{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b4e941a-06c8-4724-9c72-b8ce769ee276",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# ENGLISH TOP 20 QULAC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e702f7-925f-4747-ab37-83a85deebec0",
   "metadata": {},
   "source": [
    "First start with english dataset just to set up the runs, do it on top 20 docs. For now we dont put any parameters, we just use the docs and the query and try to predict 1 claridying query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ac16d0-c6ea-4eba-b467-98c4738ea304",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "966e332e-d77e-4323-8bf9-004ce71bff69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average token length: 86448.01502504174\n"
     ]
    }
   ],
   "source": [
    "def get_token_length(input_text):\n",
    "    tokenized_input = tokenizer(input_text, padding=False, truncation=False)\n",
    "    return len(tokenized_input[\"input_ids\"])\n",
    "\n",
    "# Calculate the average token length for all inputs\n",
    "token_lengths = [get_token_length(entry[\"input\"]) for entry in raw_data]\n",
    "\n",
    "# Calculate the average and print it\n",
    "average_token_length = sum(token_lengths) / len(token_lengths)\n",
    "print(f\"Average token length: {average_token_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a11695b-917b-40c7-8064-53399a2de450",
   "metadata": {},
   "source": [
    "# SEQ2SEQ USING T5 TINY + top 5 docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d684ddb-d503-402e-b86f-fcec6420cdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08ce92c9-cad0-4cde-98c1-7867d02e5681",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /users/Etu0/21402600/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhibaakbi\u001b[0m (\u001b[33mhibaakbi-sorbonne-universit-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# Paste your API key directly here\n",
    "wandb.login(key=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94825901-578a-4d0e-b52b-c24375c91492",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "# Load your dataset\n",
    "def tokenize_and_split_dataset(filename,modelname):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_data = json.load(f)\n",
    "    \n",
    "    # Split the raw data into train and validation sets\n",
    "    train_data, eval_data = train_test_split(raw_data, test_size=0.1)\n",
    "    \n",
    "    # Convert the splits into Hugging Face Dataset objects\n",
    "    train_dataset = Dataset.from_list(train_data)\n",
    "    eval_dataset = Dataset.from_list(eval_data)\n",
    "    \n",
    "    # Choose your model\n",
    "    model_name = modelname \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Tokenize the dataset\n",
    "    def preprocess(batch):\n",
    "        model_inputs = tokenizer(\n",
    "            batch[\"input\"],\n",
    "            max_length=512,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(\n",
    "                batch[\"output\"],\n",
    "                max_length=64,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True\n",
    "            )\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "    \n",
    "    # Tokenize the dataset\n",
    "    tokenized_train_dataset = train_dataset.map(preprocess, batched=True)\n",
    "    tokenized_eval_dataset = eval_dataset.map(preprocess, batched=True)\n",
    "\n",
    "    return tokenizer,tokenized_train_dataset,tokenized_eval_dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9949238d-143f-44fd-b35c-92f3e9ea12e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fa41b376e6b40e582c318cf701b021f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/539 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/Etu0/21402600/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4611cfdc97484bd9b86ef8d0f79183e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': '[QUERY] I\\'m looking for information on Rick Warren, the evangelical minister.\\n[DOCUMENTS]\\n[DOC 1] The XX Factor\\\\nBriefings\\\\nNews & Politics\\\\nArts\\\\nLife\\\\nBusiness & Tech\\\\nScience\\\\nPodcasts & Video\\\\nBlogs\\\\nThe XX Factor: Slate women blog about politics, etc...\\\\nHome\\\\nDialing Protective Services...\\\\nPosted\\xa0Thursday, January 08, 2009 1:35 PM\\\\n| By Melinda Henneberger\\\\nEmily, I also agree with the Obama ban on strollers at the inauguration, but not because the crowd needs to be protected from babies (and their means of transportation). Here I am wondering whether it will be safe to take newly minted teenagers into the crowds that day\\\\u0014most parents I know are leaning against it\\\\u0014and toddlers would surely be at risk, no?\\\\nUpdate: Oops, now that I have read Hanna\\'s post, I take it back: I\\'m sure it will be fine!\\\\nFiled under: Obama, inauguration, strollers\\\\nOh, My Aching Back\\\\nPosted\\xa0Thursday, January 08, 2009 1:18 PM\\\\n| By Hanna Rosin\\\\nEmily, I, like you, am long past the days when I think I am entitled to bring my baby absolutely everywhere. I, too, have embarrassing memories of nicking\\xa0ankles on the streets of San Francisco, or Brooklyn,\\xa0or downtown D.C. and blithely walking on by as the offended pedestrians\\xa0burned holes in my back\\xa0with their eyes. But here is my current reality: I live here. I have\\xa0three children, one of them an infant. I feel like\\xa0missing the opportunity to\\xa0witness this moment\\xa0because of some small, boring concerns (he needs to nap, she will get tired, blah blah blah) is pretty depressing. And frankly, a stroller will make all the difference. There are only so many hours on a winter day you can hold a baby and satisfy two other children. With a stroller, I can just shove the baby in and the snacks on the bottom and be on my merry\\xa0way. So yes, the\\xa0Park Service is probably right, but they are\\xa0seriously ruining my day.\\\\nFiled under: inauguration, strollers\\\\nDo I Really Care About the Temporary Preacher-in-Chief?\\\\nPosted\\xa0Monday, December 22, 2008 11:47 AM\\\\n| By E.J. Graff\\\\nWell, Hanna, I don\\'t think anyone is advocating censoring Warren. He has the same freedom to speak as does every other American, and certainly far more access to public forums. Nor do I think that failing to ask him to give the inaugural prayer would have been equivalent to pretending that evangelicals don\\'t exist, any more than Reagan\\'s failing to invite the late Rebbe Schneerson to give an inaugural prayer was equivalent to pretending that the Lubavitcher Chassidim didn\\'t exist. Or more to Melinda\\'s point, that Obama\\'s failing to ask Christopher Hitchens to give the inaugural antiprayer is equivalent to pretending that atheists exist. Of course they exist. Of course they are free to preach, evangelize (which Hitchens does with particular enthusiasm), organize, and speak in the public square. Go forth. Multiply. Knock yourselves out in the marketplace of theological ideas.\\\\nThe objection has been to giving an extremist-someone who thinks women who\\'ve had abortions were running concentration camps in their wombs, as Katha Pollitt put it so brilliantly in the L.A. Times-the honorary job of saying the nation\\'s prayer over the presidency. That said, over the course of this discussion, I have somehow talked myself into the other point of view. (Or maybe spending a weekend-long blizzard locked in the house with an energetic 5-year-old has just worn me down, and I\\'m willing to give in on anything that doesn\\'t involve screechy toys. Is there a special circle of hell for screechy toy manufacturers and for \\\\\"friends\\\\\" who give said toys? This is my prayer: Please, God, let it be so!) Giving Rick Warren the temporary job of preacher-in-chief is an entirely symbolic scrap thrown to the right-wing evangelicals. In more important news, Obama appears to be ready to launch a reality-based science policy, to authorize stem-cell research, to lift the global gag rule on family planning services, to roll back Don\\'t Ask Don\\'t Tell, and to take similar actions on truly urgent issues. Warren\\'s prayer won\\'t actually have much particular public effect-except to give Obama his reverse \\\\\"Sister Souljah\\\\\" moment and the cover of appearing inclusive. Fine. Fine. Prez-elect, go play with whatever preacher you want to play with. I don\\'t care, so long as I don\\'t have to listen to the screechy toys.\\\\nFiled under: Barack Obama, Christopher Hitchens, inauguration, Rick Warren\\\\nSo the Cure for Bias Is, er, Bias?\\\\nPosted\\xa0Monday, December 22, 2008 10:58 AM\\\\n| By Dahlia Lithwick\\\\nMelinda, you are precisely right that extreme right/left black/white thinking got us into this polarized, judgmental 2008 mess, and that Obama\\\\u0019s willingness to get beyond such thinking is exactly why so many of us were attracted to him. But isn\\\\u0019t the argument that Rick Warren must be a great man because he reverse tithes just as absolutist? Nobody (except maybe Hitchens) is suggesting that Warren hasn\\\\u0019t done extraordinary work toward relieving AIDS and poverty and global warming. But that doesn\\\\u0019t change the fact that not only does he not speak for all Americans, he also expressly rejects some of their very basic rights. We can debate about whether the right to marry someone you love constitutes a basic right. But\\\\u0014and here is where Hanna and I probably differ\\\\u0014I don\\\\u0019t think Warren is really interested in having that kind of argument.\\\\nI also think it\\\\u0019s not quite fair to claim that any criticism of Warren represents some kind of generalized anti-religious bias. Too many people of very deep faith don\\\\u0019t make Warren\\\\u0019s cut. That doesn\\\\u0019t make us religion-haters. It just means that you can\\\\u0019t call it\\\\nbringing people together\\\\nif you are honoring one group\\\\u0019s message while denigrating another\\\\u0019s.\\\\nFiled under: Barack Obama, inauguration, Rick Warren\\\\nLet Warren Speak!\\\\nPosted\\xa0Friday, December 19, 2008 8:23 PM\\\\n| By Julia Turner\\\\nA post from Hanna Rosin, en route to parts tropical:\\\\nI have to take a break from my vacation to object to this liberal groupthink. We elected Obama partly because he is able to talk to people with different views. Our standards for hearing out a religious leader should not be: Does he believe everything we believe? It should be: Is he willing to talk to the other side? Many months ago, Rick Warren gave the stage over to Obama\\\\u0014showing a form of open-mindedness from an evangelical leader we haven\\'t seen since Billy Graham. Now it\\'s Obama\\'s turn to reciprocate. Your strategy\\\\u0014E.J. and others\\\\u0014would involve pretending evangelicals don\\'t exist. And what good would that do?\\\\nFiled under: Barack Obama, Obama, inauguration, Rick Warren, Warren\\\\nPrayer and Consequences\\\\nPosted\\xa0Friday, December 19, 2008 4:32 PM\\\\n| By E.J. Graff\\\\nRachael, I am so glad to be on this blog with you. You do make me examine myself for intellectual double standards. Remember, though, that we\\'re critiquing the same politician\\\\u0014Barack Obama\\\\u0014for his ministerial choices, albeit different choices in different circumstances.\\\\nAnd, in response to your specific points, I do find those circumstances to be different in important ways. I found Wright\\'s views to be appalling, but I found the Republican flogging of his views to be race-baiting. I suspected\\\\u0014no evidence, just a hunch\\\\u0014that he went to that church for political reasons, small-p political, socializing with the people who could help in Chicago politics and all that. So when Obama disavowed those views and quit the church, I shrugged off his attendance. I expect certain kinds of small compromises and hypocrisy from politicians, I suppose, and that one didn\\'t seem especially large.\\\\nBut like Dahlia, I find it to be a quite different thing to give a minister a national podium\\\\u0014essentially, to ask him to give the nation\\'s prayer, to ask that minister to invoke his (can anyone remember a female in that spot?) divinity\\'s blessing on our highest national office. How would you feel if it were Wright giving that prayer? What kind of racialized uproar would we be seeing?\\\\nAnd yet take a white extremist\\\\u0014someone who espouses what most of us see as unacceptable misogyny, someone who believes in evangelizing all people to his own religion, someone who gives voice to relatively extreme antigay sentiments (as Sara pointed out)\\\\u0014and give him a podium, and the mainstream nods at how inclusive Obama is.\\\\nI see a double standard here, but not the same one that you see.\\\\nFiled under: Barack Obama, inauguration, Rick Warren, Rev. Jeremiah Wright\\\\nWright and Wrong and Warren\\\\nPosted\\xa0Friday, December 19, 2008 3:01 PM\\\\n| By Sara Mosle\\\\nLike Dahlia amd E.J., I\\'m not thrilled with Obama\\'s selection of Rick Warren to deliver the invocation at the inauguration, given Warren\\'s opposition to gay marriage and many of his other views. At a time of high divorce rates and increased infidelity\\\\u0014and I\\'m talking about hetereosexuals, who are the real threat to the institution of marriage\\\\u0014I find it almost comically perverse that conservatives are against a group of people who so earnestly want to form committed, long-term, stable relationships sanctioned by God.\\\\nIndeed, when I recently moved temporarily to Dallas, my way of finding an Episcopal church for me and my daughter was to Google \\\\\"gay,\\\\\" \\\\\"bishop,\\\\\" \\\\\"New Hampshire,\\\\\" and \\\\\"Dallas.\\\\\" And sure enough, I quickly found the one congregation where every priest on the staff had supported Gene Robinson, and I feel right at home. But it did gnaw at me at the time that I just wanted to be preached to by the converted. After all, were I more committed to gay and lesbian rights, wouldn\\'t I have joined precisely those Episcopal congregations where the issue is still an open wound\\\\u0014and believe you me, there are plenty to choose from in North Texas\\\\u0014and tried to persuade my less-enlightened congregants to see the light? I took the easy way out.\\\\nIn this one sense, I do have grudging respect for Obama\\\\u0019s choice of Warren. Yes, it\\\\u0019s clearly a political calculation\\\\u0014but political in a good sense. I do believe Obama is genuinely trying to create dialogue with those who disagree with him in hopes of bringing a few more wayward souls along. If he can get even a few evangelicals to drop their active opposition to gay rights\\\\u0014to become more agnostic, so to speak, on this one issue\\\\u0014then that might, in fact, further the cause more than I\\'m doing on Sundays by kneeling, smug and self-satisfied, next to my fellow liberal parishioners.\\\\nObama did, after all, actively campaign on bringing people together, and I remember at least thinking I supported that idea during the election. While I am sinfully spiteful enough after the damage of the Bush years to wish this unity would now take place under dark of night (or maybe involve issues I care less about), so long as Obama continues to push hard for equal rights for all Americans as a matter of policy, I have less of a problem with his otherwise entirely symbolic olive branch to Warren. However, if the result of such good-faith efforts is to provide an opportunity for right-leaners like Rachael to tar Obama again with Jeremiah Wright, then never mind: Bring back those good ole partisan politics.\\\\nFiled under: Barack Obama, gay marriage, inauguration, Rick Warren, Rev. Jeremiah Wright\\\\nThe Personal and the Political\\\\nPosted\\xa0Friday, December 19, 2008 12:26 PM\\\\n| By Rachael Larimore\\\\nDahlia, you ask if there\\'s a difference between Obama\\'s choice of a \\\\\"personal spiritual adviser\\\\\"\\xa0and the public and political act of picking Rick Warren to give the inaugural invocation. I agree there\\'s a difference, but probably not in a way that you will like.\\\\nIf Obama had attended a quiet, out-of-the-way church that focused on helping its congregants achieve spiritual growth, one where the kindly old minister made house calls to the elderly and infirm, sure, it would be unkind to compare that person to Rick Warren. But if Obama had attended that kind of church, we wouldn\\'t be having this conversation. Instead, he attended a church whose preacher sought out the spotlight and sold DVDs of sermons in which he preached anti-American views. And Obama had a 20-year relationship with that church. Isn\\'t that lengthy commitment, however personal, more telling than a brief and symbolic political act?\\\\nLet\\'s frame this another way: If the Republicans had nominated a candidate who attended Rick Warren\\'s church for 20 years, would it have been fair to question that person\\'s choice of spiritual adviser? (Heck, I\\'d have questioned it.)\\\\nFiled under: Barack Obama, inauguration, Rick Warren, Rev. Jeremiah Wright\\\\nClarifying Our Terms\\\\nPosted\\xa0Friday, December 19, 2008 11:33 AM\\\\n| By Dahlia Lithwick\\\\nTwo things: First. A reader writes to chide me for using the word religious interchangeably with\\\\nright-wing evangelical\\\\nin my last post. Point well-taken. I didn\\\\u0019t mean to suggest that all religious Americans are represented by Rick Warren. Second, Rachael, surely there is a difference between Obama\\\\u0019s personal spiritual leader and the man he picks to bless the inauguration and his presidency? The choice of Warren in this case was a public and political act.\\\\nFiled under: Barack Obama, inauguration, Rick Warren\\\\nExtremism Is Always Unattractive, Wright?\\\\nPosted\\xa0Friday, December 19, 2008 11:13 AM\\\\n| By Rachael Larimore\\\\nE.J.,\\\\nLet me start by saying that there is probably very little outside of abortion that the Rev. Rick Warren and I agree on. My righty-ness has more to do with political and economic conservatism than social issues. I am a staunch supporter of gay rights and gay marriage, and I think the best marriages are equal partnerships, not employer-employee relationships. I don\\'t know what the afterlife will bring, but I doubt it\\'s a Christians-only country club.\\\\nSo I respect and share\\xa0your concerns about the message President-elect Obama is sending by inviting Warren to do the inaugural invocation. But isn\\'t there an interesting parallel here? Obama attended Trinity United and listened to its pastor, the infamous Rev. Jeremiah Wright, for 20 years. Jeremiah Wright, who is on tape saying, \\\\\"God Damn America,\\\\\" who has claimed that the government created AIDS for the purpose of \\\\\"genocide against people of color,\\\\\" and who just a few weeks ago marked the occasion of Pearl Harbor Day by calling it the anniversary of the United States dropping the bomb on Hiroshima.\\\\nYet complaining about Obama\\'s association with Wright was verboten during the election. Conservatives who raised the issue were viewed as intolerant, racist, or muckraking. It was a silly issue blown out of proportion and gave no indication of what kind of president Obama would be, we were told.\\\\nPersonally, I\\'m no fan of extremism of any stripe. So I hope that everyone who is so up in arms about Warren right now can at least take a second and reconsider whether we righties were so wrong to complain about Wright.\\\\nFiled under: Barack Obama, inauguration, Rick Warren, Rev. Jeremiah Wright\\\\nThe Bipartisan Blues\\\\nPosted\\xa0Friday, December 19, 2008 10:36 AM\\\\n| By Dahlia Lithwick\\\\nE.J.,\\\\nI agree. Obama\\\\u0019s decision to ask Rick Warren to deliver the invocation at his inauguration shows exactly what happens when bipartisanship becomes an end in itself. The president-elect continues to confuse reaching across the aisle with being principled. Sometimes the principle is just too important to compromise. Both Obama and Warren are to be credited for reaching out across the chasm that separates liberals and evangelicals in America. Each has signaled a willingness to talk and\\\\u0014to their huge credit\\\\u0014to listen to ideas different from their own. But as you explain, Warren\\\\u0019s views on women, stem-cell research, and homosexuality are not moderate. He doesn\\\\u0019t even dress them up as moderate!\\\\nIf Obama wanted to signal his continued respect for Warren and for religious Americans, he could have done so in a thousand ways that would have welcomed them into the tent, without banishing and insulting those already inside.\\\\nFiled under: Barack Obama, inauguration, Rick Warren\\\\nRick Warren Will Give the Inaugural Prayer?!? Oh, Barack, Please Say It Ain\\'t So ...\\\\nPosted\\xa0Thursday, December 18, 2008 5:34 PM\\\\n| By E.J. Graff\\\\nSo Rick Warren is going to give the inaugural prayer? Rick Warren, Jerry Falwell in sheep\\\\u0019s clothing, the leader of the Saddleback Church (megachurch, actually, with satellite campuses and broadcast sermons and services), who, as Michelle Goldberg puts it so pleasantly in The Guardian:\\\\nHe is a man who compares legal abortion to the Holocaust and gay marriage to incest and paedophilia. He believes that Jews, Muslims, Buddhists, Hindus and other non-Christians are going to spend eternity burning in hell. He doesn\\'t believe in evolution. He recently the social gospel\\\\u0014the late 19th- and early 20th-century Protestant movement that led a religious crusade against poverty and inequality\\\\u0014as \\\\\"Marxism in Christian clothing.\\\\nOr as Linda Hirshman noted on the WAM listserv\\xa0(I\\\\u0019m posting this with her permission):\\\\nRick Warren\\\\u0019s site for educating preachers, Pastor.com, has a long essay on why women should submit to their husbands. Here\\\\u0019s the money line: \\\\\"The Greek word for \\'submit\\' is hupotasso.\\xa0Hupo means \\\\\"under\\\\\" and tasso means \\\\\"to place in order.\\\\\"\\xa0The compound word hupotasso means \\\\\"to place under or in an orderly fashion.\\\\\"\\xa0Paul didn\\'t dislike women, he liked order!\\xa0He advocated order in the church, order in government, order in business, and, yes, order in the home.\\\\nThere have been a lot of heartbroken comments about this on change.gov.\\\\nBarack, please, say you respect women\\\\u0014and nonevangelicals\\\\u0014more than this. Please?\\\\nFiled under: Barack Obama, inauguration, Rick Warren\\\\nWhy Did Obama Choose Poet Elizabeth Alexander To Read at His Inauguration?\\\\nPosted\\xa0Thursday, December 18, 2008 1:32 PM\\\\n| By Meghan O\\'Rourke\\\\nSo Barack Obama has chosen poet Elizabeth Alexander to read at this January\\'s inauguration. Who is she, and why her? It\\'s a choice that reflects his serious, pragmatic side. Alexander is an African-American, born in Harlem in 1962, who has published four books; the last, American Sublime, was a finalist for the Pulitzer Prize. A professor of African-American studies at Yale (from which she also matriculated), Alexander writes poems that are metaphorically and linguistically dense, layered, and subtle. Her work speaks about black experience (see the excerpt from The Venus Hottentot on her Web page). But she can\\'t be said to privilege identity politics over aesthetics; her poems work more at being complex than didactic. In this sense, she\\'s an analogue to Obama, who doesn\\'t privilege identity politics over his strategy of inclusiveness. Her choice also reflects Obama\\'s faith in the meritocracy: a poet with a Ph.D., Alexander comes across as methodical and hardworking. I saw her give a reading last fall at Princeton with the wonderful young poet Terrance Hayes, a witty former basketball player (whom I\\'d half-hoped Obama would choose; he would\\'ve reflected the president-elect\\'s playful side). Alexander was businesslike: There was no quipping or flirting with the audience.Though only four poets (I think) have ever read at inaugurations, Alexander won\\'t actually be the first African-American woman to receive the honor \\\\u0014Bill Clinton asked Maya Angelou to read at his 1993 inauguration. Alexander doesn\\'t have much else in common with Angelou, though; she\\'s more like Robert Frost, who read at Kennedy\\'s inauguration. Her best poems are imaginatively expansive as well as philosophical. Here\\'s a representative poem, called \\\\\"Stravinsky in L.A.\\\\\" You can imagine Obama liking the end:Stravinsky in L.A. In white pleated trousers, peering through green sunshades, looking for the way the sun is red noise, how locusts hiss to replicate the sun. What is the visual equivalent of syncopation?\\xa0 Rows of seared palms wrinkle in the heat waves through green glass.\\xa0Sprinklers tick, tick, tick.\\xa0The Watts Towers aim to split the sky into chroma, spires tiled with rubble nothing less than aspiration.\\xa0I\\'ve left minarets for sun and syncopation, sixty-seven shades of green which I have counted, beginning:\\xa0palm leaves, front and back, luncheon pickle, bottle glass, etcetera. One day I will comprehend the different grades of red. On that day I will comprehend these people, rhythms, jazz, Simon Rodia, Watts, Los Angeles, aspiration.\\\\nFiled under: Barack Obama, inauguration, elizabeth alexander, poetry\\\\n<a id=\\\\\"_ctl0____ctl0____ctl0__ctl0_bcr__ctl0___Results___postlist___EntryI\\n[DOC 2] The Bilerico Project | Calm down: Rick Warren is not a big deal\\\\nSign In\\\\nSign up for our daily digest!\\\\nJan\\\\n08\\\\nSearch\\\\nEnter your search terms\\\\nWeb\\\\nThe Bilerico Project\\\\nSubmit search form\\\\nHome\\\\n|\\\\nContributors\\\\n|\\\\nArchives\\\\n|\\\\nAdvertise on The Bilerico Project\\\\n|\\\\nContact Us\\\\n|\\\\nAbout Us\\\\nCalm down: Rick Warren is not a big deal\\\\nFiled by: Bil Browning\\\\nDecember 19, 2008\\\\n4:00 PM\\\\nYou might be surprised at my thoughts on the current Rick Warren scandal.\\\\nI\\'ve hesitated to blog about it for the past couple of days, but as this story has grown bigger and bigger, it\\'s time I stepped out and said something.\\\\nIt just needs to be said.\\\\nCalm down.\\\\nRick Warren is not a big deal.\\\\nThis tempest in a teapot will only harm our community.\\\\nWith so many prominent bloggers, activists and politicos outraged, I realize this post won\\'t be the most popular I\\'ve written.\\\\nAs I\\'ve mulled this over the past few days, certain key elements fell into place for me; I wonder why our leadership didn\\'t realize some of these basic facts before they started pounding on Warren.\\\\nAs Jim Ross writes at the Sentinel in his post, \\\\\"Obama Creates New Cabinet-Level Post for Rick Warren\\\\\":\\\\nOK, not really.\\\\nBut one would have thought so, judging by the reaction to Obama\\'s recent announcement concerning Warren.\\\\nThe outrage from the left and the coverage from the media has gone off the charts, at a level that would have been appropriate if Obama had named him, say, Secretary of Religion and Faith and then endowed him with the powers to shape the administration\\'s social and moral agenda.\\\\nBut that\\'s not at all what Obama did.\\\\nHe simply asked Warren to deliver the invocation at the inauguration.\\\\nHere\\'s what will happen:\\\\nWarren will stand up and pray something nice and religious and patriotic sounding that is carefully crafted to offend as few people as possible, and then he will sit down.\\\\nThe next day, he will leave Washington and go back to being a pastor and an author.\\\\nThat\\'s all.\\\\nIt will be that innocuous.\\\\n1. Rick Warren Is Not the Biggest Threat to the LGBT Community\\\\nWarren is a popular preacher and author.\\\\nHe\\'s a religious right leader.\\\\nHe is a pompous windbag, a liar, and a homophobe.\\\\nBig f--king deal.\\\\nAnti-gay, lying, pompous preachers are a dime a dozen.\\\\nWarren\\'s soapbox is larger than most, but 98% of these mega-ministers end up crashing to earth surrounded by prostitutes, drugs and dollars with the careers in ruin.\\\\nLGBT folk are losing their jobs in several states just because they\\'re queer.\\\\nThe violence against trans folk in Memphis, TN and across the nation calls out for more attention and aggressive watchdogs.\\\\nOur adoption and parenting rights are under attack.\\\\nHIV infections are surging. We are dying.\\\\nDon\\'t Ask Don\\'t Tell should be a priority.\\\\nA fully inclusive ENDA should be plotted and planned and put in place.\\\\nFiguring out a realistic response to today\\'s HIV/AIDS environment is needed.\\\\nLord knows, there\\'s a whole helluva lot of work to do on trans issues.\\\\nResolving the racism, sexism, ableism and other undesirable traits in our own community should be a priority.\\\\nBut Rick Warren\\'s short prayer?\\\\nThat\\'s tiny in the grand scheme of things.\\\\n2. Doesn\\'t This Signal That Homophobia Is Acceptable To Obama?\\\\nNo.\\\\nNot at all.\\\\nIt signals that Obama is a shrewd politician.\\\\nOne of the biggest presents Obama can get during the first few months of his presidency is the goodwill of the Right.\\\\nPicking Warren, who took flack during the election for his willingness to engage Obama, shows the evangelical leadership that Obama is willing to repay his favors.\\\\nSaying a prayer at the inauguration is small potatoes.\\\\nStop someone on the street and ask them who gave the invocation the last time.\\\\nOr the time before that.\\\\nHow many times has Billy Graham spoken the opening prayer?\\\\nDo you know?\\\\nIt\\'s not as if Inauguration preachers are big memory making opportunities.\\\\nCan you name the other gay-friendly minister speaking at Obama\\'s inauguration?\\\\nI couldn\\'t.\\\\nI scanned five Bilerico posts about the topic and didn\\'t see him mentioned.\\\\nI found the answer at the New York Times blog.\\\\nRev. Dr. Joseph E. Lowery.\\\\nHell, we don\\'t even care about him.\\\\nNeither does Joe Six Pack.\\\\nThe other thing most Americans don\\'t care about is LGBT rights.\\\\nFace it, most straight people just don\\'t give a s--t about our issues. They are asleep to our struggle, obviously, our Prop 8 would not have passed.\\\\nOr Amendment 2.\\\\nOr Prop 102.\\\\nOr...\\\\nWhile this would have signaled to evangelical leaders that Obama is willing to treat them generously and they could, understandably, take that to mean the constant stream of homophobic comments were a-ok, I doubt that the average American would have noticed.\\\\nIf our community had kept their mouth shut on this choice to lead a prayer, Warren wouldn\\'t get the free publicity, the goodwill from evangelicals, or the constant media repetition of the \\\\\"Obama is dissing the gays\\\\\" mantra.\\\\nWe\\'ve told more Americans about Obama\\'s signal than would ever have noticed on their own.\\\\nWe\\'ve made ourselves the victim before an uncaring audience that thinks this is a big whooptie-doo.\\\\nAfter all, several of them attend a church where their minister probably says the same or more on a regular basis.\\\\n3. We Look Like Novices\\\\nAs the LGBT community is trying to flex our muscles, this is not the time to look like a 75 pound weakling.\\\\nBy mewling and kavetching about Rick Warren\\'s minutes spent praying to an invisible man in the sky, we didn\\'t flex our biggest muscle - our brains.\\\\nEveryone knew that Warren would not be replaced.\\\\nHis selection had already been announced publicly, and replacing him would have been too big a scandal.\\\\nThe new president does not make mistakes.\\\\nRemember?\\\\nSo what did we hope to gain?\\\\nThe only person I saw asking this obvious question, was Stonewall Democrats Executive Director Jon Hoadley on the Huffington Post.\\\\nOur community -- at least from what my inbox has seen -- has been quick to share our anger at this choice. But while advocates --especially the LGBT netroots -- are rightfully telling the incoming administration that this isn\\'t right, our community has been slow to share a solution.\\\\nUnfortunately, this is a pattern in modern LGBT advocacy. We saw the same anger over rumors earlier this year that former Democratic Senator Sam Nunn of Georgia (the architect of opposition in 1993 to service by openly-gay military personnel) was being considered for the position of Vice President. Our community loudly said \\\\\"No to Nunn!\\\\\"- just as we said \\\\\"No to Nunn!\\\\\" to similar rumors in 2004 and 2000. Yet, in saying\\\\\"no\\\\\" our community failed to share what we could say \\\\\"yes\\\\\" to.\\\\nIf we want to enact pro-equality policy and change, we need to take a page out of Harvey Milk\\'s playbook: we have to give them hope. We need to say what we want, not just what we don\\'t want.\\\\nThe only thing I\\'ve heard that we wanted was a cabinet-level pick of an openly gay person.\\\\nBig deal.\\\\n(I\\'d rather Obama pick his team based on their qualifications than any tokenism.\\\\nIf he picked all black, disabled lesbians as advisors, that\\'s fine; as long as they are the best people for the job.\\\\nI\\'d rather not be a token for his administration.)\\\\nNow, all the cabinet spots are filled so we didn\\'t get that either!\\\\nSo that leaves us with what demand?\\\\nThat the transition team do the impossible?\\\\nThey won\\'t dump Warren.\\\\nWe\\'re not asking for anything else.\\\\nWe\\'re just complaining to hear ourselves whine.\\\\nIf You Can\\'t Stand the Heat, Get Outta the Kitchen\\\\nPolitics is a rough and tumble sport.\\\\nWe\\'ve been handed our asses politically lately.\\\\nWe helped get Obama elected only to get marriage amendments shoved down our throats.\\\\nWe\\'re angry.\\\\nWe\\'re prickly.\\\\nWe\\'re sensitive.\\\\nMany of us led with our hearts and not our brains on this one.\\\\nWe got upset and started sending out press releases and writing blog posts and going on TV and we poured out the well documented ills of Rick Warren and his loathsome beliefs.\\\\nWhat we should have done is head straight to Obama and pointedly \\\\\"put him on notice.\\\\\"\\\\nWe should strategize ways to get Don\\'t Ask Don\\'t Tell repealed quickly, how to get the votes for a fully-inclusive ENDA, how to get an inclusive hate crimes bill to Obama\\'s desk ASAP.\\\\nWe hold his feet to the fire to really show his support of our community by passing these important life-saving bills.\\\\nQuickly.\\\\nBut this temper tantrum that we\\'re throwing publicly?\\\\nIt\\'s not worth it.\\\\nThere\\'s no winning either the political or the pr battle.\\\\nWe need to shut the hell up and go to work on the issues that matter.\\\\nA \\\\\"nice and religious and patriotic sounding that is carefully crafted to offend as few people as possible\\\\\" prayer?\\\\nThat\\'s not it.\\\\nEditors\\' note: Did you like this post? Please help us out by voting for The Bilerico Project as the Best LGBT Blog in the 2008 Weblog Awards! You don\\'t have to sign up for anything, just follow click on \\\\\"The Bilerico Project.\\\\\" Thank you so much!\\\\nShare\\\\nLike what you read? Subscribe to The Bilerico Project by Email.\\\\nWant others to read it too? Use the buttons and links here to share this entry with others. (Move your cursor over a button or link to learn more about it.)\\\\nNeed to know more about these buttons?\\\\nView blog reactions\\\\n92 Comments\\\\nPerhaps you should take your own advice and sit this one out.\\\\nIt is time to get ugly not wait and wait and wait and let the bruises pile up.\\\\nYou need an intervention, dear.\\\\nYour party is abusing you.\\\\nhttp://www.youtube.com/watch?v=0x-fkSYDtUY\\\\npatrick | December 19, 2008\\\\n4:08 PMReply to this comment\\\\nIt\\'s not that I want to put up with every slap they give us - see my post under \\\\\"highlights\\\\\" about the Indiana Dem Party mailing out anti-gay mailers.\\\\nI regularly challenge our state party; it\\'s earned me the ire of most Dems for putting Queer above Dem.\\\\nThis just isn\\'t the right strategy for this case.\\\\nIt\\'s political reasoning that I\\'m advocating.\\\\nBil Browning | December 21, 2008\\\\n1:01 PMReply to this comment\\\\nI realize this post won\\'t be the most popular I\\'ve written.\\\\nJudging by the site stats, if you put \\\\\"penis\\\\\" in the title it actually could be.\\\\nNick | December 19, 2008\\\\n4:28 PMReply to this comment\\\\nBil, you are absolutely right. This really isn\\'t a matter for people to get their panties in a knot over.\\\\nWarren is going to give a invocation. A\\xa0prayer, if you will. He\\'s not at the inauguration to preach, or to proselytize, and won\\'t be given a chance to.\\\\nKeep in mind that Warren invited him to debate McCain, which Obama wisely accepted; Obama may not have won many converts over that night, but Warren could have just blown him off and didn\\'t.\\\\nYes, Warren\\'s no friend. He\\'s certainly opposed to same sex marriage and other GLBT rights issues. However, there are far, far worse that could have been picked. Warren does support enhanced funding for social welfare programs, for instance.\\\\nHe wouldn\\'t have been my pick (I\\'d have chosen Katherine Jefferts-Schori or Gene Robinson, but I\\'m an Episcopalian and Obama isn\\'t), but I was relieved that Obama didn\\'t pick a TD Jakes, Joel Osteen, Bernice King, or Harry Jackson, say.\\\\nThe important thing will be what Obama does about hate crimes, DADT, ENDA, and repeal of DOMA. I am prepared to wait and judge on his performance, and give him a chance to perform.\\\\nI believe he will do okay. We will be watching.\\\\nPolar\\\\n| December 19, 2008\\\\n4:31 PMReply to this comment\\\\nAgreed.\\\\nEntirely.\\\\nBil Browning | December 21, 2008\\\\n1:20 PMReply to this comment\\\\nOne of the biggest presents Obama can get during the first few months of his presidency is the goodwill of the Right.\\\\nHe won\\'t get it.\\\\nCan you name the other minister speaking at Obama\\'s inauguration?I scanned five Bilerico posts about the topic and didn\\'t see him mentioned.\\\\nWell, scan my post from yesterday or use the google function next time.\\\\nWe\\'ve told more Americans about Obama\\'s signal than would ever have noticed on their own.\\\\nThat\\'s OK, the signal wasn\\'t for them. It was for us, the leadership of the Religious Right, and the establishment political media.\\\\nEveryone knew that Warren would not be replaced. His selection had already been announced publicly, and replacing him would have been too big a scandal.\\\\nWell, that\\'s not the point. The point is responding to one off these assaults to either (a) prevent another one in the future or (b) get the community to wisen up. And since Obama has made, as a direct result of being pushed by LGBT people on this issue, some of the most significant and specific pro-LGBT statements a president has ever made in the history of the country, I\\'d say it looks like it\\'s having an effect.\\\\nIf You Can\\'t Stand the Heat, Get Outta the Kitchen\\\\nAt this point, the only person blogging on this site telling people to quiet down is you. I mean...\\\\nWhat we should have done is head straight to Obama and pointedly \\\\\"put him on notice.\\\\\"\\\\nI take this as a stamp of approval on this whole mess? Because this is what thousands representing hundreds of thousands to millions of people putting him on notice looks like. Unless you mean that it should have been some sort of quiet, backroom deal that he could have reneged on at any point because no one knew it happened, I think this is the process you\\'re looking for.\\\\nThe Right has been throwing temper tantrums for years to get what they want, although they usually call it \\\\\"drumming up the base\\\\\" instead. Getting angry, expressing your anger so that other people are angry too, and then having them express their anger.\\\\nIt looks ugly. It\\'s not fun to stand in the middle of, I understand. But I don\\'t see a competitive alternative. Work \\\\\"on the issues that matter,\\\\\" while charmingly vague, isn\\'t a course of action that is mutually exclusive with expressing our anger at this event.\\\\nAlex Blaze | December 19, 2008\\\\n4:31 PMReply to this comment\\\\nIt appears that many on the evangelical right aren\\'t exactly enthralled by the Rick Warren selection (or more properly, his acceptance) a couple of examples from E-Mails a pundit for the Christian Broadcasting Network published:\\\\n\\\\\"Unless Rick Warren has changed, he is very disappointing in the pro-life cause. Just ask pro-life leaders their opinion. He doesn\\'t like to deal with it at his church. It just seems funny that he is known as \\'pro-life\\' when he largely ignores the subject and teaches others to do the same. I fear God for these \\'men of God\\'. We have lost 50 million babies, and most won\\'t say a word. Reminds me of Nazi Germany or our slavery days. Very few spoke out. It was more comfortable to keep quiet.\\\\\"\\\\nHere\\'s another:\\\\n\\\\\"I have had about all I can stand of Rick Warren\\'s double standards. WHOSE side is he really on anyway? I\\'m beginning to think all he cares about are his questionable political connections. When I saw your article announcing his participation in \\\\\"that one\\'s\\\\\" so called inauguration ceremony it absolutely sickened me. It isn\\'t enough Obama is so full of himself that he \\\\\"thinks\\\\\" he\\'s God. - Apparently now Rick Warren believes he is too. This is a complete mockery of all things sacred.\\\\\"\\\\nRight-wing panties apparently snit, too.\\\\nDon Sherfick | December 19, 2008\\\\n4:46 PMReply to this comment\\\\nFinally had time to go back and read the other comments, and all I can say in response to Alex\\'s is&\\\\nOUCH!\\\\nMan!\\\\nI\\'m glad we\\'re on the same side of the issue on this one, Alex dear boy.\\\\n;-)\\\\nScott Kaiser | December 19, 2008\\\\n7:45 PMReply to this comment\\\\nI\\'m with you on this one, Alex.\\\\nBil couldn\\'t be more wrong.\\\\nI had written a rather pointed version of that that seems to have disappeared into cyberspace but it boiled down to the fact that moral failings are a big deal for someone for whom a big part of their job is the exercise of a bully pulpit -- that Hils was right when she questioned Obama\\'s good sense about reaching out -- terming him naive (which is the nicer alternative to his just being a jerk who doesn\\'t give a rat\\'s ass that giving a bigot such as Warren such a visible position of honor gives aid and succor to those who hang our children on fences to die and ignores the effect of WarrenRobertsonDobsonEtAl\\'s constant demeaning dehumanizing of us that digs deep into too many of our souls resulting in self-destruction both quick and slow and embeds itself deeply into the fabric of our society to keep us the highest per capita victims of hate crimes and youth homelessness.)\\\\nI talked, too, about how it is not the loud outcry that\\'s politically naive -- it\\'s ending it too soon that would be.\\\\nThat we\\'re not on Joe Sixpack\\'s or Obama\\'s radar is reason all by itself to pump up the volume -- that discussion always favors the under-discussed-but-on-the-right-side-of-the-moral-universe such as we are, even if it doesn\\'t appear so early in that discussion -- and it is early.\\\\nThe issues of morality only tend to get to the forefront after the frivolous has run its course.\\\\nThen the cracks in the veneer of civility carefully slathered on the Warrens like so much bad heavy makeup start to show even to the big media whores who have tried to ignore the obvious.\\\\nHell, even Mrs. \\\\\"I\\'d Better Not Stand Up For My People Cause CNN Likes Its People Safely Semi-Closeted\\\\\" Rosen even opined -- albeit timidly -- on Anderson Cooper last night that being the victim of Obama\\'s political expediency could be considered ample justification of our complaints.\\\\nSo get those inauguration tix but use them to boo Warren up close and call Obama the liar he is when he makes his inevitable claims of being a vehicle for inclusive change.\\\\nYou\\'ll end up eating balogne sandwiches instead of crab puffs that evening -- but you\\'ll be at least guaranteed to be spending the night with a more honest class of folks.\\\\nWho knows, if we tell the truth and that spoils the party, maybe they\\'ll decide we\\'re worth at least consulting next time.\\\\nIf not, what\\'s he going to do, not sign DADTDP repeal or whatever crap is passing for real civil rights legislation for us these days?\\\\nGet real!\\\\nMarla R. Stevens | December 19, 2008\\\\n7:57 PMReply to this comment\\\\nWell, scan my post from yesterday or use the google function next time.\\\\nI did Alex.\\\\nAll three of them that published before mine.\\\\nYou have Lowrey in a blockquote of the Obama statement in one post.\\\\nYou never mention him yourself.\\\\nMan Does Not Live By Doughnuts Alone\\\\nResponding to Obama: We Can Disgree without Being Disagreeable\\\\n(this is the one with Lowery in the blockquote)\\\\nObama Picks Arch-Homophobe Rick Warren\\\\nAs for this quote:\\\\nThat\\'s OK, the signal wasn\\'t for them. It was for us, the leadership of the Religious Right, and the establishment political media.\\\\nI thank you for agreeing with me.\\\\nYou nailed it.\\\\nBy blowing this up into a brouhaha, we\\'ve made sure everyone thinks that Obama is actually anti-gay and condones the right-wing homophobes.\\\\nWe\\'ve made it an issue when it could have been handled without a public shaming of our community.\\\\nThe point is responding to one off these assaults to either (a) prevent another one in the future or (b) get the community to wisen up. And since Obama has made, as a direct result of being pushed by LGBT people on this issue, some of the most significant and specific pro-LGBT statements a president has ever made in the history of the country, I\\'d say it looks like it\\'s having an effect.\\\\nSo the point of complaining about Warren and turning this into a major media story is to get positive statements from Obama?\\\\nWe already had that.\\\\nAnd since the controversy broke, the most he\\'s said is that he supports civil unions.\\\\nWow.\\\\nlet\\'s take that to the bank...\\\\nI say this because your point that the purpose should be \\\\\"(a) prevent another one in the future or (b) get the community to wisen up\\\\\" is too politically naive.\\\\nIt won\\'t prevent another one.\\\\nPolitics will always trump; alliances must be made to govern.\\\\nAnd \\\\\"get the community to wise up?\\\\\"\\\\nSeriously?\\\\nDid Prop 8 do that?\\\\nDid any of the other amendments?\\\\nThe lack of legislation in some states?\\\\nEllen coming out?\\\\nA federal DOMA?\\\\nDADT?\\\\nThe numerous hate crimes against the LGBT community?\\\\nGLAAD parties?\\\\nHRC e-mails?\\\\nMarches on Washington?\\\\nNo.\\\\nThis won\\'t either.\\\\nIt\\'s a rookie mistake to shake and shout over Warren.\\\\nOur leadership should have known better.\\\\nBil Browning | December 21, 2008\\\\n1:13 PMReply to this comment\\\\nYou are overthinking this entirely.\\\\nDon\\'t say anything because if we do, then they will think that and then these people will perceive this about us and we will look bad and god knows we don\\'t want to look bad...which in effect brings us right back to the topic at hand.\\\\nBy saying nothing and/or carefully mincing around trying to manage our appearance we look like cowards at best or at worse we look disingenuous.\\\\nWhy would anyone - anyone - not have a response to being poked in the eye?\\\\nThe Mormons/xtians have been peeing their pants about reactions to prop 8.\\\\nWhy is it that there is always a double standard applied to LGBTs - by our allies, by the media and worst of all by ourselves?\\\\nThat is what you propose Bil by trying to control everything and everyone\\'s reactions.\\\\nWE look like amateurs?\\\\nInstead of professionals?\\\\nInstead of looking like an Obama type that can lie and spin his way through a dust cloud by saying he is evolved when we considers bigotry to be an example of inclusion?\\\\nWe have a responsibility to encourage each other to stand up for what is right and Obama is wrong, you are wrong, and Rick Warren is evil.\\\\nSilence in the guise of nuance is actually cowardice.\\\\npatrick | December 21, 2008\\\\n5:31 PMReply to this comment\\\\nI agree with much of what you say, Bil, and as you\\'ve probably noted, I\\'ve chided Waymon a bit and challanged him to think about the impact of what I consider to be our community\\'s tendency to overuse the term \\\\\"hate\\\\\" so much that it leaves no room to really throw the \\\\\"hate\\\\\" book at Fred Phelps.\\\\nAt one time in my younger days I once went to a counsellor concerning anger control and he told me to take my frustrations out by beating a pillow with a bat or tennis racquet.\\\\nSince I don\\'t like feathers or loose foam pieces littering my bedroom I\\'ve discontinued that practive, but I\\'ve lately taken to really putting down lots of elevated thoughts on paper just to see what they look like, and then before I send them to Veritas Rex (ooops, I didn\\'t mean to go that far...) I just delete the whole thing.\\\\nBut I feel lots better.\\\\nUnfortunately the send button is always very easy to press, and I suspect in understandable frustration that it\\'s been exercised a lot lately when maybe a re-read (at least) would have been in order.\\\\nDon Sherfick | December 19, 2008\\\\n4:32 PMReply to this comment\\\\nThanks Don, but I don\\'t need to be chided.\\\\nI stand by my comments.\\\\nevery one them.\\\\ncalling a spade a spade is not wrong.\\\\nWhat Warren spews is hate, even if he smiles nicely at the end.\\\\nWaymon Hudson | December 19, 2008 11:42 PMReply to this comment\\\\nI agree with much of what you say, Bil, and as you\\'ve probably noted, I\\'ve chided Waymon a bit and challanged him to think about the impact of what I consider to be our community\\'s tendency to overuse the term \\\\\"hate\\\\\" so much that it leaves no room to really throw the \\\\\"hate\\\\\" book at Fred Phelps.\\\\nTo be fair to Waymon, the reason Phelps exists is so that everyone can throw the hate book at them and let everyone else who has the same beliefs but is more mediagenic, like Warren et al, off the hook.\\\\nMaybe we should form an extra whiny LGBT org and then we can all say that they\\'re the whiny ones, not us? I\\'d imagine them marching at straight weddings with \\\\\"God (or Goddess!) hates inequality\\\\\" and \\\\\"You\\'re making me sad\\\\\" signs.\\\\nAlex Blaze | December 20, 2008\\\\n3:00 AMReply to this comment\\\\nMaybe we should form an extra whiny LGBT org and then we can all say that they\\'re the whiny ones, not us?\\\\nDon\\'t we already have a couple of those?\\\\n*grins*\\\\nBil Browning | December 21, 2008\\\\n1:18 PMReply to this comment\\\\nAre we giving Dems a pass here? Does anyone disagree that a Republican inviting Warren to pray at such a major event (giving Warren legitimacy) would subsequently be blamed (rightly or wrongly) for all subsequent homophobic attacks from NYC to California over the next four years? This IS a big deal.\\\\nJJJJ | December 19, 2008\\\\n4:42 PMReply to this comment\\\\nGiving them a pass?\\\\nHardly.\\\\nThat\\'s not what I advocated at all.\\\\nI\\'m just saying it could have been handled differently with a much better chance at our community actually winning something from the fight.\\\\nWarren won\\'t be uninvited.\\\\nHence, we lose.\\\\nI want to win.\\\\nBil Browning | December 21, 2008\\\\n1:21 PMReply to this comment\\\\nJoe Solmonese is even whining about Warren too.\\\\nIt reminds me of a Beatles song, \\\\\"Fool on the Hill.\\\\\"\\\\nMonicaHelms | December 19, 2008\\\\n5:03 PMReply to this comment\\\\nJoe being upset about something does not make the issue wrong, Monica.\\\\nPresenting Warren front and centre as part of the first official act of the Obama Administration is not just wrong, it is also stupid.\\\\nFor a world longing for the end of extra legal policies that Bush pursued, Warren and his advocacy of violating Hague IV by assasinating the Iranian PM is sbsolutely the least reassuring gesture that Obama could have made.\\\\nFor the Gays and Lesbians languishing in Nigerian prisons because of laws pushed for by Warren\\'s friend and ally Peter Akinola, Warren\\'s presence is a kick in the teeth, as is our mixed response to him.\\\\nMauraHennessey | December 21, 2008\\\\n2:18 PMReply to this comment\\\\nBut this temper tantrum that we\\'re throwing publicly? It\\'s not worth it. There\\'s no winning either the political or the pr battle.\\\\nWTF Bil - stop complaining, since we can\\'t change anything anyway? Should we tell the pro-choice folks to stop their temper tantrum too. (read sarcasm)\\\\nWe need to shut the hell up and go to work on the i\\n[DOC 3] Obama hearts Rick Warren | Crooks and Liars\\\\nCrooks and Liars\\\\nBlue America\\\\nLate Nite Music Club\\\\nVideo Cafe\\\\nLogin | Register\\\\nJohn Amato\\\\u0019s virtual online magazine&OK, It\\\\u0019s a blog!\\\\nHomeArchives\\\\nTags Directory\\\\nContact UsDonateFAQ\\\\nAdvertisingCommentingGeneral ProblemsMedia ProblemsSupported BrowsersUser Registration\\\\nBlogs » John Amato\\'s blog\\\\nTOPICS\\\\nReligious Right\\\\n7 Diggs\\\\nObama hearts Rick Warren\\\\nBy John Amato Wednesday Dec 17, 2008 2:00pm\\\\nSay,what?\\\\nPresident-elect Barack Obama\\'s swearing-in ceremony will feature big names like minister Rick Warren and legendary singer Aretha Franklin, the Congressional Committee on Inaugural Ceremonies announced Wednesday. Warren, the prominent evangelical and founder of the Saddleback Church in Lake Forest, California, will deliver the ceremony\\'s invocation.\\\\nRight Wing Watch outlines many of the reasons why he should not have been asked to perform this function. I guess Obama doesn\\'t mind that Warren sandbagged him at the Saddleback Forum, (cone of silence) and Warren still gets the nod. PFAW have issued a statement.\\\\n\\'Profoundly Disappointed\\' that Rick Warren Will Give Invocation.\\\\nDuncan: Aside from the bigot part, Rick Warren is, you know, a liar.\\\\nWe\\'ve covered many of Warren\\'s sins (Rick Warren is the new Jerry Falwell: \\'The Bible says that God puts government on earth to punish evildoers.\\') so why did Obama bring him on for this?\\\\nI\\'ve been very supportive of Obama so far, but I have to say that Obama\\'s decision on this one is highly insulting.\\\\nTags: Obama, Religious Right, Rick Warren\\\\nEmail\\\\nPrint\\\\nRelated\\\\nReactions\\\\nObama tries to explain Rick Warren\\\\nFocus On The Family Compares Obama To Nazis\\\\nRick Warren\\'s vision of Christianity inspired by totalitarians\\\\nGay Activist Wins Hardball Debate with Preacher\\\\nGreta Von Susteren Asks Huckabee About Rick Warren\\'s Role At Obama Inauguration\\\\nSoutheast to Southwest: Feinstein\\'s Cool StatementBaseball, Music, and Real Life: Jumping off the Obama ExpressTruthdig - Ear to the Ground - Rick Warren Tapped for Obama\\\\u0019s ...12 Days of Christmas with a Cartoon Hindu Guy Dressed Like Elvis ...Why Santa Couldn\\\\u0019t Answer Your Letter « WriteChic Press\\\\nAdvertise Here\\\\nLogin or Register to post comments.\\\\n190 comments\\\\nRe: Obama hearts Rick Warren\\\\nWed, 12/17/2008 - 14:03 \\\\u0014 Captain Kangaroo\\\\nRick Warren is the new Jerry Falwell.\\\\nTherein lies the problem.\\\\nLogin or register to reply\\\\nNot really... dig deeper\\\\nWed, 12/17/2008 - 16:29 \\\\u0014 Ohmmade\\\\nThe true nature of the problem is that Obama is a christian.\\\\nAlready, you are fucking nuts if you believe any of that stuff, and Obama has shown that he lets his christian belief system, alter his view. Take gay marriage, marijuana decriminalization, and abortion for examples.\\\\nLogin or register to reply\\\\nFolks stop blaming Obama for Rick Warren.\\\\nKnow your history\\\\nWed, 12/17/2008 - 18:25 \\\\u0014 CoIntelPro.Pron...\\\\nFolks stop blaming Obama for Rick Warren.\\\\nKnow your history.\\\\nLogin or register to reply\\\\nBarack Obama is a \\\\\"progressive-liberal\\\\\" Fraud!!!!!!!!!!!!\\\\nThu, 12/18/2008 - 12:20 \\\\u0014 EastCoastLefty62\\\\nPlease. Rick Warren giving the invocation at the inaguration is more proof that Barack Obama is a fraud. I am so tired of this Obama cult movement and that he is somehow immune from criticism. Obama is just another slimy politico, folks and the sooner you all realize this fact, the\\\\nmore you will be able to absorb the lies yet to come your way. Obama is a master at deception and for this, he has been elected the president of the USA. Obama has mastered the art of con jobster, and so well, that the liberal/progressives of the Internet have never grasped. Only true leftists knew what an abject liar Obama was all the way through the election cycle. Just because McCain-Palin were more repulsive than Obama, does not make the man with many \\\\\"changes\\\\\" a true liberal or progressive. Read the writing found on www.counterpunch.org or www.blackagendareport.com if you want a real critique of the fraud known as Obama from Chicago. Wake up already folks. Rick Warren. Doug Kmiec. Donnie McClerkin. All these folks are anti-gay and anti-women and firm now in the Obama camp. Obama is a gay hater and he like Rick Warren is opposed to gay marriage!!!\\\\nI wish people who consider themselves to be either \\\\\"liberals or progressives\\\\\" stop acting like Barack Obama is actually a liberal or progressive when he is not. Obama lied in masterful rhetorical form to the gullible liberals and naive progressives who actually still think that he is some kind of \\\\\"peace-loving\\\\\" or \\\\\"justice-seeking\\\\\" man.\\\\nAll the concessions and moves that Obama has made thus far to bring about change is to move to both the center and the right. Bob Gates! Please. Remember Iran-Contra folks. Get a grip. Biden and Hillary Clinton are both war hawks and both avowed orientalists. As Glen Ford at www.blackagendareport.com put it, in his article titled, \\\\\"Obama\\'s \\\\u0018Center-Right\\' Presidency: The Die is Cast,\\\\\":\\\\n\\\\\"We already know enough. There is no more mystery as to what makes Barack Obama tick. He is your garden variety \\\\\"center-right\\\\\" American politician, as rated by no less an authority than the New York Times - the folks who practically invented the standards by which one is located in the corporate political spectrum....There are lots of political appointments to go before Obama\\'s roster is complete, but the heavy lifting is done. The ideological pillars of America\\'s first Black presidency have been planted wholly within the parameters of governance allowed by big capital and the imperial military. Obama\\'s \\\\\"transition\\\\\" is more accurately seen as a \\\\\"continuity\\\\\" of rule by the lords of finance capital and their protective screen of warriors and spies. The Obama regime, still incomplete, already wreaks of filthy rich thieves and gore-covered war criminals....Obama\\'s \\\\\"national security\\\\\" and economic lineup is an infinity of ugliness, more repulsive than I could have imagined back in the summer of 2003, when Obama\\'s rise to glory was about to begin. The supremely talented actor/state senator\\'s capacity for obfuscation; his refusal to take a firm position on any subject of real controversy; his transparently false denials of fealty to the corporate Democratic Leadership Council, which had publicly claimed him - all this should have marked Obama as bad news for Black America. But his was a fatally attractive package, like the shiny little cluster bomblets that kids pick up in places like Afghanistan....\\\\\"\\\\nPaul Street, agreed with Glen Ford as well. Street\\'s latest article, at www.blackagendareport.com, titled, >\\\\\"The\\\\nWait \\\\u0018Til He Gets In\\\\nDelusion: The President Elect is Not a Latent Lefty,\\\\\" also dismissed any notion of Obama being anything but a center-right politico, saying that:\\\\n\\\\\"One of the more recurrent refrains I heard from many of Barack Obama\\'s progressive supporters in late 2007 and through the recent election went like this: \\\\\"Oh, he has to say and do that stuff to get elected. The corporate and military powers that be will sink him if he acts as left as he really is. Just wait until he gets in: then you\\'ll see the real progressive deal.\\\\\" \\\\\"That stuff\\\\\" included Obama declaring his readiness to bomb Iran, saying that black Americans had come \\\\\"90 percent\\\\\" of the way to equality, treating Jeremiah Wright\\'s anger over American racism as inappropriate for the current era, proclaiming that the U.S. invaded Iraq with noble intentions, and saying that \\\\\"the Surge\\\\\" was \\\\\"succeeding beyond our wildest imagination.\\\\\" Other parts of the Obama campaign package: advancing nuclear power and Ethanol, claiming that leading Wall Street firms and other large corporations were as interested as anyone else in \\\\\"American renewal\\\\\" (they \\\\\"just hadn\\'t been asked\\\\\" to help the country, Obama said last year), supporting the unilateral use of military power even in \\\\\"situations beyond self-defense\\\\\" (in a 2007 Foreign Affairs essay), and calling for an expansion of U.S.-imperial armed forces.\\\\\"\\\\nStop acting like Obama is a true \\\\\"liberal or progressive\\\\\" when he is not. Obama\\'s picks have not shown any signs that he will govern to the left whatsoever. It appears that Obama is out to prove that Democrats can be good rightwing Republicans too and that Democrats also can embrace the same right-wing Evangelical Christianity that Rick Warren and James Dobson represent. Yes, Obama has brought about \\\\\"change,\\\\\" alright. In fact, the change that Obama has delivered was to bring the Democratic Party into the same religious fold as the corporate neoliberals who helped ruin the world economy and by extension, the U.S. economy.\\\\nIf you are shocked at Obama\\'s choosing Rick Warren, don\\'t be. Those bones that you all think Obama is just stalling to throw to the real liberals and progressives will not be coming anytime soon. Obama is a skilled establishmentarian, who used elite rhetoric on a very gullible and naive progressive movement found on the Internet and elsewhere. Contrary to Barack \\\\\"Fraud\\\\\" Obama, his campaign only received 25% of its funds from those \\\\\"small donors\\\\\" given through the Internet rather than the 90% lie put out by his lying campaign staff led by Mr. David Axelrod and Rahm Emanuel. Perhaps, Patrick Fitzgerald can help save liberals/progressives from themselves.\\\\nBy the way, I am a radical leftist and not a liberal or progressives. Too many liberals/progressives are dumb enough to believe the rhetoric from the likes of Barack Obama yet, never really comprehend the actual content or words he speaks of.\\\\nLogin or register to reply\\\\ninclusiveness\\\\nMon, 12/22/2008 - 06:17 \\\\u0014 Gerundo\\\\nObama campaigned on a promise of inclusiveness. Now he\\'s showing what he meant.... you go to the guy you disagree with most and set a place at the table for him. I think we all need to get a little bigger about this. Either that or stop bitching next time you want to complain about a conservative group not being inclusive.\\\\nLogin or register to reply\\\\nUm, Obama...You Won\\\\nWed, 12/17/2008 - 14:05 \\\\u0014 Mar Del Zur\\\\nYou can stop sucking up to wingnut xtians like Warren and his ilk. These are the people who should be kicked out of Washington, not invited to your damn inauguration. Unreal.\\\\nLogin or register to reply\\\\nRe: Obama hearts Rick Warren\\\\nWed, 12/17/2008 - 14:07 \\\\u0014 JerryO\\\\nPreach it brother...Hallelujah!\\\\nLogin or register to reply\\\\namong other things\\\\nWed, 12/17/2008 - 14:09 \\\\u0014 all hail the hy...\\\\nThe interesting part is that the BBC broke down the votes and it seemed Obama didn\\'t really make a dent in the white fantasy nutcase voter.\\\\nLogin or register to reply\\\\nRIGHT!!! Obama had 5 million LESS white votes than Kerry and\\\\nWed, 12/17/2008 - 14:20 \\\\u0014 YouCantHandleDaTruth\\\\n...still won the election.\\\\nHe doesn\\'t need the 99.7% white fundie nut vote IMHO, they use God as an excuse to be bigots.\\\\nLogin or register to reply\\\\nRe: Obama hearts Rick Warren\\\\nWed, 12/17/2008 - 14:05 \\\\u0014 JerryO\\\\nMaybe to score some points with the fundies?\\\\nLogin or register to reply\\\\nPoints with fundies\\\\nWed, 12/17/2008 - 16:25 \\\\u0014 sbj\\\\nThe only 2 ways for a politician to score points with the fundies is to either give them power and money or to become one of them. Short of providing one or both of these things, one remains the enemy.\\\\nLogin or register to reply\\\\nObama Can\\'t Get His Nose of the Religious Right\\'s Craw\\\\nWed, 12/17/2008 - 14:06 \\\\u0014 Christopher di ...\\\\nIncreasingly, I wonder just who is President-elect Barack Obama?\\\\nOf all the people he could have asked, he selected Rick Warren?\\\\nShame. On. You. Obama.\\\\nLogin or register to reply\\\\nYou mean\\\\nWed, 12/17/2008 - 14:57 \\\\u0014 Floridiot\\\\ncrawcth don\\'t ya?\\\\nLogin or register to reply\\\\nI know\\\\nThu, 12/18/2008 - 07:37 \\\\u0014 AngelfoodMcSpade\\\\nevery day my \\\\\"hope\\\\\" for \\\\\"change\\\\\" sinks lower.\\\\nYesterday he also picked Ken Salazar sect\\'y of the Interior..dem who has kept exclusive company with thugs from the second he won his senate seat in CO. Expect damage. Think Lieberman.\\\\nLogin or register to reply\\\\nPissed\\\\nWed, 12/17/2008 - 14:07 \\\\u0014 Leadership\\\\nNow this pisses me off.\\\\nI know in my heart of hearts that Obama is way to smart to be a Theist.\\\\nWay to fucking smart to buy into that bronze age middle eastern mythology.\\\\nHaving that snake oils salesman up there preaching to us is fucking bullshit.\\\\nIf you like the Bible you MUST be a fan of genocide, aggressive war, slavery, incest, torture and just plain fucking with people (see Abraham bullied by God to kill his son, and the whole Job episode.)\\\\nLogin or register to reply\\\\nBible fan\\\\nWed, 12/17/2008 - 15:58 \\\\u0014 LazyCosmos\\\\nThis is a swearing in ceremony for a secular job...an invocation is part of the ceremony because...???\\\\nAn invocation is basically a pleading with a deity for some benefit...it\\'s an insult to every rational person in attendance or watching the broadcast.\\\\nLogin or register to reply\\\\nWORD.\\\\nThu, 12/18/2008 - 07:44 \\\\u0014 jood42\\\\nIt\\'s utterly offensive to me that a representative of one specific religion is employed to solemnize the swearing-in of a SECULAR government position.\\\\nLogin or register to reply\\\\nHe has to prove he\\'s one of \\'us\\'\\\\nWed, 12/17/2008 - 14:08 \\\\u0014 TheBeatles120\\\\nWhat a disappointment.\\\\nI don\\'t understand why any religion has to be brought into the inauguration.\\\\nI feel like he\\'s just trying to prove to the right-wing crazies that he\\'s \\\\\"religious\\\\\" too, and they have \\\\\"common ground\\\\\" to stand on.\\\\nObama, you are really pushing my buttons!\\\\nHey don\\'t forget this asshole, Warren, helped pass a referendum to keep my sister (and millions of others) from marrying the one she loves in California.\\\\nThanks BO, Change We Can Believe In!\\\\nLogin or register to reply\\\\nYet another decision...\\\\nWed, 12/17/2008 - 14:09 \\\\u0014 DevilDog21\\\\nthat makes me wonder why Obama was seen as so much better than Kucinich. We are seeing that while he is a Dem, he certainly is not a progressive...\\\\nLogin or register to reply\\\\nHe never was\\\\nWed, 12/17/2008 - 16:27 \\\\u0014 Ohmmade\\\\nI don\\'t know why you thought he was a \\\\\"progressive\\\\\". Just because you\\'re not batshit insane, hungry for war, and ready to throw social security to the wolves, doesn\\'t mean you are progressive. It just means you\\'re not Bush.\\\\nLogin or register to reply\\\\nRe: Obama hearts Rick Warren\\\\nWed, 12/17/2008 - 22:44 \\\\u0014 motorfingaz\\\\nHuh??\\\\nLogin or register to reply\\\\nWhy\\\\nThu, 12/18/2008 - 07:52 \\\\u0014 AngelfoodMcSpade\\\\nis O appeasing this 25% extreme faction that has held this country hostage for the last 9 years? (on whose path BClinton strew rose petals?).\\\\nIt can only mean 3 things: He\\'s naive`, has a hostage mentality, is a fraud.\\\\nLogin or register to reply\\\\nWhy is this surprising?\\\\nWed, 12/17/2008 - 14:10 \\\\u0014 Ohmmade\\\\nLook, Obama is a Christian. He\\'s given speeches telling us that he doesn\\'t support gay marriage because of his religious beliefs, his god tells him abortion is wrong, and he was going to a pretty freaky church for a long time.\\\\nObama is a christian. Not as hardcore fundie as Bush lied about being, but Obama basically believes in Jesus, walking on water, and a big guy in the sky watching over us.\\\\nI wouldn\\'t be surprised if Obama thinks evolution is all part of god\\'s plan.\\\\nLogin or register to reply\\\\nExactly, but...\\\\nWed, 12/17/2008 - 15:10 \\\\u0014 pauleky\\\\n...everyone still has a right to complain about it. I had major reservations about BO, but at his worst, he\\'ll still be better than what we\\'ve had for the last eight years or any Republican, period!\\\\nLogin or register to reply\\\\nits not surprising its depressing...\\\\nWed, 12/17/2008 - 19:23 \\\\u0014 smithersSOCAL\\\\nI never expected Obama to swing the bat for the gay community where gay marriage in California was concerned.\\\\nbut this constant pandering to the evangelical fringe is so maddening.\\\\nObama doesent need to thank me for encouraging my family to vote for him even though the large bulk of my family was (and are) diehard Hillary supporters.\\\\nthere are enlightened christians....people like Cornell West...and even Jimmy Carter.. but Rick \\\\\"i have 2 gay friends\\\\\" Warren is no enlightened christian.\\\\nthis ecumenical idealism will end badly.....\\\\nLogin or register to reply\\\\nRe: Obama hearts Rick Warren\\\\nThu, 12/18/2008 - 08:03 \\\\u0014 AngelfoodMcSpade\\\\nDaily.\\\\nEvery Day.\\\\nEvery choice he is making to surround himself with these heartbreaking dull knives that are so dangerous.\\\\nOne after another.\\\\nNot one DECENT ordinary sane person has been placed in any meaningful position of power.\\\\nWe\\'ve been had.\\\\nLogin or register to reply\\\\noh\\\\nThu, 12/18/2008 - 08:08 \\\\u0014 AngelfoodMcSpade\\\\nand let\\'s not forget how Blackwater will continue to thrive and run rogue under our flag abroad and DOMESTICALLY.\\\\nNo chink in Eric Prince\\'s bank account.\\\\nLogin or register to reply\\\\nObama is making a LOT of bad choices\\\\nWed, 12/17/2008 - 14:12 \\\\u0014 Christy\\\\nYeah and choosing someone working with Chicago schools to head our education department is laughable.\\\\nIs Chicago really doing so great in the education department because I have heard otherwise.\\\\nI\\'m way disappointed. I really wanted to believe in him. Now between all this and his choosing Clinton as secretary of state I\\'m feeling pretty hopeless.\\\\nLogin or register to reply\\\\nyeap\\\\nWed, 12/17/2008 - 14:19 \\\\u0014 YouCantHandleDaTruth\\\\n\\\\\"...Is Chicago really doing so great in the education department because I have heard otherwise...\\\\\"\\\\nIf you look at it from a perspective of IMPROVEMENT then yes.\\\\nLogin or register to reply\\\\nand Salazar\\\\nWed, 12/17/2008 - 14:24 \\\\u0014 Blue Lensman\\\\nthe reich wing is loving his selections.\\\\nThis bothers me..\\\\nLogin or register to reply\\\\nI have a feeling\\\\nWed, 12/17/2008 - 14:12 \\\\u0014 Blue Rose\\\\nthat Obama is just throwing a bone to the Reich-wingers.\\\\nThat said, I really wish there was a true separation of church & state.\\\\nAny time an invocation is said at an event, I\\'ll stand by quietly but I am not very happy.\\\\nLogin or register to reply\\\\nre: I have a feeling\\\\nWed, 12/17/2008 - 14:42 \\\\u0014 shermhead\\\\nBut he didn\\'t need to; see above comments about just how little of a dent he made into the white, xian, conservative demographic.\\\\nAs was stated before, Kerry did much better.\\\\nThe Big O cleaned McCains clock without the help of the hardcore fundies and evangelicals.\\\\nThe throwing a bone argument carries no weight.\\\\nLogin or register to reply\\\\nHow Many Bones...\\\\nWed, 12/17/2008 - 15:43 \\\\u0014 Freddy Knuckles\\\\nis he going to throw to them? They already have a good-sized pile that he\\'s thrown them.\\\\nI\\'m starting to feel serious buyer\\'s remorse.\\\\nLogin or register to reply\\\\nRe: Obama hearts Rick Warren\\\\nWed, 12/17/2008 - 22:46 \\\\u0014 motorfingaz\\\\nYou are sounding as crazy as the Bush supporters. Just on the opposite end of the spectrum.\\\\nLogin or register to reply\\\\nso Freddy gets muffled\\\\nThu, 12/18/2008 - 08:30 \\\\u0014 AngelfoodMcSpade\\\\nand is not permitted to doubt.\\\\nNot \\\\\"allowed\\\\\" (according to you) to hand over his comprehending mind and just \\\\\"trust\\\\\" based on no evidence, unearned (adults earn trust).\\\\nSo he\\'s crazy and extreme as bush supporters for that?\\\\nHow the fuck old are you? 16?\\\\nLogin or register to reply\\\\nRe: Obama hearts Rick Warren\\\\nWed, 12/17/2008 - 14:12 \\\\u0014 pissed off patricia\\\\nI really don\\'t care.\\\\nThat\\'s not the part of the ceremony that I pay attention to, no matter who it is.\\\\nI\\'m much more concerned about that day coming off safe and sane due to the giant crowd that will be there.\\\\nLogin or register to reply\\\\nTrue, good perspective...1 toilet per 12,000 people is a...\\\\nWed, 12/17/2008 - 14:18 \\\\u0014 YouCantHandleDaTruth\\\\n...bigger issue along with Obama\\'s safety...\\\\nLogin or register to reply\\\\nRe: Obama hearts Rick Warren\\\\nWed, 12/17/2008 - 14:38 \\\\u0014 Captain Kangaroo\\\\nyeah, I\\'m much more worried about Iraq and the economy. But I\\'m thinking that he could have had Reverend Wright do this instead of Falwell Jr.\\\\nLogin or register to reply\\\\nRe: Obama hearts Rick Warren\\\\nThu, 12/18/2008 - 07:47 \\\\u0014 jood42\\\\nI pay attention to that part of the ceremony because it specifically disenfranchises me.\\\\nPersonally.\\\\nBush was proclaimed president eight years ago by a minister who solemnized the deal with, \\\\\"In the name of our Lord Jesus Christ.\\\\\"\\\\nGuess what: Jesus Christ is not everybody\\'s Lord.\\\\nWay to alienate every religious minority in this country, folks, including rational, disgusted atheists.\\\\nEnough of religion as a ceremonial part of politics.\\\\nDidn\\'t they learn their lesson after the Rev. Moon \\\\\"crowning\\\\\" backlash?\\\\nLogin or register to reply\\\\nWell...\\\\nWed, 12/17/2008 - 14:13 \\\\u0014 DanD\\\\nPeople are out of work, or on their way out of work, South Asia\\'s more on the brink than they usually are and we have eight years of damage to every aspect of American public policy to clean up.\\\\nForgive me if I couldn\\'t care less who introduces Barack Obama to god on January 20.\\\\nWarren is fairly popular with a great number of people I disagree with.\\\\nBut PFAW either weren\\'t paying attention when Obama said he wouldn\\'t be living in a liberal bubble, or they should have voted for someone else.\\\\nGo ahead, hear what the guy has to say.\\\\nI\\'m pleased Barack Obama\\'s going to have the final word for the next 4 years and I\\'m not worried he\\'s so fragile he\\'s going to be infected by conservative germs.\\\\nLogin or register to reply\\\\nIt is traditional..\\\\nWed, 12/17/2008 - 14:17 \\\\u0014 Amitola\\\\n...to have an invocation and sometimes a benediction at these kinds of events.\\\\nI have the feeling that Barack is not inviting Warren because he supports his kind of theocracy, but because he doesn\\'t.\\\\nWhat better way to make the evangelical Warren-worshippers shut the F**k up than to let him \\\\\"pray\\\\\" at the inauguaration?\\\\nThen, we never have to hear from him again and none of them can say Barack slighted them.... one and done.\\\\nLogin or register to reply\\\\n<div class=\\\\\"clear-b\\n[DOC 4] The Rev. Chuck Currie: Rick Warren Wrong Voice For Inauguration\\\\nThe Rev. Chuck Currie\\\\nViews from a United Church of Christ Minister\\\\nAbout Rev. Currie\\\\nThe Rev. Charles Stephen Currie, Jr.\\\\nContact Information\\\\nRecent News\\\\nNotice\\\\nE-Mail sent to my address may be published.\\\\nPeople are encouraged to leave comments.\\\\nHowever, comments meant to simply offend others \\\\u0013 or that are personal attacks \\\\u0013 will either be deleted or the commenter banned from the site.\\\\nWe can disagree with each other and still be respectful.\\\\nI also reserve the right not to post comments from people using fake names and / or e-mail addresses.\\\\nBe willing to take responsibility for comments you make here.\\\\nDisclaimer:\\\\nViews expressed here are my own and unless otherwise stated do not reflect the opinion of any UCC congregation, related body or any other organization.\\\\nAdd me to your TypePad People list\\\\nSubscribe to this blog\\'s feed\\\\nSubscribe to my Podcast\\\\nYahoo\\\\nClick here to add this blog to your My Yahoo!\\\\nNews\\\\nAir America Radio\\\\nCatholic Media Report\\\\nCommon Dreams News\\\\nDisciples World\\\\nKGW Portland\\\\nKMOV St. Louis\\\\nNPR\\\\nOPB\\\\nReligion and Ethics NewsWeekly\\\\nSojourners\\\\nWorldwide Faith News\\\\nFriends Online\\\\nAndrew Stelzer\\\\nErik Sten\\\\nGretchen Kafoury\\\\nLes AuCoin\\\\nPaul Nickell\\\\nRuss Dondero\\\\nSam Chase\\\\nSunset High School Class of \\'87 Reunion\\\\nThe Rev. Aaron Roberts\\\\nThe Rev. Scott Elliott\\\\nThe Roberts Family\\\\nTina Kotek\\\\nWillie Deuel\\\\nWinston Calvert\\\\nMySpace\\\\nChuck\\'s MySpace Page\\\\nSee how we\\'re connected\\\\nTechnorati\\\\nArchives\\\\nJanuary 2009\\\\nDecember 2008\\\\nNovember 2008\\\\nOctober 2008\\\\nSeptember 2008\\\\nAugust 2008\\\\nJuly 2008\\\\nJune 2008\\\\nMay 2008\\\\nApril 2008\\\\nMore...\\\\n« Weekly Podcast From Parkrose Community United Church of Christ For Dec. 17, 2008 |\\\\nMain\\\\n| Catch Me at 1:10 PM (Pacific) On Chicago\\'s WVON »\\\\nWednesday, December 17, 2008\\\\nRick Warren Wrong Voice For Inauguration\\\\nI am deeply troubled that President-elect Obama has invited Rick Warren to offer the invocation at the inauguration.\\xa0 Warren stands opposed to the progressive agenda and to many of the core values that Barack Obama campaigned on.\\xa0 The symbolism of offering such as prodigious place in history to a figure such as Warren is upsetting.\\\\nWhat is Warren\\'s record?\\xa0 People for the American Way reports:As we\\'ve pointed out several times before, in 2004 Warren declared that marriage, reproductive choice, and stem cell research were \\\\\"non-negotiable\\\\\" issues for Christian voters and has admitted that the main difference between himself and James Dobson is a matter of tone.\\xa0 He criticized Obama\\'s answers at the Faith Forum he hosted before the election and vowed to continue to pressure him to change his views on the issue of reproductive choice.\\xa0 He came out strongly in support of Prop 8, saying \\\\\"there is no need to change the universal, historical definition of marriage to appease 2 percent of our population ... This is not a political issue -- it is a moral issue that God has spoken clearly about.\\\\\" He\\'s declared that those who do not believe in God should not be allowed to hold public office.Warren is a good spokesman for the Religious Right but does not represent mainstream Christianity.\\\\nUpdate:\\xa0 I\\'m delighted, however, to learn that The Reverend Dr. Joseph E. Lowery will provide the benediction.\\xa0 Rev. Lowery is an American hero whose ministry of tolerance and justice stands in stark contrast to those who have used religion as a tool to divide Americans.\\\\nUpdate:\\xa0 The Huffington Post has picked up the story...On Wednesday, the transition team announced that Rick Warren, pastor of the powerful Saddleback Church, would give the invocation on January 20th. The selection may not have been incredibly surprising. Obama and Warren are reportedly close -- Obama praised the Megachurch leader in his second book \\\\\"The Audacity of Hope.\\\\\" Warren, meanwhile, hosted a values forum between Obama and McCain during the general election. Nevertheless, the announcement is being greeted with deep skepticism in progressive religious and political circles.\\\\n\\\\\"My blood pressure is really high right now,\\\\\" said Rev. Chuck Currie, minister at Parkrose Community United Church of Christ in Portland, Oregon. \\\\\"Rick Warren does some really good stuff and there are some areas that I have admired his ability to build bridges between evangelicals and mainline religious and political figures... but he is also very established in the religious right and his position on social issues like gay rights, stem cell research and women\\'s rights are all out of the mainstream and are very much opposed to the progressive agenda that Obama ran on. I think that he is very much the wrong person to put on the stage with the president that day.\\\\\" Click here to read the full article.\\\\nPosted at 11:52 in President Barack Obama\\\\n| Permalink\\\\nTechnorati Tags:\\\\ninauguration, Rick Warren\\\\nDigg This\\\\n| Save to del.icio.us\\\\nTrackBack\\\\nTrackBack URL for this entry:http://www.typepad.com/t/trackback/8008/37203868\\\\nListed below are links to weblogs that reference Rick Warren Wrong Voice For Inauguration:\\\\nRick Warren Wrong Voice For Inauguration\\\\nI am deeply troubled that President-elect Obama has invited Rick Warren to offer the invocation at the inauguration.\\xa0 Warren stands opposed to the progressive agenda and to many of the core values that Barack Obama campaigned on.\\xa0 The symbolism of offering such as prodigious place in history to a figure such as Warren is upsetting.\\\\nWhat is Warren\\'s record?\\xa0 People for the American Way reports:As we\\'ve pointed out several times before, in 2004 Warren declared that marriage, reproductive choice, and stem cell research were \\\\\"non-negotiable\\\\\" issues for Christian voters and has admitted that the main difference between himself and James Dobson is a matter of tone.\\xa0 He criticized Obama\\'s answers at the Faith Forum he hosted before the election and vowed to continue to pressure him to change his views on the issue of reproductive choice.\\xa0 He came out strongly in support of Prop 8, saying \\\\\"there is no need to change the universal, historical definition of marriage to appease 2 percent of our population ... This is not a political issue -- it is a moral issue that God has spoken clearly about.\\\\\" He\\'s declared that those who do not believe in God should not be allowed to hold public office.Warren is a good spokesman for the Religious Right but does not represent mainstream Christianity.\\\\nUpdate:\\xa0 I\\'m delighted, however, to learn that The Reverend Dr. Joseph E. Lowery will provide the benediction.\\xa0 Rev. Lowery is an American hero whose ministry of tolerance and justice stands in stark contrast to those who have used religion as a tool to divide Americans.\\\\nUpdate:\\xa0 The Huffington Post has picked up the story...On Wednesday, the transition team announced that Rick Warren, pastor of the powerful Saddleback Church, would give the invocation on January 20th. The selection may not have been incredibly surprising. Obama and Warren are reportedly close -- Obama praised the Megachurch leader in his second book \\\\\"The Audacity of Hope.\\\\\" Warren, meanwhile, hosted a values forum between Obama and McCain during the general election. Nevertheless, the announcement is being greeted with deep skepticism in progressive religious and political circles.\\\\n\\\\\"My blood pressure is really high right now,\\\\\" said Rev. Chuck Currie, minister at Parkrose Community United Church of Christ in Portland, Oregon. \\\\\"Rick Warren does some really good stuff and there are some areas that I have admired his ability to build bridges between evangelicals and mainline religious and political figures... but he is also very established in the religious right and his position on social issues like gay rights, stem cell research and women\\'s rights are all out of the mainstream and are very much opposed to the progressive agenda that Obama ran on. I think that he is very much the wrong person to put on the stage with the president that day.\\\\\" Click here to read the full article.\\\\nView the entire comment thread.\\\\nUnited Church News Blog\\\\nVisit The Rev. Chuck Currie now daily at the United Church News Blog.\\\\nRev. Currie\\'s Posts On United Church News Blog\\\\nRecent Comments\\\\nLaura on Rick Warren Wrong Voice For Inauguration\\\\nMichael on Rick Warren Wrong Voice For Inauguration\\\\nMark Mullins on Rick Warren Wrong Voice For Inauguration\\\\nAdrian Pyle on Rick Warren Wrong Voice For Inauguration\\\\nBrian on Rick Warren Wrong Voice For Inauguration\\\\nTom on Rick Warren Wrong Voice For Inauguration\\\\nMavin Lance Wiser on Rick Warren Wrong Voice For Inauguration\\\\nThe Rev. Chuck Currie on Rick Warren Wrong Voice For Inauguration\\\\nHooman on Rick Warren Wrong Voice For Inauguration\\\\nBrian on Rick Warren Wrong Voice For Inauguration\\\\nUnited Church of Christ\\\\nCentral Pacific Conference UCC\\\\nEden Theological Seminary\\\\nFirst Congregational UCC Portland\\\\nGod Is Still Speaking\\\\nParkrose Community UCC (Portland)\\\\nUCC Education\\\\nUCC Faith\\\\nUCC LGBT Coalition\\\\nUCC Ministries\\\\nUCC News\\\\nUCC Peace & Justice\\\\nUCC Worship\\\\nUnited Church of Christ\\\\nFaith Blogs\\\\na believer\\'s journal\\\\nA Guy in the Pew\\\\nA Lie A Day\\\\nA Religious Liberal Blog\\\\nAbundancetrek\\\\nany day a beautiful change\\\\nBeatitudes Blog\\\\nBehind the surface\\\\nbethquick.com\\\\nBlog of the Greateful Bear\\\\nBoy In The Bands\\\\nconnexions\\\\nCracked Posts\\\\nEven You\\\\nFaith In Society\\\\nFaith Jouneys\\\\nfaithCommons.org\\\\nFaithful Progressive\\\\nFaithfully Liberal\\\\nFar Country Tell\\\\nGay Religion\\\\nGay Spirituality & Culture\\\\nJesus Politics\\\\nLiberal Christians\\\\nMelissa Rogers\\\\nMy Blurred Vision\\\\nPhilocrites: commentary on liberal religion and politics\\\\nPhilosophy Over Coffee\\\\nPonderings on a\\\\nJourney\\\\nProgressive Revival\\\\nRadical Hapa\\\\nSpiritBlog\\\\nthe big daddy weave\\\\nThe Ivy Bush\\\\nThe Mountaintop\\\\nThe Rev. Dr. Rev. Susan Brooks Thistlethwaite\\\\nThe Revealer\\\\ntheoblog.ca\\\\nTheolog: The Blog of The Christian Century\\\\nThree Sumach-Red Dogs I Run With\\\\nUnright Christian Blogs\\\\nFaith Web Sites\\\\nBeliefnet.com\\\\nBread for the World\\\\nCall To Renewal\\\\nCenter for Progressive Christianity\\\\nChristian Peacemaker Teams\\\\nChurch Folks for a Better America\\\\nChurch World Service\\\\nClergy for Choice\\\\nClergy Leadership Network\\\\nDay 1\\\\nEcumenical Ministries of Oregon\\\\nFaithfulAmerica.org\\\\nGeorge W. Bush & the Unreligious Right\\\\nJewish Voice for Peace\\\\nJubilee USA Network\\\\nNational Council of Churches\\\\nPax Christi USA\\\\nProgressive Theology\\\\nThe Beatitudes Society\\\\nThe Interfaith Alliance\\\\nUnited Methodist Church\\\\nWorld Council of Churches\\\\nBlogs\\\\nAbstraction: The Daily Rantings of an Ascended Mind...\\\\nAn old soul...\\\\nAngry Bear\\\\nBlog for America\\\\nDistance\\\\nEx-Gay Watch\\\\nJesus\\' General\\\\nLiberal Oasis\\\\nMarkham\\'s Behavioral Health\\\\nNet Politik\\\\nNotes on the Atrocities\\\\nPro-war.com\\\\nSTL Bloggers\\\\nThe American Street\\\\nThe Daou Report\\\\nTop Blogs\\\\nTristero\\\\nTwistedchick\\'s Free Speech Soapbox\\\\nWaveflux\\\\nWeb Sites\\\\nArthritis Foundation\\\\nBarack Obama\\\\nCenter for American Progress\\\\nHuman Rights Watch\\\\nInterfaith Committee on Homelessness (Washington County, Oregon)\\\\nNational Coalition for the Homeless\\\\nOne America Committee\\\\nThe National Psoriasis Foundation\\\\nTom Potter for Mayor\\\\nOregon Blogs\\\\nAbout It All.com\\\\nBean Anderson\\\\nBlue Oregon\\\\nGayAmerican.org\\\\nIsaac Laquedem\\\\nLittle Lost Robot\\\\nLoaded Orygun\\\\nLong Story * Short Pier\\\\nMy Whim Is Law\\\\npenn\\\\nPortland Blogs\\\\nScott Jensen\\'s Blog\\\\nThe One True b!X: PORTLAND COMMUNIQUE\\\\nThe Oregon Blog\\\\nwhat if...?\\\\nBlog powered by TypePad\\\\nMember since 10/2003\\\\nCategories\\\\n2004 Election\\\\n2006 Midterm Elections\\\\n2007 Federal Budget\\\\n2008 Election\\\\n2008 Federal Budget\\\\n9/11\\\\nBooks\\\\nCivil Rights\\\\nClergy Leadership Network\\\\nCurrent Affairs\\\\nEco-Justice\\\\nFamily\\\\nFilm\\\\nFood and Drink\\\\nFriends\\\\nHealth\\\\nHomelessness\\\\nHurricane Gustav\\\\nHurricane Katrina\\\\nIran\\\\nIraq\\\\nIsrael-Lebanon Conflict\\\\nMeasure 30\\\\nMedia\\\\nMusic [1]\\\\nNational Council of Churches\\\\nOregon\\\\nPodcast\\\\nPortland\\\\nPoverty\\\\nPoverty / Hunger\\\\nPresident Barack Obama\\\\nReligion\\\\nScience\\\\nSports\\\\nSunday Offertory\\\\nTelevision\\\\nThe Courts\\\\nThe Twins\\\\nUnited Church New Blog\\\\nUnited Church News Blog\\\\nUnited Church of Christ\\\\nWeb/Tech\\\\nWeblogs', 'output': \"I'm looking for background and biographical information on Rick Warren.\", 'input_ids': [784, 15367, 11824, 908, 27, 31, 51, 479, 21, 251, 30, 11066, 16700, 6, 8, 30682, 6323, 5, 784, 9857, 5211, 11810, 134, 908, 784, 26472, 209, 908, 37, 3, 4, 4, 11543, 127, 2, 29, 279, 1753, 89, 53, 7, 2, 29, 6861, 7, 3, 184, 14984, 7, 2, 29, 7754, 7, 2, 29, 16427, 2, 29, 26269, 3, 184, 7130, 2, 29, 31385, 2, 29, 16665, 5254, 7, 3, 184, 3953, 2, 29, 279, 2152, 7, 2, 29, 634, 3, 4, 4, 11543, 127, 10, 180, 5867, 887, 875, 81, 6525, 6, 672, 233, 2, 29, 19040, 2, 29, 23770, 697, 12714, 757, 1799, 233, 2, 29, 10288, 2721, 6, 1762, 12046, 6, 2464, 209, 10, 2469, 3246, 2, 29, 9175, 938, 1212, 9230, 9, 454, 5990, 16170, 2, 29, 427, 51, 9203, 6, 27, 92, 2065, 28, 8, 4534, 4514, 30, 13593, 277, 44, 8, 3, 30634, 6, 68, 59, 250, 8, 4374, 523, 12, 36, 5046, 45, 9388, 41, 232, 70, 598, 13, 5127, 137, 947, 27, 183, 7918, 823, 34, 56, 36, 1346, 12, 240, 6164, 3, 8215, 26, 12357, 7, 139, 8, 4374, 7, 24, 239, 2, 76, 1206, 2534, 5463, 1362, 27, 214, 33, 90, 152, 53, 581, 34, 2, 76, 1206, 2534, 232, 13817, 7, 133, 8460, 36, 44, 1020, 6, 150, 58, 2, 29, 29423, 10, 411, 9280, 6, 230, 24, 27, 43, 608, 20767, 9, 31, 7, 442, 6, 27, 240, 34, 223, 10, 27, 31, 51, 417, 34, 56, 36, 1399, 55, 2, 29, 3183, 1361, 365, 10, 4534, 6, 3, 30634, 6, 13593, 277, 2, 29, 15046, 6, 499, 71, 8509, 3195, 2, 29, 10288, 2721, 6, 1762, 12046, 6, 2464, 19520, 927, 3246, 2, 29, 9175, 938, 20767, 9, 7963, 77, 2, 29, 427, 51, 9203, 6, 27, 6, 114, 25, 6, 183, 307, 657, 8, 477, 116, 27, 317, 27, 183, 7201, 12, 830, 82, 1871, 2997, 6531, 5, 27, 6, 396, 6, 43, 27445, 5655, 13, 3, 11191, 53, 16594, 7, 30, 8, 6162, 13, 1051, 5901, 6, 42, 12805, 6, 42, 7092, 309, 5, 254, 5, 11, 3, 7437, 532, 120, 3214, 30, 57, 38, 8, 326, 14550, 16400, 7, 16644, 8034, 16, 82, 223, 28, 70, 2053, 5, 299, 270, 19, 82, 750, 2669, 10, 27, 619, 270, 5, 27, 43, 386, 502, 6, 80, 13, 135, 46, 9806, 5, 27, 473, 114, 3586, 8, 1004, 12, 9051, 48, 798, 250, 13, 128, 422, 6, 13006, 3315, 41, 88, 523, 12, 12522, 6, 255, 56, 129, 7718, 6, 3, 4605, 107, 3, 4605, 107, 3, 4605, 107, 61, 19, 1134, 20, 4715, 53, 5, 275, 26200, 6, 3, 9, 13593, 49, 56, 143, 66, 8, 1750, 5, 290, 33, 163, 78, 186, 716, 30, 3, 9, 2265, 239, 25, 54, 1520, 3, 9, 1871, 11, 11132, 192, 119, 502, 5, 438, 3, 9, 13593, 49, 6, 27, 54, 131, 24727, 8, 1871, 16, 11, 8, 12751, 30, 8, 2007, 11, 36, 30, 82, 3, 935, 651, 194, 5, 264, 4273, 6, 8, 1061, 1387, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [27, 31, 51, 479, 21, 2458, 11, 2392, 16982, 251, 30, 11066, 16700, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "tokenizer,tokenized_train_dataset, tokenized_eval_dataset= tokenize_and_split_dataset(\"training_top5_qulac_PREPROCESSED_FOR_MODEL.json\",\"google/t5-efficient-tiny\")\n",
    "print(tokenized_train_dataset[0])  # Optional: check one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eca98c-d837-4af0-8465-291d7362f22c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0f3abc-97b0-4a37-98f8-2fd7d7bf0969",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "085d12e2-cb0a-4511-bc09-560f309474f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24a83916-aab0-4c34-b54d-922ba0cb813f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 23:11:33.397038: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-12 23:11:34.418103: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747084294.707124   30855 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747084294.782840   30855 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747084295.568091   30855 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747084295.568131   30855 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747084295.568134   30855 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747084295.568136   30855 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-12 23:11:35.671759: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /users/Etu0/21402600/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /users/Etu0/21402600/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /users/Etu0/21402600/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "\n",
    "bleu = load(\"bleu\")\n",
    "rouge = load(\"rouge\")\n",
    "meteor = load(\"meteor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec32dee1-8326-45f9-b21e-dce60bd17046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    This function calculates BLEU, ROUGE, and METEOR for the model predictions.\n",
    "    eval_pred: tuple (predictions, references), where predictions are model outputs\n",
    "    and references are the true target labels.\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions= predictions[0]\n",
    "    pred_ids = np.argmax(predictions, axis=-1) \n",
    "    # Decode the model's predicted tokens and true labels\n",
    "    predicted_texts = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    true_texts = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Calculate BLEU score\n",
    "    bleu_score = bleu.compute(predictions=predicted_texts, references=true_texts)\n",
    "    rouge_score = rouge.compute(predictions=predicted_texts, references=true_texts)\n",
    "    meteor_score = meteor.compute(predictions=predicted_texts, references=true_texts)\n",
    "   \n",
    "    # Combine the metrics into a dictionary\n",
    "    return {\n",
    "        \"bleu\": bleu_score[\"bleu\"],\n",
    "        \"rouge1\": rouge_score[\"rouge1\"],\n",
    "        \"rouge2\": rouge_score[\"rouge2\"],\n",
    "        \"rougeL\": rouge_score[\"rougeL\"],\n",
    "        \"meteor\": meteor_score[\"meteor\"]\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0e10cdd8-1069-4894-9eaa-1e5cc6688473",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import torch\n",
    "device= torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "adb17c1e-9f90-427c-a141-95b920b901d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from datasets import Dataset\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(modelname, filename, tokenizer, tokenized_train_dataset, tokenized_eval_dataset, nb_epochs=30):\n",
    "    model_name =  modelname\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name,from_tf=True)\n",
    "    \n",
    "    output_dir = \"./results\"\n",
    "    logging_dir = os.path.join(output_dir, \"logs\")\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",  # Output directory for logs and checkpoints (optional)\n",
    "        eval_strategy=\"epoch\",  # Evaluate after every epoch\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=5e-5,  # Learning rate\n",
    "        per_device_train_batch_size=8,  # Batch size per device for training\n",
    "        per_device_eval_batch_size=8,  # Batch size per device for evaluation\n",
    "        num_train_epochs=nb_epochs,  # Number of training epochs\n",
    "        logging_dir=logging_dir,\n",
    "        use_cpu=True,\n",
    "        logging_first_step=True,\n",
    "        weight_decay=0.01,  # Weight decay for regularization\n",
    "        logging_steps=50,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\", \n",
    "        greater_is_better=False,\n",
    "        report_to=\"wandb\",\n",
    "        run_name=wandb.run.name\n",
    "    )\n",
    "    \n",
    "    os.makedirs(logging_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_eval_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,  # Pass the tokenizer for automatic tokenization during training\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "        \n",
    "    )\n",
    "    \n",
    "    # Start the training (without saving)\n",
    "    trainer.train()\n",
    "    \n",
    "    # You can evaluate the model here if you need\n",
    "    # Example: Evaluate after training\n",
    "    evaluation_results = trainer.evaluate()\n",
    "    print(evaluation_results)\n",
    "    \n",
    "    \n",
    "    predictions = trainer.predict(tokenized_eval_dataset)\n",
    "    \n",
    "    pred_ids = np.argmax(predictions.predictions[0], axis=-1)\n",
    "    predicted_texts = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    true_texts = tokenizer.batch_decode(predictions.label_ids, skip_special_tokens=True)\n",
    "    input_ids = tokenized_eval_dataset[\"input_ids\"]\n",
    "    input_texts = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "    \n",
    "    output_predictions_file = filename\n",
    "    with open(output_predictions_file, \"w\", encoding=\"utf-8\") as writer:\n",
    "        for input_text, true_text, predicted_text in tqdm(zip(input_texts, true_texts, predicted_texts), total=len(true_texts)):\n",
    "            output_dict = {\n",
    "                \"input\": input_text,\n",
    "                \"true\": true_text,\n",
    "                \"predicted\": predicted_text\n",
    "            }\n",
    "            writer.write(json.dumps(output_dict) + \"\\n\")\n",
    "    \n",
    "    print(f\"Final predictions with inputs saved to {output_predictions_file}\")\n",
    "\n",
    "    wandb.finish()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b0f3ed70-e026-4cbe-a86d-c065831a784f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/users/nfs/Etu0/21402600/wandb/run-20250504_220833-a0qk063s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/a0qk063s' target=\"_blank\">T5 Tiny + TO5 DOCS</a></strong> to <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/a0qk063s' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/a0qk063s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1123707/2720556636.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2720' max='2720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2720/2720 17:12, Epoch 40/40]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>13.234300</td>\n",
       "      <td>1.517482</td>\n",
       "      <td>0.014952</td>\n",
       "      <td>0.211643</td>\n",
       "      <td>0.026537</td>\n",
       "      <td>0.205198</td>\n",
       "      <td>0.110794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.799100</td>\n",
       "      <td>1.271452</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.201595</td>\n",
       "      <td>0.017517</td>\n",
       "      <td>0.202409</td>\n",
       "      <td>0.115615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.452900</td>\n",
       "      <td>1.084230</td>\n",
       "      <td>0.027275</td>\n",
       "      <td>0.260028</td>\n",
       "      <td>0.067534</td>\n",
       "      <td>0.259609</td>\n",
       "      <td>0.184927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.326900</td>\n",
       "      <td>0.944666</td>\n",
       "      <td>0.053019</td>\n",
       "      <td>0.303056</td>\n",
       "      <td>0.096960</td>\n",
       "      <td>0.302544</td>\n",
       "      <td>0.248124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.182600</td>\n",
       "      <td>0.820040</td>\n",
       "      <td>0.059990</td>\n",
       "      <td>0.323862</td>\n",
       "      <td>0.111305</td>\n",
       "      <td>0.322215</td>\n",
       "      <td>0.255717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.011800</td>\n",
       "      <td>0.746734</td>\n",
       "      <td>0.061839</td>\n",
       "      <td>0.325240</td>\n",
       "      <td>0.105657</td>\n",
       "      <td>0.321485</td>\n",
       "      <td>0.263440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.983700</td>\n",
       "      <td>0.705895</td>\n",
       "      <td>0.073363</td>\n",
       "      <td>0.342629</td>\n",
       "      <td>0.120120</td>\n",
       "      <td>0.337879</td>\n",
       "      <td>0.281048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.891700</td>\n",
       "      <td>0.682155</td>\n",
       "      <td>0.084416</td>\n",
       "      <td>0.362100</td>\n",
       "      <td>0.130386</td>\n",
       "      <td>0.359398</td>\n",
       "      <td>0.303273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.864700</td>\n",
       "      <td>0.668440</td>\n",
       "      <td>0.095278</td>\n",
       "      <td>0.377438</td>\n",
       "      <td>0.144101</td>\n",
       "      <td>0.371821</td>\n",
       "      <td>0.323610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.837300</td>\n",
       "      <td>0.656724</td>\n",
       "      <td>0.101825</td>\n",
       "      <td>0.389156</td>\n",
       "      <td>0.154621</td>\n",
       "      <td>0.384049</td>\n",
       "      <td>0.325958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.836500</td>\n",
       "      <td>0.649312</td>\n",
       "      <td>0.097403</td>\n",
       "      <td>0.385955</td>\n",
       "      <td>0.149819</td>\n",
       "      <td>0.382141</td>\n",
       "      <td>0.328511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.778600</td>\n",
       "      <td>0.646154</td>\n",
       "      <td>0.108318</td>\n",
       "      <td>0.387567</td>\n",
       "      <td>0.159687</td>\n",
       "      <td>0.385335</td>\n",
       "      <td>0.334930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.785600</td>\n",
       "      <td>0.636994</td>\n",
       "      <td>0.115581</td>\n",
       "      <td>0.403781</td>\n",
       "      <td>0.172163</td>\n",
       "      <td>0.394638</td>\n",
       "      <td>0.350829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.770200</td>\n",
       "      <td>0.629739</td>\n",
       "      <td>0.110842</td>\n",
       "      <td>0.410395</td>\n",
       "      <td>0.175945</td>\n",
       "      <td>0.399715</td>\n",
       "      <td>0.356543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.773800</td>\n",
       "      <td>0.628085</td>\n",
       "      <td>0.104804</td>\n",
       "      <td>0.399132</td>\n",
       "      <td>0.167280</td>\n",
       "      <td>0.389492</td>\n",
       "      <td>0.352378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.750100</td>\n",
       "      <td>0.628125</td>\n",
       "      <td>0.112952</td>\n",
       "      <td>0.408972</td>\n",
       "      <td>0.170384</td>\n",
       "      <td>0.397224</td>\n",
       "      <td>0.355581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.743100</td>\n",
       "      <td>0.619520</td>\n",
       "      <td>0.102707</td>\n",
       "      <td>0.404894</td>\n",
       "      <td>0.166046</td>\n",
       "      <td>0.394024</td>\n",
       "      <td>0.351058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.709200</td>\n",
       "      <td>0.615554</td>\n",
       "      <td>0.111367</td>\n",
       "      <td>0.411961</td>\n",
       "      <td>0.172817</td>\n",
       "      <td>0.400502</td>\n",
       "      <td>0.364337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.716800</td>\n",
       "      <td>0.615468</td>\n",
       "      <td>0.117244</td>\n",
       "      <td>0.418957</td>\n",
       "      <td>0.176206</td>\n",
       "      <td>0.406571</td>\n",
       "      <td>0.363969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.720700</td>\n",
       "      <td>0.609589</td>\n",
       "      <td>0.118809</td>\n",
       "      <td>0.415633</td>\n",
       "      <td>0.177373</td>\n",
       "      <td>0.407316</td>\n",
       "      <td>0.369632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.725900</td>\n",
       "      <td>0.608189</td>\n",
       "      <td>0.122059</td>\n",
       "      <td>0.419105</td>\n",
       "      <td>0.178609</td>\n",
       "      <td>0.407782</td>\n",
       "      <td>0.368244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.688700</td>\n",
       "      <td>0.604163</td>\n",
       "      <td>0.122731</td>\n",
       "      <td>0.425521</td>\n",
       "      <td>0.179935</td>\n",
       "      <td>0.416233</td>\n",
       "      <td>0.372139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.691700</td>\n",
       "      <td>0.603528</td>\n",
       "      <td>0.123620</td>\n",
       "      <td>0.420918</td>\n",
       "      <td>0.183784</td>\n",
       "      <td>0.411220</td>\n",
       "      <td>0.373995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.705700</td>\n",
       "      <td>0.601638</td>\n",
       "      <td>0.125238</td>\n",
       "      <td>0.423663</td>\n",
       "      <td>0.187189</td>\n",
       "      <td>0.412444</td>\n",
       "      <td>0.375190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.685700</td>\n",
       "      <td>0.598599</td>\n",
       "      <td>0.123597</td>\n",
       "      <td>0.423307</td>\n",
       "      <td>0.189868</td>\n",
       "      <td>0.414908</td>\n",
       "      <td>0.384072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.690300</td>\n",
       "      <td>0.598421</td>\n",
       "      <td>0.127281</td>\n",
       "      <td>0.429126</td>\n",
       "      <td>0.189896</td>\n",
       "      <td>0.417358</td>\n",
       "      <td>0.383872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.681500</td>\n",
       "      <td>0.598000</td>\n",
       "      <td>0.124947</td>\n",
       "      <td>0.427424</td>\n",
       "      <td>0.189799</td>\n",
       "      <td>0.417754</td>\n",
       "      <td>0.384089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.673100</td>\n",
       "      <td>0.597120</td>\n",
       "      <td>0.126013</td>\n",
       "      <td>0.426318</td>\n",
       "      <td>0.189600</td>\n",
       "      <td>0.417617</td>\n",
       "      <td>0.384775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.697700</td>\n",
       "      <td>0.596599</td>\n",
       "      <td>0.132896</td>\n",
       "      <td>0.432317</td>\n",
       "      <td>0.194090</td>\n",
       "      <td>0.423326</td>\n",
       "      <td>0.387550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.660700</td>\n",
       "      <td>0.595381</td>\n",
       "      <td>0.133185</td>\n",
       "      <td>0.431697</td>\n",
       "      <td>0.196608</td>\n",
       "      <td>0.423130</td>\n",
       "      <td>0.389312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.668600</td>\n",
       "      <td>0.594052</td>\n",
       "      <td>0.133244</td>\n",
       "      <td>0.430575</td>\n",
       "      <td>0.191526</td>\n",
       "      <td>0.421031</td>\n",
       "      <td>0.385276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.684200</td>\n",
       "      <td>0.593734</td>\n",
       "      <td>0.137865</td>\n",
       "      <td>0.429648</td>\n",
       "      <td>0.198025</td>\n",
       "      <td>0.420479</td>\n",
       "      <td>0.386644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.681300</td>\n",
       "      <td>0.593189</td>\n",
       "      <td>0.138571</td>\n",
       "      <td>0.436165</td>\n",
       "      <td>0.200775</td>\n",
       "      <td>0.426925</td>\n",
       "      <td>0.392286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.670800</td>\n",
       "      <td>0.593278</td>\n",
       "      <td>0.138113</td>\n",
       "      <td>0.434839</td>\n",
       "      <td>0.199312</td>\n",
       "      <td>0.425867</td>\n",
       "      <td>0.390755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.664300</td>\n",
       "      <td>0.592617</td>\n",
       "      <td>0.137989</td>\n",
       "      <td>0.431627</td>\n",
       "      <td>0.199312</td>\n",
       "      <td>0.422283</td>\n",
       "      <td>0.388942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.662600</td>\n",
       "      <td>0.591460</td>\n",
       "      <td>0.138069</td>\n",
       "      <td>0.433293</td>\n",
       "      <td>0.199180</td>\n",
       "      <td>0.423898</td>\n",
       "      <td>0.390066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.662600</td>\n",
       "      <td>0.591313</td>\n",
       "      <td>0.138069</td>\n",
       "      <td>0.432904</td>\n",
       "      <td>0.199180</td>\n",
       "      <td>0.423769</td>\n",
       "      <td>0.391223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.656500</td>\n",
       "      <td>0.591473</td>\n",
       "      <td>0.138069</td>\n",
       "      <td>0.432904</td>\n",
       "      <td>0.199180</td>\n",
       "      <td>0.423769</td>\n",
       "      <td>0.391223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.649800</td>\n",
       "      <td>0.591461</td>\n",
       "      <td>0.138069</td>\n",
       "      <td>0.432904</td>\n",
       "      <td>0.199180</td>\n",
       "      <td>0.423769</td>\n",
       "      <td>0.391223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.656300</td>\n",
       "      <td>0.591441</td>\n",
       "      <td>0.138069</td>\n",
       "      <td>0.432904</td>\n",
       "      <td>0.199180</td>\n",
       "      <td>0.423769</td>\n",
       "      <td>0.391223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5913133025169373, 'eval_bleu': 0.13806879921087598, 'eval_rouge1': 0.432904255543699, 'eval_rouge2': 0.19917965151874492, 'eval_rougeL': 0.4237685060228919, 'eval_meteor': 0.3912234163144579, 'eval_runtime': 0.9983, 'eval_samples_per_second': 60.105, 'eval_steps_per_second': 8.014, 'epoch': 40.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:00<00:00, 67414.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final predictions with inputs saved to predicted_clarif_T5TINY_TOP5DOC.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/bleu</td><td>▂▁▂▄▄▄▅▅▆▆▆▆▇▇▆▇▆▇▇▇▇▇▇▇▇▇▇▇████████████</td></tr><tr><td>eval/loss</td><td>█▆▅▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/meteor</td><td>▁▁▃▄▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇██████████████████</td></tr><tr><td>eval/rouge1</td><td>▁▁▃▄▅▅▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇███████████████████</td></tr><tr><td>eval/rouge2</td><td>▁▁▃▄▅▄▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇████████████████</td></tr><tr><td>eval/rougeL</td><td>▁▁▃▄▅▅▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇███████████████████</td></tr><tr><td>eval/runtime</td><td>▂▂█▁▁▁▂▁▂▅▁▁▁▁▁▁▁▁▂▂▂▂▁▁▁▂▂▁▂▂▂▂▂▂▁▂▂▂▁▂</td></tr><tr><td>eval/samples_per_second</td><td>▇▇▁▇▇▇▇▇▇▃▇███████▆▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇</td></tr><tr><td>eval/steps_per_second</td><td>▇▇▁▇▇▇▇▇▇▃▇███████▆▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇</td></tr><tr><td>test/bleu</td><td>▁</td></tr><tr><td>test/loss</td><td>▁</td></tr><tr><td>test/meteor</td><td>▁</td></tr><tr><td>test/rouge1</td><td>▁</td></tr><tr><td>test/rouge2</td><td>▁</td></tr><tr><td>test/rougeL</td><td>▁</td></tr><tr><td>test/runtime</td><td>▁</td></tr><tr><td>test/samples_per_second</td><td>▁</td></tr><tr><td>test/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/grad_norm</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▆▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/bleu</td><td>0.13807</td></tr><tr><td>eval/loss</td><td>0.59131</td></tr><tr><td>eval/meteor</td><td>0.39122</td></tr><tr><td>eval/rouge1</td><td>0.4329</td></tr><tr><td>eval/rouge2</td><td>0.19918</td></tr><tr><td>eval/rougeL</td><td>0.42377</td></tr><tr><td>eval/runtime</td><td>0.9983</td></tr><tr><td>eval/samples_per_second</td><td>60.105</td></tr><tr><td>eval/steps_per_second</td><td>8.014</td></tr><tr><td>test/bleu</td><td>0.13807</td></tr><tr><td>test/loss</td><td>0.59131</td></tr><tr><td>test/meteor</td><td>0.39122</td></tr><tr><td>test/rouge1</td><td>0.4329</td></tr><tr><td>test/rouge2</td><td>0.19918</td></tr><tr><td>test/rougeL</td><td>0.42377</td></tr><tr><td>test/runtime</td><td>0.9819</td></tr><tr><td>test/samples_per_second</td><td>61.107</td></tr><tr><td>test/steps_per_second</td><td>8.148</td></tr><tr><td>total_flos</td><td>486520368660480.0</td></tr><tr><td>train/epoch</td><td>40</td></tr><tr><td>train/global_step</td><td>2720</td></tr><tr><td>train/grad_norm</td><td>0.91794</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6563</td></tr><tr><td>train_loss</td><td>1.09959</td></tr><tr><td>train_runtime</td><td>1032.634</td></tr><tr><td>train_samples_per_second</td><td>20.879</td></tr><tr><td>train_steps_per_second</td><td>2.634</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">T5 Tiny + TO5 DOCS</strong> at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/a0qk063s' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/a0qk063s</a><br> View project at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250504_220833-a0qk063s/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"T5-Clarifications\",  \n",
    "    name=\"T5 Tiny + TO5 DOCS\",\n",
    "    reinit=True                     \n",
    ")\n",
    "train(\"google/t5-efficient-tiny\", \"predicted_clarif_T5TINY_TOP5DOC.jsonl\",tokenizer, tokenized_train_dataset, tokenized_eval_dataset,40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbca1b80-7580-4ff6-a573-1c43e81e3622",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# SEQ2SEQ T5 SMALL, + TOP5DOCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0319809a-83ba-48db-a569-c367360769c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c6d9dd011340769ffe2322ca6086e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/539 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f977735db3d44a093a7290f6fbaa2d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': '[QUERY] Find information about solar panels and companies that manufacture them.\\n[DOCUMENTS]\\n[DOC 1] Cooler Planet\\\\nCooler Planet\\\\nCooler Planet’s Blog\\\\nSkip to content\\\\n« Older posts\\\\nNow is a Great Time to Install Solar\\xa0Power\\\\nJanuary 14, 2009 – 11:41 am\\\\nWhat, you say? The economy is in a shambles, two of my neighbors have been laid off and my heating bill is half again as much as it was last winter. How can this be a good time to install solar power?\\\\nLet\\\\u0019s take it step by step. The losses on Wall Street may impact the economy, but they are not the economy. Almost all the factories, farms, growers, and retailers who started 2008 are still around. More important, the cows, chickens, pigs, wheat and corn fields, cotton fields and forests \\\\u0013 the goods that make up the\\\\nreal\\\\neconomy \\\\u0013 are still growing.\\\\nYes, people are losing jobs in the banking and retail sectors, but these job losses are exacerbated by cutbacks in spending, leading to more losses. I\\\\u0019m not suggesting you buy frivolous items like plastic shoes from China, but if you want to encourage the manufacturing sector, buy American. Buy the one thing that is still truly\\\\nmade in America\\\\n; solar power.\\\\nYour heating bill is likely from 25 to 40 percent higher. This is because the utilities lost money over the summer as a result of consumers cutting back on electricity. You can make them see the error of their ways \\\\u0013 and the futility of raising prices to cover losses \\\\u0013 by installing solar power and generating some, or all, of your electricity needs yourself.\\\\nLet\\\\u0019s take a look at other incentives, beginning with The Clean Energy Tax Stimulus Act of 2008 (amended as part of the recent bailout bill, passed Oct. 1, 2008):\\\\nTaxpayers can \\\\tclaim a renewable energy credit for 10 years, beginning on the date \\\\tthe qualified facility is placed in service. In order to qualify, \\\\tfacilities must be placed in service by December 31, 2009 (the \\\\toriginal date was 2008). The credit is equal to 30 percent of \\\\tqualifying expenditures, and the former cap of $2,000 for each \\\\tsystem was also repealed as part of the bailout package. This 30 \\\\tpercent credit is not a tax deduction, but an actual credit \\\\u0013 \\\\tit comes right off the top of your income.\\\\nUnder current law, \\\\ttaxpayers can also claim a 30 percent business energy credit \\\\tfor purchases of qualified solar energy property. Credits apply to \\\\tperiods after December 31, 2005 and before January 1, 2008.\\\\nWhat this represents to you, the average American, is a gift from the very Senate that gave those silly bankers $700 billion. The least you can do is take advantage of it, since it\\\\u0019s probably the only bailout you, the consumer, are likely to see.\\\\nIf the federal incentive isn\\\\u0019t enough to whet your appetite, check out the database of state incentives. For example, in Minnesota solar installations escape sales tax and qualify for up to $22,500 in incentives (at a rate of $2.25 per watt) through Xcel Energy\\\\u0019s Renewable Development Fund. The incentives are available until the fund is exhausted.\\\\nIn San Francisco, California, solar installations qualify for up to a $6,000 rebate. That\\\\u0019s cash in hand, so to speak. In Illinois, you can get up to $10,000 in incentives (for a $50,000 system). You can also check out Cooler Planet\\\\u0019s blog on solar power state ratings.\\\\nSo what are you waiting for? As the economic downturn impacts state budgets and funds are used up, the opportunities to benefit from a solar power system can only grow smaller. To find a qualified installer and purchase your solar system, visit our resource page.\\\\nBy lavendula13\\\\n|\\\\nPosted in Barak Obama,\\\\nenergy efficiency,\\\\nenergy policy,\\\\ngreen living,\\\\nrenewable energy,\\\\nsolar energy,\\\\nsolar information,\\\\nsolar power\\\\n|\\\\nTagged solar energy, solar power, solar panels, solar, solar cells, solar rebates, clean energy, economy, solar incentives, solar installation, downturn\\\\n|\\\\nComments (0)\\\\nThe difference between Do It Yourself and Professional\\xa0Installation\\\\nJanuary 14, 2009 – 11:21 am\\\\nI am one of the biggest do-it-yourself advocates on the planet. Using books, plans, the right tools, having my own DIY business, backed up withschooling and apprenticeship certifications, I am confident in my own ability to build and make just about anything. But when it comes to solar panel installation, I\\\\u0019d much rather let the pros in the business handle it all. And here\\\\u0019s why.\\\\nSolar cells and panels are the cutting edge in sustainable energy usage. They need to be handled a certain way and put together in certain ways that will allow them to work in the most efficient way possible. This type of knowledge does not just come from books and classes, this type of knowledge comes from actual hands on experience. The pros in this business are absolutely confident in their ability to get the hardware to the job and get it installed 100% correctly, guaranteed.\\\\nThere is no DIY person that can give 100% assurance that an application of cells or panels is going to work like it is supposed to work. Even with all of my knowledge, I couldn\\\\u0019t do that either. Sure, you might save money and attempt to get an intricate solar panel display installed yourself, but you also may end of with a solar roof ornament that does nothing except take up space and look impressive.\\\\nThe knowledge and the expertise, to me anyway, far outweigh any savings there may be in trying to rig up a set of functional solar panels. I would much rather pay the pros to do it right the first time, then to erect a solar array myself and have to call the pros in later to have them fix the mess I left behind.\\\\nBy daleythegreenguy\\\\n|\\\\nPosted in Uncategorized\\\\n|\\\\nTagged DIY, do it yourself, renewable energy, solar, solar cells, solar contractor, solar energy, solar installer, solar panels, solar power\\\\n|\\\\nComments (0)\\\\nMaking Solar\\xa0Accessible\\\\nJanuary 13, 2009 – 2:06 pm\\\\nSolar energy is likely the largest, single answer to this nation\\\\u0019s dependence on fossil fuels. With enough solar energy, U.S. citizens can ignore the threat of\\xa0Peak Oil, the more visible threat of\\xa0oil embargoesfrom Mideast countries, the rising cost of fossil fuels and the air pollution that is making large cities virtually uninhabitable.\\\\nGetting to that goal isn\\\\u0019t easy. There are a lot of solar incentives out there, from utility companies sponsoring solar installation to local, state and federal governments offering funding or tax rebates to homeowners installing a photovoltaic (PV) system. Unfortunately, these incentives are all over the board, vary from state to state and even region to region depending on the utility, are rarely publicly advertised to make homeowners aware of their existence, and change from one year to the next depending on the whims of Congress.\\\\nMost homeowners, when presented with the 10,000 hoops required to get some kind of incentive package, shrug their shoulders and walk away. Life is complicated enough. If president-elect Barack Obama truly wants to meet his\\xa0eco-friendly energy goals\\xa0\\\\u0013 five million\\\\ngreen collar\\\\njobs, negative oil imports in by 2018, and 10 percent of energy coming from renewable sources like solar \\\\u0013 he will first need to translate this mishmash of incentives into a cohesive, comprehensive and comprehensible national policy.\\\\nWhat if, for example, instead of searching\\xa0DSIRE\\\\u0019s\\xa0(Database of State Incentives for Renewable Energy) map for your state, then searching the more than 100 entries for a program that matches your qualifications and expectations, you could simply rely on the fact that state, regional and utility incentives all offered a specific program?\\\\nBetter yet, what if an agency of the federal government, like the U.S. Department of Energy, took over all incentives and offered a flat rebate not subject to change every time Congress was in session? Can you imagine the numbers of individuals who \\\\u0013 currently stymied by a maze of regulations and stipulations \\\\u0013 would flock to install solar energy panels? I can, and it would give the solar energy industry a boost not seen since Henry Ford first introduced an affordable automobile.\\\\nYou can make this happen by writing your representative and demanding a uniform, timeless, renewable energy incentive program. Better yet, join a social networking site and start your own petition, or contact renewable energy advocate sites (the American Solar Energy Society is a good start) and ask them to start a petition asking Obama and his transition team to develop such a program.\\\\nIf, on the other hand, you have done your due diligence and know what kinds of solar incentives are available to you, go to Cooler Planet\\\\u0019s\\xa0preferred provider page\\xa0and get cracking!\\\\nThe future of solar energy is you, and tomorrow is already here.\\\\nBy lavendula13\\\\n|\\\\nPosted in energy efficiency,\\\\nenergy policy,\\\\nrenewable energy,\\\\nsolar energy,\\\\nsolar power\\\\n|\\\\nTagged cheap solar power, eco friendly, energy, green energy, solar, solar cells, solar energy, solar power\\\\n|\\\\nComments (0)\\\\nSolar Energy Affordable\\xa0Program\\\\nJanuary 13, 2009 – 1:41 pm\\\\nI just read about One Block off the Grid (1BOG), a San Francisco consumer advocacy group for residential renewable energy, has joined forces with Solar City in developing a new community purchase program to offer special pricing and affordable financing options on residential solar systems to all the homeowners in the San Francisco Bay area.\\\\nParticipants in the 1BOG/ Solar City Program are eligible to receive 25% off the retail cash price of a home solar system and a comprehensive design and installation package that includes monitoring and energy audit services. Participants who choose to finance the installation through Solar City’s Solar Lease option are also eligible to receive their first three months free if they sign up through 1BOG before January 31, 2009.\\\\nJanuary 31, 2009 might seem like a short notice, but I bet you my bottom dollar that this program will get extended if enough homeowners in the SF Bay area come forward and sign up for the renewable energy program.\\\\nAlthough some claim solar power is not affordable yet, I beg to differ with programs like this one forming, and we mustn’t forget about the tax credits.\\\\nIt is affordable programs like this one that puts a smile on my face.\\xa0 The future of solar energy looks mighty bright to me.\\\\nOnce the homeowners use solar power, with consumer programs like this one, they become more knowledgeable about the returns on investment.\\xa0 The news will spread concerning the benefits, and then even more homeowners will join the solar movement.\\\\nBy kennamc\\\\n|\\\\nPosted in California,\\\\ngreen living,\\\\nrenewable energy,\\\\nsolar energy,\\\\nsolar information,\\\\nsolar power\\\\n|\\\\nTagged 1BOG, affordable solar, California, San Francisco, solar financing, solar power, solar program, solar system\\\\n|\\\\nComments (0)\\\\nSolar Rebates Running\\xa0Dry\\\\nJanuary 13, 2009 – 1:10 pm\\\\nA recent New York Times article points out that Connecticut\\\\u0019s solar rebate funds are running dry. As it happens, the program has been so successful, that there are no more funds to go around. According to the article, since 2005 more than $8.5 million has been given out for 815 residential solar projects and 127 business and government projects. A whopping 300 percent growth spike in the purchase of residential systems in the latter half of 2008 essentially drained all the available funding.\\\\nWe can find echoes of this sticky situation in Florida and in New Jersey. During the second half of 2008 over 3,000 Floridians applied for solar rebates from the state; while 1,300 have been approved, the remaining 1,700 are still waiting in the wings. The problem is, there is no money left. Again, we see the same thing happening in New Jersey, where a surfeit of applications has outpaced the rebate money by far.\\\\nThis is somewhat of a false dilemma as it signals something more profound than legislative quagmires. What\\\\u0019s truly of standout quality here is that Americans are more than ready for solar. We no longer want to be a nation of oil guzzlers. Applying for solar rebates is just one of the first steps in weaning ourselves from oil, which can get pretty pricey on more than one level.\\\\nNow the obvious drawback of course, is the cost. Rebates are effective, but limited. State support can only go so far. But, let\\\\u0019s take a step back. At the moment, solar energy panels are indeed expensive, hence the rebates. Nevertheless, the predictions look quite rosy. We may see a price drop of as much as 43% on the cost of solar silicon\\\\u0014the stuff used to make solar panels\\\\u0014by the end of the year. In the next five years, we could see even more cuts, with silicon prices falling by as much as 67%. In particularly sunny regions, the cost of solar energy will be on par with electricity, making going solar very affordable. So, although government incentives lag, the future of solar looks sunny.\\\\nBy nagalabs\\\\n|\\\\nPosted in Connecticut,\\\\nFlorida,\\\\nenergy efficiency,\\\\nenergy policy,\\\\ngreen living,\\\\nsolar energy,\\\\nsolar power\\\\n|\\\\nTagged solar energy, solar power, solar panels, solar cells, solar rebates, solar system, solar incentives, energy rebates, legislature, conneticuit, Flordia, New Jersey\\\\n|\\\\nComments (0)\\\\nPlastic Solar Cell Efficiency Gets a\\xa0Bump\\\\nJanuary 13, 2009 – 12:39 pm\\\\nCurrently, most solar technologies are delivered via silicon crystals, whose highly refined nature and relative scarcity make them expensive components in the solar process. This makes the end-product equally expensive, and cost is one of the factors limiting the use of solar panels to generate energy.\\\\nPlastic, or polymer, solar cells are relative newcomers to solar technology, but their potential advantages \\\\u0013 lower cost, lighter weight and greater flexibility \\\\u0013 promise to sweep the solar industry, once sourcing and manufacturing are refined.\\\\nPolymers are plastic-type substances, usually made from petroleum. Organic plastics, typically represented by such products as amber and shellac (or tree sap), may soon be available from cellulose, or food products like corn, making organic polymer solar technology not only inexpensive but environmentally friendly; i.e., disposable.\\\\nKonarka Technologies, Inc., recently announced that their flagship product, Power Plastic®, was rated at 6 percent efficiency. This may not seem like much, but solar panels currently in use rarely boast more than 12- to 14-percent efficiency, and polymer cell technology is still in its infancy.\\\\nFor Konarka to achieve 6 percent with its flexible organic based photovoltaic (PV) solar is truly an important milestone, as co-developer Dr. Alan Heeger of the University of California (Santa Barbara) notes.\\\\nThis progress gives us confidence that we are on a technology pathway toward the vision of high efficiency, low cost \\\\u0018plastic\\\\u0019 solar cells.\\\\nHeeger, one of the co-founders of Konarka, received the Nobel Prize in Chemistry in 2000. He and his colleagues at UCSB are currently focused on issues related to the fundamental electronic structure of polymer solar cells, and hopes in the near future to bump that efficiency rating to a full 10 percent, which would make it highly competitive with silicon-based solar.\\\\nAnother discovery, from the UCLA Henry Samueli School of Engineering and Applied Science, promises to improve polymer solar material by substituting a silicon atom (or a crystalline) for a carbon atom in the backbone of the polymer. Eventually, says UCLA researcher and co-author Hsiang-Yu Chen, solar cells may be as thin as paper, attachable to any surface, and colored to match different applications.\\\\nImagine hanging a solar panel alongside your deck or patio that looks like a Van Gogh!\\\\nBy lavendula13\\\\n|\\\\nPosted in climate change,\\\\nenergy efficiency,\\\\nenergy policy,\\\\nrenewable energy,\\\\nsolar energy,\\\\nsolar power\\\\n|\\\\nTagged photovoltaic, solar energy, solar power, solar panels, solar, renewable energy, green energy, solar cells, Konarka Technologies\\\\n|\\\\nComments (0)\\\\nSolar Number Two in Energy Source\\xa0Ratings\\\\nJanuary 9, 2009 – 12:33 pm\\\\nAccording to professor Mark Z. Jacobson, who teaches civil and environmental engineering at Stanford, solar technology is one of the best ways for America to improve energy security, mitigate climate change and reduce the amount of air pollution linked to disease and death.\\\\nOf course, we at Cooler Planet already knew this. What struck us about Jacobson\\\\u0019s study was that it received no funding from any government agency, energy company or public/private interest group. This paradigm of unbiased reporting on alternative energy technologies leads us to conclude that Jacobson\\\\u0019s study is perhaps the only one to come down the pike in a decade that reflects the energy situation in America as it truly is.\\\\nThis quantitative, scientific study, which appraises proposed solutions not only for their ability to provide energy but also their impacts on climate change, energy security, space requirements, ecological health, water pollution, reliability and sustainability, concludes that the best paths are, by order of preference: wind, concentrated solar power (CSP), geothermal power, tidal power, solar photovoltaics (PV), wave power and hydroelectric power.\\\\nJacobson also notes that the energy sources getting the most attention (clean coal, nuclear, bioethanol) are also the ones contributing either the most, or the most dangerous, pollution. The paper, which will be published in the next issue of Energy and Environmental Science, notes that putting people to work building solar, wind and geothermal plants would not only create jobs to heal the economy but would reduce health care costs and provide better growing conditions for food crops.\\\\nThough Jacobson favors wind, he admits that wind alone isn\\\\u0019t the solution. “It’s got to be a package deal, with energy also being produced by other sources such as solar, tidal, wave and geothermal power.”\\\\nPresident-elect Barack Obama has announced a plan to save or create 2.5 million jobs in the next few years, many of which would come from implementing green technologies. We at Cooler Planet think this is a great idea, but opponents insist the initiative will cost more than it generates in wages or profits.\\\\nWe at Cooler Planet believe this is misguided thinking \\\\u0013 like equating the cost of a loaf of bread to the pound of wheat grown to make it, instead of the life saved by eating it. Sometimes economists get so wrapped up in their cost/benefit analyses they lose sight of the big picture. By the same token, investing heavily in alternative technologies would bring new developments, drive down costs and make it easier for consumers, pinched by the recession, to commit to\\\\ngreen\\\\nalternatives.\\\\nIn addition, using up finite resources to generate energy while neglecting infinite resources like sun and wind may leave us unexpectedly exposed to an energy nightmare if gas and coal reserves are being seriously overestimated, as more and more experts say is happening.\\\\nBy lavendula13\\\\n|\\\\nPosted in Presidential candidates,\\\\nenergy efficiency,\\\\nenergy policy,\\\\nrenewable energy,\\\\nsolar energy,\\\\nsolar power\\\\n|\\\\nTagged Barrack, brarrack obama, geothermal power, green energy, green jobs, hydroelectric power, Obama, solar, solar cells, solar energy, solar photovoltaics, solar power, tidal power, wave power\\\\n|\\\\nComments (0)\\\\nAerosol and Solar\\xa0Mix?\\\\nJanuary 8, 2009 – 10:59 am\\\\nTracking advances in solar cell technology, I’ve stumbled across another landmark efficiency gain.\\\\nThis one, obtained by switching from conventional screen printing to a non-contact aerosol jet printer, has upped efficiencies by 2 percent on thin-film solar cells.\\\\nThin-film solar cells (TFSC) are defined as solar cells made by depositing one or more thin layers (nanometers to micrometers) of photovoltaic material on a substrate. These thin-film cells are classed as: amorphous silicon, cadmium telluride, copper indium gallium selenide (CIGS), polysilicon, dye-sensitized solar cells, or organic (plastic, or polymer) solar cells \\\\u0013 the latter three still in development stages.\\\\nPrinting is a deposition method, and the use of aerosol jet printing could raise the copper indium gallium selenide efficiency \\\\u0013 already reported at 9.5 percent (with reports from NREL in excess of 19 percent) \\\\u0013 to a value competitive with silicone-based solar at about half the cost to manufacture.\\\\nThe advance, coming from the Fraunhofer Institute\\\\u0019s Solar Energy Systems in Germany, reports that using an Optomec printer enabled thinner strata deposition of metallic, semiconducting and insulating inks via nanoscale pigments developed by Applied Nanotech Inc. enabled efficiences in excess of 20 percent, as compared to previous figures of 16 to 18 percent.\\\\nFraunhofer also used the printer to fabricate front-side metallization lines on otherwise conventional solar cells, which enabled it to shrink the metallization area and boost efficiency by reducing shading losses.\\\\nScreen-printed solar cells can be manufactured faster than ink-jet printing methods, and the Optomec\\\\u0019s 40-nozzle head can print a solar cell in under 3 seconds. What this means to you, the consumer, is a future of truly affordable solar. What it means to the environment is of inestimable value.\\\\nOf course, waiting to save a few bucks doesn\\\\u0019t make a lot of sense in terms of the environment. News coming out of the United Nations Climate Change Conference in PoznaD, Poland (December 5 to 12) indicates that climate change is not only upon us, but likely to reach that critical\\\\ntipping point\\\\nin as little as a decade.\\\\nYou can do your part now by consulting Cooler Planet\\\\u0019s resource page and contacting a reputable, knowledgeable solar energy contractor. If we all wait for cheaper solar, we may be buying the horse after the proverbial barn door has been irrevocably broken.\\\\nBy lavendula13\\\\n|\\\\nPosted in energy efficiency,\\\\nenergy policy,\\\\nrenewable energy,\\\\nsolar energy,\\\\nsolar power\\\\n|\\\\nTagged solar energy, solar power, solar panel, solar, Cooler Planet, alternative energy, green energy, solar cells, green, solar system, aerosol printer, aerosol jet printer\\\\n|\\\\nComments (0)\\\\nSolar LA, I love this\\xa0Plan!!!\\\\nJanuary 7, 2009 – 3:47 pm\\\\nI just read about a project that Los Angeles Mayor Antonio Villaraigosa, City Council President Eric Garcetti, Councilmember Jan Perry and the Los Angeles Department of Water and Power are sponsoring called Solar LA. Los Angeles is going green by sponsoring the installation of a 1.3 gigawatts of solar power. That’s a lot.\\\\nFrom what I read about Solar LA, it is the largest solar project undertaken by any single city in the world.\\xa0 Solar LA will cut Los Angeles’ carbon emissions and give the City a steady supply of clean, renewable energy to power its future. The whole project will be done by 2020 with installation being done in increments through the years with the final phase installed by 2020.\\\\nSolar LA also includes Expand Residential Program, using $313 million in State funds for solar projects that offer rebates programs to encourage residents to install solar panels on their roofs. I want to tap into this program, big time. Plus, there will be low interest loans while low income communities will be provide with free systems. All walks of life will benefit from LA’s solar power plan.\\\\nThe program also includes solar systems on rooftops, reservoirs and parking lots on City-owned property. A total of 400 MW solar systems will be installed by 2014. That is more than the total solar systems operating in California now.\\\\nThe plan includes research and development, manufacturing and warehouse operations. I am sure new jobs will be created in the field of green-tech.\\\\nThe neighboring Mojave Desert will be a solar resource for utility-scale solar power projects developed by third-party solar companies, which LA plans to purchase after 8 years of operation.\\\\nAs a consumer I am very excited because this will set a green-trend for other cities to follow. If you think about it, having solar power in a city like LA, solves the energy and public health problems facing cities throughout the country. As a resident I’d like to know more about this program. How I can tap into using solar power?\\\\nBy kennamc\\\\n|\\\\nPosted in California,\\\\ncarbon emissions,\\\\ngreen living,\\\\nsolar energy,\\\\nsolar information,\\\\nsolar power\\\\n|\\\\nTagged alternative energy, green, green energy, green power, Los Angeles Solar, solar cell, solar cells, solar energy, solar incentives, solar LA, solar power, solar program, solar rebates\\\\n|\\\\nComments (0)\\\\nSolar Cells here and Now, Ready for Prime\\xa0Time\\\\nJanuary 6, 2009 – 10:27 am\\\\nIf you listen closely, you can almost here the music and a famous announcer saying, Da da da da, Solar Cells, Ready for Prime Time! And with the coming of the new year, it might be the time to turn those dreams of solar cells into reality.\\\\nFirst of all, in an article I just wrote a couple of weeks ago, there might be a buyers market for solar cells next year. Some of the major solar cell manufacturers may cut prices on solar panels, because solar cell growth is not as robust as they want it. (the solar cell industry only grew 25% in 2008 compared to 35% and 40% growth in previous years)\\\\nSolar cell efficiency is at the highest it\\\\u0019s ever been, while overall prices have not gone up. That\\\\u0019s one of the reasons why growth has been so phenomenal, since you get more bang for the buck, or as they say in the business, more kilowatts for the dollar, at the same price.\\\\nMore states will be giving tax benefits and rebates in the next year, mainly due to the new government administration set to take office in January. Our government is set to pass major incentives for anyone who wants to use alternative energy, and that means easier access and much more cost effectiveness for you.\\\\nThe best part is that, only in the past few years have appliance manufacturers gotten on the alternative energy wagon. Almost every appliance you can think of, from refrigerators to microwave ovens, have been designed to run on 12 volts of power. And we are not talking about house trailer sized models here, I mean full sized household appliances that can run directly off of the energy stored from solar cells.\\\\nSo there you have it, with all these advances in solar cells, this might just be the right time to become a part of this form of alternative energy. Because here and now, solar cells are ready for prime time!\\\\nBy daleythegreenguy\\\\n|\\\\nPosted in climate change,\\\\nenergy efficiency,\\\\nenergy policy,\\\\nrenewable energy,\\\\nsolar energy,\\\\nsolar power\\\\n|\\\\nTagged solar power, solar, renewable energy, alternative energy, green energy, solar cells, solar rebates, solar growth, solar industry, solar efficiency, solar incentives\\\\n|\\\\nComments (0)\\\\n« Older posts\\\\nJanuary 2009\\\\nS\\\\nM\\\\nT\\\\nW\\\\nT\\\\nF\\\\nS\\\\n« Dec\\\\n123\\\\n45678910\\\\n11<\\n[DOC 2] What is a Solar Panel?\\\\nWhat is a Solar Panel?A solar panel is a device that collects photons of sunlight, which are very small packets of electromagnetic radiation energy, and converts them into electrical current that can be used to power electrical loads.\\\\nATTENTION CUSTOMERS!\\\\nEffective IMMEDIATELY! Qualified commercial and residential renewable energy systems will be eligible for a 30% tax credit on solar and renewable energy investments.More Details »\\\\nHome\\\\nSolar Panels\\\\nControllers\\\\nDC to AC Inverters\\\\nBatteries\\\\nPV Mounting\\\\nWind Power\\\\nWiring & Cable\\\\nStore\\\\nSpecial\\\\nThank you for visiting MrSolar.com!Hrs: M-F 8am - 7pm ET • Ph: 888-680-2427\\xa0\\xa0\\xa0\\xa0HOME | STORE | CONTACT | BOOKMARK\\\\nBy Ron Curtis\\\\nAbout Solar Panels\\\\nSolar energy begins with the sun. Solar panels, also known as photovoltaics, are used to convert light from the sun, which is composed of particles of energy called \\\\\"photons\\\\\", into electricity that can be used to power elecrical loads. Light from the sun is a renewable energy resource which provides clean energy, produced by solar panels.\\\\nSolar panels can be used for a wide variety of applications including remote power systems for cabins, telecommunications equipment, remote sensing, and of course for the production of electricity by residential and commercial solar panel systems.\\\\nOn this page, we will discuss the history, technology, and benefits of solar panels. We will learn how solar panels work, how solar panels are made, where you can buy solar panels, and how solar panels create electricity.\\\\nA Short History of Solar Panels\\\\nThe development of solar energy goes back more than 100 years. In the early days, solar power was used primarily for the production of steam which could be used to drive machinery. But it wasn\\'t until the discovery of the \\\\\"photovoltaic effect\\\\\" by Henri Becquerel that would allow the conversion of sunlight solar electric energy. Becquerel\\'s discovery then led to the invention in 1893 by Charles Fritts of the first genuine solar cell which was formed by coating sheets of selenium with a thin layer of gold. And from this humble beginning would arise the device we know today as the solar panel.\\\\nRussel Ohl, an American inventor on the payroll of Bell Laboratories, patented the world\\'s first silicon solar cell in 1941. Ohl\\'s invention led to the production of the first solar panel in 1954 by the same company. The new-fangled solar panels found their first mainstream use in space satellites. For most people, the first solar panel in their life was probably embedded in their new calculator - circa the 1970s!\\\\nToday, solar panels and complete solar panel systems are used to power a wide variety of applications. Yes, solar panels in the form of solar cells are still being used in calculators. However, they are also being used to provide solar power to entire homes and commercial buildings, such as Google\\'s headquarters in California.\\\\nHow Do Solar Panels Work?\\\\nSolar panels collect clean renewable energy in the form of sunlight and convert that light into electricity which can then be used to provide power for electrical loads. Solar panels are comprised of several individual solar cells which are themselves composed of layers of silicon, phosphorous (which provides the negative charge), and boron (which provides the positive charge). Solar panels absorb the photons and in doing so initiate an electric current. The resulting energy generated from photons striking the surface of the solar panel allows electrons to be knocked out of their atomic orbits and released into the electric field generated by the solar cells which then pull these free electrons into a directional current This entire process is known as the Photovoltaic Effect.\\\\nAn average home has more than enough roof area for the necessary number of solar panels to produce enough solar electricrity to supply all of its power needs. Assisted by an inverter, a device that converts the direct current (or DC current), generated by a solar panel into alternating current (or AC current), solar panel arrays can be sized to meet the most demanding electrical load requirements. The AC current can be used to power loads in your home or commercial building, your recreational vehicle or your boat (RV/Marine Solar Panels), your remote cabin or home, and remote traffic controls, telecommunications equipment, oil and gas flow monitoring, RTU, SCADA, and much more.\\\\nThe Benefits of Solar Panels\\\\nUsing solar panels is a very practical way to produce electricity for many applications. The obvious would have to be off-grid living. Living off-grid means living in a location that is not serviced by the main electric utility grid. Remote homes and cabins benefit nicely from solar power systems. No longer is it necessary to pay huge fees for the installation of electric utility poles and cabling from the nearest main grid access point. A solar electric system is potentially less expensive and can provide power for upwards of three decades if properly maintained.\\\\nBesides the fact that solar panels make it possible to live off-grid, perhaps the greatest benefit that you would enjoy from the use of solar power is that it is both a clean and a renewable source of energy. With the advent of global climate change, it has become more important that we do whatever we can to reduce the pressure on our atmosphere from the emission of greenhouse gases. Solar panels have no moving parts and require little maintenance. They are ruggedly built and last for decades when porperly maintained.\\\\nLast, but not least, of the benefits of solar panels and solar power is that, once a system has paid for its initial installation costs, the electricity it produces for the remainder of the system\\'s lifespan, which could be as much as 15-20 years depending on the quality of the system, is absolutely free! For grid-tie solar power system owners, the benefits begin from the moment the system comes online, potentially eliminating monthy electric bills or, and this is the best part, actually earning the system\\'s owner additional income from the electric company. How? If you use less power than your solar electric system produces, that excess power can be sold, sometimes at a premium, to your electric utility company!\\\\nThere are many other applications and benefits of using solar panels to generate your electricity needs - too many to list here. But as you browse our website, you\\'ll gain a good general knowledge of just how versatile and convenient solar power can be.\\\\nWhere can I buy solar panels?\\\\nWell, right here on this website, of course!\\\\nOur solar panel brands include the most respected manufacturers in the solar panel business. These brands include such names as BP Solar, General Electric, and Sharp, among others. We feature only the highest quality solar panels from manufacturers with a proven track record in solar panel technology. With over 30 years in the solar panel business, you can be sure that at MrSolar.com, we know solar panels!\\\\nHome\\\\nStorefront\\\\nLogin\\\\nContact Us\\\\nSearch\\\\nProduct List\\\\nSite Map\\\\nLink to Us!\\\\nSolar Power Products\\\\nSolar Panels\\\\nControllers\\\\nDC to AC Inverters\\\\nBatteries\\\\nWind Power\\\\nShurflo Pumps\\\\nWiring & Cable\\\\nPV Mounting Hardware\\\\nPre-Packaged Systems\\\\nGrid-Tied Systems\\\\nOff Grid Systems\\\\n10-160W Pole Mount Kits\\\\nPower Ready Systems\\\\nRV & Marine Kits\\\\nSolar Sign Lighting\\\\nPortable Solar Power\\\\nSunwize Systems\\\\nRelevant Links\\\\nQuick Contact\\\\nSales:\\\\nsales@mrsolar.com\\\\nCustomer Support:\\\\nservice@mrsolar.com\\\\nWebmaster:\\\\nsiteadmin@mrsolar.com\\\\nQuick Nav\\\\nHelp & Sizing\\\\nUniRac Pricing Sheet\\\\nOff-Grid Calculator\\\\nSolar Power FAQs\\\\nSolar System Sizing\\\\nRV System Worksheet\\\\nIndustrial Sizing\\\\nTilt Angle Chart\\\\nArray Orientation\\\\nSolar Insolation Maps\\\\nSolar Power Glossary\\\\nSite Assessment\\\\nDesign Assistance\\\\nGovernment Incentives\\\\nBattery Technical Manual\\\\nRelated Stuff\\\\nHow It\\'s Made\\\\nWhat\\'s a Solar Panel?\\\\nPhoton Lounge\\\\nRV Solar Panel Kits\\\\nWhat is solar power?\\\\nFlexible Solar Panels\\\\nPhotovoltaics (PV)\\\\nSolar Panels |\\\\nControllers |\\\\nInverters |\\\\nBatteries |\\\\nWind Power |\\\\nSolar Pumps |\\\\nWiring & Cable |\\\\nPV Mounting Hardware\\\\nWe Offer The Highest Quality Solar Power Product Components Anywhere.\\\\nWelcome to MrSolar.com, the website of Online Solar, Inc., with offices in Maryland, Texas, and California. We ship from distribution centers in New York, Texas, and California. We offer a wide variety of solar panels, solar charge controllers, DC to AC Inverters, batteries, enclosures, cable and wiring and more. We specialize in industrial solar power applications such as solar telecommunications, solar RTU/SCADA, and oil and gas pipeline data monitoring. We have several types of pre-packaged solar power system kits inlcuding grid-tied solar power systems, off-grid solar energy systems, 5-110 watt kits, power ready systems, RV & marine kits, solar sign lighting kits, powerable solar power, and complete Sunwize grid tie systems. Click on any of the links on this page to find more information on our solar energy system products and components.\\\\nThank you for visiting MrSolar.com! We know solar panels!\\\\nCopyright © 2007 Online Solar, Inc. & MrSolar.com\\\\nHOME |\\\\nABOUT |\\\\nPRIVACY |\\\\nCONTACT |\\\\nLINK TO US |\\\\nSITE MAP\\\\nOnline Solar, Inc.\\\\nPO Box: 1506\\\\nHunt Valley, MD 21030\\\\nPh: 888-680-2427\\n[DOC 3] Solar Panels, 800.292.7648\\\\nThe\\\\nBugdugle Blog Network\\\\nSolar Panels, 800.292.7648\\\\n2008-May-18 - Dow Corning Going Solar\\\\nFREELAND - If you think solar power peaked in the 1970s, think again.\\\\nIn five years, energy from the sun will be competitive with traditional sources like coal and nuclear, predicts Eric Peeters, executive director of Dow Corning Solar Solutions. You\\'ll be able to power your home with more efficient, less costly and smaller solar panels, for instance.\\\\nThe innovations will be hatched at a $3 million, 27,000-square-foot Solar Solutions Application Center that Dow Corning Corp. opened Friday at an existing company building in Freeland.\\\\nRight now, solar panels aren\\'t as cost competitive with traditional power sources, or efficient as they could be.\\\\nBut Dow Corning says it has the answer - a process called encapsulation - that will use silicone to make panels that can harness more of the sun\\'s energy than existing technologies.\\\\n\\'\\'The future of solar is brilliant,\\'\\' Peeters said. \\'\\'It\\'s going to become a real part of our lives.\\'\\'\\\\nIn one hour, the sun puts out enough energy to power the entire planet for one year, Peeters said. Harnessing that power is the key.\\\\nMost solar panels today are made with EVA, or ethylene vinyl acetate, said Chad Steiner, a program manager at the Freeland facility.\\\\nDow Corning, based in Bay County\\'s Williams Township, plans to use silicone to make photovoltaic, or solar, panels.\\\\nThe method is meant to protect, seal and prevent corrosion of the panels.\\\\nSilicone also is more transparent than EVA, allowing more visible and ultraviolet light to reach the cell and be captured and harnessed, Steiner said.\\\\nThat increases efficiency by \\'\\'1-3 relative percentage points,\\'\\' he said, allowing panels to generate more watts and more money for manufacturers.\\\\nThe Solar Solutions Application Center aims to show manufacturers how to improve their existing solar technologies through a collaborative, hands-on process.\\\\n\\'\\'This is where all the fun stuff is going to happen,\\'\\' said Don Sheets, a vice president at Dow Corning.\\\\nPermanent Link\\\\n2008-May-17 - German Solar Industry Wants Solar Incentives\\\\nTHALHEIM, Germany: This sad stretch of eastern Germany, with its deserted coal mines and corroded factories, epitomizes post-industrial gloom. It is a place where even the clouds rarely seem to\\xa0part.\\\\nYet the sun was shining here the other day - and nowhere more brightly than at Q-Cells, a German company that last year overtook Sharp, a Japanese company, to become the world\\'s largest maker of photovoltaic solar cells. Q-Cells is the anchor tenant in a flowering cluster of solar start-ups here, known as Solar\\xa0Valley.\\\\nThanks to its aggressive push into renewable energies, a cloud-wreathed Germany has become an unlikely world leader in the race to harness the sun\\'s energy. It has by far the largest market for photovoltaic systems, which convert sunlight into electricity, with roughly half of the world\\'s total installed capacity. And it is the third-largest producer of solar cells and modules, after China and\\xa0Japan.\\\\nNow, though, with so many solar panels on so many rooftops, critics say Germany has too much of a good thing. Even at a time of record oil prices, solar is encountering resistance from conservative lawmakers who want to pare back its generous state incentives. They say it is growing at an unhealthy pace, threatening to burden consumers with too many costs in the form of higher electricity\\xa0bills.\\\\nSolar-energy entrepreneurs warn that reducing these incentives would deprive Germany of its pole position in an industry of tomorrow. They liken Germany to the United States and Japan, which were both once solar stars but faded as their subsidies became less\\xa0attractive.\\\\nPermanent Link\\\\n2008-May-16 - Google Money Ups for BrightSource\\\\nGoogle has stepped up its commitment to solar power with an investment of $10m in local company BrightSource Energy.\\\\nThe investment in the California-based firm was made through Google.org, the company\\'s non-profit philanthropic venture.\\\\nThe BrightSource deal is the second such investment Google has made in solar power, after taking a stake in eSolar last month.\\\\nBrightSource is raising cash in the hope of constructing a series of huge solar power plants in California\\'s Mojave Desert capable of supplying up to 900MW of electricity.\\\\n\\\\\"Solar thermal energy generation is one of the key emerging industries addressing the changing global climate and we are excited about our current investments in solar thermal technology,\\\\\" Google investment strategy manager Chris Busselle and green energy strategy team member Kevin Chen said in a blog posting.\\\\n\\\\\"We believe that by supporting researchers and entrepreneurs taking different, ambitious approaches and risks to generate clean energy, we can help to accelerate, progress and increase the collective economic value of these new clean energy industries.\\\\\"\\\\nThe investments are part of Google\\'s \\'Renewable Energy Cheaper than Coal\\' initiative.\\\\nThe company is assembling a team of researchers and engineers in the hope of creating a Gigawatt of energy for less than it costs to generate the same amount at a coal-burning power plant.\\\\nPermanent Link\\\\n2008-May-15 - Israeli Solar Panels Heavy Investor Action\\\\nA conference organized by Israeli President Shimon Peres to mark the country\\'s 60th anniversary kicked off May 13 with a showcase of 60 pioneering Israeli firms that have the potential to shake up medicine, biotech, clean energy, transportation, information technology, and communications. Among them was Luz II, a Jerusalem thermal solar energy company with more than one reason to celebrate its newfound recognition.\\\\nEarly the next day, Luz II announced it had raised $115 million in financing from a high-profile group of investors including Google.org—the philanthropy offshoot of Internet giant Google (GOOG)—and Black River Asset Management, an asset management unit of agricultural giant Cargill. Also participating in the funding are the investment arms of two major oil companies, BP (BP) Alternative Energy and StatoilHydro Venture.\\\\nAll of Luz II\\'s previous investors also kicked in to the latest round, including the Chevron Technology Ventures unit of Chevron (CVX), Silicon Valley venture firm Draper Fisher Jurvetson, Morgan Stanley (MS), and DBL Investors, a former unit of JP Morgan (JPM), bringing the company\\'s total funding to date to more than $160 million.\\\\nPermanent Link\\\\n2008-May-14 - Alegeria Building Desert Solar Panel Power Plant\\\\nALGIERS, Algeria (Dow Jones)--An Algerian state-controlled power company is erecting a forest of billboard-sized mirrors in the middle of the desert in the country\\'s first large-scale attempt to harvest solar energy from the Sahara and potentially transmit it to consumers thousands of miles away in Europe.\\\\nAlgeria wants to lay high voltage power lines to connect its electricity grid directly to Spain and Italy: a critical step to sending electricity across vast distances to new markets. Doing so would position the North African country - already a key natural gas supplier in Europe - to snap up an even larger share of the European energy market, which is moving to boost the role of renewable sources in its energy mix. \\'There\\'s no question we want to do it: we have the space for it, we have the (solar) radiation,\\' Algeria\\'s Oil Minister Chakib Khelil said in a recent interview in the Algerian capital.\\\\nSo far the Algerian government is having trouble winning commitments to sell its solar-powered electricity to European utilities at a premium to cover costs because governments are reluctant to pay more to Algieria suppliers when they are paying subsidies to promote their own renewable energy industries.\\\\nBut the European Commission aims for a 20% increase in energy from renewable sources by 2020 as part of a climate and energy package seeking approval from the European Parliament and member states. And experts like Luis Crespo, secretary-general of Protermo Solar, a solar energy industry group in Spain, say that the European Union will have to import renewable power to reach those goals.\\\\nCrespo says the continent will have to look elsewhere if it\\'s to meet those proposed targets because installing sprawling solar and wind farms in Europe on the scale necessary would be hampered by a lack of appropriate space. \\'We\\'re talking about all of Europe getting 20% of its primary energy from renewable sources in 2020 - for that, it\\'s certain that Europe will have to count on the importation of clean energy from Africa,\\' Crespo says. The technology to generate that proportion of renewable solar energy is already at work in Algeria and elsewhere.\\\\nPermanent Link\\\\nSuntrek\\\\nSolar Panels manufactured by Suntrek Industries are the best available in the solar industry.\\\\n«\\xa0 January\\xa02009 \\xa0»\\\\nMonTueWedThuFriSatSun\\xa01234\\\\n567891011\\\\n12131415161718\\\\n19202122232425\\\\n262728293031\\\\nRecent Posts\\\\nDow Corning Going Solar\\\\nGerman Solar Industry Wants Solar Incentives\\\\nLinks\\\\nHome\\\\nView my profile\\\\nArchives\\\\nFriends\\\\nEmail Me\\\\nMy Blog\\'s RSS\\\\nSolar PanelsThousand Oaks Real EstateGlass Thousand OaksCabinet GlassBoard Up ServicesPersonal TrainerEmbroidery Thousand OaksCommercial Real Estate Westlake VillageTool and DieWeb DesignWebsite EnhancementH2S ScavengerCalifornia Home LoansSpiritual EnlightenmentCO2 ScavengerMarble Floor CareAudio Video TeleconferencingVentura Criminal Defense Attorney\\\\nFriends\\\\nPage 1 of 73\\\\nLast Page\\\\n|\\\\nNext Page\\\\nPowered By Bugdugle Blog\\\\nHosting\\n[DOC 4] Sharp 165W Solar Panel (NE165U1) - 150 Watts & Up Solar Panels\\\\n@ AltE\\\\nstore\\\\nlearn\\\\ncommunity\\\\n>\\\\nStore >\\\\nSolar Panels >\\\\n150 Watts & Up Solar Panels >\\\\nSharp 165W Solar Panel (NE165U1)\\\\nList Price:\\\\n$1,070.40\\\\nOur Price:\\\\n$829.56\\\\nSave: $240.84 (23%)\\\\nModel:\\\\nNE165U1\\\\nBrand:\\\\nSharp\\\\nAvailability:\\\\nCall for Availability\\\\nQuantity\\\\nThis item ships truck freight only, shipping will not automatically be calculated.\\\\nNeed help selecting components and designing your system? Give our Knowledgeable Sales Folks a call at\\\\n877-878-4060.\\\\nBasic Stats click to view hide\\\\nVolts: 24\\xa0VAmps: 4.77\\xa0AWatts: 165\\xa0WConnector Type: MC Solarline 1\\xa0Cell Tech: Polycrystalline\\\\nOur Code: SHA165-NE165U1\\\\nAlt-E Video Tips\\\\nclick to view hide\\\\nMC Connectors\\\\nLearn about the different MC connectors and how to use them in conjunction with your solar panels.Solar Panel TestingLearn how to test your solar panel with a multimeter.\\\\nReviews click to view hide\\\\nThere are currently no reviews.\\\\nIf you own this product write a review!\\\\nMounts click to view hide\\\\nThis is not a complete list of mounts for this panel, just a selection to help make renewable doable.\\\\nNum PanelsMountMount Details\\\\n4DPW Roof/Ground Mount SH165 4 Mods w/ TLRoof/Ground6DPW Roof/Ground Mount SH165 6 Mods w/ TLRoof/Ground, Truck Freight8DPW Roof/Ground Mount SH165 8 Mods w/ TLRoof/Ground, Truck Freight\\\\nProduct Documentation click to view hide\\\\nSharp_Module_Warranty_New.pdf\\\\nsharp-165.pdf\\\\nDetails click to view hide\\\\nRead about how this product was used in Absorbing Solar, a real-life story from one of our customers!\\\\nSharp\\\\nNE165U1 - 165 Watt Solar Module\\\\nA\\\\nsafe, clean, reliable source of energy, the Sharp NE165U1 photovoltaic module\\\\nis designed for large electrical power requirements. Based on the technology of\\\\ncrystal silicon solar cells developed over 35 years, this solar panel has superb\\\\ndurability to withstand rigourous operating conditions and is suitable for grid\\\\nconnected systems. Common applications include residences, office buildings,\\\\nsolar power stations, solar villages, radio relay stations, beacons and traffic\\\\nlights. As the world\\'s leading manufacturer of photovoltaic modules, Sharp produces\\\\nan extensive line of high power solar panels for every electrical power requirement.\\\\nFeatures\\\\nHigh-power solar module (165W) using 125mm square multi-crystal\\\\nsilicon solar cells with 12.68% module conversion efficiency.\\\\nPhotovoltaic module with bypass diode minimizes the\\\\npower drop caused by shade.\\\\nTextured cell surface to reduce the reflection of sunlight\\\\nand BSF (Back Surface Field) structure to improve cell conversion efficiency:\\\\n14.55%.\\\\nWhite tempered glass, EVA resin, and a weatherproof\\\\nfilm, plus aluminum frame for extended outdoor use.\\\\nNominal 24 DC output, perfect for grid connected systems\\\\nOutput terminal: Lead wire with waterproof connector\\\\nCertifications: UL 1703, cUL\\\\nSHARP solar panels are manufactured in ISO 9001 certified\\\\nfactories\\\\nCell\\\\nMulti-Cyrstal Silicon Solar Cells\\\\nNumber of Cells and Connections\\\\n72 in Series\\\\nOpen Circuit Voltage (Voc)\\\\n43.1 volts\\\\nMaximum Power Voltage (Vmp)\\\\n34.6 volts\\\\nShort Circuit Current (Isc)\\\\n5.46 amps\\\\nMaximum Power Current (Imp)\\\\n4.77 amps\\\\nMaximum Power\\\\n165.0 watts\\\\nMinimum Power\\\\n148.5 watts\\\\nEcanpsulated Solar Cell Efficiency\\\\n14.55\\\\nPTC Rating\\\\n144.87 watts\\\\nMaximum System Voltage\\\\nDC 600V\\\\nSeries Fuse Rating\\\\n10 amps\\\\nType of Output Terminal\\\\nLead Wire with MC Connector\\\\nDimensions\\\\n62.01\\\\\" x 32.52\\\\\" x 1.81\\\\\"\\\\n(1575 x 826 x 46 mm)\\\\nWeight\\\\n37.485 lbs (17.0 kg)\\\\nOperating/Storage Temperature\\\\n-40 to +90 C\\\\nPlease note that we cannot sell these modules to customers in Europe per request of the manufacturer.\\\\nCustomers Also Purchased click to view hide\\\\nCustomers who bought the Sharp 165W Solar Panel (NE165U1) also purchased:\\\\nXantrex XW Solar Charge Controller MPPT 865-1030Xantrex Cm/R-50 Digitalmeter-C12/35/40/60-50\\'30\\' MC1 Connector #10 AWG Male/Female50\\' MC1 Connector #10 AWG Male/FemaleMc Branch Cable Coupler M-2FMc Branch Cable Coupler F-2M\\\\nSend to a friend\\\\nWrite a review\\\\nRequest more info\\\\nNeed help designing your system?Call Us Toll Free!877-878-4060Se habla Español\\\\ncart\\\\n0 items\\\\nproducts\\\\nBatteries\\\\nBooks, Classes & Webinars\\\\nCables & Wiring\\\\nCharge Controllers\\\\nComposting\\\\nDC Voltage Converters\\\\nEnclosures, Electrical and Safety\\\\nEnergy Efficient Products\\\\nHydropower\\\\nInverters\\\\nKits and Package Deals\\\\nLighting & Fans\\\\nMeters, Communications & Site Analysis\\\\nOpen Box / Returned to Stock\\\\nOverstock, Closeouts & Clearance!\\\\nPortable Power\\\\nPower Panels\\\\nSolar Air Heaters\\\\nSolar Panels\\\\n1 to 50 Watt Solar Panels\\\\n51 to 99 Watt Solar Panels\\\\n100 to 149 Watts Solar Panels\\\\n150 Watts & Up Solar Panels\\\\nFlexible / Rollable Solar Panels\\\\nFoldable Solar Panels\\\\nSolar Electricity Education\\\\nSolar Panels by the Pallet\\\\nSolar Pool Heaters and Pumps\\\\nSolar Trackers & Solar Panel Mounts\\\\nSolar Water Heaters\\\\nSolar Water Pumps\\\\nWind Turbines\\\\nbrowse by brand\\\\nPlease SelectAEEAlternate Energy TechnologiesAlternative Energy StoreAmerican Technical PublishersApollo SolarBergey WindpowerBlue Oak PV ProductsBlue Sea SystemsBlue Sky EnergyBogart EngineeringBP SolarBZ ProductsC. CraneCanadian Solar, Inc.Carmanah TechnologiesConcordeDanbyDankoff SolarDelta Lightning ArrestorsDirect Power and WaterDometic Gas RefrigeratorsDuracellEnvironmental Solar SystemsEvergreen SolarExeltechFroniusGEGlobal SolarGNBGo Power!GrundfosHeliodyneIOTA EngineeringIronRidge (Two Seas)Kaco SolarKanekaKyoceraLaingLeveredgeLorentzMadgeTechMagnum InvertersMaximum IncMidnite Solar, Inc.Miller RefrigerationMitsubishi ElectricMK BatteryMorningstar CorpNatural LightOutbackP3 International (Kill-A-Watt)Paloma TanklessPhocosPower UpPowerFilm Inc.PV PoweredRCH FanworksSamlexSanyo SolarSharpShellShurfloSMA - AmericaSolar Cone, Inc.Solar ConvertersSolar WorldSolarRoofsSolectria RenewablesSouthwest Wind PowerSpecialty ConceptsSquare DStecaSterling PublishingSun FrostSun PumpsSun-MarSundanzerSunTechSunwizeSurretteTacoUniracUnisolarUniversal BatteryVoltaic SystemsWattsunWiley ElectronicsXantrexYour Solar Home IncZomeworks\\\\nWhy buy from us?\\\\nSign up for our Newsletters\\\\nSave / Earn Money\\\\nHave a suggestion? Found a bug?\\\\nCustomer testimonials\\\\n“My order was received just in the nick of time as we have an appointment tomarrow at\\\\n9 am for help in installing this unit and the other 2 items on this order.\\\\nThanks again, and if I ever need anything else I will surely consider your store.”\\\\nCarl - Livingston, TX\\\\n“Thank you to all involved. I\\'ve dealt with quite a few vendors in the past year and it\\'s always refreshing to find folks like you that care about your job, your company and your customer. Please, keep up the great work! As a self-described customer representative, we appreciate it.”\\\\nKen - Walnut Creek, CA\\\\n“Thanks a lot for the info. You guys have excellent customer service!”\\\\nBob - Dallas, TX\\\\nmore\\xa0testimonials\\xa0...\\\\nAbout us | Contact us |\\\\nHelp | Terms & conditions\\\\nPress\\\\n| Affiliate Program\\\\n| Popular Searches| Site Map\\\\n| Job Openings\\\\n© Alternative Energy Store. All rights reserved.', 'output': 'Find information about nano-technological solar power.', 'input_ids': [784, 15367, 11824, 908, 2588, 251, 81, 3693, 7065, 11, 688, 24, 9421, 135, 5, 784, 9857, 5211, 11810, 134, 908, 784, 26472, 209, 908, 7823, 49, 12601, 2, 29, 3881, 32, 1171, 12601, 2, 29, 3881, 32, 1171, 12601, 22, 7, 5039, 2, 29, 134, 2168, 102, 12, 738, 2, 29, 7474, 3525, 49, 3489, 2, 29, 17527, 19, 3, 9, 1651, 2900, 12, 11985, 9334, 2621, 2, 29, 30404, 11363, 2464, 3, 104, 850, 10, 4853, 183, 2, 29, 5680, 6, 25, 497, 58, 37, 2717, 19, 16, 3, 9, 3, 7, 1483, 2296, 7, 6, 192, 13, 82, 11195, 43, 118, 7245, 326, 11, 82, 5866, 2876, 19, 985, 541, 38, 231, 38, 34, 47, 336, 2265, 5, 571, 54, 48, 36, 3, 9, 207, 97, 12, 2438, 3693, 579, 58, 2, 29, 2796, 17, 2, 76, 1206, 2294, 7, 240, 34, 1147, 57, 1147, 5, 37, 8467, 30, 3556, 1887, 164, 1113, 8, 2717, 6, 68, 79, 33, 59, 8, 2717, 5, 3, 13283, 66, 8, 23295, 6, 16537, 6, 1604, 277, 6, 11, 10481, 113, 708, 2628, 33, 341, 300, 5, 1537, 359, 6, 8, 9321, 7, 6, 3832, 7, 6, 3, 9905, 7, 6, 13221, 11, 7329, 4120, 6, 7282, 4120, 11, 15099, 3, 2, 76, 1206, 2368, 8, 4968, 24, 143, 95, 8, 2, 29, 6644, 2, 29, 15, 12036, 63, 3, 2, 76, 1206, 2368, 33, 341, 1710, 5, 2, 29, 19739, 6, 151, 33, 5489, 2476, 16, 8, 8175, 11, 3549, 8981, 6, 68, 175, 613, 8467, 33, 3, 31488, 26, 57, 1340, 1549, 7, 16, 2887, 6, 1374, 12, 72, 8467, 5, 27, 2, 76, 1206, 2294, 51, 59, 15495, 25, 805, 9030, 4571, 1162, 1173, 114, 2343, 4439, 45, 1473, 6, 68, 3, 99, 25, 241, 12, 2454, 8, 3732, 2393, 6, 805, 797, 5, 4783, 8, 80, 589, 24, 19, 341, 1892, 2, 29, 4725, 16, 1371, 2, 29, 117, 3693, 579, 5, 2, 29, 21425, 5866, 2876, 19, 952, 45, 944, 12, 1283, 1093, 1146, 5, 100, 19, 250, 8, 17930, 1513, 540, 147, 8, 1248, 38, 3, 9, 741, 13, 3674, 3753, 223, 30, 6373, 5, 148, 54, 143, 135, 217, 8, 3505, 13, 70, 1155, 3, 2, 76, 1206, 2368, 11, 8, 9620, 14277, 13, 7954, 1596, 12, 1189, 8467, 3, 2, 76, 1206, 2368, 57, 9873, 3693, 579, 11, 3, 11600, 128, 6, 42, 66, 6, 13, 39, 6373, 523, 909, 5, 2, 29, 2796, 17, 2, 76, 1206, 2294, 7, 240, 3, 9, 320, 44, 119, 16844, 6, 1849, 28, 37, 7433, 4654, 5287, 472, 603, 83, 302, 1983, 13, 2628, 41, 265, 14550, 38, 294, 13, 8, 1100, 15794, 670, 2876, 6, 2804, 6416, 5, 1914, 2628, 61, 10, 2, 29, 382, 9, 226, 8832, 277, 54, 3, 2, 17, 15085, 3, 9, 12475, 827, 998, 21, 335, 203, 6, 1849, 30, 8, 833, 3, 2, 17, 532, 4717, 3064, 19, 2681, 16, 313, 5, 86, 455, 12, 9448, 6, 3, 2, 17, 89, 9, 13067, 3010, 398, 36, 2681, 16, 313, 57, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [2588, 251, 81, 13944, 18, 3470, 29, 4478, 3693, 579, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "tokenizer,tokenized_train_dataset, tokenized_eval_dataset= tokenize_and_split_dataset(\"training_top5_qulac_PREPROCESSED_FOR_MODEL.json\",\"t5-small\")\n",
    "print(tokenized_train_dataset[0])  # Optional: check one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4c34785e-2e1d-456b-9488-723ece681367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">T5 Base + TO5 DOCS SUMMARIZED</strong> at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/0ugc1l4s' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/0ugc1l4s</a><br> View project at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_034946-0ugc1l4s/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/users/nfs/Etu0/21402600/wandb/run-20250505_035144-j778adx3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/j778adx3' target=\"_blank\">T5 Small + TO5 DOCS</a></strong> to <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/j778adx3' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/j778adx3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "523bf25439e0404ebf18dd50cbd690f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)4339abf00c2d751f99a73c1c4b127be5d8a7c7d9:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1746409912.909384 1123707 gpu_device.cc:2341] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1123707/4023069370.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='952' max='2040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 952/2040 16:59 < 19:27, 0.93 it/s, Epoch 14/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7.069100</td>\n",
       "      <td>0.771933</td>\n",
       "      <td>0.149806</td>\n",
       "      <td>0.404954</td>\n",
       "      <td>0.176795</td>\n",
       "      <td>0.393308</td>\n",
       "      <td>0.334293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.969000</td>\n",
       "      <td>0.549294</td>\n",
       "      <td>0.174324</td>\n",
       "      <td>0.437280</td>\n",
       "      <td>0.207905</td>\n",
       "      <td>0.424256</td>\n",
       "      <td>0.386653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.571200</td>\n",
       "      <td>0.470458</td>\n",
       "      <td>0.208231</td>\n",
       "      <td>0.497650</td>\n",
       "      <td>0.239241</td>\n",
       "      <td>0.489789</td>\n",
       "      <td>0.447375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.530600</td>\n",
       "      <td>0.454921</td>\n",
       "      <td>0.191625</td>\n",
       "      <td>0.493873</td>\n",
       "      <td>0.227596</td>\n",
       "      <td>0.483214</td>\n",
       "      <td>0.435498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.509200</td>\n",
       "      <td>0.447265</td>\n",
       "      <td>0.196918</td>\n",
       "      <td>0.506433</td>\n",
       "      <td>0.239022</td>\n",
       "      <td>0.494439</td>\n",
       "      <td>0.455201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.486100</td>\n",
       "      <td>0.439352</td>\n",
       "      <td>0.208233</td>\n",
       "      <td>0.503904</td>\n",
       "      <td>0.245829</td>\n",
       "      <td>0.491247</td>\n",
       "      <td>0.453277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.448400</td>\n",
       "      <td>0.434164</td>\n",
       "      <td>0.223288</td>\n",
       "      <td>0.517980</td>\n",
       "      <td>0.259163</td>\n",
       "      <td>0.509867</td>\n",
       "      <td>0.471452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.451100</td>\n",
       "      <td>0.433264</td>\n",
       "      <td>0.223910</td>\n",
       "      <td>0.519740</td>\n",
       "      <td>0.259430</td>\n",
       "      <td>0.509617</td>\n",
       "      <td>0.472943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.432900</td>\n",
       "      <td>0.432279</td>\n",
       "      <td>0.219700</td>\n",
       "      <td>0.513982</td>\n",
       "      <td>0.251678</td>\n",
       "      <td>0.503930</td>\n",
       "      <td>0.462324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.432700</td>\n",
       "      <td>0.428119</td>\n",
       "      <td>0.221895</td>\n",
       "      <td>0.515922</td>\n",
       "      <td>0.256638</td>\n",
       "      <td>0.505348</td>\n",
       "      <td>0.461391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.421000</td>\n",
       "      <td>0.426875</td>\n",
       "      <td>0.231302</td>\n",
       "      <td>0.525081</td>\n",
       "      <td>0.267280</td>\n",
       "      <td>0.517264</td>\n",
       "      <td>0.475457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.405500</td>\n",
       "      <td>0.426925</td>\n",
       "      <td>0.227921</td>\n",
       "      <td>0.520674</td>\n",
       "      <td>0.262532</td>\n",
       "      <td>0.513510</td>\n",
       "      <td>0.469692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.401500</td>\n",
       "      <td>0.427032</td>\n",
       "      <td>0.225849</td>\n",
       "      <td>0.512133</td>\n",
       "      <td>0.260503</td>\n",
       "      <td>0.504980</td>\n",
       "      <td>0.466281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.398200</td>\n",
       "      <td>0.427655</td>\n",
       "      <td>0.226261</td>\n",
       "      <td>0.511071</td>\n",
       "      <td>0.262460</td>\n",
       "      <td>0.503215</td>\n",
       "      <td>0.461330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4268746078014374, 'eval_bleu': 0.23130231703063925, 'eval_rouge1': 0.5250809284537804, 'eval_rouge2': 0.26727968353866854, 'eval_rougeL': 0.5172638348759238, 'eval_meteor': 0.4754573042921671, 'eval_runtime': 2.2709, 'eval_samples_per_second': 26.421, 'eval_steps_per_second': 3.523, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:00<00:00, 61380.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final predictions with inputs saved to predicted_clarif_T5SMALL_TOP5DOC.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/bleu</td><td>▁▃▆▅▅▆▇▇▇▇█████</td></tr><tr><td>eval/loss</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/meteor</td><td>▁▄▇▆▇▇██▇▇███▇█</td></tr><tr><td>eval/rouge1</td><td>▁▃▆▆▇▇██▇▇██▇▇█</td></tr><tr><td>eval/rouge2</td><td>▁▃▆▅▆▆▇▇▇▇██▇██</td></tr><tr><td>eval/rougeL</td><td>▁▃▆▆▇▇██▇▇██▇▇█</td></tr><tr><td>eval/runtime</td><td>██████▇▁▅▅▄▄▄▅▅</td></tr><tr><td>eval/samples_per_second</td><td>▁▁▁▁▁▁▂█▄▄▅▅▄▄▄</td></tr><tr><td>eval/steps_per_second</td><td>▁▁▁▁▁▁▂█▄▄▅▅▄▄▄</td></tr><tr><td>test/bleu</td><td>▁</td></tr><tr><td>test/loss</td><td>▁</td></tr><tr><td>test/meteor</td><td>▁</td></tr><tr><td>test/rouge1</td><td>▁</td></tr><tr><td>test/rouge2</td><td>▁</td></tr><tr><td>test/rougeL</td><td>▁</td></tr><tr><td>test/runtime</td><td>▁</td></tr><tr><td>test/samples_per_second</td><td>▁</td></tr><tr><td>test/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇██████</td></tr><tr><td>train/grad_norm</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/bleu</td><td>0.2313</td></tr><tr><td>eval/loss</td><td>0.42687</td></tr><tr><td>eval/meteor</td><td>0.47546</td></tr><tr><td>eval/rouge1</td><td>0.52508</td></tr><tr><td>eval/rouge2</td><td>0.26728</td></tr><tr><td>eval/rougeL</td><td>0.51726</td></tr><tr><td>eval/runtime</td><td>2.2709</td></tr><tr><td>eval/samples_per_second</td><td>26.421</td></tr><tr><td>eval/steps_per_second</td><td>3.523</td></tr><tr><td>test/bleu</td><td>0.2313</td></tr><tr><td>test/loss</td><td>0.42687</td></tr><tr><td>test/meteor</td><td>0.47546</td></tr><tr><td>test/rouge1</td><td>0.52508</td></tr><tr><td>test/rouge2</td><td>0.26728</td></tr><tr><td>test/rougeL</td><td>0.51726</td></tr><tr><td>test/runtime</td><td>2.2521</td></tr><tr><td>test/samples_per_second</td><td>26.642</td></tr><tr><td>test/steps_per_second</td><td>3.552</td></tr><tr><td>total_flos</td><td>1021289233907712.0</td></tr><tr><td>train/epoch</td><td>14</td></tr><tr><td>train/global_step</td><td>952</td></tr><tr><td>train/grad_norm</td><td>0.84918</td></tr><tr><td>train/learning_rate</td><td>3e-05</td></tr><tr><td>train/loss</td><td>0.3982</td></tr><tr><td>train_loss</td><td>0.85507</td></tr><tr><td>train_runtime</td><td>1020.2309</td></tr><tr><td>train_samples_per_second</td><td>15.849</td></tr><tr><td>train_steps_per_second</td><td>2</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">T5 Small + TO5 DOCS</strong> at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/j778adx3' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/j778adx3</a><br> View project at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_035144-j778adx3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"T5-Clarifications\",  \n",
    "    name=\"T5 Small + TO5 DOCS\"\n",
    ")\n",
    "train(\"t5-small\", \"predicted_clarif_T5SMALL_TOP5DOC.jsonl\",tokenizer, tokenized_train_dataset, tokenized_eval_dataset,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c4c16d-3e0e-42c3-93c0-1a09801204e7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## same run doen earlier before wandb -IGNORE\n",
    "do it for 30 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bb9084fb-6dc9-4e79-a299-9b1efd656e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_674230/1610004705.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='850' max='1020' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 850/1020 27:18 < 05:28, 0.52 it/s, Epoch 25/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>21.891900</td>\n",
       "      <td>0.822087</td>\n",
       "      <td>0.124927</td>\n",
       "      <td>0.377650</td>\n",
       "      <td>0.152483</td>\n",
       "      <td>0.372101</td>\n",
       "      <td>0.286300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.389900</td>\n",
       "      <td>0.734556</td>\n",
       "      <td>0.141975</td>\n",
       "      <td>0.400126</td>\n",
       "      <td>0.167344</td>\n",
       "      <td>0.385960</td>\n",
       "      <td>0.343383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.887300</td>\n",
       "      <td>0.604645</td>\n",
       "      <td>0.167292</td>\n",
       "      <td>0.434119</td>\n",
       "      <td>0.207284</td>\n",
       "      <td>0.424159</td>\n",
       "      <td>0.387383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.887300</td>\n",
       "      <td>0.501504</td>\n",
       "      <td>0.186885</td>\n",
       "      <td>0.457414</td>\n",
       "      <td>0.221770</td>\n",
       "      <td>0.449804</td>\n",
       "      <td>0.413395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.656200</td>\n",
       "      <td>0.471018</td>\n",
       "      <td>0.194042</td>\n",
       "      <td>0.497377</td>\n",
       "      <td>0.225994</td>\n",
       "      <td>0.490047</td>\n",
       "      <td>0.437352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.541100</td>\n",
       "      <td>0.461405</td>\n",
       "      <td>0.199731</td>\n",
       "      <td>0.495848</td>\n",
       "      <td>0.231933</td>\n",
       "      <td>0.489790</td>\n",
       "      <td>0.445434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.541100</td>\n",
       "      <td>0.455307</td>\n",
       "      <td>0.197389</td>\n",
       "      <td>0.493090</td>\n",
       "      <td>0.232453</td>\n",
       "      <td>0.489384</td>\n",
       "      <td>0.441052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.493800</td>\n",
       "      <td>0.449578</td>\n",
       "      <td>0.193624</td>\n",
       "      <td>0.504263</td>\n",
       "      <td>0.233002</td>\n",
       "      <td>0.494275</td>\n",
       "      <td>0.448787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.483700</td>\n",
       "      <td>0.446134</td>\n",
       "      <td>0.198189</td>\n",
       "      <td>0.506828</td>\n",
       "      <td>0.245310</td>\n",
       "      <td>0.500292</td>\n",
       "      <td>0.455046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.483700</td>\n",
       "      <td>0.442523</td>\n",
       "      <td>0.208601</td>\n",
       "      <td>0.510335</td>\n",
       "      <td>0.255664</td>\n",
       "      <td>0.504310</td>\n",
       "      <td>0.463587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.468200</td>\n",
       "      <td>0.441586</td>\n",
       "      <td>0.205029</td>\n",
       "      <td>0.508835</td>\n",
       "      <td>0.251193</td>\n",
       "      <td>0.501002</td>\n",
       "      <td>0.461990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.448100</td>\n",
       "      <td>0.439791</td>\n",
       "      <td>0.213837</td>\n",
       "      <td>0.515503</td>\n",
       "      <td>0.258877</td>\n",
       "      <td>0.507266</td>\n",
       "      <td>0.472156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.448100</td>\n",
       "      <td>0.437662</td>\n",
       "      <td>0.218052</td>\n",
       "      <td>0.513573</td>\n",
       "      <td>0.259710</td>\n",
       "      <td>0.505766</td>\n",
       "      <td>0.470409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.436900</td>\n",
       "      <td>0.436570</td>\n",
       "      <td>0.217980</td>\n",
       "      <td>0.513096</td>\n",
       "      <td>0.259350</td>\n",
       "      <td>0.505314</td>\n",
       "      <td>0.470337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.437700</td>\n",
       "      <td>0.435639</td>\n",
       "      <td>0.223214</td>\n",
       "      <td>0.517984</td>\n",
       "      <td>0.261367</td>\n",
       "      <td>0.509783</td>\n",
       "      <td>0.471340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.437700</td>\n",
       "      <td>0.434608</td>\n",
       "      <td>0.221294</td>\n",
       "      <td>0.516467</td>\n",
       "      <td>0.260075</td>\n",
       "      <td>0.508025</td>\n",
       "      <td>0.470396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.433100</td>\n",
       "      <td>0.434120</td>\n",
       "      <td>0.221914</td>\n",
       "      <td>0.517952</td>\n",
       "      <td>0.261147</td>\n",
       "      <td>0.510485</td>\n",
       "      <td>0.469828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.418700</td>\n",
       "      <td>0.433966</td>\n",
       "      <td>0.226801</td>\n",
       "      <td>0.522563</td>\n",
       "      <td>0.267834</td>\n",
       "      <td>0.514209</td>\n",
       "      <td>0.477093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.418700</td>\n",
       "      <td>0.433418</td>\n",
       "      <td>0.228328</td>\n",
       "      <td>0.519418</td>\n",
       "      <td>0.264893</td>\n",
       "      <td>0.511472</td>\n",
       "      <td>0.471591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.413400</td>\n",
       "      <td>0.432969</td>\n",
       "      <td>0.227067</td>\n",
       "      <td>0.518008</td>\n",
       "      <td>0.263706</td>\n",
       "      <td>0.510033</td>\n",
       "      <td>0.470315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.413100</td>\n",
       "      <td>0.432017</td>\n",
       "      <td>0.224526</td>\n",
       "      <td>0.515032</td>\n",
       "      <td>0.261322</td>\n",
       "      <td>0.508169</td>\n",
       "      <td>0.463329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.413100</td>\n",
       "      <td>0.430861</td>\n",
       "      <td>0.225405</td>\n",
       "      <td>0.519022</td>\n",
       "      <td>0.261620</td>\n",
       "      <td>0.510562</td>\n",
       "      <td>0.466577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.403700</td>\n",
       "      <td>0.431396</td>\n",
       "      <td>0.229211</td>\n",
       "      <td>0.521909</td>\n",
       "      <td>0.266835</td>\n",
       "      <td>0.513628</td>\n",
       "      <td>0.470085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.403900</td>\n",
       "      <td>0.431024</td>\n",
       "      <td>0.225510</td>\n",
       "      <td>0.519528</td>\n",
       "      <td>0.261620</td>\n",
       "      <td>0.510999</td>\n",
       "      <td>0.466577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.398300</td>\n",
       "      <td>0.430949</td>\n",
       "      <td>0.225331</td>\n",
       "      <td>0.518801</td>\n",
       "      <td>0.261620</td>\n",
       "      <td>0.510999</td>\n",
       "      <td>0.465932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.43086084723472595, 'eval_bleu': 0.22540464040675906, 'eval_rouge1': 0.5190223773296081, 'eval_rouge2': 0.26161976710516216, 'eval_rougeL': 0.5105622466603756, 'eval_meteor': 0.4665770243147761, 'eval_runtime': 2.1916, 'eval_samples_per_second': 27.377, 'eval_steps_per_second': 1.825, 'epoch': 25.0}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (2, 60) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(evaluation_results)\n\u001b[1;32m     51\u001b[0m predictions \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mpredict(tokenized_eval_dataset)\n\u001b[0;32m---> 52\u001b[0m pred_ids \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m predicted_texts \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(pred_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     54\u001b[0m true_texts \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(predictions\u001b[38;5;241m.\u001b[39mlabel_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:1229\u001b[0m, in \u001b[0;36margmax\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;124;03mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m-> 1229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:56\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, method, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bound \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:45\u001b[0m, in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     wrap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(asarray(obj), method)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrap:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, mu\u001b[38;5;241m.\u001b[39mndarray):\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (2, 60) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "# from transformers import AutoModelForSeq2SeqLM, Trainer, TrainingArguments,EarlyStoppingCallback\n",
    "# from datasets import Dataset\n",
    "# import os\n",
    "\n",
    "\n",
    "# model_name = \"t5-small\"\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# output_dir = \"./results\"\n",
    "# logging_dir = os.path.join(output_dir, \"logs\")\n",
    "# # Define training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",  # Output directory for logs and checkpoints (optional)\n",
    "#     eval_strategy=\"epoch\",  # Evaluate after every epoch\n",
    "#     save_strategy=\"epoch\",\n",
    "#     learning_rate=5e-5,  # Learning rate\n",
    "#     per_device_train_batch_size=16,  # Batch size per device for training\n",
    "#     per_device_eval_batch_size=16,  # Batch size per device for evaluation\n",
    "#     num_train_epochs=30,  # Number of training epochs\n",
    "#     logging_dir=logging_dir,\n",
    "#     use_cpu=True,\n",
    "#     logging_first_step=True,\n",
    "#     weight_decay=0.01,  # Weight decay for regularization\n",
    "#     logging_steps=50,\n",
    "#     load_best_model_at_end=True,\n",
    "#     metric_for_best_model=\"eval_loss\", \n",
    "#     greater_is_better=False\n",
    "# )\n",
    "\n",
    "# os.makedirs(logging_dir, exist_ok=True)\n",
    "\n",
    "# # Initialize the Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_train_dataset,\n",
    "#     eval_dataset=tokenized_eval_dataset,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     tokenizer=tokenizer,  # Pass the tokenizer for automatic tokenization during training\n",
    "#     callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "# )\n",
    "\n",
    "# # Start the training (without saving)\n",
    "# trainer.train()\n",
    "\n",
    "# # You can evaluate the model here if you need\n",
    "# # Example: Evaluate after training\n",
    "# evaluation_results = trainer.evaluate()\n",
    "# print(evaluation_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a7b5ace7-d9cc-4cfa-8a41-6b1441404838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final predictions saved to predicted_clarif_TINYT5_30EPOCH.jsonl\n"
     ]
    }
   ],
   "source": [
    "# predictions = trainer.predict(tokenized_eval_dataset)\n",
    "\n",
    "# pred_ids = np.argmax(predictions.predictions[0], axis=-1)\n",
    "# predicted_texts = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "# true_texts = tokenizer.batch_decode(predictions.label_ids, skip_special_tokens=True)\n",
    "\n",
    "# output_predictions_file = \"predicted_clarif_TINYT5_30EPOCH.jsonl\"\n",
    "# with open(output_predictions_file, \"w\", encoding=\"utf-8\") as writer:\n",
    "#     for true_text, predicted_text in zip(true_texts, predicted_texts):\n",
    "#         output_dict = {\"true\": true_text, \"predicted\": predicted_text}\n",
    "#         writer.write(json.dumps(output_dict) + \"\\n\")\n",
    "\n",
    "# print(f\"Final predictions saved to {output_predictions_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5a652c-766f-4673-a13e-0ae78a3ebf03",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# SE2SEQ T5 SMALL + TO5 DOCS RUN ON WANDB - just ignore set up wasnt finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ffbb7cfa-73e3-4da6-af86-e3636df096a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1123707/1094267103.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/users/nfs/Etu0/21402600/wandb/run-20250504_213114-n1ehdka6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hibaakbi-sorbonne-universit-/huggingface/runs/n1ehdka6' target=\"_blank\">T5-SMALL TOP5DOCS</a></strong> to <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/huggingface' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/huggingface/runs/n1ehdka6' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/huggingface/runs/n1ehdka6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='952' max='2040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 952/2040 16:24 < 18:47, 0.96 it/s, Epoch 14/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.781100</td>\n",
       "      <td>0.812190</td>\n",
       "      <td>0.117157</td>\n",
       "      <td>0.391130</td>\n",
       "      <td>0.181558</td>\n",
       "      <td>0.377999</td>\n",
       "      <td>0.326605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.946100</td>\n",
       "      <td>0.601260</td>\n",
       "      <td>0.154446</td>\n",
       "      <td>0.424232</td>\n",
       "      <td>0.224240</td>\n",
       "      <td>0.417415</td>\n",
       "      <td>0.387250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.596800</td>\n",
       "      <td>0.516339</td>\n",
       "      <td>0.169055</td>\n",
       "      <td>0.465668</td>\n",
       "      <td>0.233193</td>\n",
       "      <td>0.457669</td>\n",
       "      <td>0.422066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.527900</td>\n",
       "      <td>0.501994</td>\n",
       "      <td>0.157821</td>\n",
       "      <td>0.473764</td>\n",
       "      <td>0.230911</td>\n",
       "      <td>0.465590</td>\n",
       "      <td>0.434874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.496200</td>\n",
       "      <td>0.494720</td>\n",
       "      <td>0.160844</td>\n",
       "      <td>0.466397</td>\n",
       "      <td>0.226154</td>\n",
       "      <td>0.458146</td>\n",
       "      <td>0.430444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.460600</td>\n",
       "      <td>0.490341</td>\n",
       "      <td>0.165680</td>\n",
       "      <td>0.480229</td>\n",
       "      <td>0.240972</td>\n",
       "      <td>0.471348</td>\n",
       "      <td>0.439784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.466800</td>\n",
       "      <td>0.483008</td>\n",
       "      <td>0.164266</td>\n",
       "      <td>0.481503</td>\n",
       "      <td>0.240175</td>\n",
       "      <td>0.474668</td>\n",
       "      <td>0.445833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.432900</td>\n",
       "      <td>0.483343</td>\n",
       "      <td>0.166211</td>\n",
       "      <td>0.481178</td>\n",
       "      <td>0.234927</td>\n",
       "      <td>0.473599</td>\n",
       "      <td>0.443244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.433700</td>\n",
       "      <td>0.480551</td>\n",
       "      <td>0.165734</td>\n",
       "      <td>0.478390</td>\n",
       "      <td>0.231603</td>\n",
       "      <td>0.470114</td>\n",
       "      <td>0.439596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.414700</td>\n",
       "      <td>0.480789</td>\n",
       "      <td>0.171579</td>\n",
       "      <td>0.496970</td>\n",
       "      <td>0.243226</td>\n",
       "      <td>0.486260</td>\n",
       "      <td>0.455332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.429400</td>\n",
       "      <td>0.478722</td>\n",
       "      <td>0.172439</td>\n",
       "      <td>0.493218</td>\n",
       "      <td>0.240594</td>\n",
       "      <td>0.480130</td>\n",
       "      <td>0.448876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.389400</td>\n",
       "      <td>0.479441</td>\n",
       "      <td>0.170657</td>\n",
       "      <td>0.490131</td>\n",
       "      <td>0.236828</td>\n",
       "      <td>0.480351</td>\n",
       "      <td>0.440880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.395000</td>\n",
       "      <td>0.481211</td>\n",
       "      <td>0.177209</td>\n",
       "      <td>0.498229</td>\n",
       "      <td>0.246129</td>\n",
       "      <td>0.485537</td>\n",
       "      <td>0.447034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.387800</td>\n",
       "      <td>0.480203</td>\n",
       "      <td>0.180397</td>\n",
       "      <td>0.495965</td>\n",
       "      <td>0.247697</td>\n",
       "      <td>0.486698</td>\n",
       "      <td>0.446693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4787222743034363, 'eval_bleu': 0.17243909861428214, 'eval_rouge1': 0.49321804168360456, 'eval_rouge2': 0.24059370319927537, 'eval_rougeL': 0.4801302646389882, 'eval_meteor': 0.44887556862862926, 'eval_runtime': 2.2903, 'eval_samples_per_second': 26.197, 'eval_steps_per_second': 3.493, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:00<00:00, 73930.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final predictions with inputs saved to predicted_clarif_T5SMALL_TOP5DOC_WNDB.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#WANDB\n",
    "train(\"t5-small\", \"predicted_clarif_T5SMALL_TOP5DOC_WNDB.jsonl\",\"T5-SMALL TOP5DOCS\",tokenizer, tokenized_train_dataset, tokenized_eval_datset,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bacabf-a93d-4b3f-a436-e0b6e88fcee6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# SE2SEQ T5 SMALL + QUERY ONLY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "71be7093-39a0-4467-a041-37995cbbb958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e30566a9464ab8ba536b58d624b7db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/539 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/Etu0/21402600/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb55014d045d47c19183ffcd5a0aa550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': \"I'm looking for information on Rick Warren, the evangelical minister.\", 'output': \"I'm looking for background and biographical information on Rick Warren.\", 'input_ids': [27, 31, 51, 479, 21, 251, 30, 11066, 16700, 6, 8, 30682, 6323, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [27, 31, 51, 479, 21, 2458, 11, 2392, 16982, 251, 30, 11066, 16700, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "tokenizer,tokenized_train_dataset, tokenized_eval_dataset= tokenize_and_split_dataset(\"training_queryonly_qulac_PREPROCESSED_FOR_MODEL.json\",\"t5-small\")\n",
    "print(tokenized_train_dataset[0])  # Optional: check one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d6592e79-3fea-4973-b97b-ff92459e1b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/users/nfs/Etu0/21402600/wandb/run-20250505_040903-ag9cok1h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/ag9cok1h' target=\"_blank\">T5 Small + Query ONLY</a></strong> to <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/ag9cok1h' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/ag9cok1h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1123707/4023069370.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1564' max='2040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1564/2040 24:03 < 07:19, 1.08 it/s, Epoch 23/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.926500</td>\n",
       "      <td>0.909643</td>\n",
       "      <td>0.110387</td>\n",
       "      <td>0.357508</td>\n",
       "      <td>0.179116</td>\n",
       "      <td>0.348935</td>\n",
       "      <td>0.296328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.184500</td>\n",
       "      <td>0.728811</td>\n",
       "      <td>0.120721</td>\n",
       "      <td>0.389918</td>\n",
       "      <td>0.193348</td>\n",
       "      <td>0.380868</td>\n",
       "      <td>0.332936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.716800</td>\n",
       "      <td>0.575246</td>\n",
       "      <td>0.151035</td>\n",
       "      <td>0.434860</td>\n",
       "      <td>0.223656</td>\n",
       "      <td>0.426650</td>\n",
       "      <td>0.393620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.608900</td>\n",
       "      <td>0.536134</td>\n",
       "      <td>0.153890</td>\n",
       "      <td>0.447716</td>\n",
       "      <td>0.224096</td>\n",
       "      <td>0.435802</td>\n",
       "      <td>0.406838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.567000</td>\n",
       "      <td>0.521996</td>\n",
       "      <td>0.156953</td>\n",
       "      <td>0.453647</td>\n",
       "      <td>0.224420</td>\n",
       "      <td>0.442497</td>\n",
       "      <td>0.411973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.520900</td>\n",
       "      <td>0.513575</td>\n",
       "      <td>0.161386</td>\n",
       "      <td>0.462814</td>\n",
       "      <td>0.233679</td>\n",
       "      <td>0.452933</td>\n",
       "      <td>0.419951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.519400</td>\n",
       "      <td>0.507583</td>\n",
       "      <td>0.164982</td>\n",
       "      <td>0.459551</td>\n",
       "      <td>0.236061</td>\n",
       "      <td>0.454089</td>\n",
       "      <td>0.420570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.483600</td>\n",
       "      <td>0.504109</td>\n",
       "      <td>0.158770</td>\n",
       "      <td>0.457573</td>\n",
       "      <td>0.223498</td>\n",
       "      <td>0.449212</td>\n",
       "      <td>0.415757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.485600</td>\n",
       "      <td>0.500099</td>\n",
       "      <td>0.160056</td>\n",
       "      <td>0.467163</td>\n",
       "      <td>0.233950</td>\n",
       "      <td>0.459600</td>\n",
       "      <td>0.427607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.462600</td>\n",
       "      <td>0.498216</td>\n",
       "      <td>0.159606</td>\n",
       "      <td>0.466377</td>\n",
       "      <td>0.235319</td>\n",
       "      <td>0.455765</td>\n",
       "      <td>0.424606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.480300</td>\n",
       "      <td>0.495615</td>\n",
       "      <td>0.154318</td>\n",
       "      <td>0.459450</td>\n",
       "      <td>0.226074</td>\n",
       "      <td>0.448600</td>\n",
       "      <td>0.417270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.442400</td>\n",
       "      <td>0.494839</td>\n",
       "      <td>0.158950</td>\n",
       "      <td>0.462530</td>\n",
       "      <td>0.230412</td>\n",
       "      <td>0.451076</td>\n",
       "      <td>0.421837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.494353</td>\n",
       "      <td>0.167586</td>\n",
       "      <td>0.464723</td>\n",
       "      <td>0.234480</td>\n",
       "      <td>0.455842</td>\n",
       "      <td>0.424495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.442400</td>\n",
       "      <td>0.492404</td>\n",
       "      <td>0.174801</td>\n",
       "      <td>0.470691</td>\n",
       "      <td>0.248502</td>\n",
       "      <td>0.461481</td>\n",
       "      <td>0.432741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.436400</td>\n",
       "      <td>0.492634</td>\n",
       "      <td>0.170652</td>\n",
       "      <td>0.471504</td>\n",
       "      <td>0.243543</td>\n",
       "      <td>0.461574</td>\n",
       "      <td>0.428834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.438500</td>\n",
       "      <td>0.491708</td>\n",
       "      <td>0.170457</td>\n",
       "      <td>0.469410</td>\n",
       "      <td>0.243543</td>\n",
       "      <td>0.459686</td>\n",
       "      <td>0.426735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.423400</td>\n",
       "      <td>0.491379</td>\n",
       "      <td>0.171088</td>\n",
       "      <td>0.477358</td>\n",
       "      <td>0.247597</td>\n",
       "      <td>0.464094</td>\n",
       "      <td>0.433112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.408500</td>\n",
       "      <td>0.490885</td>\n",
       "      <td>0.170794</td>\n",
       "      <td>0.476739</td>\n",
       "      <td>0.243785</td>\n",
       "      <td>0.463362</td>\n",
       "      <td>0.430803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.408100</td>\n",
       "      <td>0.490637</td>\n",
       "      <td>0.171940</td>\n",
       "      <td>0.478911</td>\n",
       "      <td>0.243040</td>\n",
       "      <td>0.463519</td>\n",
       "      <td>0.434919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.410500</td>\n",
       "      <td>0.489294</td>\n",
       "      <td>0.172863</td>\n",
       "      <td>0.481748</td>\n",
       "      <td>0.245706</td>\n",
       "      <td>0.466922</td>\n",
       "      <td>0.437927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.414600</td>\n",
       "      <td>0.489354</td>\n",
       "      <td>0.171179</td>\n",
       "      <td>0.477657</td>\n",
       "      <td>0.239604</td>\n",
       "      <td>0.463975</td>\n",
       "      <td>0.431657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.393800</td>\n",
       "      <td>0.490258</td>\n",
       "      <td>0.173347</td>\n",
       "      <td>0.485814</td>\n",
       "      <td>0.249448</td>\n",
       "      <td>0.469409</td>\n",
       "      <td>0.441746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.389800</td>\n",
       "      <td>0.489864</td>\n",
       "      <td>0.173205</td>\n",
       "      <td>0.483152</td>\n",
       "      <td>0.248342</td>\n",
       "      <td>0.467893</td>\n",
       "      <td>0.438052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.48929399251937866, 'eval_bleu': 0.17286262087859694, 'eval_rouge1': 0.48174823070272255, 'eval_rouge2': 0.24570588499476026, 'eval_rougeL': 0.4669216196461488, 'eval_meteor': 0.4379267959319926, 'eval_runtime': 1.9192, 'eval_samples_per_second': 31.263, 'eval_steps_per_second': 4.168, 'epoch': 23.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:00<00:00, 163096.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final predictions with inputs saved to predicted_clarif_T5SMALL_queryonly_WNDB.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/bleu</td><td>▁▂▅▆▆▇▇▆▆▆▆▆▇███████████</td></tr><tr><td>eval/loss</td><td>█▅▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/meteor</td><td>▁▃▆▆▇▇▇▇▇▇▇▇▇█▇▇█▇██████</td></tr><tr><td>eval/rouge1</td><td>▁▃▅▆▆▇▇▆▇▇▇▇▇▇▇▇████████</td></tr><tr><td>eval/rouge2</td><td>▁▂▅▅▆▆▇▅▆▇▆▆▇█▇▇█▇▇█▇███</td></tr><tr><td>eval/rougeL</td><td>▁▃▆▆▆▇▇▇▇▇▇▇▇██▇████████</td></tr><tr><td>eval/runtime</td><td>██████▂▁▁▁▂▁▁▁▂▁▂▂▁▁▁▃▁▂</td></tr><tr><td>eval/samples_per_second</td><td>▁▁▁▁▁▁▇▇▇█▇██▇▇█▇▇▇█▇▆▇▇</td></tr><tr><td>eval/steps_per_second</td><td>▁▁▁▁▁▁▇▇▇█▇███▇█▇▇▇█▇▆▇▇</td></tr><tr><td>test/bleu</td><td>▁</td></tr><tr><td>test/loss</td><td>▁</td></tr><tr><td>test/meteor</td><td>▁</td></tr><tr><td>test/rouge1</td><td>▁</td></tr><tr><td>test/rouge2</td><td>▁</td></tr><tr><td>test/rougeL</td><td>▁</td></tr><tr><td>test/runtime</td><td>▁</td></tr><tr><td>test/samples_per_second</td><td>▁</td></tr><tr><td>test/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>█▂▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/bleu</td><td>0.17286</td></tr><tr><td>eval/loss</td><td>0.48929</td></tr><tr><td>eval/meteor</td><td>0.43793</td></tr><tr><td>eval/rouge1</td><td>0.48175</td></tr><tr><td>eval/rouge2</td><td>0.24571</td></tr><tr><td>eval/rougeL</td><td>0.46692</td></tr><tr><td>eval/runtime</td><td>1.9192</td></tr><tr><td>eval/samples_per_second</td><td>31.263</td></tr><tr><td>eval/steps_per_second</td><td>4.168</td></tr><tr><td>test/bleu</td><td>0.17286</td></tr><tr><td>test/loss</td><td>0.48929</td></tr><tr><td>test/meteor</td><td>0.43793</td></tr><tr><td>test/rouge1</td><td>0.48175</td></tr><tr><td>test/rouge2</td><td>0.24571</td></tr><tr><td>test/rougeL</td><td>0.46692</td></tr><tr><td>test/runtime</td><td>1.901</td></tr><tr><td>test/samples_per_second</td><td>31.563</td></tr><tr><td>test/steps_per_second</td><td>4.208</td></tr><tr><td>total_flos</td><td>1677832312848384.0</td></tr><tr><td>train/epoch</td><td>23</td></tr><tr><td>train/global_step</td><td>1564</td></tr><tr><td>train/grad_norm</td><td>0.7764</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.3898</td></tr><tr><td>train_loss</td><td>0.64938</td></tr><tr><td>train_runtime</td><td>1444.352</td></tr><tr><td>train_samples_per_second</td><td>11.195</td></tr><tr><td>train_steps_per_second</td><td>1.412</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">T5 Small + Query ONLY</strong> at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/ag9cok1h' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/ag9cok1h</a><br> View project at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_040903-ag9cok1h/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#WANDB\n",
    "wandb.init(\n",
    "    project=\"T5-Clarifications\",  \n",
    "    name=\"T5 Small + Query ONLY\"\n",
    ")\n",
    "train(\"t5-small\", \"predicted_clarif_T5SMALL_queryonly_WNDB.jsonl\",tokenizer, tokenized_train_dataset, tokenized_eval_dataset,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126b02cf-3097-4e34-af38-192370f5f416",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# SEQ2SEQ T5 SMALL + TOP 5 SUMMARIZED DOCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3f2f2e89-78ea-4938-a3cd-807e7429df54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59f901984eff42c0a8fdb35ae3e84c36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/539 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c140e7ce1c87485e9939d59258b17124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': \"[QUERY] Find information about solar panels and companies that manufacture them.\\n[DOCUMENTS]\\n[DOC 1] Incentives are incentives for people who want to make money from incentives . The incentives for incentives are still relevant in the United States .\\n[DOC 2] Online Solar, Inc. & Mr.Solar.com offer solar panels . We know solar panels!\\n[DOC 3] A criminal defense attorney, a criminal defense lawyer and a home loaner, has been featured on this blog . The site is designed by Bugdugle Blog Design .\\n[DOC 4] Xantrex XW Solar Charge Controller MPPT 865-1030Xantrex Cm/R-50 Digitalmeter-C12/35/40/60-50'30' MC1 Connector #10 AWG Male/Female .\\n[DOC 5] Solar Technology Company (IST) has developed a concentrator system that incorporates the reflective surface as part of the collector structure . This results in a very lightweight, low-cost concentrator module that is also very strong .\", 'output': 'Find information about nano-technological solar power.', 'input_ids': [784, 15367, 11824, 908, 2588, 251, 81, 3693, 7065, 11, 688, 24, 9421, 135, 5, 784, 9857, 5211, 11810, 134, 908, 784, 26472, 209, 908, 86, 3728, 8763, 33, 16844, 21, 151, 113, 241, 12, 143, 540, 45, 16844, 3, 5, 37, 16844, 21, 16844, 33, 341, 2193, 16, 8, 907, 1323, 3, 5, 784, 26472, 204, 908, 1777, 9334, 6, 1542, 5, 3, 184, 1363, 5, 134, 17401, 5, 287, 462, 3693, 7065, 3, 5, 101, 214, 3693, 7065, 55, 784, 26472, 220, 908, 71, 4336, 4453, 4917, 6, 3, 9, 4336, 4453, 6297, 11, 3, 9, 234, 2289, 49, 6, 65, 118, 4510, 30, 48, 875, 3, 5, 37, 353, 19, 876, 57, 16381, 1259, 3537, 5039, 1642, 3, 5, 784, 26472, 314, 908, 3, 4, 288, 60, 226, 3, 4, 518, 9334, 15907, 23105, 5220, 6383, 505, 4122, 4536, 1458, 4, 288, 60, 226, 205, 51, 87, 448, 19431, 4190, 4401, 18, 254, 2122, 87, 2469, 87, 2445, 87, 3328, 19431, 31, 1458, 31, 3, 3698, 536, 7878, 127, 1713, 1714, 71, 22202, 17830, 87, 371, 15, 13513, 3, 5, 784, 26472, 305, 908, 9334, 3669, 1958, 41, 13582, 61, 65, 1597, 3, 9, 28038, 127, 358, 24, 6300, 7, 8, 22891, 1774, 38, 294, 13, 8, 12501, 1809, 3, 5, 100, 772, 16, 3, 9, 182, 10182, 6, 731, 18, 11290, 28038, 127, 6008, 24, 19, 92, 182, 1101, 3, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [2588, 251, 81, 13944, 18, 3470, 29, 4478, 3693, 579, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "tokenizer,tokenized_train_dataset, tokenized_eval_dataset= tokenize_and_split_dataset(\"training_top5_qulac_SUMMARIZED_FOR_MODEL.json\",\"t5-small\")\n",
    "print(tokenized_train_dataset[0])  # Optional: check one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f3c0dd92-2b9e-4c87-a117-d3b7c80361c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/users/nfs/Etu0/21402600/wandb/run-20250505_043318-aan9abky</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/aan9abky' target=\"_blank\">T5 SMALL + TO5 DOCS SUMMARIZED</a></strong> to <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/aan9abky' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/aan9abky</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1123707/4023069370.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1088' max='2040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1088/2040 15:58 < 14:00, 1.13 it/s, Epoch 16/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.934200</td>\n",
       "      <td>0.742373</td>\n",
       "      <td>0.124662</td>\n",
       "      <td>0.386160</td>\n",
       "      <td>0.183804</td>\n",
       "      <td>0.374544</td>\n",
       "      <td>0.311506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.977200</td>\n",
       "      <td>0.523263</td>\n",
       "      <td>0.181451</td>\n",
       "      <td>0.448648</td>\n",
       "      <td>0.241939</td>\n",
       "      <td>0.443039</td>\n",
       "      <td>0.399988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.551300</td>\n",
       "      <td>0.465223</td>\n",
       "      <td>0.189024</td>\n",
       "      <td>0.471199</td>\n",
       "      <td>0.237287</td>\n",
       "      <td>0.468122</td>\n",
       "      <td>0.441595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.523500</td>\n",
       "      <td>0.450799</td>\n",
       "      <td>0.199623</td>\n",
       "      <td>0.482653</td>\n",
       "      <td>0.245469</td>\n",
       "      <td>0.477889</td>\n",
       "      <td>0.452465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.506200</td>\n",
       "      <td>0.444649</td>\n",
       "      <td>0.197712</td>\n",
       "      <td>0.483458</td>\n",
       "      <td>0.248663</td>\n",
       "      <td>0.480008</td>\n",
       "      <td>0.452078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.482700</td>\n",
       "      <td>0.435452</td>\n",
       "      <td>0.205548</td>\n",
       "      <td>0.496380</td>\n",
       "      <td>0.261015</td>\n",
       "      <td>0.491287</td>\n",
       "      <td>0.461446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.438000</td>\n",
       "      <td>0.432831</td>\n",
       "      <td>0.219565</td>\n",
       "      <td>0.510013</td>\n",
       "      <td>0.267423</td>\n",
       "      <td>0.504820</td>\n",
       "      <td>0.475660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.446300</td>\n",
       "      <td>0.431656</td>\n",
       "      <td>0.226739</td>\n",
       "      <td>0.513442</td>\n",
       "      <td>0.271191</td>\n",
       "      <td>0.508927</td>\n",
       "      <td>0.472623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.431700</td>\n",
       "      <td>0.428373</td>\n",
       "      <td>0.233349</td>\n",
       "      <td>0.517839</td>\n",
       "      <td>0.272771</td>\n",
       "      <td>0.512893</td>\n",
       "      <td>0.478949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.428700</td>\n",
       "      <td>0.426955</td>\n",
       "      <td>0.236857</td>\n",
       "      <td>0.522491</td>\n",
       "      <td>0.278750</td>\n",
       "      <td>0.516506</td>\n",
       "      <td>0.480049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.414800</td>\n",
       "      <td>0.426569</td>\n",
       "      <td>0.234319</td>\n",
       "      <td>0.520185</td>\n",
       "      <td>0.267510</td>\n",
       "      <td>0.513151</td>\n",
       "      <td>0.478572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.401600</td>\n",
       "      <td>0.425577</td>\n",
       "      <td>0.238488</td>\n",
       "      <td>0.524881</td>\n",
       "      <td>0.281168</td>\n",
       "      <td>0.519983</td>\n",
       "      <td>0.482447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.397300</td>\n",
       "      <td>0.424112</td>\n",
       "      <td>0.225735</td>\n",
       "      <td>0.518514</td>\n",
       "      <td>0.266528</td>\n",
       "      <td>0.510256</td>\n",
       "      <td>0.477252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.393400</td>\n",
       "      <td>0.424642</td>\n",
       "      <td>0.231405</td>\n",
       "      <td>0.521158</td>\n",
       "      <td>0.274260</td>\n",
       "      <td>0.514539</td>\n",
       "      <td>0.482135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.372400</td>\n",
       "      <td>0.424577</td>\n",
       "      <td>0.231847</td>\n",
       "      <td>0.520942</td>\n",
       "      <td>0.270108</td>\n",
       "      <td>0.514763</td>\n",
       "      <td>0.487080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.383800</td>\n",
       "      <td>0.425438</td>\n",
       "      <td>0.233672</td>\n",
       "      <td>0.524631</td>\n",
       "      <td>0.276647</td>\n",
       "      <td>0.518743</td>\n",
       "      <td>0.491517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.42411214113235474, 'eval_bleu': 0.22573487255751198, 'eval_rouge1': 0.5185140104023928, 'eval_rouge2': 0.2665282907380432, 'eval_rougeL': 0.5102561977502414, 'eval_meteor': 0.47725188098889065, 'eval_runtime': 1.9468, 'eval_samples_per_second': 30.819, 'eval_steps_per_second': 4.109, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:00<00:00, 74411.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final predictions with inputs saved to predicted_clarif_T5SMALL_TOP5DOC_SUMMARIZED.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/bleu</td><td>▁▄▅▆▅▆▇▇████▇███▇</td></tr><tr><td>eval/loss</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/meteor</td><td>▁▄▆▆▆▇▇▇██▇█▇███▇</td></tr><tr><td>eval/rouge1</td><td>▁▄▅▆▆▇▇▇█████████</td></tr><tr><td>eval/rouge2</td><td>▁▅▅▅▆▇▇▇▇█▇█▇█▇█▇</td></tr><tr><td>eval/rougeL</td><td>▁▄▆▆▆▇▇▇█████████</td></tr><tr><td>eval/runtime</td><td>▅▅▅▅▆▅▅▅▆▅▄▂▂█▁▂▅</td></tr><tr><td>eval/samples_per_second</td><td>▄▄▄▄▃▄▄▄▃▄▅▇▇▁█▇▄</td></tr><tr><td>eval/steps_per_second</td><td>▃▄▄▄▃▄▄▄▃▄▅▇▇▁█▇▄</td></tr><tr><td>test/bleu</td><td>▁</td></tr><tr><td>test/loss</td><td>▁</td></tr><tr><td>test/meteor</td><td>▁</td></tr><tr><td>test/rouge1</td><td>▁</td></tr><tr><td>test/rouge2</td><td>▁</td></tr><tr><td>test/rougeL</td><td>▁</td></tr><tr><td>test/runtime</td><td>▁</td></tr><tr><td>test/samples_per_second</td><td>▁</td></tr><tr><td>test/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train/grad_norm</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▆▅▅▅▄▄▄▃▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/bleu</td><td>0.22573</td></tr><tr><td>eval/loss</td><td>0.42411</td></tr><tr><td>eval/meteor</td><td>0.47725</td></tr><tr><td>eval/rouge1</td><td>0.51851</td></tr><tr><td>eval/rouge2</td><td>0.26653</td></tr><tr><td>eval/rougeL</td><td>0.51026</td></tr><tr><td>eval/runtime</td><td>1.9468</td></tr><tr><td>eval/samples_per_second</td><td>30.819</td></tr><tr><td>eval/steps_per_second</td><td>4.109</td></tr><tr><td>test/bleu</td><td>0.22573</td></tr><tr><td>test/loss</td><td>0.42411</td></tr><tr><td>test/meteor</td><td>0.47725</td></tr><tr><td>test/rouge1</td><td>0.51851</td></tr><tr><td>test/rouge2</td><td>0.26653</td></tr><tr><td>test/rougeL</td><td>0.51026</td></tr><tr><td>test/runtime</td><td>1.9205</td></tr><tr><td>test/samples_per_second</td><td>31.243</td></tr><tr><td>test/steps_per_second</td><td>4.166</td></tr><tr><td>total_flos</td><td>1167187695894528.0</td></tr><tr><td>train/epoch</td><td>16</td></tr><tr><td>train/global_step</td><td>1088</td></tr><tr><td>train/grad_norm</td><td>0.69069</td></tr><tr><td>train/learning_rate</td><td>2e-05</td></tr><tr><td>train/loss</td><td>0.3838</td></tr><tr><td>train_loss</td><td>0.73868</td></tr><tr><td>train_runtime</td><td>959.3204</td></tr><tr><td>train_samples_per_second</td><td>16.856</td></tr><tr><td>train_steps_per_second</td><td>2.127</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">T5 SMALL + TO5 DOCS SUMMARIZED</strong> at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/aan9abky' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/aan9abky</a><br> View project at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_043318-aan9abky/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"T5-Clarifications\",  \n",
    "    name=\"T5 SMALL + TO5 DOCS SUMMARIZED\"\n",
    ")\n",
    "train(\"t5-small\", \"predicted_clarif_T5SMALL_TOP5DOC_SUMMARIZED.jsonl\",tokenizer, tokenized_train_dataset, tokenized_eval_dataset,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9100e5-1fac-414c-b14a-e7a45bf5ae7a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# SEQ2SEQ T5 BASE + TOP 5 SUMMARIZED DOCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "80e4cb5f-d534-44d3-b921-4fe1f0fec38f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df71340afd214605895691bb9e241bff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/539 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/Etu0/21402600/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "080201eec45c4306b1e8b39fb72e1012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': \"[QUERY] I'm looking for information on Rick Warren, the evangelical minister.\\n[DOCUMENTS]\\n[DOC 1] Elizabeth Alexander is an African-American, born in Harlem in 1962, who has published four books . Her work speaks about black experience, but her poems work more at being complex than didactic . Her best poems are imaginatively expansive as well as philosophical .\\n[DOC 2] Presenting Warren front and centre as part of the first official act of the Obama Administration is not just wrong, it is also stupid . For a world longing for the end of extra legal policies that Bush pursued .\\n[DOC 3] The Rev. Moon is invited to introduce Barack Obama to god on January 20 . Atheists say Obama is inviting Warren not because he supports his kind of theocracy, but because he doesn't .\\n[DOC 4] Blogs include: A Guy in the Pew, A Lie A Day, A Religious Liberal Blog, A Blog of the Greateful Bear, The Rev. Rev. Susan Brooks Thistlethwaite, The Revealer, Theolog: The Blog of The Christian Century .\\n[DOC 5] PE Obama’s selection of Rick Warren to give the invocation is called “Leading by example”\", 'output': \"I'm looking for background and biographical information on Rick Warren.\", 'input_ids': [784, 15367, 11824, 908, 27, 31, 51, 479, 21, 251, 30, 11066, 16700, 6, 8, 30682, 6323, 5, 784, 9857, 5211, 11810, 134, 908, 784, 26472, 209, 908, 9066, 10135, 19, 46, 3850, 18, 8778, 6, 2170, 16, 3504, 109, 51, 16, 20236, 6, 113, 65, 1790, 662, 1335, 3, 5, 1347, 161, 12192, 81, 1001, 351, 6, 68, 160, 18460, 161, 72, 44, 271, 1561, 145, 22994, 3, 5, 1347, 200, 18460, 33, 26443, 120, 23303, 38, 168, 38, 22466, 3, 5, 784, 26472, 204, 908, 18795, 53, 16700, 851, 11, 2050, 38, 294, 13, 8, 166, 2314, 1810, 13, 8, 4534, 6863, 19, 59, 131, 1786, 6, 34, 19, 92, 13721, 3, 5, 242, 3, 9, 296, 307, 53, 21, 8, 414, 13, 996, 1281, 3101, 24, 8905, 6665, 26, 3, 5, 784, 26472, 220, 908, 37, 6342, 5, 9023, 19, 5374, 12, 4277, 20653, 4534, 12, 8581, 30, 1762, 460, 3, 5, 71, 532, 343, 7, 497, 4534, 19, 14256, 16700, 59, 250, 3, 88, 4951, 112, 773, 13, 8, 32, 2935, 75, 63, 6, 68, 250, 3, 88, 744, 31, 17, 3, 5, 784, 26472, 314, 908, 5039, 7, 560, 10, 71, 14507, 16, 8, 1276, 210, 6, 71, 3, 19079, 71, 1430, 6, 71, 28789, 18587, 5039, 6, 71, 5039, 13, 8, 1651, 15, 1329, 9034, 6, 37, 6342, 5, 6342, 5, 10445, 9083, 7, 100, 17, 109, 189, 26745, 15, 6, 37, 6342, 15, 9, 1171, 6, 37, 32, 2152, 10, 37, 5039, 13, 37, 2826, 12336, 3, 5, 784, 26472, 305, 908, 11012, 4534, 22, 7, 1801, 13, 11066, 16700, 12, 428, 8, 16, 15044, 19, 718, 105, 2796, 9, 26, 53, 57, 677, 153, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [27, 31, 51, 479, 21, 2458, 11, 2392, 16982, 251, 30, 11066, 16700, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "tokenizer,tokenized_train_dataset, tokenized_eval_dataset= tokenize_and_split_dataset(\"training_top5_qulac_SUMMARIZED_FOR_MODEL.json\",\"t5-base\")\n",
    "print(tokenized_train_dataset[0])  # Optional: check one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "98e74264-15e1-4242-9308-d1761d562a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">T5 BASE + TO5 DOCS SUMMARIZED</strong> at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/38akjybr' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/38akjybr</a><br> View project at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_044927-38akjybr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/users/nfs/Etu0/21402600/wandb/run-20250505_045253-ito9s66s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/ito9s66s' target=\"_blank\">T5 BASE + TO5 DOCS SUMMARIZED</a></strong> to <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/ito9s66s' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/ito9s66s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0d79472cc79476c86d8d7d9f802dd9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tf_model.h5:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f17affa3d714acf8676d0832eeca134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1123707/4023069370.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='476' max='2040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 476/2040 24:01 < 1:19:15, 0.33 it/s, Epoch 7/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.482500</td>\n",
       "      <td>0.445160</td>\n",
       "      <td>0.201440</td>\n",
       "      <td>0.492116</td>\n",
       "      <td>0.263184</td>\n",
       "      <td>0.485924</td>\n",
       "      <td>0.454750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.490100</td>\n",
       "      <td>0.386635</td>\n",
       "      <td>0.220864</td>\n",
       "      <td>0.535610</td>\n",
       "      <td>0.299028</td>\n",
       "      <td>0.523957</td>\n",
       "      <td>0.499894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.381200</td>\n",
       "      <td>0.380820</td>\n",
       "      <td>0.225631</td>\n",
       "      <td>0.534435</td>\n",
       "      <td>0.300286</td>\n",
       "      <td>0.524182</td>\n",
       "      <td>0.501660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.341100</td>\n",
       "      <td>0.372675</td>\n",
       "      <td>0.223258</td>\n",
       "      <td>0.541405</td>\n",
       "      <td>0.290740</td>\n",
       "      <td>0.528428</td>\n",
       "      <td>0.505760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.321100</td>\n",
       "      <td>0.376765</td>\n",
       "      <td>0.235075</td>\n",
       "      <td>0.537244</td>\n",
       "      <td>0.293451</td>\n",
       "      <td>0.522374</td>\n",
       "      <td>0.508591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.279500</td>\n",
       "      <td>0.383681</td>\n",
       "      <td>0.235180</td>\n",
       "      <td>0.529471</td>\n",
       "      <td>0.287502</td>\n",
       "      <td>0.515008</td>\n",
       "      <td>0.503549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.271100</td>\n",
       "      <td>0.392567</td>\n",
       "      <td>0.226530</td>\n",
       "      <td>0.517833</td>\n",
       "      <td>0.275241</td>\n",
       "      <td>0.504018</td>\n",
       "      <td>0.490656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3726748824119568, 'eval_bleu': 0.22325809117239986, 'eval_rouge1': 0.5414049646830954, 'eval_rouge2': 0.2907402855429171, 'eval_rougeL': 0.5284277816775549, 'eval_meteor': 0.5057596460391415, 'eval_runtime': 5.6346, 'eval_samples_per_second': 10.648, 'eval_steps_per_second': 1.42, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:00<00:00, 69060.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final predictions with inputs saved to predicted_clarif_T5BASE_TOP5DOC_SUMMARIZED.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/bleu</td><td>▁▅▆▆██▆▆</td></tr><tr><td>eval/loss</td><td>█▂▂▁▁▂▃▁</td></tr><tr><td>eval/meteor</td><td>▁▇▇██▇▆█</td></tr><tr><td>eval/rouge1</td><td>▁▇▇█▇▆▅█</td></tr><tr><td>eval/rouge2</td><td>▁██▆▇▆▃▆</td></tr><tr><td>eval/rougeL</td><td>▁▇▇█▇▆▄█</td></tr><tr><td>eval/runtime</td><td>▂▅▃▃▁▇▂█</td></tr><tr><td>eval/samples_per_second</td><td>▇▄▆▆█▂▇▁</td></tr><tr><td>eval/steps_per_second</td><td>▇▃▆▆█▂▇▁</td></tr><tr><td>test/bleu</td><td>▁</td></tr><tr><td>test/loss</td><td>▁</td></tr><tr><td>test/meteor</td><td>▁</td></tr><tr><td>test/rouge1</td><td>▁</td></tr><tr><td>test/rouge2</td><td>▁</td></tr><tr><td>test/rougeL</td><td>▁</td></tr><tr><td>test/runtime</td><td>▁</td></tr><tr><td>test/samples_per_second</td><td>▁</td></tr><tr><td>test/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▂▂▂▃▃▄▄▅▅▅▆▆▇▇████</td></tr><tr><td>train/global_step</td><td>▁▂▂▂▃▃▄▄▅▅▅▆▆▇▇█████</td></tr><tr><td>train/grad_norm</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>█▇▆▆▅▄▃▃▂▁</td></tr><tr><td>train/loss</td><td>█▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/bleu</td><td>0.22326</td></tr><tr><td>eval/loss</td><td>0.37267</td></tr><tr><td>eval/meteor</td><td>0.50576</td></tr><tr><td>eval/rouge1</td><td>0.5414</td></tr><tr><td>eval/rouge2</td><td>0.29074</td></tr><tr><td>eval/rougeL</td><td>0.52843</td></tr><tr><td>eval/runtime</td><td>5.6346</td></tr><tr><td>eval/samples_per_second</td><td>10.648</td></tr><tr><td>eval/steps_per_second</td><td>1.42</td></tr><tr><td>test/bleu</td><td>0.22326</td></tr><tr><td>test/loss</td><td>0.37267</td></tr><tr><td>test/meteor</td><td>0.50576</td></tr><tr><td>test/rouge1</td><td>0.5414</td></tr><tr><td>test/rouge2</td><td>0.29074</td></tr><tr><td>test/rougeL</td><td>0.52843</td></tr><tr><td>test/runtime</td><td>5.4418</td></tr><tr><td>test/samples_per_second</td><td>11.026</td></tr><tr><td>test/steps_per_second</td><td>1.47</td></tr><tr><td>total_flos</td><td>2297598121082880.0</td></tr><tr><td>train/epoch</td><td>7</td></tr><tr><td>train/global_step</td><td>476</td></tr><tr><td>train/grad_norm</td><td>0.91392</td></tr><tr><td>train/learning_rate</td><td>4e-05</td></tr><tr><td>train/loss</td><td>0.2711</td></tr><tr><td>train_loss</td><td>0.59241</td></tr><tr><td>train_runtime</td><td>1443.9383</td></tr><tr><td>train_samples_per_second</td><td>11.199</td></tr><tr><td>train_steps_per_second</td><td>1.413</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">T5 BASE + TO5 DOCS SUMMARIZED</strong> at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/ito9s66s' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/ito9s66s</a><br> View project at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_045253-ito9s66s/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"T5-Clarifications\",  \n",
    "    name=\"T5 BASE + TO5 DOCS SUMMARIZED\"\n",
    ")\n",
    "train(\"t5-base\", \"predicted_clarif_T5BASE_TOP5DOC_SUMMARIZED.jsonl\",tokenizer, tokenized_train_dataset, tokenized_eval_dataset,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eca462a-5e6f-41be-8100-eafe58c78bef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# TFIDF DOC REP + T5 SMALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cfeae9d6-f43b-4b80-8d10-ed628d3474df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b304f1b357854c21ba4631ccc4e85b60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/539 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/Etu0/21402600/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5875c2fd2164d06b5bc46e261a8cc59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': \"[QUERY] I'm looking for information on Rick Warren, the evangelical minister.\\n[DOCUMENTS]\\n[DOC 1] obama warren inauguration rick nfiled barack prayer wright nposted jeremiah u0014and u0019s alexander dahlia u0019t gay church inaugural hitchens hanna\\n[DOC 2] obama warren pmreply lgbt bilerico rick comment nbil december ni community nwe ve hate browning bil inauguration 2008 say prayer\\n[DOC 3] obama u0014 nwed nlogin reply warren register rick 12 17 2008 barack hearts nre nthu 14 progressives progressive angelfoodmcspade invocation\\n[DOC 4] warren rick inauguration obama rev currie nucc church wrong voice christ progressive blog religious ucc chuck portland nfaith parkrose oregon\\n[DOC 5] 17th et pm obama 2008 warren gay invocation rick evangelical people ni nobama god choice president nrick bible barack evangelicals\", 'output': \"I'm looking for background and biographical information on Rick Warren.\", 'input_ids': [784, 15367, 11824, 908, 27, 31, 51, 479, 21, 251, 30, 11066, 16700, 6, 8, 30682, 6323, 5, 784, 9857, 5211, 11810, 134, 908, 784, 26472, 209, 908, 3, 32, 115, 265, 9, 615, 1536, 3, 30634, 3, 5206, 3, 29, 11966, 26, 1207, 4365, 7029, 3, 210, 3535, 3, 29, 2748, 1054, 3, 12488, 11658, 107, 3, 76, 1206, 2534, 232, 3, 76, 1206, 2294, 7, 1240, 226, 11849, 836, 107, 40, 23, 9, 3, 76, 1206, 2294, 17, 16998, 2078, 22145, 1560, 1559, 7, 3, 107, 10878, 784, 26472, 204, 908, 3, 32, 115, 265, 9, 615, 1536, 6366, 60, 102, 120, 3, 40, 122, 115, 17, 3, 3727, 49, 5807, 3, 5206, 1670, 3, 29, 3727, 20, 75, 18247, 3, 29, 23, 573, 3, 29, 1123, 3, 162, 5591, 4216, 53, 3, 3727, 3, 30634, 2628, 497, 7029, 784, 26472, 220, 908, 3, 32, 115, 265, 9, 3, 76, 1206, 2534, 3, 29, 1123, 26, 3, 29, 2152, 77, 8776, 615, 1536, 3691, 3, 5206, 586, 1003, 2628, 1207, 4365, 9812, 3, 29, 60, 3, 29, 189, 76, 968, 9018, 7, 9018, 11831, 12437, 51, 75, 7, 5612, 15, 16, 15044, 784, 26472, 314, 908, 615, 1536, 3, 5206, 3, 30634, 3, 32, 115, 265, 9, 5109, 5495, 1753, 3, 29, 17431, 2078, 1786, 2249, 3, 15294, 9018, 875, 4761, 3, 17431, 3, 24238, 2147, 40, 232, 3, 29, 10699, 107, 2447, 8115, 42, 15, 5307, 784, 26472, 305, 908, 1003, 189, 3, 15, 17, 6366, 3, 32, 115, 265, 9, 2628, 615, 1536, 16998, 16, 15044, 3, 5206, 30682, 151, 3, 29, 23, 150, 115, 265, 9, 8581, 1160, 2753, 3, 29, 5206, 26996, 1207, 4365, 30682, 7, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [27, 31, 51, 479, 21, 2458, 11, 2392, 16982, 251, 30, 11066, 16700, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "tokenizer,tokenized_train_dataset, tokenized_eval_dataset= tokenize_and_split_dataset(\"training_top5_qulac_PREPROCESSED_FOR_MODEL_TFIDF.json\",\"t5-small\")\n",
    "print(tokenized_train_dataset[0])  # Optional: check one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5523dbab-6d11-490a-aa55-8af5649b42f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/users/nfs/Etu0/21402600/wandb/run-20250506_204354-9vhl845z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/9vhl845z' target=\"_blank\">T5 SMALL + TO5 DOCS TFIDF v2. </a></strong> to <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/9vhl845z' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/9vhl845z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1389011/287362180.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='952' max='2720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 952/2720 17:10 < 31:58, 0.92 it/s, Epoch 14/40]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.430100</td>\n",
       "      <td>0.748300</td>\n",
       "      <td>0.126248</td>\n",
       "      <td>0.397713</td>\n",
       "      <td>0.194673</td>\n",
       "      <td>0.388640</td>\n",
       "      <td>0.340054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.908400</td>\n",
       "      <td>0.568506</td>\n",
       "      <td>0.146624</td>\n",
       "      <td>0.430685</td>\n",
       "      <td>0.214592</td>\n",
       "      <td>0.423728</td>\n",
       "      <td>0.378188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.598700</td>\n",
       "      <td>0.517328</td>\n",
       "      <td>0.163846</td>\n",
       "      <td>0.460022</td>\n",
       "      <td>0.229027</td>\n",
       "      <td>0.450214</td>\n",
       "      <td>0.416359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.532000</td>\n",
       "      <td>0.501637</td>\n",
       "      <td>0.163275</td>\n",
       "      <td>0.463841</td>\n",
       "      <td>0.227477</td>\n",
       "      <td>0.453015</td>\n",
       "      <td>0.416833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.508400</td>\n",
       "      <td>0.493275</td>\n",
       "      <td>0.169298</td>\n",
       "      <td>0.462386</td>\n",
       "      <td>0.233447</td>\n",
       "      <td>0.455440</td>\n",
       "      <td>0.417341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.466000</td>\n",
       "      <td>0.486814</td>\n",
       "      <td>0.178139</td>\n",
       "      <td>0.473974</td>\n",
       "      <td>0.240766</td>\n",
       "      <td>0.466193</td>\n",
       "      <td>0.433736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.469000</td>\n",
       "      <td>0.481031</td>\n",
       "      <td>0.180324</td>\n",
       "      <td>0.480001</td>\n",
       "      <td>0.259340</td>\n",
       "      <td>0.474954</td>\n",
       "      <td>0.443831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.436300</td>\n",
       "      <td>0.480974</td>\n",
       "      <td>0.190290</td>\n",
       "      <td>0.487991</td>\n",
       "      <td>0.264242</td>\n",
       "      <td>0.481337</td>\n",
       "      <td>0.448620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.436600</td>\n",
       "      <td>0.478462</td>\n",
       "      <td>0.187314</td>\n",
       "      <td>0.490617</td>\n",
       "      <td>0.263446</td>\n",
       "      <td>0.482397</td>\n",
       "      <td>0.452956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.416800</td>\n",
       "      <td>0.476689</td>\n",
       "      <td>0.180338</td>\n",
       "      <td>0.483891</td>\n",
       "      <td>0.257500</td>\n",
       "      <td>0.476725</td>\n",
       "      <td>0.449168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.427500</td>\n",
       "      <td>0.475296</td>\n",
       "      <td>0.187371</td>\n",
       "      <td>0.494812</td>\n",
       "      <td>0.263929</td>\n",
       "      <td>0.484346</td>\n",
       "      <td>0.459091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.389800</td>\n",
       "      <td>0.476161</td>\n",
       "      <td>0.194889</td>\n",
       "      <td>0.498244</td>\n",
       "      <td>0.268623</td>\n",
       "      <td>0.485364</td>\n",
       "      <td>0.460796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.398300</td>\n",
       "      <td>0.476301</td>\n",
       "      <td>0.185981</td>\n",
       "      <td>0.494078</td>\n",
       "      <td>0.254988</td>\n",
       "      <td>0.480204</td>\n",
       "      <td>0.453289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.388200</td>\n",
       "      <td>0.475467</td>\n",
       "      <td>0.194723</td>\n",
       "      <td>0.496102</td>\n",
       "      <td>0.264462</td>\n",
       "      <td>0.480892</td>\n",
       "      <td>0.460875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.47529616951942444, 'eval_bleu': 0.18737064277859514, 'eval_rouge1': 0.49481248246904985, 'eval_rouge2': 0.26392867554129157, 'eval_rougeL': 0.4843460519697246, 'eval_meteor': 0.459091339323111, 'eval_runtime': 2.0702, 'eval_samples_per_second': 28.982, 'eval_steps_per_second': 3.864, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:00<00:00, 98419.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final predictions with inputs saved to predicted_clarif_T5SMALL_TOP5DOC_TFIDF_v2.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/bleu</td><td>▁▃▅▅▅▆▇█▇▇▇█▇█▇</td></tr><tr><td>eval/loss</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/meteor</td><td>▁▃▅▅▅▆▇▇█▇█████</td></tr><tr><td>eval/rouge1</td><td>▁▃▅▆▆▆▇▇▇▇█████</td></tr><tr><td>eval/rouge2</td><td>▁▃▄▄▅▅▇██▇██▇██</td></tr><tr><td>eval/rougeL</td><td>▁▄▅▆▆▇▇██▇█████</td></tr><tr><td>eval/runtime</td><td>▁▂▂▃▅▆▆▅▅█▄▅▇▄▄</td></tr><tr><td>eval/samples_per_second</td><td>█▇▇▆▄▃▃▄▄▁▅▄▂▅▅</td></tr><tr><td>eval/steps_per_second</td><td>█▇▇▆▄▃▃▄▄▁▅▄▂▅▅</td></tr><tr><td>test/bleu</td><td>▁</td></tr><tr><td>test/loss</td><td>▁</td></tr><tr><td>test/meteor</td><td>▁</td></tr><tr><td>test/rouge1</td><td>▁</td></tr><tr><td>test/rouge2</td><td>▁</td></tr><tr><td>test/rougeL</td><td>▁</td></tr><tr><td>test/runtime</td><td>▁</td></tr><tr><td>test/samples_per_second</td><td>▁</td></tr><tr><td>test/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇██████</td></tr><tr><td>train/grad_norm</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/bleu</td><td>0.18737</td></tr><tr><td>eval/loss</td><td>0.4753</td></tr><tr><td>eval/meteor</td><td>0.45909</td></tr><tr><td>eval/rouge1</td><td>0.49481</td></tr><tr><td>eval/rouge2</td><td>0.26393</td></tr><tr><td>eval/rougeL</td><td>0.48435</td></tr><tr><td>eval/runtime</td><td>2.0702</td></tr><tr><td>eval/samples_per_second</td><td>28.982</td></tr><tr><td>eval/steps_per_second</td><td>3.864</td></tr><tr><td>test/bleu</td><td>0.18737</td></tr><tr><td>test/loss</td><td>0.4753</td></tr><tr><td>test/meteor</td><td>0.45909</td></tr><tr><td>test/rouge1</td><td>0.49481</td></tr><tr><td>test/rouge2</td><td>0.26393</td></tr><tr><td>test/rougeL</td><td>0.48435</td></tr><tr><td>test/runtime</td><td>2.0582</td></tr><tr><td>test/samples_per_second</td><td>29.152</td></tr><tr><td>test/steps_per_second</td><td>3.887</td></tr><tr><td>total_flos</td><td>1021289233907712.0</td></tr><tr><td>train/epoch</td><td>14</td></tr><tr><td>train/global_step</td><td>952</td></tr><tr><td>train/grad_norm</td><td>1.02336</td></tr><tr><td>train/learning_rate</td><td>3e-05</td></tr><tr><td>train/loss</td><td>0.3882</td></tr><tr><td>train_loss</td><td>0.76336</td></tr><tr><td>train_runtime</td><td>1031.9254</td></tr><tr><td>train_samples_per_second</td><td>20.893</td></tr><tr><td>train_steps_per_second</td><td>2.636</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">T5 SMALL + TO5 DOCS TFIDF v2. </strong> at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/9vhl845z' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/9vhl845z</a><br> View project at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250506_204354-9vhl845z/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"T5-Clarifications\",  \n",
    "    name=\"T5 SMALL + TO5 DOCS TFIDF v2. \"                   \n",
    ")\n",
    "train(\"t5-small\", \"predicted_clarif_T5SMALL_TOP5DOC_TFIDF_v2.jsonl\",tokenizer, tokenized_train_dataset, tokenized_eval_dataset,40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b434a933-456f-48b8-9126-b46bf7de3136",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# TFIDF AVEC 20 DOCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4836d52e-12a0-450e-8153-7f0b38b30da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d24566fa0584973a6f49a17dd2b27b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/539 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/Etu0/21402600/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1589cd0b68af4a7aa7acc6df0a9d7a37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': \"[QUERY] I'm looking for information on Rick Warren, the evangelical minister.\\n[DOCUMENTS]\\n[DOC 1] obama warren inauguration rick nfiled barack prayer wright nposted jeremiah u0014and u0019s alexander dahlia u0019t gay church inaugural hitchens hanna\\n[DOC 2] obama warren pmreply lgbt bilerico rick comment nbil december ni community nwe ve hate browning bil inauguration 2008 say prayer\\n[DOC 3] obama u0014 nwed nlogin reply warren register rick 12 17 2008 barack hearts nre nthu 14 progressives progressive angelfoodmcspade invocation\\n[DOC 4] warren rick inauguration obama rev currie nucc church wrong voice christ progressive blog religious ucc chuck portland nfaith parkrose oregon\\n[DOC 5] 17th et pm obama 2008 warren gay invocation rick evangelical people ni nobama god choice president nrick bible barack evangelicals\\n[DOC 6] obama flag warren dec reply nposted gay marriage civil nno roosevelt 23 gays rights washington 2008 japhy pm unions lgbt\\n[DOC 7] warren church cri rick driven ditc churches npurpose god christ movement jesus hanegraaf bible faith apostles apostolic saddleback unbiblical purpose\\n[DOC 8] obama evangelical warren invocation pastor inauguration gay rick keywords barack president elect inaugural npr marriage nreverbiage yo choice 18th dec\\n[DOC 9] gay obama warren disagree invocation inauguration rick marriage nobama pastor wright hate people fan huffpost souljah rapists barack gays bailout\\n[DOC 10] warren rick gay npennyjane obama marriage ndecember mr nsatsuma nposted 2008 spirituality hrc ni azariah pm god invited ok myoutspirit\\n[DOC 11] nposted respond 08 obama 12 non warren pm heathen ni ngodless 18 religion people nrick nobama rick right abortion evangelical\\n[DOC 12] obama warren rick invocation gay inauguration president progressives barack progressive choice transition chellew pining huffpost lesbian meme hodge dissatisfaction marriage\\n[DOC 13] obama warren nre gay dec rick nradioleft ruined sister cohen washingtonpost party comment gays nradio macelveen quasisuspectclass fierstein left u0019t\\n[DOC 14] warren church saddleback rick nwarren retrieved http evangelical ministry california 2008 schiavo obama leaders newsweek baptist www pastors christian purpose\\n[DOC 15] warren rick obama inauguration gay invocation lgbt elect inaugural media glaad 08 controversy faith n12 president coverage rev nmsnbc pm\\n[DOC 16] warren obama invocation rick pastor prayer nwatch inauguration graham pray pulliam god u0019s u0019t president nhe jesus sarah december nrick\\n[DOC 17] warren obama inauguration gay marriage rick pastor cnn crash plane speech invocation said abortion caylee evangelical popular nmore conservative river\\n[DOC 18] nposted comment warren prejudice christ gay rick jesus lauren poor obama inauguration dec pigface alternet reply maddow rating nadvertisement post\\n[DOC 19] rod r20 obama gay warren gays church rick pastor barack anti invocation nfar haggard televangelist evangelical nobama deliver advocate supremacist\\n[DOC 20] warren rick obama black dunamis women ministry nsaid church dominance blessings gay strategy ndecember think abortion peace political spiritual marriage\", 'output': \"I'm looking for background and biographical information on Rick Warren.\", 'input_ids': [784, 15367, 11824, 908, 27, 31, 51, 479, 21, 251, 30, 11066, 16700, 6, 8, 30682, 6323, 5, 784, 9857, 5211, 11810, 134, 908, 784, 26472, 209, 908, 3, 32, 115, 265, 9, 615, 1536, 3, 30634, 3, 5206, 3, 29, 11966, 26, 1207, 4365, 7029, 3, 210, 3535, 3, 29, 2748, 1054, 3, 12488, 11658, 107, 3, 76, 1206, 2534, 232, 3, 76, 1206, 2294, 7, 1240, 226, 11849, 836, 107, 40, 23, 9, 3, 76, 1206, 2294, 17, 16998, 2078, 22145, 1560, 1559, 7, 3, 107, 10878, 784, 26472, 204, 908, 3, 32, 115, 265, 9, 615, 1536, 6366, 60, 102, 120, 3, 40, 122, 115, 17, 3, 3727, 49, 5807, 3, 5206, 1670, 3, 29, 3727, 20, 75, 18247, 3, 29, 23, 573, 3, 29, 1123, 3, 162, 5591, 4216, 53, 3, 3727, 3, 30634, 2628, 497, 7029, 784, 26472, 220, 908, 3, 32, 115, 265, 9, 3, 76, 1206, 2534, 3, 29, 1123, 26, 3, 29, 2152, 77, 8776, 615, 1536, 3691, 3, 5206, 586, 1003, 2628, 1207, 4365, 9812, 3, 29, 60, 3, 29, 189, 76, 968, 9018, 7, 9018, 11831, 12437, 51, 75, 7, 5612, 15, 16, 15044, 784, 26472, 314, 908, 615, 1536, 3, 5206, 3, 30634, 3, 32, 115, 265, 9, 5109, 5495, 1753, 3, 29, 17431, 2078, 1786, 2249, 3, 15294, 9018, 875, 4761, 3, 17431, 3, 24238, 2147, 40, 232, 3, 29, 10699, 107, 2447, 8115, 42, 15, 5307, 784, 26472, 305, 908, 1003, 189, 3, 15, 17, 6366, 3, 32, 115, 265, 9, 2628, 615, 1536, 16998, 16, 15044, 3, 5206, 30682, 151, 3, 29, 23, 150, 115, 265, 9, 8581, 1160, 2753, 3, 29, 5206, 26996, 1207, 4365, 30682, 7, 784, 26472, 431, 908, 3, 32, 115, 265, 9, 5692, 615, 1536, 20, 75, 8776, 3, 29, 2748, 1054, 16998, 5281, 3095, 3, 29, 29, 32, 3, 52, 32, 32, 7, 15, 4911, 17, 1902, 16998, 7, 2166, 6179, 6029, 2628, 2662, 6941, 6366, 7021, 7, 3, 40, 122, 115, 17, 784, 26472, 489, 908, 615, 1536, 2078, 3, 2685, 3, 5206, 6737, 1227, 17, 75, 9894, 3, 29, 19681, 8581, 3, 15294, 2426, 528, 7, 302, 3, 2618, 15, 3484, 9, 89, 26996, 3251, 29932, 7, 3, 2521, 235, 2176, 23922, 1549, 73, 22456, 40, 1950, 1730, 784, 26472, 505, 908, 3, 32, 115, 265, 9, 30682, 615, 1536, 16, 15044, 13018, 3, 30634, 16998, 3, 5206, 12545, 1207, 4365, 2753, 11924, 22145, 3, 29, 102, 52, 5281, 3, 29, 60, 11868, 23, 545, 3, 63, 32, 1160, 507, 189, 20, 75, 784, 26472, 668, 908, 16998, 3, 32, 115, 265, 9, 615, 1536, 15788, 16, 15044, 3, 30634, 3, 5206, 5281, 150, 115, 265, 9, 13018, 3, 210, 3535, 5591, 151, 1819, 3, 107, 2999, 5950, 3668, 1191, 107, 3, 5846, 343, 7, 1207, 4365, 16998, 7, 15794, 670, 784, 26472, 335, 908, 615, 1536, 3, 5206, 16998, 3, 29, 3208, 29, 63, 7066, 15, 3, 32, 115, 265, 9, 5281, 3, 29, 221, 75, 18247, 3, 51, 52, 3, 29, 7, 144, 4078, 9, 3, 29, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [27, 31, 51, 479, 21, 2458, 11, 2392, 16982, 251, 30, 11066, 16700, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "tokenizer,tokenized_train_dataset, tokenized_eval_dataset= tokenize_and_split_dataset(\"training_top20_qulac_PREPROCESSED_FOR_MODEL_TFIDF.json\",\"t5-small\")\n",
    "print(tokenized_train_dataset[0])  # Optional: check one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e89e55c-63a6-4b5b-954c-5b01420eb01d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/users/nfs/Etu0/21402600/wandb/run-20250506_164535-b64xoabo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/b64xoabo' target=\"_blank\">T5 SMALL + TOP 20 DOCS TFIDF </a></strong> to <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/b64xoabo' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/b64xoabo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1389011/1914975726.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='595' max='680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [595/680 07:19 < 01:02, 1.35 it/s, Epoch 35/40]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>20.627000</td>\n",
       "      <td>3.924397</td>\n",
       "      <td>0.025245</td>\n",
       "      <td>0.355990</td>\n",
       "      <td>0.172432</td>\n",
       "      <td>0.347286</td>\n",
       "      <td>0.229766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>20.627000</td>\n",
       "      <td>0.820348</td>\n",
       "      <td>0.104493</td>\n",
       "      <td>0.363480</td>\n",
       "      <td>0.161490</td>\n",
       "      <td>0.354000</td>\n",
       "      <td>0.291797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.525100</td>\n",
       "      <td>0.759679</td>\n",
       "      <td>0.124591</td>\n",
       "      <td>0.390347</td>\n",
       "      <td>0.189394</td>\n",
       "      <td>0.379454</td>\n",
       "      <td>0.332368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.525100</td>\n",
       "      <td>0.677909</td>\n",
       "      <td>0.151313</td>\n",
       "      <td>0.414139</td>\n",
       "      <td>0.206800</td>\n",
       "      <td>0.402357</td>\n",
       "      <td>0.359550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.525100</td>\n",
       "      <td>0.624031</td>\n",
       "      <td>0.137112</td>\n",
       "      <td>0.408605</td>\n",
       "      <td>0.197611</td>\n",
       "      <td>0.401066</td>\n",
       "      <td>0.358688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.791100</td>\n",
       "      <td>0.578247</td>\n",
       "      <td>0.143713</td>\n",
       "      <td>0.414690</td>\n",
       "      <td>0.204435</td>\n",
       "      <td>0.406755</td>\n",
       "      <td>0.366876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.791100</td>\n",
       "      <td>0.535245</td>\n",
       "      <td>0.154061</td>\n",
       "      <td>0.436243</td>\n",
       "      <td>0.216300</td>\n",
       "      <td>0.427037</td>\n",
       "      <td>0.394729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.791100</td>\n",
       "      <td>0.520411</td>\n",
       "      <td>0.151011</td>\n",
       "      <td>0.453964</td>\n",
       "      <td>0.221205</td>\n",
       "      <td>0.445504</td>\n",
       "      <td>0.408364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>0.511297</td>\n",
       "      <td>0.158875</td>\n",
       "      <td>0.464410</td>\n",
       "      <td>0.227780</td>\n",
       "      <td>0.452663</td>\n",
       "      <td>0.417638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>0.504855</td>\n",
       "      <td>0.159205</td>\n",
       "      <td>0.467033</td>\n",
       "      <td>0.227396</td>\n",
       "      <td>0.457840</td>\n",
       "      <td>0.422107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>0.499625</td>\n",
       "      <td>0.163129</td>\n",
       "      <td>0.468461</td>\n",
       "      <td>0.231995</td>\n",
       "      <td>0.459411</td>\n",
       "      <td>0.421826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.510600</td>\n",
       "      <td>0.496308</td>\n",
       "      <td>0.163079</td>\n",
       "      <td>0.467333</td>\n",
       "      <td>0.231111</td>\n",
       "      <td>0.458242</td>\n",
       "      <td>0.422311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.510600</td>\n",
       "      <td>0.494861</td>\n",
       "      <td>0.165444</td>\n",
       "      <td>0.468384</td>\n",
       "      <td>0.234190</td>\n",
       "      <td>0.461005</td>\n",
       "      <td>0.425219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.510600</td>\n",
       "      <td>0.490368</td>\n",
       "      <td>0.164966</td>\n",
       "      <td>0.470879</td>\n",
       "      <td>0.232668</td>\n",
       "      <td>0.464215</td>\n",
       "      <td>0.424454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.487900</td>\n",
       "      <td>0.488172</td>\n",
       "      <td>0.166260</td>\n",
       "      <td>0.474036</td>\n",
       "      <td>0.234509</td>\n",
       "      <td>0.466537</td>\n",
       "      <td>0.427350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.487900</td>\n",
       "      <td>0.487193</td>\n",
       "      <td>0.166883</td>\n",
       "      <td>0.472056</td>\n",
       "      <td>0.236448</td>\n",
       "      <td>0.464323</td>\n",
       "      <td>0.426245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.487900</td>\n",
       "      <td>0.486104</td>\n",
       "      <td>0.176445</td>\n",
       "      <td>0.476641</td>\n",
       "      <td>0.242102</td>\n",
       "      <td>0.468663</td>\n",
       "      <td>0.431680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.464100</td>\n",
       "      <td>0.484030</td>\n",
       "      <td>0.175576</td>\n",
       "      <td>0.484778</td>\n",
       "      <td>0.252091</td>\n",
       "      <td>0.476563</td>\n",
       "      <td>0.442108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.464100</td>\n",
       "      <td>0.482876</td>\n",
       "      <td>0.175522</td>\n",
       "      <td>0.482844</td>\n",
       "      <td>0.249533</td>\n",
       "      <td>0.474236</td>\n",
       "      <td>0.441503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.464100</td>\n",
       "      <td>0.481568</td>\n",
       "      <td>0.176823</td>\n",
       "      <td>0.482258</td>\n",
       "      <td>0.255116</td>\n",
       "      <td>0.473920</td>\n",
       "      <td>0.442390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.456100</td>\n",
       "      <td>0.481535</td>\n",
       "      <td>0.178352</td>\n",
       "      <td>0.484348</td>\n",
       "      <td>0.256461</td>\n",
       "      <td>0.475827</td>\n",
       "      <td>0.445808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.456100</td>\n",
       "      <td>0.481352</td>\n",
       "      <td>0.191489</td>\n",
       "      <td>0.491737</td>\n",
       "      <td>0.266627</td>\n",
       "      <td>0.482324</td>\n",
       "      <td>0.457139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.456100</td>\n",
       "      <td>0.481318</td>\n",
       "      <td>0.191643</td>\n",
       "      <td>0.494141</td>\n",
       "      <td>0.266627</td>\n",
       "      <td>0.481045</td>\n",
       "      <td>0.459937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.436100</td>\n",
       "      <td>0.479515</td>\n",
       "      <td>0.191489</td>\n",
       "      <td>0.492129</td>\n",
       "      <td>0.266627</td>\n",
       "      <td>0.481045</td>\n",
       "      <td>0.458349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.436100</td>\n",
       "      <td>0.478706</td>\n",
       "      <td>0.191702</td>\n",
       "      <td>0.492696</td>\n",
       "      <td>0.267003</td>\n",
       "      <td>0.481723</td>\n",
       "      <td>0.458436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.436100</td>\n",
       "      <td>0.478427</td>\n",
       "      <td>0.191643</td>\n",
       "      <td>0.492129</td>\n",
       "      <td>0.266627</td>\n",
       "      <td>0.481045</td>\n",
       "      <td>0.458349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.432700</td>\n",
       "      <td>0.478216</td>\n",
       "      <td>0.191856</td>\n",
       "      <td>0.492869</td>\n",
       "      <td>0.266627</td>\n",
       "      <td>0.481639</td>\n",
       "      <td>0.460438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.432700</td>\n",
       "      <td>0.477476</td>\n",
       "      <td>0.191760</td>\n",
       "      <td>0.496059</td>\n",
       "      <td>0.267005</td>\n",
       "      <td>0.484718</td>\n",
       "      <td>0.460203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.432700</td>\n",
       "      <td>0.477224</td>\n",
       "      <td>0.191760</td>\n",
       "      <td>0.494077</td>\n",
       "      <td>0.267387</td>\n",
       "      <td>0.483061</td>\n",
       "      <td>0.459784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.424800</td>\n",
       "      <td>0.476470</td>\n",
       "      <td>0.191914</td>\n",
       "      <td>0.496635</td>\n",
       "      <td>0.267387</td>\n",
       "      <td>0.485380</td>\n",
       "      <td>0.461036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.424800</td>\n",
       "      <td>0.476451</td>\n",
       "      <td>0.191760</td>\n",
       "      <td>0.493926</td>\n",
       "      <td>0.267005</td>\n",
       "      <td>0.482617</td>\n",
       "      <td>0.459736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.424800</td>\n",
       "      <td>0.476443</td>\n",
       "      <td>0.191606</td>\n",
       "      <td>0.493422</td>\n",
       "      <td>0.268802</td>\n",
       "      <td>0.484053</td>\n",
       "      <td>0.458348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.422300</td>\n",
       "      <td>0.476877</td>\n",
       "      <td>0.191722</td>\n",
       "      <td>0.494247</td>\n",
       "      <td>0.268802</td>\n",
       "      <td>0.484733</td>\n",
       "      <td>0.459802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.422300</td>\n",
       "      <td>0.476951</td>\n",
       "      <td>0.191722</td>\n",
       "      <td>0.494247</td>\n",
       "      <td>0.267005</td>\n",
       "      <td>0.483176</td>\n",
       "      <td>0.459802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.422300</td>\n",
       "      <td>0.477085</td>\n",
       "      <td>0.191780</td>\n",
       "      <td>0.494822</td>\n",
       "      <td>0.267387</td>\n",
       "      <td>0.483790</td>\n",
       "      <td>0.459921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.47644302248954773, 'eval_bleu': 0.19160596633706883, 'eval_rouge1': 0.4934219280997374, 'eval_rouge2': 0.2688020232562909, 'eval_rougeL': 0.48405262705132923, 'eval_meteor': 0.45834750682011727, 'eval_runtime': 0.6782, 'eval_samples_per_second': 88.475, 'eval_steps_per_second': 2.949, 'epoch': 35.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:00<00:00, 75709.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final predictions with inputs saved to predicted_clarif_T5SMALL_TOP20DOC_TFIDF.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/bleu</td><td>▁▄▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>eval/loss</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/meteor</td><td>▁▃▄▅▅▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇████████████████</td></tr><tr><td>eval/rouge1</td><td>▁▁▃▄▄▄▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>eval/rouge2</td><td>▂▁▃▄▃▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇███████████████</td></tr><tr><td>eval/rougeL</td><td>▁▁▃▄▄▄▅▆▆▇▇▇▇▇▇▇▇█▇▇████████████████</td></tr><tr><td>eval/runtime</td><td>▂▂▁▃▂▁█▂▂▂▂▂▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁</td></tr><tr><td>eval/samples_per_second</td><td>▆▇█▅▆█▁▆▇▇▇▆██▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇█</td></tr><tr><td>eval/steps_per_second</td><td>▆▇█▅▆█▁▆▇▇▇▆██▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇█</td></tr><tr><td>test/bleu</td><td>▁</td></tr><tr><td>test/loss</td><td>▁</td></tr><tr><td>test/meteor</td><td>▁</td></tr><tr><td>test/rouge1</td><td>▁</td></tr><tr><td>test/rouge2</td><td>▁</td></tr><tr><td>test/rougeL</td><td>▁</td></tr><tr><td>test/runtime</td><td>▁</td></tr><tr><td>test/samples_per_second</td><td>▁</td></tr><tr><td>test/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>█▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>█▇▇▆▅▅▄▄▃▂▂▁</td></tr><tr><td>train/loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/bleu</td><td>0.19161</td></tr><tr><td>eval/loss</td><td>0.47644</td></tr><tr><td>eval/meteor</td><td>0.45835</td></tr><tr><td>eval/rouge1</td><td>0.49342</td></tr><tr><td>eval/rouge2</td><td>0.2688</td></tr><tr><td>eval/rougeL</td><td>0.48405</td></tr><tr><td>eval/runtime</td><td>0.6782</td></tr><tr><td>eval/samples_per_second</td><td>88.475</td></tr><tr><td>eval/steps_per_second</td><td>2.949</td></tr><tr><td>test/bleu</td><td>0.19161</td></tr><tr><td>test/loss</td><td>0.47644</td></tr><tr><td>test/meteor</td><td>0.45835</td></tr><tr><td>test/rouge1</td><td>0.49342</td></tr><tr><td>test/rouge2</td><td>0.2688</td></tr><tr><td>test/rougeL</td><td>0.48405</td></tr><tr><td>test/runtime</td><td>0.6791</td></tr><tr><td>test/samples_per_second</td><td>88.35</td></tr><tr><td>test/steps_per_second</td><td>2.945</td></tr><tr><td>total_flos</td><td>2553223084769280.0</td></tr><tr><td>train/epoch</td><td>35</td></tr><tr><td>train/global_step</td><td>595</td></tr><tr><td>train/grad_norm</td><td>0.5412</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.4223</td></tr><tr><td>train_loss</td><td>0.85965</td></tr><tr><td>train_runtime</td><td>441.5936</td></tr><tr><td>train_samples_per_second</td><td>48.823</td></tr><tr><td>train_steps_per_second</td><td>1.54</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">T5 SMALL + TOP 20 DOCS TFIDF </strong> at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/b64xoabo' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/b64xoabo</a><br> View project at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250506_164535-b64xoabo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"T5-Clarifications\",  \n",
    "    name=\"T5 SMALL + TOP 20 DOCS TFIDF \"                   \n",
    ")\n",
    "train(\"t5-small\", \"predicted_clarif_T5SMALL_TOP20DOC_TFIDF.jsonl\",tokenizer, tokenized_train_dataset, tokenized_eval_dataset,40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13d58b6-7b90-4286-b8fb-e8741c24aa50",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# TEST AVEC TOP10 - TFIDF-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d410427-cbc2-46c1-a482-3d3ca3aa3adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9069ac5da50d4ad5838b4daf6a46f224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/539 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d52e33866c4ae2abcd1c0859d31602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': \"[QUERY] I'm looking for information on Rick Warren, the evangelical minister.\\n[DOCUMENTS]\\n[DOC 1] obama warren inauguration rick nfiled barack prayer wright nposted jeremiah u0014and u0019s alexander dahlia u0019t gay church inaugural hitchens hanna\\n[DOC 2] obama warren pmreply lgbt bilerico rick comment nbil december ni community nwe ve hate browning bil inauguration 2008 say prayer\\n[DOC 3] obama u0014 nwed nlogin reply warren register rick 12 17 2008 barack hearts nre nthu 14 progressives progressive angelfoodmcspade invocation\\n[DOC 4] warren rick inauguration obama rev currie nucc church wrong voice christ progressive blog religious ucc chuck portland nfaith parkrose oregon\\n[DOC 5] 17th et pm obama 2008 warren gay invocation rick evangelical people ni nobama god choice president nrick bible barack evangelicals\\n[DOC 6] obama flag warren dec reply nposted gay marriage civil nno roosevelt 23 gays rights washington 2008 japhy pm unions lgbt\\n[DOC 7] warren church cri rick driven ditc churches npurpose god christ movement jesus hanegraaf bible faith apostles apostolic saddleback unbiblical purpose\\n[DOC 8] obama evangelical warren invocation pastor inauguration gay rick keywords barack president elect inaugural npr marriage nreverbiage yo choice 18th dec\\n[DOC 9] gay obama warren disagree invocation inauguration rick marriage nobama pastor wright hate people fan huffpost souljah rapists barack gays bailout\\n[DOC 10] warren rick gay npennyjane obama marriage ndecember mr nsatsuma nposted 2008 spirituality hrc ni azariah pm god invited ok myoutspirit\", 'output': \"I'm looking for background and biographical information on Rick Warren.\", 'input_ids': [784, 15367, 11824, 908, 27, 31, 51, 479, 21, 251, 30, 11066, 16700, 6, 8, 30682, 6323, 5, 784, 9857, 5211, 11810, 134, 908, 784, 26472, 209, 908, 3, 32, 115, 265, 9, 615, 1536, 3, 30634, 3, 5206, 3, 29, 11966, 26, 1207, 4365, 7029, 3, 210, 3535, 3, 29, 2748, 1054, 3, 12488, 11658, 107, 3, 76, 1206, 2534, 232, 3, 76, 1206, 2294, 7, 1240, 226, 11849, 836, 107, 40, 23, 9, 3, 76, 1206, 2294, 17, 16998, 2078, 22145, 1560, 1559, 7, 3, 107, 10878, 784, 26472, 204, 908, 3, 32, 115, 265, 9, 615, 1536, 6366, 60, 102, 120, 3, 40, 122, 115, 17, 3, 3727, 49, 5807, 3, 5206, 1670, 3, 29, 3727, 20, 75, 18247, 3, 29, 23, 573, 3, 29, 1123, 3, 162, 5591, 4216, 53, 3, 3727, 3, 30634, 2628, 497, 7029, 784, 26472, 220, 908, 3, 32, 115, 265, 9, 3, 76, 1206, 2534, 3, 29, 1123, 26, 3, 29, 2152, 77, 8776, 615, 1536, 3691, 3, 5206, 586, 1003, 2628, 1207, 4365, 9812, 3, 29, 60, 3, 29, 189, 76, 968, 9018, 7, 9018, 11831, 12437, 51, 75, 7, 5612, 15, 16, 15044, 784, 26472, 314, 908, 615, 1536, 3, 5206, 3, 30634, 3, 32, 115, 265, 9, 5109, 5495, 1753, 3, 29, 17431, 2078, 1786, 2249, 3, 15294, 9018, 875, 4761, 3, 17431, 3, 24238, 2147, 40, 232, 3, 29, 10699, 107, 2447, 8115, 42, 15, 5307, 784, 26472, 305, 908, 1003, 189, 3, 15, 17, 6366, 3, 32, 115, 265, 9, 2628, 615, 1536, 16998, 16, 15044, 3, 5206, 30682, 151, 3, 29, 23, 150, 115, 265, 9, 8581, 1160, 2753, 3, 29, 5206, 26996, 1207, 4365, 30682, 7, 784, 26472, 431, 908, 3, 32, 115, 265, 9, 5692, 615, 1536, 20, 75, 8776, 3, 29, 2748, 1054, 16998, 5281, 3095, 3, 29, 29, 32, 3, 52, 32, 32, 7, 15, 4911, 17, 1902, 16998, 7, 2166, 6179, 6029, 2628, 2662, 6941, 6366, 7021, 7, 3, 40, 122, 115, 17, 784, 26472, 489, 908, 615, 1536, 2078, 3, 2685, 3, 5206, 6737, 1227, 17, 75, 9894, 3, 29, 19681, 8581, 3, 15294, 2426, 528, 7, 302, 3, 2618, 15, 3484, 9, 89, 26996, 3251, 29932, 7, 3, 2521, 235, 2176, 23922, 1549, 73, 22456, 40, 1950, 1730, 784, 26472, 505, 908, 3, 32, 115, 265, 9, 30682, 615, 1536, 16, 15044, 13018, 3, 30634, 16998, 3, 5206, 12545, 1207, 4365, 2753, 11924, 22145, 3, 29, 102, 52, 5281, 3, 29, 60, 11868, 23, 545, 3, 63, 32, 1160, 507, 189, 20, 75, 784, 26472, 668, 908, 16998, 3, 32, 115, 265, 9, 615, 1536, 15788, 16, 15044, 3, 30634, 3, 5206, 5281, 150, 115, 265, 9, 13018, 3, 210, 3535, 5591, 151, 1819, 3, 107, 2999, 5950, 3668, 1191, 107, 3, 5846, 343, 7, 1207, 4365, 16998, 7, 15794, 670, 784, 26472, 335, 908, 615, 1536, 3, 5206, 16998, 3, 29, 3208, 29, 63, 7066, 15, 3, 32, 115, 265, 9, 5281, 3, 29, 221, 75, 18247, 3, 51, 52, 3, 29, 7, 144, 4078, 9, 3, 29, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [27, 31, 51, 479, 21, 2458, 11, 2392, 16982, 251, 30, 11066, 16700, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "tokenizer,tokenized_train_dataset, tokenized_eval_dataset= tokenize_and_split_dataset(\"training_top10_qulac_PREPROCESSED_FOR_MODEL_TFIDF.json\",\"t5-small\")\n",
    "print(tokenized_train_dataset[0])  # Optional: check one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3f3aa23-3bff-469b-b65d-e5c342c21dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">T5 SMALL + TOP 10 DOCS TFIDF </strong> at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/tx2fybs4' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/tx2fybs4</a><br> View project at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250506_171004-tx2fybs4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/users/nfs/Etu0/21402600/wandb/run-20250506_171109-3jp9lf30</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/3jp9lf30' target=\"_blank\">T5 SMALL + TOP 10 DOCS TFIDF </a></strong> to <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/3jp9lf30' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/3jp9lf30</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1389011/287362180.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='952' max='2720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 952/2720 17:26 < 32:27, 0.91 it/s, Epoch 14/40]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.056500</td>\n",
       "      <td>0.714510</td>\n",
       "      <td>0.130299</td>\n",
       "      <td>0.393877</td>\n",
       "      <td>0.195397</td>\n",
       "      <td>0.382513</td>\n",
       "      <td>0.342864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.879100</td>\n",
       "      <td>0.564642</td>\n",
       "      <td>0.146328</td>\n",
       "      <td>0.435212</td>\n",
       "      <td>0.206842</td>\n",
       "      <td>0.424295</td>\n",
       "      <td>0.386815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.599400</td>\n",
       "      <td>0.513678</td>\n",
       "      <td>0.153190</td>\n",
       "      <td>0.446186</td>\n",
       "      <td>0.211427</td>\n",
       "      <td>0.437752</td>\n",
       "      <td>0.398847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.533400</td>\n",
       "      <td>0.496594</td>\n",
       "      <td>0.158607</td>\n",
       "      <td>0.464045</td>\n",
       "      <td>0.223636</td>\n",
       "      <td>0.454576</td>\n",
       "      <td>0.418017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.507500</td>\n",
       "      <td>0.488396</td>\n",
       "      <td>0.164677</td>\n",
       "      <td>0.469698</td>\n",
       "      <td>0.229723</td>\n",
       "      <td>0.460327</td>\n",
       "      <td>0.424169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.465700</td>\n",
       "      <td>0.482871</td>\n",
       "      <td>0.163358</td>\n",
       "      <td>0.474385</td>\n",
       "      <td>0.241108</td>\n",
       "      <td>0.466911</td>\n",
       "      <td>0.434065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.467600</td>\n",
       "      <td>0.478042</td>\n",
       "      <td>0.163872</td>\n",
       "      <td>0.477605</td>\n",
       "      <td>0.239284</td>\n",
       "      <td>0.468635</td>\n",
       "      <td>0.437644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>0.476956</td>\n",
       "      <td>0.187689</td>\n",
       "      <td>0.494259</td>\n",
       "      <td>0.258788</td>\n",
       "      <td>0.483407</td>\n",
       "      <td>0.456956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.435100</td>\n",
       "      <td>0.475347</td>\n",
       "      <td>0.188789</td>\n",
       "      <td>0.494190</td>\n",
       "      <td>0.262514</td>\n",
       "      <td>0.485608</td>\n",
       "      <td>0.459629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.416600</td>\n",
       "      <td>0.475664</td>\n",
       "      <td>0.189321</td>\n",
       "      <td>0.492966</td>\n",
       "      <td>0.258012</td>\n",
       "      <td>0.482432</td>\n",
       "      <td>0.461262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.426200</td>\n",
       "      <td>0.472294</td>\n",
       "      <td>0.191566</td>\n",
       "      <td>0.498215</td>\n",
       "      <td>0.260375</td>\n",
       "      <td>0.486085</td>\n",
       "      <td>0.462606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.387000</td>\n",
       "      <td>0.475296</td>\n",
       "      <td>0.188116</td>\n",
       "      <td>0.494976</td>\n",
       "      <td>0.257997</td>\n",
       "      <td>0.483716</td>\n",
       "      <td>0.461994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.398500</td>\n",
       "      <td>0.474627</td>\n",
       "      <td>0.191006</td>\n",
       "      <td>0.502415</td>\n",
       "      <td>0.262698</td>\n",
       "      <td>0.487376</td>\n",
       "      <td>0.462964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.387200</td>\n",
       "      <td>0.474338</td>\n",
       "      <td>0.195885</td>\n",
       "      <td>0.500090</td>\n",
       "      <td>0.265107</td>\n",
       "      <td>0.486494</td>\n",
       "      <td>0.460413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.47229379415512085, 'eval_bleu': 0.19156588038375424, 'eval_rouge1': 0.49821472572638525, 'eval_rouge2': 0.26037547294190294, 'eval_rougeL': 0.48608466436493974, 'eval_meteor': 0.4626056059854938, 'eval_runtime': 1.979, 'eval_samples_per_second': 30.319, 'eval_steps_per_second': 4.042, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:00<00:00, 71615.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final predictions with inputs saved to predicted_clarif_T5SMALL_TOP10DOC_TFIDF.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/bleu</td><td>▁▃▃▄▅▅▅▇▇▇█▇▇██</td></tr><tr><td>eval/loss</td><td>█▄▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/meteor</td><td>▁▄▄▅▆▆▇████████</td></tr><tr><td>eval/rouge1</td><td>▁▄▄▆▆▆▆▇▇▇█████</td></tr><tr><td>eval/rouge2</td><td>▁▂▃▄▄▆▅▇█▇█▇███</td></tr><tr><td>eval/rougeL</td><td>▁▄▅▆▆▇▇████████</td></tr><tr><td>eval/runtime</td><td>▇███▃▃▂▁▂▁▃▂▂▄▄</td></tr><tr><td>eval/samples_per_second</td><td>▂▁▁▁▆▆▇█▇█▆▇▇▅▅</td></tr><tr><td>eval/steps_per_second</td><td>▂▁▁▁▆▆▆█▇█▆▇▇▅▅</td></tr><tr><td>test/bleu</td><td>▁</td></tr><tr><td>test/loss</td><td>▁</td></tr><tr><td>test/meteor</td><td>▁</td></tr><tr><td>test/rouge1</td><td>▁</td></tr><tr><td>test/rouge2</td><td>▁</td></tr><tr><td>test/rougeL</td><td>▁</td></tr><tr><td>test/runtime</td><td>▁</td></tr><tr><td>test/samples_per_second</td><td>▁</td></tr><tr><td>test/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇██████</td></tr><tr><td>train/grad_norm</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/bleu</td><td>0.19157</td></tr><tr><td>eval/loss</td><td>0.47229</td></tr><tr><td>eval/meteor</td><td>0.46261</td></tr><tr><td>eval/rouge1</td><td>0.49821</td></tr><tr><td>eval/rouge2</td><td>0.26038</td></tr><tr><td>eval/rougeL</td><td>0.48608</td></tr><tr><td>eval/runtime</td><td>1.979</td></tr><tr><td>eval/samples_per_second</td><td>30.319</td></tr><tr><td>eval/steps_per_second</td><td>4.042</td></tr><tr><td>test/bleu</td><td>0.19157</td></tr><tr><td>test/loss</td><td>0.47229</td></tr><tr><td>test/meteor</td><td>0.46261</td></tr><tr><td>test/rouge1</td><td>0.49821</td></tr><tr><td>test/rouge2</td><td>0.26038</td></tr><tr><td>test/rougeL</td><td>0.48608</td></tr><tr><td>test/runtime</td><td>1.9536</td></tr><tr><td>test/samples_per_second</td><td>30.712</td></tr><tr><td>test/steps_per_second</td><td>4.095</td></tr><tr><td>total_flos</td><td>1021289233907712.0</td></tr><tr><td>train/epoch</td><td>14</td></tr><tr><td>train/global_step</td><td>952</td></tr><tr><td>train/grad_norm</td><td>1.07465</td></tr><tr><td>train/learning_rate</td><td>3e-05</td></tr><tr><td>train/loss</td><td>0.3872</td></tr><tr><td>train_loss</td><td>0.74192</td></tr><tr><td>train_runtime</td><td>1047.3613</td></tr><tr><td>train_samples_per_second</td><td>20.585</td></tr><tr><td>train_steps_per_second</td><td>2.597</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">T5 SMALL + TOP 10 DOCS TFIDF </strong> at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/3jp9lf30' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/3jp9lf30</a><br> View project at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250506_171109-3jp9lf30/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"T5-Clarifications\",  \n",
    "    name=\"T5 SMALL + TOP 10 DOCS TFIDF \"                   \n",
    ")\n",
    "train(\"t5-small\", \"predicted_clarif_T5SMALL_TOP10DOC_TFIDF.jsonl\",tokenizer, tokenized_train_dataset, tokenized_eval_dataset,40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aba6d6-8fdd-468d-ae5e-48f4b55c3414",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# TOP 10 DOCUMENTS, T5 SMALL, TDIFD CLEANED !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e07ea802-1edf-47d4-8380-69f44f8ae695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e372b1abf4034fb8b959e5bf376c5003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/539 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/Etu0/21402600/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f83dcdede47e40a8a49d3873fee8e5cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': \"[QUERY] I'm looking for information on Rick Warren, the evangelical minister.\\n[DOCUMENTS]\\n[DOC 1] obama warren inauguration rick barack wright prayer jeremiah dahlia alexander evangelical gay post church ctl hanna inaugural rachael stroller political\\n[DOC 2] warren obama pmreply bil lgbt bilerico rick comment december hate ve big inauguration community don gay waymon prayer whiny right\\n[DOC 3] obama warren reply login rick register progressive barack liberal fundie angelfoodmcspade invocation thu gay heart folk ceremony evangelical youcanthandledatruth ohmmade\\n[DOC 4] warren ucc rick inauguration obama church rev currie wrong blog voice progressive christ portland chuck parkrose religious faith opposed oregon\\n[DOC 5] et obama pm warren evangelical rick gay invocation people barack god president choice bible elect mccain think right abortion religious\\n[DOC 6] obama gay warren dec flag reply marriage civil japhy evangelical post roosevelt right washington white str invite minority pm lgbt\\n[DOC 7] warren church rick cri ditc christ god purpose apostle drive movement book jesus saddleback hanegraaf unbiblical apologetic bible faith apostolic\\n[DOC 8] obama evangelical warren invocation pastor inauguration gay rick keyword barack president reverbiage npr inaugural marriage elect yo choice supporter dec\\n[DOC 9] gay obama warren disagree rick invocation inauguration marriage barack hate pedophile pastor wright huffpost rapist comment people bailout souljah supporter\\n[DOC 10] warren rick gay pennyjane spirituality mr obama marriage satsuma myoutspirit god invite azariah hrc december right pm man love woman\", 'output': \"I'm looking for background and biographical information on Rick Warren.\", 'input_ids': [784, 15367, 11824, 908, 27, 31, 51, 479, 21, 251, 30, 11066, 16700, 6, 8, 30682, 6323, 5, 784, 9857, 5211, 11810, 134, 908, 784, 26472, 209, 908, 3, 32, 115, 265, 9, 615, 1536, 3, 30634, 3, 5206, 1207, 4365, 3, 210, 3535, 7029, 3, 12488, 11658, 107, 836, 107, 40, 23, 9, 1240, 226, 11849, 30682, 16998, 442, 2078, 3, 75, 17, 40, 3, 107, 10878, 22145, 3, 21136, 9, 15, 40, 13593, 49, 1827, 784, 26472, 204, 908, 615, 1536, 3, 32, 115, 265, 9, 6366, 60, 102, 120, 3, 3727, 3, 40, 122, 115, 17, 3, 3727, 49, 5807, 3, 5206, 1670, 20, 75, 18247, 5591, 3, 162, 600, 3, 30634, 573, 278, 16998, 194, 2157, 7029, 14228, 77, 63, 269, 784, 26472, 220, 908, 3, 32, 115, 265, 9, 615, 1536, 8776, 11255, 3, 5206, 3691, 9018, 1207, 4365, 10215, 694, 2498, 11831, 12437, 51, 75, 7, 5612, 15, 16, 15044, 3, 189, 76, 16998, 842, 12702, 7252, 30682, 25, 1608, 17, 2894, 1361, 9, 2666, 189, 3, 32, 107, 51, 4725, 784, 26472, 314, 908, 615, 1536, 3, 17431, 3, 5206, 3, 30634, 3, 32, 115, 265, 9, 2078, 5109, 5495, 1753, 1786, 875, 2249, 9018, 3, 15294, 2147, 40, 232, 3, 24238, 2447, 8115, 4761, 3251, 8560, 42, 15, 5307, 784, 26472, 305, 908, 3, 15, 17, 3, 32, 115, 265, 9, 6366, 615, 1536, 30682, 3, 5206, 16998, 16, 15044, 151, 1207, 4365, 8581, 2753, 1160, 26996, 11924, 3, 51, 12464, 77, 317, 269, 20526, 4761, 784, 26472, 431, 908, 3, 32, 115, 265, 9, 16998, 615, 1536, 20, 75, 5692, 8776, 5281, 3095, 2662, 6941, 30682, 442, 3, 52, 32, 32, 7, 15, 4911, 17, 269, 6179, 6029, 872, 5765, 5484, 16392, 6366, 3, 40, 122, 115, 17, 784, 26472, 489, 908, 615, 1536, 2078, 3, 5206, 3, 2685, 1227, 17, 75, 3, 15294, 8581, 1730, 29932, 1262, 2426, 484, 528, 7, 302, 23922, 1549, 3, 2618, 15, 3484, 9, 89, 73, 22456, 40, 1950, 3, 9521, 2152, 7578, 26996, 3251, 3, 2521, 235, 2176, 784, 26472, 505, 908, 3, 32, 115, 265, 9, 30682, 615, 1536, 16, 15044, 13018, 3, 30634, 16998, 3, 5206, 15693, 1207, 4365, 2753, 3, 60, 11868, 23, 545, 3, 29, 102, 52, 22145, 5281, 11924, 3, 63, 32, 1160, 380, 49, 20, 75, 784, 26472, 668, 908, 16998, 3, 32, 115, 265, 9, 615, 1536, 15788, 3, 5206, 16, 15044, 3, 30634, 5281, 1207, 4365, 5591, 3, 3138, 21144, 15, 13018, 3, 210, 3535, 3, 107, 2999, 5950, 3, 5846, 343, 1670, 151, 15794, 670, 3668, 1191, 107, 380, 49, 784, 26472, 335, 908, 615, 1536, 3, 5206, 16998, 23925, 7066, 15, 3428, 485, 3, 51, 52, 3, 32, 115, 265, 9, 5281, 3, 7, 144, 4078, 9, 82, 670, 7, 2388, 155, 8581, 5484, 3, 9, 172, 6286, 107, 3, 107, 52, 75, 20, 75, 18247, 269, 6366, 388, 333, 2335, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [27, 31, 51, 479, 21, 2458, 11, 2392, 16982, 251, 30, 11066, 16700, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer,tokenized_train_dataset, tokenized_eval_dataset= tokenize_and_split_dataset(\"training_top10_qulac_PREPROCESSED_FOR_MODEL_TFIDF_CLEANED.json\",\"t5-small\")\n",
    "print(tokenized_train_dataset[0])  # Optional: check one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "789a3b86-1ab4-4fb2-ad8b-6d0fdaa66507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/users/nfs/Etu0/21402600/wandb/run-20250506_194747-gx5a38ay</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/gx5a38ay' target=\"_blank\">T5 SMALL + TOP 10 DOCS TFIDF CLEANED </a></strong> to <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/gx5a38ay' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/gx5a38ay</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1389011/3298686746.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='680' max='680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [680/680 08:28, Epoch 40/40]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>20.694700</td>\n",
       "      <td>4.167738</td>\n",
       "      <td>0.024244</td>\n",
       "      <td>0.371587</td>\n",
       "      <td>0.191891</td>\n",
       "      <td>0.364078</td>\n",
       "      <td>0.222026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>20.694700</td>\n",
       "      <td>0.811859</td>\n",
       "      <td>0.107319</td>\n",
       "      <td>0.357107</td>\n",
       "      <td>0.168929</td>\n",
       "      <td>0.349676</td>\n",
       "      <td>0.286539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.591700</td>\n",
       "      <td>0.742543</td>\n",
       "      <td>0.128234</td>\n",
       "      <td>0.391908</td>\n",
       "      <td>0.192670</td>\n",
       "      <td>0.383771</td>\n",
       "      <td>0.330736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.591700</td>\n",
       "      <td>0.689369</td>\n",
       "      <td>0.135222</td>\n",
       "      <td>0.403324</td>\n",
       "      <td>0.195763</td>\n",
       "      <td>0.390824</td>\n",
       "      <td>0.349692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.591700</td>\n",
       "      <td>0.642927</td>\n",
       "      <td>0.132013</td>\n",
       "      <td>0.404858</td>\n",
       "      <td>0.201291</td>\n",
       "      <td>0.392272</td>\n",
       "      <td>0.354776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.807600</td>\n",
       "      <td>0.573485</td>\n",
       "      <td>0.161868</td>\n",
       "      <td>0.441045</td>\n",
       "      <td>0.229638</td>\n",
       "      <td>0.428007</td>\n",
       "      <td>0.390647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.807600</td>\n",
       "      <td>0.540802</td>\n",
       "      <td>0.157325</td>\n",
       "      <td>0.442252</td>\n",
       "      <td>0.226629</td>\n",
       "      <td>0.434822</td>\n",
       "      <td>0.396413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.807600</td>\n",
       "      <td>0.520051</td>\n",
       "      <td>0.167385</td>\n",
       "      <td>0.460135</td>\n",
       "      <td>0.234141</td>\n",
       "      <td>0.450140</td>\n",
       "      <td>0.411276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.587700</td>\n",
       "      <td>0.510913</td>\n",
       "      <td>0.161534</td>\n",
       "      <td>0.463689</td>\n",
       "      <td>0.238924</td>\n",
       "      <td>0.453761</td>\n",
       "      <td>0.420154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.587700</td>\n",
       "      <td>0.505721</td>\n",
       "      <td>0.167697</td>\n",
       "      <td>0.469235</td>\n",
       "      <td>0.242106</td>\n",
       "      <td>0.460769</td>\n",
       "      <td>0.423740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.587700</td>\n",
       "      <td>0.501582</td>\n",
       "      <td>0.161771</td>\n",
       "      <td>0.471276</td>\n",
       "      <td>0.237627</td>\n",
       "      <td>0.463077</td>\n",
       "      <td>0.418901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.514000</td>\n",
       "      <td>0.499200</td>\n",
       "      <td>0.167908</td>\n",
       "      <td>0.473731</td>\n",
       "      <td>0.243037</td>\n",
       "      <td>0.464966</td>\n",
       "      <td>0.425214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.514000</td>\n",
       "      <td>0.496880</td>\n",
       "      <td>0.165357</td>\n",
       "      <td>0.470466</td>\n",
       "      <td>0.235982</td>\n",
       "      <td>0.461830</td>\n",
       "      <td>0.423265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.514000</td>\n",
       "      <td>0.493122</td>\n",
       "      <td>0.168439</td>\n",
       "      <td>0.475549</td>\n",
       "      <td>0.246142</td>\n",
       "      <td>0.466926</td>\n",
       "      <td>0.428778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.489900</td>\n",
       "      <td>0.490091</td>\n",
       "      <td>0.168439</td>\n",
       "      <td>0.477532</td>\n",
       "      <td>0.248005</td>\n",
       "      <td>0.468976</td>\n",
       "      <td>0.430508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.489900</td>\n",
       "      <td>0.488445</td>\n",
       "      <td>0.172323</td>\n",
       "      <td>0.476143</td>\n",
       "      <td>0.248005</td>\n",
       "      <td>0.467483</td>\n",
       "      <td>0.432304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.489900</td>\n",
       "      <td>0.487077</td>\n",
       "      <td>0.173079</td>\n",
       "      <td>0.478371</td>\n",
       "      <td>0.247993</td>\n",
       "      <td>0.468481</td>\n",
       "      <td>0.432848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.467400</td>\n",
       "      <td>0.484947</td>\n",
       "      <td>0.170695</td>\n",
       "      <td>0.479440</td>\n",
       "      <td>0.248149</td>\n",
       "      <td>0.468262</td>\n",
       "      <td>0.432469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.467400</td>\n",
       "      <td>0.483396</td>\n",
       "      <td>0.179946</td>\n",
       "      <td>0.483564</td>\n",
       "      <td>0.254444</td>\n",
       "      <td>0.472234</td>\n",
       "      <td>0.439125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.467400</td>\n",
       "      <td>0.482132</td>\n",
       "      <td>0.180192</td>\n",
       "      <td>0.484146</td>\n",
       "      <td>0.256395</td>\n",
       "      <td>0.472794</td>\n",
       "      <td>0.440997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.460300</td>\n",
       "      <td>0.480892</td>\n",
       "      <td>0.183885</td>\n",
       "      <td>0.485212</td>\n",
       "      <td>0.258442</td>\n",
       "      <td>0.473851</td>\n",
       "      <td>0.440981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.460300</td>\n",
       "      <td>0.479534</td>\n",
       "      <td>0.184662</td>\n",
       "      <td>0.485330</td>\n",
       "      <td>0.259598</td>\n",
       "      <td>0.474444</td>\n",
       "      <td>0.443169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.460300</td>\n",
       "      <td>0.479208</td>\n",
       "      <td>0.186873</td>\n",
       "      <td>0.488326</td>\n",
       "      <td>0.265965</td>\n",
       "      <td>0.478421</td>\n",
       "      <td>0.449687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.440900</td>\n",
       "      <td>0.478189</td>\n",
       "      <td>0.186932</td>\n",
       "      <td>0.491320</td>\n",
       "      <td>0.262919</td>\n",
       "      <td>0.479268</td>\n",
       "      <td>0.448625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.440900</td>\n",
       "      <td>0.477512</td>\n",
       "      <td>0.190143</td>\n",
       "      <td>0.487595</td>\n",
       "      <td>0.264898</td>\n",
       "      <td>0.477568</td>\n",
       "      <td>0.447431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.440900</td>\n",
       "      <td>0.477421</td>\n",
       "      <td>0.193947</td>\n",
       "      <td>0.488598</td>\n",
       "      <td>0.268031</td>\n",
       "      <td>0.478424</td>\n",
       "      <td>0.451019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.436600</td>\n",
       "      <td>0.477448</td>\n",
       "      <td>0.192902</td>\n",
       "      <td>0.489965</td>\n",
       "      <td>0.265184</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>0.449465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.436600</td>\n",
       "      <td>0.476948</td>\n",
       "      <td>0.194630</td>\n",
       "      <td>0.493566</td>\n",
       "      <td>0.268031</td>\n",
       "      <td>0.481195</td>\n",
       "      <td>0.454211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.436600</td>\n",
       "      <td>0.476406</td>\n",
       "      <td>0.194374</td>\n",
       "      <td>0.490378</td>\n",
       "      <td>0.268031</td>\n",
       "      <td>0.481354</td>\n",
       "      <td>0.453534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.428800</td>\n",
       "      <td>0.475904</td>\n",
       "      <td>0.194374</td>\n",
       "      <td>0.491473</td>\n",
       "      <td>0.268031</td>\n",
       "      <td>0.481141</td>\n",
       "      <td>0.453534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.428800</td>\n",
       "      <td>0.475899</td>\n",
       "      <td>0.194374</td>\n",
       "      <td>0.490967</td>\n",
       "      <td>0.268031</td>\n",
       "      <td>0.480791</td>\n",
       "      <td>0.453534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.428800</td>\n",
       "      <td>0.475801</td>\n",
       "      <td>0.194276</td>\n",
       "      <td>0.491473</td>\n",
       "      <td>0.268031</td>\n",
       "      <td>0.481141</td>\n",
       "      <td>0.451732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.427500</td>\n",
       "      <td>0.475969</td>\n",
       "      <td>0.195934</td>\n",
       "      <td>0.492115</td>\n",
       "      <td>0.269089</td>\n",
       "      <td>0.482271</td>\n",
       "      <td>0.457122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.427500</td>\n",
       "      <td>0.475975</td>\n",
       "      <td>0.195775</td>\n",
       "      <td>0.490175</td>\n",
       "      <td>0.267041</td>\n",
       "      <td>0.479709</td>\n",
       "      <td>0.454714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.427500</td>\n",
       "      <td>0.475688</td>\n",
       "      <td>0.195676</td>\n",
       "      <td>0.491983</td>\n",
       "      <td>0.269089</td>\n",
       "      <td>0.482165</td>\n",
       "      <td>0.454821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.417500</td>\n",
       "      <td>0.475633</td>\n",
       "      <td>0.195836</td>\n",
       "      <td>0.493953</td>\n",
       "      <td>0.268414</td>\n",
       "      <td>0.484456</td>\n",
       "      <td>0.456352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.417500</td>\n",
       "      <td>0.475612</td>\n",
       "      <td>0.195996</td>\n",
       "      <td>0.495255</td>\n",
       "      <td>0.268414</td>\n",
       "      <td>0.485180</td>\n",
       "      <td>0.457398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.417500</td>\n",
       "      <td>0.475803</td>\n",
       "      <td>0.193259</td>\n",
       "      <td>0.494433</td>\n",
       "      <td>0.267361</td>\n",
       "      <td>0.483114</td>\n",
       "      <td>0.455681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.419900</td>\n",
       "      <td>0.475790</td>\n",
       "      <td>0.195594</td>\n",
       "      <td>0.492697</td>\n",
       "      <td>0.268414</td>\n",
       "      <td>0.480678</td>\n",
       "      <td>0.454857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.419900</td>\n",
       "      <td>0.475811</td>\n",
       "      <td>0.195594</td>\n",
       "      <td>0.492697</td>\n",
       "      <td>0.268414</td>\n",
       "      <td>0.480678</td>\n",
       "      <td>0.454857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.47561225295066833, 'eval_bleu': 0.19599575082949333, 'eval_rouge1': 0.4952546724913716, 'eval_rouge2': 0.2684139504411648, 'eval_rougeL': 0.4851800011878169, 'eval_meteor': 0.45739784690501534, 'eval_runtime': 0.6696, 'eval_samples_per_second': 89.609, 'eval_steps_per_second': 2.987, 'epoch': 40.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:00<00:00, 72712.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final predictions with inputs saved to predicted_clarif_T5SMALL_TOP10DOC_TFIDF_CLEANED.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/bleu</td><td>▁▄▅▆▅▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇████████████████████</td></tr><tr><td>eval/loss</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/meteor</td><td>▁▃▄▅▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇█████████████████████</td></tr><tr><td>eval/rouge1</td><td>▂▁▃▃▃▅▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇██████████████████</td></tr><tr><td>eval/rouge2</td><td>▃▁▃▃▃▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇██████████████████</td></tr><tr><td>eval/rougeL</td><td>▂▁▃▃▃▅▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇██████████████████</td></tr><tr><td>eval/runtime</td><td>▂▂▂▂▁▂▂▂▂▂▂█▂▂▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▁</td></tr><tr><td>eval/samples_per_second</td><td>▆▇▇▇▇▇▇▇▇▇▇▁▇▇▇▇▇▇▇▇▆▇▇▇▇▆▇▇▇▇▇▇▇▆▇▇▇▇▇█</td></tr><tr><td>eval/steps_per_second</td><td>▆▇▇▇▇▇▇▇▇▇▇▁▇▇▇▇▇▇▇▇▆▇▇▇▇▆▇▇▇▇▇▇▇▆▇▇▇▇▇█</td></tr><tr><td>test/bleu</td><td>▁</td></tr><tr><td>test/loss</td><td>▁</td></tr><tr><td>test/meteor</td><td>▁</td></tr><tr><td>test/rouge1</td><td>▁</td></tr><tr><td>test/rouge2</td><td>▁</td></tr><tr><td>test/rougeL</td><td>▁</td></tr><tr><td>test/runtime</td><td>▁</td></tr><tr><td>test/samples_per_second</td><td>▁</td></tr><tr><td>test/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train/grad_norm</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>█▇▇▆▆▅▅▄▄▃▃▂▂▁</td></tr><tr><td>train/loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/bleu</td><td>0.196</td></tr><tr><td>eval/loss</td><td>0.47561</td></tr><tr><td>eval/meteor</td><td>0.4574</td></tr><tr><td>eval/rouge1</td><td>0.49525</td></tr><tr><td>eval/rouge2</td><td>0.26841</td></tr><tr><td>eval/rougeL</td><td>0.48518</td></tr><tr><td>eval/runtime</td><td>0.6696</td></tr><tr><td>eval/samples_per_second</td><td>89.609</td></tr><tr><td>eval/steps_per_second</td><td>2.987</td></tr><tr><td>test/bleu</td><td>0.196</td></tr><tr><td>test/loss</td><td>0.47561</td></tr><tr><td>test/meteor</td><td>0.4574</td></tr><tr><td>test/rouge1</td><td>0.49525</td></tr><tr><td>test/rouge2</td><td>0.26841</td></tr><tr><td>test/rougeL</td><td>0.48518</td></tr><tr><td>test/runtime</td><td>0.6658</td></tr><tr><td>test/samples_per_second</td><td>90.12</td></tr><tr><td>test/steps_per_second</td><td>3.004</td></tr><tr><td>total_flos</td><td>2917969239736320.0</td></tr><tr><td>train/epoch</td><td>40</td></tr><tr><td>train/global_step</td><td>680</td></tr><tr><td>train/grad_norm</td><td>0.36822</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4199</td></tr><tr><td>train_loss</td><td>0.81325</td></tr><tr><td>train_runtime</td><td>509.3719</td></tr><tr><td>train_samples_per_second</td><td>42.327</td></tr><tr><td>train_steps_per_second</td><td>1.335</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">T5 SMALL + TOP 10 DOCS TFIDF CLEANED </strong> at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/gx5a38ay' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/gx5a38ay</a><br> View project at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250506_194747-gx5a38ay/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"T5-Clarifications\",  \n",
    "    name=\"T5 SMALL + TOP 10 DOCS TFIDF CLEANED \"                   \n",
    ")\n",
    "train(\"t5-small\", \"predicted_clarif_T5SMALL_TOP10DOC_TFIDF_CLEANED.jsonl\",tokenizer, tokenized_train_dataset, tokenized_eval_dataset,40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e98519e-4475-426d-8ee1-97df80f01b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIFDF CLEANED TOP 15 WITH T5SMALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e9b1a3fc-355b-450c-9fb6-eab9f4959226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a0e88b51cb4eda82483b11511cc472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/539 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8ffa2f20e5a49ec965bb9579b04a77e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': \"[QUERY] I'm looking for information on Rick Warren, the evangelical minister.\\n[DOCUMENTS]\\n[DOC 1] obama warren inauguration rick barack wright prayer jeremiah dahlia alexander evangelical gay post church ctl hanna inaugural rachael stroller political\\n[DOC 2] warren obama pmreply bil lgbt bilerico rick comment december hate ve big inauguration community don gay waymon prayer whiny right\\n[DOC 3] obama warren reply login rick register progressive barack liberal fundie angelfoodmcspade invocation thu gay heart folk ceremony evangelical youcanthandledatruth ohmmade\\n[DOC 4] warren ucc rick inauguration obama church rev currie wrong blog voice progressive christ portland chuck parkrose religious faith opposed oregon\\n[DOC 5] et obama pm warren evangelical rick gay invocation people barack god president choice bible elect mccain think right abortion religious\", 'output': \"I'm looking for background and biographical information on Rick Warren.\", 'input_ids': [784, 15367, 11824, 908, 27, 31, 51, 479, 21, 251, 30, 11066, 16700, 6, 8, 30682, 6323, 5, 784, 9857, 5211, 11810, 134, 908, 784, 26472, 209, 908, 3, 32, 115, 265, 9, 615, 1536, 3, 30634, 3, 5206, 1207, 4365, 3, 210, 3535, 7029, 3, 12488, 11658, 107, 836, 107, 40, 23, 9, 1240, 226, 11849, 30682, 16998, 442, 2078, 3, 75, 17, 40, 3, 107, 10878, 22145, 3, 21136, 9, 15, 40, 13593, 49, 1827, 784, 26472, 204, 908, 615, 1536, 3, 32, 115, 265, 9, 6366, 60, 102, 120, 3, 3727, 3, 40, 122, 115, 17, 3, 3727, 49, 5807, 3, 5206, 1670, 20, 75, 18247, 5591, 3, 162, 600, 3, 30634, 573, 278, 16998, 194, 2157, 7029, 14228, 77, 63, 269, 784, 26472, 220, 908, 3, 32, 115, 265, 9, 615, 1536, 8776, 11255, 3, 5206, 3691, 9018, 1207, 4365, 10215, 694, 2498, 11831, 12437, 51, 75, 7, 5612, 15, 16, 15044, 3, 189, 76, 16998, 842, 12702, 7252, 30682, 25, 1608, 17, 2894, 1361, 9, 2666, 189, 3, 32, 107, 51, 4725, 784, 26472, 314, 908, 615, 1536, 3, 17431, 3, 5206, 3, 30634, 3, 32, 115, 265, 9, 2078, 5109, 5495, 1753, 1786, 875, 2249, 9018, 3, 15294, 2147, 40, 232, 3, 24238, 2447, 8115, 4761, 3251, 8560, 42, 15, 5307, 784, 26472, 305, 908, 3, 15, 17, 3, 32, 115, 265, 9, 6366, 615, 1536, 30682, 3, 5206, 16998, 16, 15044, 151, 1207, 4365, 8581, 2753, 1160, 26996, 11924, 3, 51, 12464, 77, 317, 269, 20526, 4761, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [27, 31, 51, 479, 21, 2458, 11, 2392, 16982, 251, 30, 11066, 16700, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer,tokenized_train_dataset, tokenized_eval_dataset= tokenize_and_split_dataset(\"training_top5_qulac_PREPROCESSED_FOR_MODEL_TFIDF_CLEANED.json\",\"t5-small\")\n",
    "print(tokenized_train_dataset[0])  # Optional: check one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9ae95ac9-800d-4cbf-af5e-04725c525af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/users/nfs/Etu0/21402600/wandb/run-20250506_200750-3p5ajmym</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/3p5ajmym' target=\"_blank\">T5 SMALL + TOP5 DOCS TFIDF CLEANED </a></strong> to <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/3p5ajmym' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/3p5ajmym</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1389011/287362180.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1156' max='2720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1156/2720 21:47 < 29:32, 0.88 it/s, Epoch 17/40]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.020100</td>\n",
       "      <td>0.750591</td>\n",
       "      <td>0.111133</td>\n",
       "      <td>0.382052</td>\n",
       "      <td>0.180609</td>\n",
       "      <td>0.370262</td>\n",
       "      <td>0.314686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.998600</td>\n",
       "      <td>0.590119</td>\n",
       "      <td>0.144853</td>\n",
       "      <td>0.425262</td>\n",
       "      <td>0.210200</td>\n",
       "      <td>0.417045</td>\n",
       "      <td>0.370997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.612400</td>\n",
       "      <td>0.521921</td>\n",
       "      <td>0.167089</td>\n",
       "      <td>0.457482</td>\n",
       "      <td>0.234683</td>\n",
       "      <td>0.446116</td>\n",
       "      <td>0.406463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.540400</td>\n",
       "      <td>0.503287</td>\n",
       "      <td>0.169154</td>\n",
       "      <td>0.456124</td>\n",
       "      <td>0.229099</td>\n",
       "      <td>0.448066</td>\n",
       "      <td>0.409783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.511800</td>\n",
       "      <td>0.497196</td>\n",
       "      <td>0.164821</td>\n",
       "      <td>0.464146</td>\n",
       "      <td>0.226652</td>\n",
       "      <td>0.455479</td>\n",
       "      <td>0.415864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.470900</td>\n",
       "      <td>0.488893</td>\n",
       "      <td>0.157548</td>\n",
       "      <td>0.463477</td>\n",
       "      <td>0.226462</td>\n",
       "      <td>0.456482</td>\n",
       "      <td>0.416965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.476000</td>\n",
       "      <td>0.485329</td>\n",
       "      <td>0.161877</td>\n",
       "      <td>0.470216</td>\n",
       "      <td>0.234615</td>\n",
       "      <td>0.460682</td>\n",
       "      <td>0.427468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.438600</td>\n",
       "      <td>0.482569</td>\n",
       "      <td>0.167427</td>\n",
       "      <td>0.471517</td>\n",
       "      <td>0.231280</td>\n",
       "      <td>0.463674</td>\n",
       "      <td>0.425976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.442000</td>\n",
       "      <td>0.480201</td>\n",
       "      <td>0.180948</td>\n",
       "      <td>0.479859</td>\n",
       "      <td>0.243026</td>\n",
       "      <td>0.471497</td>\n",
       "      <td>0.436291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.419600</td>\n",
       "      <td>0.478255</td>\n",
       "      <td>0.177844</td>\n",
       "      <td>0.478104</td>\n",
       "      <td>0.237394</td>\n",
       "      <td>0.468293</td>\n",
       "      <td>0.436926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.431200</td>\n",
       "      <td>0.478307</td>\n",
       "      <td>0.180593</td>\n",
       "      <td>0.486939</td>\n",
       "      <td>0.240059</td>\n",
       "      <td>0.474711</td>\n",
       "      <td>0.445145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.395100</td>\n",
       "      <td>0.477345</td>\n",
       "      <td>0.178779</td>\n",
       "      <td>0.482365</td>\n",
       "      <td>0.237124</td>\n",
       "      <td>0.469018</td>\n",
       "      <td>0.441017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.400200</td>\n",
       "      <td>0.477351</td>\n",
       "      <td>0.176040</td>\n",
       "      <td>0.483121</td>\n",
       "      <td>0.235603</td>\n",
       "      <td>0.470342</td>\n",
       "      <td>0.440584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.392700</td>\n",
       "      <td>0.477144</td>\n",
       "      <td>0.179970</td>\n",
       "      <td>0.481783</td>\n",
       "      <td>0.241314</td>\n",
       "      <td>0.469983</td>\n",
       "      <td>0.439511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.388200</td>\n",
       "      <td>0.478082</td>\n",
       "      <td>0.179554</td>\n",
       "      <td>0.480953</td>\n",
       "      <td>0.239062</td>\n",
       "      <td>0.469109</td>\n",
       "      <td>0.438359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.389300</td>\n",
       "      <td>0.480177</td>\n",
       "      <td>0.177532</td>\n",
       "      <td>0.479911</td>\n",
       "      <td>0.238214</td>\n",
       "      <td>0.467707</td>\n",
       "      <td>0.437251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.368900</td>\n",
       "      <td>0.481274</td>\n",
       "      <td>0.187379</td>\n",
       "      <td>0.488286</td>\n",
       "      <td>0.244361</td>\n",
       "      <td>0.473199</td>\n",
       "      <td>0.444802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4771442413330078, 'eval_bleu': 0.17996959865803996, 'eval_rouge1': 0.4817831447099876, 'eval_rouge2': 0.24131369566392785, 'eval_rougeL': 0.4699829025534148, 'eval_meteor': 0.4395113427286545, 'eval_runtime': 2.0622, 'eval_samples_per_second': 29.095, 'eval_steps_per_second': 3.879, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:00<00:00, 97504.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final predictions with inputs saved to predicted_clarif_T5SMALL_TOP5DOC_TFIDF_CLEANED.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/bleu</td><td>▁▄▆▆▆▅▆▆▇▇▇▇▇▇▇▇█▇</td></tr><tr><td>eval/loss</td><td>█▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/meteor</td><td>▁▄▆▆▆▆▇▇██████████</td></tr><tr><td>eval/rouge1</td><td>▁▄▆▆▆▆▇▇▇▇█████▇██</td></tr><tr><td>eval/rouge2</td><td>▁▄▇▆▆▆▇▇█▇█▇▇█▇▇██</td></tr><tr><td>eval/rougeL</td><td>▁▄▆▆▇▇▇▇██████████</td></tr><tr><td>eval/runtime</td><td>▆▄▄▂▁▂▃▄▃▃▆▃▃▃▅▅▆█</td></tr><tr><td>eval/samples_per_second</td><td>▃▅▅▇█▇▆▅▆▆▃▅▆▆▄▄▃▁</td></tr><tr><td>eval/steps_per_second</td><td>▃▅▅▇█▇▆▅▆▆▃▅▆▆▄▄▃▁</td></tr><tr><td>test/bleu</td><td>▁</td></tr><tr><td>test/loss</td><td>▁</td></tr><tr><td>test/meteor</td><td>▁</td></tr><tr><td>test/rouge1</td><td>▁</td></tr><tr><td>test/rouge2</td><td>▁</td></tr><tr><td>test/rougeL</td><td>▁</td></tr><tr><td>test/runtime</td><td>▁</td></tr><tr><td>test/samples_per_second</td><td>▁</td></tr><tr><td>test/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇██████</td></tr><tr><td>train/grad_norm</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/bleu</td><td>0.17997</td></tr><tr><td>eval/loss</td><td>0.47714</td></tr><tr><td>eval/meteor</td><td>0.43951</td></tr><tr><td>eval/rouge1</td><td>0.48178</td></tr><tr><td>eval/rouge2</td><td>0.24131</td></tr><tr><td>eval/rougeL</td><td>0.46998</td></tr><tr><td>eval/runtime</td><td>2.0622</td></tr><tr><td>eval/samples_per_second</td><td>29.095</td></tr><tr><td>eval/steps_per_second</td><td>3.879</td></tr><tr><td>test/bleu</td><td>0.17997</td></tr><tr><td>test/loss</td><td>0.47714</td></tr><tr><td>test/meteor</td><td>0.43951</td></tr><tr><td>test/rouge1</td><td>0.48178</td></tr><tr><td>test/rouge2</td><td>0.24131</td></tr><tr><td>test/rougeL</td><td>0.46998</td></tr><tr><td>test/runtime</td><td>2.0215</td></tr><tr><td>test/samples_per_second</td><td>29.681</td></tr><tr><td>test/steps_per_second</td><td>3.957</td></tr><tr><td>total_flos</td><td>1240136926887936.0</td></tr><tr><td>train/epoch</td><td>17</td></tr><tr><td>train/global_step</td><td>1156</td></tr><tr><td>train/grad_norm</td><td>0.77817</td></tr><tr><td>train/learning_rate</td><td>3e-05</td></tr><tr><td>train/loss</td><td>0.3689</td></tr><tr><td>train_loss</td><td>0.72789</td></tr><tr><td>train_runtime</td><td>1309.1375</td></tr><tr><td>train_samples_per_second</td><td>16.469</td></tr><tr><td>train_steps_per_second</td><td>2.078</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">T5 SMALL + TOP5 DOCS TFIDF CLEANED </strong> at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/3p5ajmym' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications/runs/3p5ajmym</a><br> View project at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/T5-Clarifications</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250506_200750-3p5ajmym/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"T5-Clarifications\",  \n",
    "    name=\"T5 SMALL + TOP5 DOCS TFIDF CLEANED \"                   \n",
    ")\n",
    "train(\"t5-small\", \"predicted_clarif_T5SMALL_TOP5DOC_TFIDF_CLEANED.jsonl\",tokenizer, tokenized_train_dataset, tokenized_eval_dataset,40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e828684-37a6-4539-a488-7b4a8cb1187a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6e312c7-1f22-48b7-892b-b22142021c16",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29e4133-8414-421c-abfd-f5a28e8d078b",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4a597e-50a1-4652-9925-ac74241f6f28",
   "metadata": {},
   "source": [
    "# T5 EFFICIENT -------------- TOP5 RAW DOCS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "577268e3-90af-41c5-a08c-b6a4dd48e205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "#fucntion to tokenize the dataset based on the model\n",
    "def tokenize_and_split_dataset(filename, modelname):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_data = json.load(f)\n",
    "        \n",
    "    # Choose your model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(modelname)\n",
    "\n",
    "    # Tokenization function\n",
    "    def preprocess(batch):\n",
    "        model_inputs = tokenizer(\n",
    "            batch[\"input\"],\n",
    "            max_length=512,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(\n",
    "                batch[\"output\"],\n",
    "                max_length=64,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True\n",
    "            )\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "    \n",
    "    # Convert raw data to Dataset object\n",
    "    dataset = Dataset.from_list(raw_data)\n",
    "    tokenized_dataset = dataset.map(preprocess, batched=True)\n",
    "\n",
    "    return tokenizer, tokenized_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84b05fa6-a374-45dc-ade1-8544fd37135c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#fucntion to split dataset into folds\n",
    "def generate_folds(tokenized_dataset, n_splits=5, seed=42):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    folds = list(kf.split(tokenized_dataset))\n",
    "    return folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4abe23d8-a598-474f-a764-1e28f14cee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from transformers import AutoModelForSeq2SeqLM, Trainer, TrainingArguments, EarlyStoppingCallback, TrainerCallback\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Custom callback to log metrics after each evaluation step\n",
    "class WandbLoggingCallback(TrainerCallback):\n",
    "    def __init__(self, fold_num):\n",
    "        self.fold_num = fold_num\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        if state.is_world_process_zero:\n",
    "            log_data = {f\"Fold_{self.fold_num}/eval/{key}\": value for key, value in metrics.items()}\n",
    "            wandb.log(log_data)\n",
    "\n",
    "def cross_val_train(modelname, filename, run_name, tokenizer, tokenized_dataset, folds, nb_epochs=30):\n",
    "    # Start a single WandB run for all folds\n",
    "    wandb.init(project=\"cross_val_T5\", name=run_name, config={\"epochs\": nb_epochs})\n",
    "    filenamebis= filename.split(\".jsonl\")[0]\n",
    "    fold_metrics = []\n",
    "\n",
    "    for fold_num, (train_idx, test_idx) in enumerate(folds):\n",
    "        print(f\"Processing Fold {fold_num + 1}\")\n",
    "\n",
    "        # Split the dataset\n",
    "        train_dataset = tokenized_dataset.select(train_idx)\n",
    "        test_dataset = tokenized_dataset.select(test_idx)\n",
    "\n",
    "        # Initialize the model\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(modelname, from_tf=True)\n",
    "\n",
    "        # Training arguments with early stopping\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"./results/{run_name}/fold_{fold_num+1}\",\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=5e-5,\n",
    "            per_device_train_batch_size=8,\n",
    "            per_device_eval_batch_size=8,\n",
    "            num_train_epochs=nb_epochs,\n",
    "            load_best_model_at_end=True,\n",
    "            logging_steps=20,\n",
    "            logging_first_step=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            seed=42,\n",
    "        )\n",
    "\n",
    "        # Initialize Trainer with the logging callback and early stopping\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=test_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            # compute_metrics=compute_metrics,\n",
    "            compute_metrics=lambda eval_pred: compute_metrics(eval_pred, tokenizer),\n",
    "            callbacks=[\n",
    "                WandbLoggingCallback(fold_num + 1),\n",
    "                EarlyStoppingCallback(early_stopping_patience=3)\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "\n",
    "        # Final evaluation for the fold\n",
    "        eval_results = trainer.evaluate()\n",
    "        print(f\"Fold {fold_num + 1} results: {eval_results}\")\n",
    "\n",
    "        # Store the final evaluation results for this fold\n",
    "        fold_metrics.append(eval_results)\n",
    "\n",
    "        # Predict on the test dataset\n",
    "        predictions = trainer.predict(test_dataset)\n",
    "\n",
    "    \n",
    "        # Decode predictions and save to file\n",
    "        pred_ids = np.argmax(predictions.predictions[0], axis=-1)\n",
    "        predicted_texts = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "        true_texts = tokenizer.batch_decode(predictions.label_ids, skip_special_tokens=True)\n",
    "        input_ids = test_dataset[\"input_ids\"]\n",
    "        input_texts = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Save predictions to a file\n",
    "\n",
    "        output_predictions_file = f\"{filenamebis}_fold_{fold_num+1}.jsonl\"\n",
    "        with open(output_predictions_file, \"w\", encoding=\"utf-8\") as writer:\n",
    "            for input_text, true_text, predicted_text in tqdm(zip(input_texts, true_texts, predicted_texts), total=len(true_texts)):\n",
    "                output_dict = {\n",
    "                    \"input\": input_text,\n",
    "                    \"true\": true_text,\n",
    "                    \"predicted\": predicted_text\n",
    "                }\n",
    "                writer.write(json.dumps(output_dict, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "        print(f\"Predictions for fold {fold_num + 1} saved to {output_predictions_file}\")\n",
    "\n",
    "    # Calculate and log average metrics\n",
    "    avg_metrics = {key: np.mean([fold[key] for fold in fold_metrics]) for key in fold_metrics[0]}\n",
    "    wandb.log({f\"Averages/{key}\": value for key, value in avg_metrics.items()})\n",
    "    print(\"Average metrics over all folds:\", avg_metrics)\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    #merge all to a single file for predcitions\n",
    "\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as merged_file:\n",
    "        for fold_num in range(len(folds)):\n",
    "            fold_file = f\"{filenamebis}_fold_{fold_num+1}.jsonl\"\n",
    "            if os.path.exists(fold_file):\n",
    "                with open(fold_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                    for line in f:\n",
    "                        merged_file.write(line)\n",
    "    print(f\"Final predicitons for all folds foudn in {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9b495fc4-79cb-4ea2-af6e-03c66e43c07c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d6f12980c5849faa9d76c88a186f7b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/Etu0/21402600/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': '[QUERY] Find information on President Barack Obama\\'s family history, including genealogy, national origins, places and dates of birth, etc.\\n[DOCUMENTS]\\n[DOC 1] Family of Barack Obama - Wikipedia, the free encyclopedia\\\\nFamily of Barack Obama\\\\nFrom Wikipedia, the free encyclopedia\\\\n(Redirected from Sasha Obama)\\\\nJump to: navigation, search\\\\nObama Family\\\\nPresident Barack Obama, First Lady Michelle, and daughters Malia and Sasha wave to the crowd after his inaugural address Jan. 20, 2009, on the west steps of the U.S. Capitol.[1]\\\\nCurrent region\\\\nWashington, DC\\\\nInformation\\\\nPlace of origin\\\\nUnited States\\\\nNotable members\\\\nBarack Obama, Michelle Obama, Ann Dunham, Barack Obama, Sr., etc.\\\\nConnected families\\\\nRobinson, Dunham, Soetoro, Ng\\\\nThis article is part of a series about\\\\nBarack Obama\\\\nBackground \\xa0ýý Illinois Senate \\xa0ýý U.S. Senate\\\\nPolitical positions\\xa0ýý Public image\\xa0ýý Family\\\\n2008 primaries\\xa0ýý ObamaýýýBiden campaign\\\\nTransition\\xa0ýý Inauguration\\xa0ýý Electoral history\\\\nPresidency (Timeline, First 100 days)\\\\nMalia, Michelle and Sasha on stage at the 2008 Democratic National Convention\\\\nThe Family of Barack Obama is an extended clan of African American, English, Indonesian, and Kenyan (Luo) heritage known through the writings and political career of Barack Obama, the President of the United States of America,[2][3][4][5] and other reports. His immediate family is the First Family of the United States. The Obamas are the first First Family of African American descent in the United States and the youngest to enter the White House since the Kennedys. One columnist wrote, ýýýObama\\'s young, energetic family harks back to days of Camelot.ýýý[6]\\\\nContents\\\\n1 Immediate family\\\\n2 Extended family - maternal relations\\\\n3 Extended family - paternal relations\\\\n4 Michelle Robinson Obama\\'s extended family\\\\n5 Genealogical charts\\\\n5.1 Obama ancestry\\\\n5.2 Family trees\\\\n6 Distant relations\\\\n7 See also\\\\n8 References\\\\n9 External links\\\\nImmediate family\\\\nMichelle Obama\\\\nMichelle Obama, nýýe Robinson, the wife of Barack Obama, was born on January 17, 1964 in Chicago, Illinois. She is a lawyer and was a University of Chicago Hospital Vice-President. She is the First Lady of the United States.\\\\nMalia Obama and Sasha Obama\\\\nBarack and Michelle Obama have two daughters: Malia Ann (pronounced /mýýýýliýýýý/), born in 1998,[7] and Natasha (known as Sasha) /ýýsýýýýýýýý/), born in 2001. Sasha is the youngest child to reside in the White House since John F. Kennedy, Jr, arrived as an infant in 1961.[8]\\\\nBefore his inauguration, President Obama published an open letter to his daughters in Parade magazine, describing what he wants for them and every child in America: \\\\\"to grow up in a world with no limits on your dreams and no achievements beyond your reach, and to grow into compassionate, committed women who will help build that world.\\\\\"[9]\\\\nWhile living in Chicago, they kept busy schedules, as the Associated Press reports: \\\\\"soccer, dance and drama for Malia, gymnastics and tap for Sasha, piano and tennis for both.\\\\\"[10][11] In July 2008, the family gave an interview to the television series Access Hollywood; Obama later said they regretted allowing the children to be included.[12]\\\\nIn his victory speech on the night of his election, President Obama repeated his promise to Sasha and Malia to get a puppy to take with them to the White House.[13] However the selection of a dog has been slow because Malia is allergic to animal dander;[14] the president subsequently said that the choice has been narrowed down to either a labradoodle or Portuguese Water Dog, and they are hoping to find a shelter animal.[15]\\\\nMalia and Sasha attend the private Sidwell Friends School in Washington, DC, the same school as attended by Chelsea Clinton, Tricia Nixon Cox, and Archibald Roosevelt, and currently the grandchildren of Vice President Joe Biden.[16] The Obama girls began classes there on January 5, 2009.[17] While in Chicago, both attended the private University of Chicago Laboratory School.\\\\nMarian Shields Robinson\\\\nMichelle Obama\\'s mother (birthname Marian Shields, born July 1937), now widowed, married Michelle\\'s father, Fraser Robinson, in 1960.[18][19] Robinson was formerly a secretary at Spiegel catalog and a bank. While Michelle and Barack Obama were campaigning in 2008, Robinson tended the Obama\\'s young children and she intends to do the same while in Washington, DC. Robinson is currently living in the White House itself as part of the First Family;[20] she is the first live-in grandmother there since Elivera M. Doud during the Eisenhower administration.[21] Some media outlets have dubbed Robinson the \\\\\"First Granny\\\\\".[22][21]\\\\nExtended family - maternal relations\\\\nRight-to-left: Barack Obama and Maya Soetoro with their mother Ann Dunham and grandfather Stanley Dunham in Hawaii (early 1970s)\\\\nWikinews has related news:\\\\nBarack Obama elected 44th President of the United States\\\\nGrandmother of Barack Obama dies at 86\\\\nMadelyn Dunham with her daughter Ann\\\\nAccording to Barack Obama\\'s Dreams from My Father, his great-grandmother Leona McCurry was part Native American, which Obama believed Leona held as a \\\\\"source of considerable shame\\\\\" and \\\\\"blanched whenever someone mentioned the subject and hoped to carry the secret to her grave\\\\\"; whereas McCurry\\'s daughter (Obama\\'s maternal grandmother) \\\\\"would turn her head in profile to show off her beaked nose, which along with a pair of jet-black eyes, was offered as proof of Cherokee blood.\\\\\"[23] To date, no concrete evidence has surfaced of Cherokee heritage. Obama\\'s maternal heritage consists mostly of English ancestry, with much smaller amounts of German, Irish, Scottish, Welsh, Swiss, and French ancestry.[2]\\\\nAnn Dunham\\\\nMother of Barack Obama born in 1942, died in 1995. Birthname Stanley Ann Dunham. Anthropologist in Hawaii and Indonesia.\\\\nMadelyn Lee Payne Dunham\\\\nBarack Obama\\'s maternal grandmother, born in 1922 and died on November 2, 2008.[24] She was a bank vice president in Hawaii. Obama said that when he was a child, his grandmother \\\\\"read me the opening lines of the Declaration of Independence and told me about the men and women who marched for equality because they believed those words put to paper two centuries ago should mean something.\\\\\"[9]\\\\nStanley Armour Dunham\\\\nGrandfather of Barack Obama, born 1918, died 1992. World War II U.S. Army sergeant, furniture salesman in Hawaii.\\\\nCharles T. Payne\\\\nGreat-uncle of Barack Obama, younger brother of Madelyn Dunham, born 1925. Served during World War II in the U.S. Army 89th Infantry Division.[25] Obama has often described Payne\\'s role in liberating Ohrdruf forced labor camp.[26] There was brief media attention when Obama mistakenly identified the camp as Auschwitz during the campaign.[27] Payne appeared in the visitor\\'s gallery at the Democratic National Convention in Denver, Colorado, when his great-nephew was nominated for President.[28] He was the assistant director of the University of Chicago\\'s Library.[26]\\\\nMaya Soetoro-Ng\\\\nHalf-sister of Barack Obama, born August 15, 1970, in Jakarta, Indonesia.[29] She is married to Konrad Ng, with whom she has a daughter, Suhaila. Maya Soetoro-Ng is a teacher in Hawaii.\\\\nKonrad Ng\\\\nBrother-in-law of Barack Obama, born 1974. A Canadian whose parents are Malaysian Chinese immigrants, he is an assistant professor at the University of Hawaii\\'s Academy of Creative Media.[30] His parents are from Kudat and Sandakan, two small towns in Sabah, Malaysia, and he was born and raised in Burlington, Ontario.[31] He married Maya Soetoro-Ng at the end of 2003 in Hawaii.[32] They have one daughter, Suhaila.[33][34][35] Konrad Ng is now a US citizen.[36]\\\\nLolo Soetoro\\\\nStepfather of Barack Obama, born in Indonesia 1936, died 1987.\\\\nExtended family - paternal relations\\\\nThe Obamas are members of the Luo, Kenya\\'s third-largest ethnic group, which is part of a larger family of ethnic groups, collectively also known as Luo. This group belongs to the Eastern Sudanic branch of the Nilo-Saharan phylum. The Obama family is largely concentrated in the western province of Nyanza.\\\\nFront row (left to right): Auma Obama (Barack\\'s half-sister), Kezia Obama (Barack\\'s step-mother), Sarah Hussein Onyango Obama (third wife of Barack\\'s paternal grandfather), Zeituni Onyango (Barack\\'s aunt)\\\\nBack row (left to right): Said Obama (Barack\\'s uncle), Barack Obama, Abongo [Roy] Obama (Barack\\'s half-brother), unidentified woman, Bernard Obama (Barack\\'s half-brother), Abo Obama (Barack\\'s half-brother).\\\\nBarack Obama, Sr.\\\\nBarack Obama\\'s father, (1936ýýý1982). Government economist in Kenya. In addition to President Obama, Barack Obama Sr. fathered six other sons and a daughter.[37]\\\\nHussein Onyango Obama\\\\nBarack Obama\\'s paternal grandfather (c. 1895ýýý1979);[38] he worked as a mission cook. He joined the British Army during World War I. (One source gives 1870ýýý1975 as his dates of birth and death based on his tombstone reading \\\\\"Mzee Hussein Onyango Obama\\\\\" in his home village.[39] The term \\\\\"mzee\\\\\" is a Kenyan honorific meaning \\\\\"old man\\\\\" or \\\\\"elder.\\\\\") According to his third wife, Sarah, he originally converted to Catholicism, but took the name Hussein when he later converted to Islam; she said he passed the name, not the religion, on to his children.[40]\\\\nHabiba Akumu Obama\\\\nBarack Obama\\'s paternal grandmother, and the second wife of Hussein Onyango Obama. A photograph of her holding her son, Barack Sr, on her lap is on the cover of her grandson\\'s memoirs titled Dreams from my Father.[41]\\\\nSarah Obama\\\\nThird wife of Obama\\'s paternal grandfather, born 1922.[39] Also known, through the addition of her late husband\\'s name, as Sarah Onyango Obama,[42] and sometimes referred to as Sarah Ogwel, Sarah Hussein Obama or Sarah Anyango Obama,[43] she lives in Nyangýýýoma Kogelo village, 30 miles west of western Kenya\\'s main town, Kisumu, on the edge of Lake Victoria.[44][45]\\\\nAlthough not a blood relation, Barack Obama calls her \\\\\"Granny Sarah\\\\\".[43][46] Sarah, who speaks Luo and only a few words of English, communicates with President Obama through an interpreter.\\\\nOn July 4, 2008, she attended the United States Independence Day celebrations in Nairobi, hosted by Michael Ranneberger, the US ambassador in Kenya.[47]\\\\nDuring the campaign she protested attempts to portray Obama as a foreigner to the United States or a Muslim, saying that while Obama\\'s grandfather had been a Muslim, \\\\\"In the world of today, children have different religions from their parents.\\\\\"[40] Sarah Obama herself is \\\\\"a strong believer of the Islamic faith,ýýý in her words.[48]\\\\nKezia Obama\\\\nFirst wife of Barack Obama\\'s father, born c. 1940. She is Barack Obama Sr.\\'s first wife, whom he married in Kenya before studying abroad in the United States. Also known as Kezia Grace Obama.[49][50] She currently lives in Bracknell, Berkshire, England.[51][52] Her sister, Jane, is the \\'Auntie Jane\\' mentioned at the very start of Dreams from My Father when she telephoned President Obama to inform him that his father had been killed in a car accident.[53]\\\\nMalik Obama\\\\nBarack Obama\\'s half-brother, also known as Abongo or Roy, born c. March, 1958,[51] son of Barack Obama, Sr. with his first wife, Kezia.[54] Malik Obama was born and raised in Nairobi, Kenya.[55] He earned a degree in accounting from the University of Nairobi.[56] He met his half-brother for the first time in 1985[55] when Barack flew from Chicago to Washington, D.C. to visit him.[57] Malik and his half-brother Barack were best men at each other\\'s weddings.[55] Barack Obama brought his wife Michelle to Kenya three years later, and they met with Malik again while Barack was introducing Michelle to many other new relatives.[58]\\\\nAlthough much of the Obama family has dispersed throughout Kenya and overseas, most, including Malik Obama, still considered their rural village on the shores of Lake Victoria to be their true home, and feel that those who have left the village have become culturally \\\\\"lost\\\\\".[59] A frequent visitor to the United States,[58] and consultant in Washington, D.C. for several months per year,[55] he nevertheless settled in the Obamas\\' ancestral home, Nyangýýýoma Kogelo, a village of several hundred people that he prefers to the city for its slow pace.[55] He runs a small electronics shop a half hour drive outside of town.[55]\\\\nDuring his brother\\'s presidential campaign, Malik Obama was a spokesman for the extended Obama family in Kenya, dealing with safety and privacy concerns arising from increased attention from the press.[60]\\\\nAbo Obama\\\\nBarack Obama\\'s half-brother, born 1968. International telephone store manager in Kenya.\\\\nAuma Obama\\\\nBarack Obama\\'s half-sister, born c. 1960.[61] As of July 2008, development worker in Kenya.[62] She studied German at the University of Heidelberg from 1981 to 1987. After her graduation in Heidelberg she went on for graduate studies at the University of Bayreuth, which awarded her a PhD in 1996. Her dissertation was about the conception of labor in Germany and its literary reflections.[62] Auma Obama lives in London, and in 1996 married an Englishman, Ian Manners. They have a daughter named Akinyi (b. 1997).[62][verification needed]\\\\nBernard Obama\\\\nBarack Obama\\'s half-brother, born 1970, son of Barack Obama, Sr. and his first wife, Kezia. He had been an auto parts supplier in Nairobi, Kenya, and has one child. Bernard converted to Islam as an adult and has said: \\\\\"Iýýým a Muslim, I donýýýt deny it. My father was raised a Muslim. But itýýýs not an issue. I donýýýt know what all the hullabaloo is about.\\\\\"[63] He currently resides in Bracknell, England, with his mother Kezia.[63]\\\\nRuth Ndesandjo\\\\nBorn Ruth Nidesand, in US c. 1940s, Barack Obama Sr.\\'s third wife and a private kindergarten director in Kenya.[64] Ruth\\'s two sons with Barack Obama, Sr., are Mark and David Ndesandjo; her third son, Joseph Ndesandjo, was born c. 1980 from a subsequent marriage to a Tanzanian.[65][66]\\\\nMark Ndesandjo\\\\nBarack Obama\\'s half-brother, son of Ruth Nidesand and Barack Obama Sr.[67] He runs an Internet company called WorldNexus that advises Chinese corporations how best to reach international customers.[68] Mark graduated from Brown University, studied physics at Stanford University, received an MBA from Emory University, and has lived in Shenzhen, China, since 2002 and is married to a Chinese woman.[69] He is also an accomplished pianist.[70]\\\\nDavid Ndesandjo\\\\nDavid Ndesandjo\\\\nBarack Obama\\'s half-brother (also known as David Opiyo Obama), son of Ruth Nidesand and Barack Obama Sr. Killed in a motorcycle accident.[71]\\\\nGeorge Hussein Onyango Obama\\\\nYoungest half-brother of Barack Obama, born c.1982, son of Barack Obama Sr.[72] and Jael (now a resident of Atlanta, Georgia).[73][74] George was six months old when his father died in an automobile accident, after which he was raised in Nairobi by his mother and a French step-father. He later lived in South Korea for two years while his mother resided there for business reasons.[73] Returning to Kenya, George Obama \\\\\"slept rough for several years,\\\\\" until his aunt gave him a six-by-eight foot corrugated metal shack in the Nairobi, Kenya, slum of Huruma Flats.[73] As of August 2008, Obama was studying to become a mechanic.[73] George received little attention until being featured in an article in the Italian language edition of Vanity Fair in August 2008, which portrayed him as living in poverty, shame, and obscurity.[75] The article quoted Obama as saying that he lived \\\\\"on less than a dollar a month\\\\\" and stated that he \\\\\"does not mention his famous half-brother in conversation\\\\\" out of shame at his poverty.[76] In later interviews George contradicted this picture. In an interview with The Times, Obama \\\\\"said that he was furious at subsequent reports that he had been abandoned by the Obama family and that he was filled with shame about living in a slum.\\\\\"[74] He told The Times, \\\\\"Life in Huruma is good.\\\\\" Obama said that he expects no favors, that he was supported by relatives, and that reports he lived on a dollar a month were \\\\\"all lies by people who donýýýt want my brother to win.ýýý[74] He told The Telegraph that he was inspired by his half-brother.[73] According to Time, George \\\\\"has repeatedly denied...that he feels abandoned by Obama.\\\\\"[77] CNN quoted him as saying, \\\\\"I was brought up well. I live well even now. The magazines, they have exaggerated everything... I think I kind of like it here. There are some challenges, but maybe it is just like where you come from, there are the same challenges.\\\\\"[75] George\\'s reported poverty was seized on by conservative critics of Barack Obama. Columnist Dinesh D\\'Souza solicited donations for George Obama from his readers,[78] while Jerome Corsi planned to give him a $1,000 check during a trip to Kenya (Corsi was expelled from the country by immigration authorities).[77]\\\\nOmar Obama\\\\nHalf-uncle of Barack Obama,[79] born on June 3, 1944 in Nyangýýýoma Kogelo. Oldest son of Onyango and Sarah Obama, resides in Boston, Massachusetts.[citation needed]\\\\nZeituni Onyango\\\\nHalf-aunt of Barack Obama,[80] born May 29, 1952, in Kenya,[81] Onyango is referred to as \\\\\"Aunti Zeituni\\\\\" in President Obama\\'s memoir, Dreams from My Father.[82]\\\\nYusuf Obama\\\\nHalf-uncle of Barack Obama,[79] born c. 1950s in Nyangýýýoma Kogelo; son of Onyango and Sarah Obama.[citation needed]\\\\nSaid Obama\\\\nHalf-uncle of Barack Obama,[79][83] born c. 1950s in Nyangýýýoma Kogelo; son of Onyango and Sarah Obama.[citation needed]\\\\nMichelle Robinson Obama\\'s extended family\\\\nFraser Robinson, Sr. (1884ýýý1936) of South Carolina, shown in an old photo along with his wife, Rosella Cohen Robinson, in its background\\\\nBarack Obama has called his wife Michelle \\\\\"the most quintessentially American woman I know.\\\\\"[3] Her family is of African American heritage, descendents of Africans of the American Colonial Era.[3] Michelle Obama\\'s family history traces back from slavery to Reconstruction to the Great Migration North. Some of Michelle\\'s relatives still reside in South Carolina.\\\\nMichelle\\'s earliest known relative is her great-great grandfather Jim Robinson, born in the 1850s, who was an American slave on the Friendfield plantation in South Carolina. The family believes that after the Civil War he remained a Friendfield plantation sharecropper for the rest of his life and that he was buried there in an unmarked grave.[3]\\\\nJim had two sons, Gabriel and Fraser, Michelle Obama\\'s great-grandfather. Fraser had an arm amputated as a result of a boyhood injury. He worked as a shoemaker, newspaper salesman and in a lumber mill and was married to Rosella Cohen.[3] Carrie Nelson, Gabriel Robinson\\'s daughter, now 80, is the oldest living Robinson and the keeper of family lore.[3]\\\\nAt least three of Michelle Obama\\'s great-uncles served in the military of the United States. One aunt moved to Princeton, New Jersey, where she worked as a maid, and cooked Southern-style meals for Michelle and her brother, Craig, when they were students at Princeton University.\\\\nCraig Robinson\\\\nMichelle Obama\\'s brother, born 1962. He is currently head coach of men\\'s basketball at Oregon State University.[84]\\\\nFraser Robinson III\\\\nMichelle Obama\\'s father, born 1935, died 1991, married Michelle\\'s mother, Marian Shields, in 1960.[85][19] Robinson was a pump worker at the City of Chicago water plant.[3]\\\\nFraser Robinson, Jr.\\\\nMichelle Obama\\'s grandfather was born on August 24, 1912 in Georgetown, South Carolina, and died on November 9, 1996, aged 84. He was a good student and orator, but moved from South Carolina to Chicago to find better work than he could find at home, eventually becoming a worker for the United States Postal Service. He was married to LaVaughn Johnson. When he retired, they moved back to South Carolina.[3]\\\\nCapers C. Funnye Jr.\\\\nMichelle Obama\\'s first cousin once removed: Funnyeýýýs mother, Verdelle Robinson Funnye (born Verdelle Robinson; August 22, 1930 ýýý April 16, 2000) and Michelle Obamaýýýs paternal grandfather, Fraser Robinson Jr., were siblings. One of America\\'s most prominent African American Jews, known for acting as a bridge between mainstream Jewry and African Americans.[86]\\\\nGenealogical charts\\\\nObama ancestry\\\\n16. Opiyo\\\\n8. Obama\\\\n4. Hussein Onyango Obama\\\\n9. Nyaoke\\\\n2. Barack Hussein Obama, Sr.\\\\n5. Habiba Akumu\\\\n1. Barack Hussein Obama II\\\\n24. Jacob William Dunham\\\\n12. Ralph Waldo Emerson Dunham, Sr.\\\\n25. Mary Ann Kearney\\\\n6. Stanley Armour Dunham\\\\n26. Harry Ellington Armour\\\\n13. Ruth Lucille Armour\\\\n27. Gabriella Clark\\\\n3. Stanley Ann Dunham\\\\n28. Charles T. Payne\\\\n14. Rolla Charles Payne\\\\n29. Della L. Wolfley\\\\n7. Madelyn Lee Payne\\\\n30. Thomas Creekmore McCurry\\\\n15. Leona Belle McCurry\\\\n31. Margaret Belle Wright\\\\nFamily trees\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nBarack Obama\\\\nStanley\\\\nDunham\\\\n1918ýýý1992\\\\nMadelyn\\\\nPayne\\\\n1922ýýý2008\\\\nHabiba\\\\nAkumu\\\\nOnyango\\\\nObama\\\\nc.\\xa01895ýýý1979\\\\nSarah\\\\nOgwel\\\\n1922ýýý\\\\nLolo\\\\nSoetoro\\\\n1936ýýý1987\\\\nAnn\\\\nDunham\\\\n1942ýýý1995\\\\nBarack\\\\nObama, Sr. *\\\\n1936ýýý1982\\\\nKezia\\\\nGrace\\\\nc. 1940ýýý\\\\nOmar\\\\nObama\\\\n1944ýýý\\\\nZeituni\\\\nOnyango\\\\n1952ýýý\\\\nYusuf\\\\nObama\\\\nc. 1950sýýý\\\\nSaid\\\\nObama\\\\nc. 1950sýýý\\\\nKonrad\\\\nNg\\\\nc. 1974ýýý\\\\nMaya\\\\nSoetoro\\\\n1970ýýý\\\\nBarack\\\\nObama\\\\n1961ýýý\\\\nMichelle\\\\nRobinson\\\\n1964ýýý\\\\nM. Abongo\\\\nObama\\\\n1958ýýý\\\\nAuma\\\\nObama\\\\nc. 1960ýýý\\\\nAbo\\\\nObama\\\\n1968ýýý\\\\nBernard\\\\nObama\\\\n1970ýýý\\\\nSuhaila\\\\nNg\\\\nc. 2005ýýý\\\\nMalia Ann\\\\nObama\\\\n1998ýýý\\\\nSasha\\\\nObama\\\\n2001ýýý\\\\n* Barack\\\\nObama, Sr.\\'s\\\\nadditional\\\\nRuth\\\\nNidesandjo\\\\nc. 1940sýýý\\\\nJael\\\\nOtieno\\\\nrelationships:\\\\nMark\\\\nNdesandjo\\\\nDavid\\\\nNdesandjo\\\\ndied\\xa0c.\\xa01987\\\\nGeorge\\\\nObama\\\\nc. 1982ýýý\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nFirst Lady Michelle Obama\\\\nFraser\\\\nRobinson, Jr.\\\\nýýý\\\\nSouth Carolina\\\\n(1912ýýý1996)\\\\nSawmill worker\\\\nLaVaughn D.\\\\nJohnson\\\\nýýý\\\\nIllinois\\\\n(1915ýýý2002)\\\\nCapers C.\\\\nFunnye, Jr.\\\\nýýý\\\\n(born c. 1952;\\\\nnephew of\\\\nFraser\\\\nRobinson, Jr.)\\\\nRabbi in Chicago\\\\nFraser\\\\nRobinson III\\\\nýýý\\\\nIllinois\\\\n(1935ýýý1991)\\\\nEnjoyed boxing\\\\nin youth;\\\\nwater plant\\\\npump operator\\\\nin Chicago\\\\nMarian\\\\nShields\\\\nýýý\\\\nIllinois\\\\n(born 1937)\\\\nSecretary at\\\\nSpiegel catalog\\\\nin Chicago;\\\\nU.S.\\'s First\\\\nGrandmother\\\\nCraig\\\\nRobinson\\\\nýýý\\\\nIllinois\\\\n(born 1962)\\\\nHead coach of\\\\nOregon State\\\\nBeavers men\\'s\\\\nbasketball\\\\nMichelle\\\\nRobinson\\\\nýýý\\\\nIllinois\\\\n(born 1964)\\\\nFirst Lady\\\\nof the United States\\\\nDistant relations\\\\nSee also: List of United States Presidents by genealogical relationship\\\\nAccording to genealogists, Barack Obama\\'s distant cousins include the multitude of descendants of his maternal ancestors from all along the early-American Atlantic seaboard as well as paternal, Kenyan relations belonging to the Luo tribe, many descending from a 17th century ancestor named Owiny.[87][88] For example, George W. Bush, the 43rd U.S. president, is the eleventh cousin of Barack Obama.[89] The New York Times science writer Nicholas Wade argues that with eleven generations leading back to their common progenitor, Samuel Hinckley, the relationship between the 43rd President and the 44th President is \\\\\"genetically meaningless\\\\\".[90]\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nSelected genealogical relationships\\\\nBill Hickok\\\\nAccording to Barack Obama\\'s family lore (and confirmed by the New England Historic Genealogical Society), the President and Hickok are sixth cousins, six-times removed.[91]\\\\nRobert Duvall\\\\nGoodnight helped inspire Pulitzer Prize-winning author Larry McMurtry to create a protagonist for his novel series Lonesome Dove: Woodrow Call. In Dove\\'s television novela, Woodrow Call\\'s partner is Gus McCrae, portrayed by Obama\\'s eighth cousin, twice removed, actor Robert Duvall.[92]\\\\nJames Madison\\\\nObama is also distantly related to U.S. Presidents James Madison, Lyndon Johnson, Gerald Ford, and U.S. Vice President Dick Cheney, British Prime Minister Sir Winston Churchill, U.S. Civil War General Robert E. Lee, and actor Brad Pitt.[93][94][95]\\\\nCharles Goodnight\\\\nAccording to Chicago Sun-Times reporter Scott Fornek, another Obama progenitor, Catherine Goodnight, was the grandniece of George Goodnight, who was in turn great-grandfather of famed cattleman Charles Goodnight.[92]\\\\nHarry S. Truman\\\\nActor Duvall is distant cousins with United States President Harry Truman, who\\'s likewise a fourth cousin, four times removed, of Obama\\'s.[92]\\\\nGeorgia O\\'Keeffe\\\\nNotable women Obama is related to include painter Georgia OýýýKeeffe.[93]\\\\nSee also\\\\nU.S. Presidential genealogical relationships\\\\nNew England Historic Genealogical Society\\\\nGary Boyd Roberts\\\\nReferences\\\\n^ Keck, Kristi (4 June 2008). \\\\\"Obama crosses historic milestone\\\\\". CNN. http://news.yahoo.com/s/ap/20081122/ap_on_go_pr_wh/obama_school_12. Retrieved on 2008-11-21.\\\\n^ a b Reitwiesner, William Addams. \\\\\"Ancestry of Barack Obama\\\\\". http://www.wargs.com/political/obama.html. Retrieved on 2008-10-09.\\\\n^ a b c d e f g h Murray, Shailagh (2 October 2008). \\\\\"A Family Tree Rooted In American Soil: Michelle Obama Learns About Her Slave Ancestors, Herself and Her Country\\\\\". The Washington Post: p.\\xa0C01. http://www.washingtonpost.com/wp-dyn/content/article/2008/10/01/AR2008100103169.html. Retrieved on 2008-10-10.\\\\n^ Sheridan, Michael (5 February 2007). \\\\\"Secrets of Obama Family Unlocked\\\\\". Muslim Observer. http://news.newamericamedia.org/news/view_article.html?article_id=ae5895fc29971b172938790be94ab107. Retrieved on 2008-11-21.\\\\n^ RTE news report March, 2007:Obamas Irish family links discovered by ancestry.co.uk\\\\n^ Noveck, Jocelyn (2008-11-07). \\\\\"Deseret News | Obama\\'s young, energetic family harks back to days of Camelot\\\\\". Secure.deseretnews.com. https://secure.deseretnews.com/article/1,5143,705261276,00.html. Retrieved on 2009-01-31.\\\\n^ Liza Mundy, Michelle: A Biography (Simon and Schuster, 2008), p. 129.\\\\n^ \\\\\"Sasha Obama\\\\\". Baltimore Sun. http://www.baltimoresun.com/topic/politics/sasha-obama-PECLB004381.topic. Retrieved on 2009-01-31.\\\\n^ a b Obama, Barack (13 January 2009). \\\\\"\\'What I Want for You--And Every Child in America\\'\\\\\". http://www.parade.com/news/2009/01/barack-obama-letter-to-my-daughters.html.\\\\n^ Sobieraj Westfall, Sandra (23 June 2008). \\\\\"Barack Obama Gives Daughter $1 Allowance a Week\\\\\". People Magazine. http://www.people.com/people/article/0,,20214569_1,00.html. Retrieved on 2008-11-21.\\\\n^ Lester, Will (July 23, 2008). \\\\\"Obama daughters keep hectic schedules of their own\\\\\". Associated Press. http://elections.apnews.com/apelect/db_6911/contentdetail.htm;jsessionid=8314A43012AB5FF1D0697247362D8752?contentguid=H95QubFb&full=true. Retrieved on 2008-08-04.\\\\n^ Hiro, Anne. \\\\\"Obama regrets letting \\\\\"Access Hollywood\\\\\" interview daughters. Won\\'t do it again. MSNBC\\'s Dan Abrams gets the story behind the story. - Lynn Sweet\\\\\". Blogs.suntimes.com. http://blogs.suntimes.com/sweet/2008/07/obama_regrets_letting_access_h.html. Retrieved on 2009-01-31.\\\\n^ Ahmed, Saeed (5 November 2008). \\\\\"Move over Barney, new dog moving into White House\\\\\". CNN. http://www.cnn.com/2008/LIVING/wayoflife/11/05/presidential.pets/index.html. Retrieved on 2008-11-21.\\\\n^ \\\\\"Obama: Getting a dog isn\\'t easy\\\\\". Associated Press. 7 November 2008. http://www.mercurynews.com/ci_10927292. Retrieved on 2008-11-21.\\\\n^ Janice Lloyd (2009-01-12). \\\\\"Obamas down to Labradoodle or Portuguese water dog\\\\\". USA Today. http://www.usatoday.com/news/washington/2009-01-11-obama-dog_N.htm. Retrieved on 2009-01-28.\\\\n^ Swarns, Rachel (21 November 2008). \\\\\"And the Winner Is ýýý Sidwell Friends\\\\\". The New York Times. http://thecaucus.blogs.nytimes.com/2008/11/21/and-the-winner-is-sidwell-friends/. Retrieved on 2008-11-21.\\\\n^ Tolin, Lisa (2009-01-05). \\\\\"Obama girls start school with photographers in tow\\\\\". The Associated Press. http://www.google.com/hostednews/ap/article/ALeqM5g6mv_lkODQMmQdpyIEnr8Zpm5mogD95H8KA80. Retrieved on 2009-01-06.\\\\n^ Taylor Marsh (2008-08-25). \\\\\"Political Analysis, National Security and Breaking News\\\\\". Taylor Marsh. http://www.taylormarsh.com/archives_view.php?id=28286. Retrieved on 2009-01-31.\\\\n^ a b Lia LoBello (2008-01-02). \\\\\"First Families: Radar introduces you to the next president\\'s relatives\\\\\". Radar Online. http://www.radaronline.com/features/2008/07/john_mccain_barack_obama_michelle_cindy_dunham_roberta_wrigh_04.php. Retrieved on 2009-01-28.\\\\n^ Rachel L. Swarns (2009-01-09). \\\\\"Obamaýýýs Mother-in-Law to Move Into the White House\\\\\". The New York Times. http://thecaucus.blogs.nytimes.com/2009/01/09/obamas-mother-in-law-to-move-into-the-white-house/?hp. Retrieved on 2009-01-09.\\\\n^ a b \\\\\"Will Obama mum-in-law make it a family affair in the White House?\\\\\". Agence France Presse. 2008-11-22. http://www.google.com/hostednews/afp/article/ALeqM5gN_i2jrCVkJQgfMbSDRRrNk8U4Sw. Retrieved on 2009-01-09.\\\\n^ Philip Sherwell (2008-2008-11-09). \\\\\"Michelle Obama persuades First Granny to join new White House team\\\\\". The Telegraph (UK). http://www.telegraph.co.uk/news/3407525/Michelle-Obama-persuades-First-Granny-to-join-new-White-House-team.html. Retrieved on 2009-01-09.\\\\n^ \\\\\"ýýýTootýýý: Obama grandmother a force that shaped him\\\\\". via Associated Press. 2008-08-25. http://www.thekansan.com/news/x1311851415/-Toot-Obama-grandmother-a-force-that-shaped-him. Retrieved on 2008-08-29.\\\\n^ \\\\\"CNN: \\\\\"Obama\\'s grandmother dies after battle with cancer\\\\\"\\\\\". http://www.cnn.com/2008/POLITICS/11/03/obama.grandma/index.html. Retrieved on 2008-11-04.\\\\n^ The 89th Infantry Division, United States Holocaust Memorial Museum\\\\n^ a b Obama\\'s great-uncle recalls liberating Nazi camp, Boston.com, July 22, 2008\\\\n^ Major Garrett (2008-05-27). \\\\\"Obama Campaign Scrambles to Correct the Record on Uncle\\'s War Service\\\\\". FOXNews.com. http://elections.foxnews.com/2008/05/27/recollection-of-obama-familys-service-missing-key-details. Retrieved on 2009-01-31.\\\\n^ \\\\\"Democrats salute Obamaýýýs great uncle\\\\\". Jewish Telegraphic Agency. August 28, 2008. http://jta.org/news/article/2008/08/28/110123/obamapayne. Retrieved on 31 January 2009.\\\\n^ Obama Family Tree dgmweb.net\\\\n^ Chicago Sun Times article with her picture\\\\n^ Obama has links to Malaysia\\\\n^ Nolan, Daniel (2008-06-11). \\\\\"Relative: Obama\\'s got \\'a good handle on Canada\\'\\\\\". The Hamilton Spectator. http://www.thespec.com/burlingtonlife/article/384475. Retrieved on 2008-07-03.\\\\n^ Nolan, Daniel (June 11, 2008). \\\\\"Obama\\'s Burlington connection\\\\\". The Hamilton Spectator. http://www.thespec.com/article/384307. Retrieved on 2008-06-21.\\\\n^ Misner, Jason (2008-06-20). \\\\\"Barack Obama was here\\\\\". Burlington Post. http://www.burlingtonpost.com/printarticle/186215. Retrieved on 2008-07-03.\\\\n^ Fornek, Scott (2007-09-09). \\\\\"\\'He helped me find my voice\\'\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545473,BSX-News-wotreehh09.article.\\\\n^ Cooper, Tom (2009-01-20). \\\\\"Keep watch for Obama\\\\\". TheSpec.com. http://www.thespec.com/Opinions/article/499161. Retrieved on 2009-01-28.\\\\n^ Ancestry of Barack Obama\\\\n^ Dreams from My Father, p. 376\\\\n^ a b Kenya: Special Report: Sleepy Little Village Where Obama Traces His Own Roots (Page 2 of 2)\\\\n^ a b \\\\\"Obama\\'s grandma slams \\'untruths\\'\\\\\". Associated Press. 2008-03-05. http://www.usatoday.com/news/world/2008-03-05-obama-kin_N.htm.\\xa0 See also this correction.\\\\n^ \\\\\"Q&A ON THE NEWS\\\\\". Atlanta Journal-Constitution. 2009-02-25. http://www.ajc.com/services/content/metro/stories/2009/02/25/questi0225.html. Retrieved on 2009-02-27.\\\\n^ In Kenya, Barack Obamaýýýs family prays for end to conflict - Times Online\\\\n^ a b Crilly, Rob (February 27, 2008). \\\\\"Dreams from Obama\\'s Grandmother\\\\\". Time Magazine, Inc.. http://www.time.com/time/world/article/0,8599,1717590,00.html?xid=rss-topstories. Retrieved on 2008-07-03.\\\\n^ Pflanz, Mike (2008-01-11). \\\\\"Barack Obama\\'s Kenyan relatives keep faith\\\\\". Daily Telegraph. http://www.telegraph.co.uk/news/main.jhtml?xml=/news/2008/01/09/wuspols1009.xml.\\\\n^ Fornek, Scott (2007-09-09). \\\\\"Sarah Obama - \\'Sparkling, laughing eyes\\'\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545459,BSX-News-wotreeu09.article.\\\\n^ \\\\\"Barack Obama in Kenya\\\\\". CNN. http://www.youtube.com/watch?v=Ikg6gj71U9k.\\\\n^ Daily Nation, July 8, 2008: Obama granny\\'s day out with envoys and top politicians\\\\n^ \\\\\"A Candidate, His Minister and the Search for Faith\\\\\". New York Times. 2007-04-30. http://www.nytimes.com/2007/04/30/us/politics/30obama.html?_r=2&pagewanted=all&oref=slogin&oref=slogin.\\\\n^ \\\\\"Kenya: All Obama kin to spend voting day in Kogelo\\\\\". afrika.no. 2008-10-27. http://www.afrika.no/Detailed/17321.html. Retrieved on 2009-01-31.\\\\n^ Cohen, Roger (2008-03-06). \\\\\"The Obamas of the World - New York Times\\\\\". Nytimes.com. http://www.nytimes.com/2008/03/06/opinion/06cohen.html. Retrieved on 2009-01-31.\\\\n^ a b Sanderson, Elizabeth (2008-01-06). \\\\\"Barack Obama\\'s stepmother living in Bracknell reveals the close bond with him ... and his mother\\\\\". Daily Mail. http://www.dailymail.co.uk/news/article-506338/Barack-Obamas-stepmother-living-Bracknell-reveals-close-bond---mother.html.\\\\n^ Lindsay, Anna (2009-01-20). \\\\\"Barack\\'s bingo-loving stepmother\\\\\". BBC News. http://news.bbc.co.uk/1/hi/england/berkshire/7834368.stm.\\\\n^ http://www.newvision.co.ug/D/8/26/666733\\\\n^ jpt (2008-06-18). \\\\\"From the Fact Check Desk: What Did Obama\\'s Half-Brother Say About Obama\\'s Background\\\\\". ABC News. http://blogs.abcnews.com/politicalpunch/2008/06/from-the-fact-c.html.\\\\n^ a b c d e f Maliti, Tom (2004-10-26). \\\\\"Obama\\'s Brother Chooses Life in Slow Lane\\\\\". The Associated Press. http://www.msnbc.msn.com/id/6333496.\\\\n^ Obama, Dreams from my Father, 2004, p. 265.\\\\n^ Obama, Dreams from my Father, 2004, p. 262.\\\\n^ a b *Oywa, John (2004-08-15). \\\\\"Sleepy Little Village Where Obama Traces His Own Roots\\\\\". The Daily Nation. http://allafrica.com/stories/200408160533.html.\\\\n^ Philip Ochieng (2004-11-01). \\\\\"From Home Squared to the US Senate: How Barack Obama Was Lost and Found\\\\\". The East African. http://www.nationmedia.com/EastAfrican/01112004/Features/PA2-11.html. Retrieved on 2008-03-23.\\\\n^ Warah, Rasna (2008-06-09). \\\\\"We cannot lay claims on Obama; he\\'s not one of us - Obama in this world\\\\\". Daily Nation. http://www.nationmedia.com. Retrieved on 2008-07-10.\\\\n^ Scott Fornek (2007-09-09). \\\\\"AUMA OBAMA: \\'Her restlessness, her independence\\'\\\\\". Chicago Sun Times. http://www.suntimes.com/news/politics/obama/familytree/545465,BSX-News-wotreew09.article. Retrieved on 2008-03-23.\\\\n^ a b c Gathmann, Florian; Gregor Peter Schmitz, Jochen Schýýnmann (July 24, 2008). \\\\\"Studentin in der Bundesrepublik: Wie Auma Obama mit Deutschland haderte\\\\\" (in German). Spiegel Online. http://www.spiegel.de/politik/ausland/0,1518,567286,00.html. Retrieved on 2008-07-24.\\\\n^ a b Harvey, Oliver (07-26 2008). \\\\\"Obama\\'s brother is in Bracknell\\\\\". The Sun. http://www.thesun.co.uk/sol/homepage/news/the_real_american_idol/article1472877.ece. Retrieved on 2008-10-06.\\\\n^ \\\\\"Madari Kindergarten\\\\\". http://www.madarikindergarten.com/.\\\\n^ \\\\\"Welcome To MedWeek San Antonio 2007\\\\\". Medweeksa.org. http://www.medweeksa.org/awardwinners/techfirm.htm. Retrieved on 2009-01-31.\\\\n^ \\\\\"PIDE - Partners for International Development & Education Inc\\\\\". Pideafrica.org. http://pideafrica.org/aboutus.htm. Retrieved on 2009-01-31.\\\\n^ Barack Obamaýýýs brother pushes Chinese imports on US - Times Online\\\\n^ Obama half-brother runs Internet company in China\\\\n^ Roger Cohen (2008-03-17). \\\\\"Obama\\'s Brother in China\\\\\". The New York Times. http://www.nytimes.com/2008/03/17/opinion/29cohen.html. Retrieved on 2008-03-23.\\\\n^ \\\\\"Youku Buzz (daily)\\xa0ýý Blog Archive\\xa0ýý Barack Obamaýýýs Half-Brother in Concert\\\\\". Buzz.youku.com. 2009-01-18. http://buzz.youku.com/2009/01/18/barack-obamas-half-brother-in-concert/. Retrieved on 2009-01-31.\\\\n^ jaketapper (2008-07-28). \\\\\"Political Punch: Barack Obama\\'s Branch-y Family Tree\\\\\". Blogs.abcnews.com. http://blogs.abcnews.com/politicalpunch/2008/07/barack-obamas-1.html. Retrieved on 2009-01-31.\\\\n^ Fornek, Scott (September 9, 2007). \\\\\"HALF-BROTHER GEORGE: \\'I would be there for him\\'\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545447,BSX-News-wotreecc09.stng. Retrieved on 2008-08-04.\\\\n^ a b c d e Pflanz, Mike (August 21, 2008). \\\\\"Barack Obama is my inspiration, says lost brother\\\\\". The Daily Telegraph. http://www.telegraph.co.uk/news/newstopics/uselection2008/barackobama/2595688/Barack-Obama-is-my-inspiration-says-lost-brother.html. Retrieved on 2008-08-23.\\\\n^ a b c Crilly, Rob (August 22, 2008). \\\\\"Life is good in my Nairobi slum, says Barack Obama\\'s younger brother\\\\\". The Times. http://www.timesonline.co.uk/tol/news/world/us_and_americas/us_elections/article4583353.ece. Retrieved on 2008-08-23.\\\\n^ a b McKenzie, David (2008-08-23). \\\\\"Behind the Scenes: Meet George Obama\\\\\". CNN. http://www.cnn.com/2008/POLITICS/08/22/bts.obama.brother/. Retrieved on 2008-10-26.\\\\n^ Pisa, Nick (August 20, 2008). \\\\\"Barack Obama\\'s \\'lost\\' brother found in Kenya\\\\\". Daily Telegraph. http://www.telegraph.co.uk/news/newstopics/uselection2008/barackobama/2590614/Barack-Obamas-lost-brother-found-in-Kenya.html. Retrieved on 2008-08-20.\\\\n^ a b Wadhams, Nick (2008-10-07). \\\\\"Corsi in Kenya: Obama\\'s Nation Boots Obama Nation Author\\\\\". TIME. http://www.time.com/time/world/article/0,8599,1847965,00.html?imw=Y. Retrieved on 2009-01-31.\\\\n^ by Dinesh D\\'Souza. \\\\\"Dinesh D\\'Souza\\xa0: George Obama, Start Packing\\\\\". Townhall.com. http://townhall.com/columnists/DineshDSouza/2008/09/22/george_obama,_start_packing. Retrieved on 2009-01-31.\\\\n^ a b c \\\\\"The Obama Family Tree\\\\\" (PDF). Chicago Sun-Times. September 9, 2007. http://www.suntimes.com/images/cds/MP3/obamatree.pdf. Retrieved on 2008-11-23.\\\\n^ First read, MSNBC\\\\n^ \\\\\"Barack Obama\\'s aunt found living in rundown public housing estate | The Australian\\\\\". Theaustralian.news.com.au. 2008-10-31. http://www.theaustralian.news.com.au/story/0,25197,24578185-5017121,00.html. Retrieved on 2009-01-31.\\\\n^ Boston Housing Authority ýýýflabbergasteredýýý Barack Obamaýýýs aunt living in Southie\\\\n^ Kilner, Derek (2008-11-05). \\\\\"Kenya Celebrates President Obama as Native Son\\\\\". Voice Of America. http://www.voanews.com/english/archive/2008-11/2008-11-05-voa45.cfm. Retrieved on 2008-12-24.\\\\n^ \\\\\"Oregon State University Beavers: Craig Robinson bio\\\\\". http://www.osubeavers.com/ViewArticle.dbml?SPSID=106239&SPID=1954&DB_OEM_ID=4700&ATCLID=1436883&Q_SEASON=2008. Retrieved on 2008-08-21.\\\\n^ \\\\\"RootsWeb\\'s WorldConnect Project: Dowling Family Genealogy\\\\\". Wc.rootsweb.ancestry.com. http://wc.rootsweb.ancestry.com/cgi-bin/igm.cgi?op=GET&db=dowfam3&id=I105855. Retrieved on 2009-01-31.\\\\n^ Weiss, Anthony (September 2, 2008). \\\\\"Michelle Obama Has a Rabbi in Her Family\\\\\". The Forward. http://www.forward.com/articles/14121/. Retrieved on 2008-10-09.\\\\n^ Gary Boyd Roberts. \\\\\"Notes on the Ancestry of Senator Barack Hussein Obama, Jr\\\\\". New England Historic Genealogical Society. http://www.newenglandancestors.org/research/services/articles_ancestry_barack_obama.asp. Retrieved on 2009-01-31.\\\\n^ Scott Fornek (2007-09-09). \\\\\"Obama Family Tree\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545463,BSX-News-wotreer09.stng. Retrieved on 2009-01-31.\\\\n^ \\\\\"Obama-Bush (family tree)\\\\\" (PDF). New England Historic Genealogical Society. http://www.newenglandancestors.org/pdfs/obama_bush.pdf. Retrieved on 2009-01-31.\\\\n^ Wade, Nicholas (2007-10-21). \\\\\"Barack Obama - Dick Cheney - Ancestry and Genealogy - Washington - New York Times\\\\\". Nytimes.com. http://www.nytimes.com/2007/10/21/weekinreview/21basic.html. Retrieved on 2009-01-31.\\\\n^ Eastman, Dick (2008-07-30). \\\\\"Barack Obama is Related to Wild Bill Hickok\\\\\". Blog.eogn.com. http://blog.eogn.com/eastmans_online_genealogy/2008/07/barack-obama-is.html. Retrieved on 2009-01-31.\\\\n^ a b c Scott Fornek (2007-09-09). \\\\\"Obama Family Tree\\\\\". Suntimes.com. http://www.suntimes.com/news/politics/obama/familytree/545441,BSX-News-wotreec09.stng. Retrieved on 2009-01-31.\\\\n^ a b \\\\\"Notes on the Ancestry of Senator Barack Hussein Obama, Jr\\\\\". New England Historic Genealogical Society. 2002-08-14. http://www.newenglandancestors.org/research/services/articles_ancestry_barack_obama.asp. Retrieved on 2009-01-31.\\\\n^ \\\\\"Obama, Clinton and McCain have some famous relations\\\\\". HeraldNet - AP. 2008-03-26. http://www.heraldnet.com/article/20080326/NEWS02/151343661. Retrieved on 2009-01-31.\\\\n^ \\\\\"Barack Obama and Joe Biden: The Change We Need\\\\\". My.barackobama.com. 2008-07-31. http://my.barackobama.com/page/community/post/williambrehm/gG5TVR. Retrieved on 2009-01-31.\\\\nExternal links\\\\nBarack Obama\\'s Family Tree - Photo Essays - TIME\\\\n\\\\\"Though Obama Had to Leave to Find Himself, It Is Hawaii That Made His Rise Possible,\\\\\" by David Maraniss\\\\nBarack Obama\\'s Branch-y Family Tree by Jake Tapper\\\\n\\\\\"Obama Family Tree\\\\\" series, by Scott Fornek\\\\n\\\\\"Six Degrees of Barack Obama\\\\\"\\\\n\\\\\"Notes on the Ancestry of Senator Barack Hussein Obama, Jr.,\\\\\" by Gary Boyd Roberts\\\\n\\\\\"Obama, Clinton and McCain have some famous relations,\\\\\" by The Associated Press\\\\n\\\\\"Obama\\'s Patriotic Family Tree,\\\\\" by Bill Brehm\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nBarack Obama\\\\nPresidency\\\\nTransition\\xa0ýý Inauguration\\xa0ýý Timeline\\xa0ýý Cabinet\\xa0ýý Judiciary\\xa0ýý Foreign policy\\xa0ýý First 100 days\\\\nEarly life and\\\\npolitical career\\\\nPublic image\\xa0ýý Illinois Senate career\\xa0ýý 2004 Democratic National Convention\\xa0ýý U.S. Senate election in Illinois\\xa0ýý U.S. Senate career\\xa0ýý Presidential primary campaign\\xa0ýý ObamaýýýBiden 2008\\xa0ýý Electoral history\\xa0ýý Political positions\\\\nBooks\\\\nDreams from My Father\\xa0ýý The Audacity of Hope\\\\nSpeeches\\\\nThe Audacity of Hope\\xa0ýý A More Perfect Union\\xa0ýý Change Has Come to America\\xa0ýý 2009 speech to joint session of Congress\\\\nFamily\\\\nMichelle Obama\\xa0ýý Barack Obama, Sr.\\xa0ýý Ann Dunham\\xa0ýý Lolo Soetoro (stepfather)\\xa0ýý Maya Soetoro-Ng (half-sister)\\xa0ýý Marian Robinson (mother-in-law)\\xa0ýý Stanley Armour Dunham (grandfather)\\xa0ýý Madelyn Dunham (grandmother)\\xa0ýý Extended family\\xa0ýý Family tree\\\\nRetrieved from \\\\\"http://en.wikipedia.org/wiki/Family_of_Barack_Obama#Malia_and_Sasha_Obama\\\\\"\\\\nCategories: Obama family | African American history | African American families | Luo Kenyans | People of mixed Black African-European ethnicity | African Americans | Asian Americans | Dutch Americans | English Americans | French Americans | German-Americans | Irish-Americans | Indonesian Americans | Kenyan-Americans | Scottish-Americans | Chinese Canadians | People of mixed Asian-European ethnicity | American families | First Families of the United States | Family treesHidden categories: Wikipedia semi-protected pages | Wikipedia indefinitely move-protected pages | All pages needing cleanup | Wikipedia articles needing factual verification since October 2008 | All pages needing factual verification | All articles with unsourced statements | Articles with unsourced statements since November 2008\\\\nViews\\\\nArticle\\\\nDiscussion\\\\nView source\\\\nHistory\\\\nPersonal tools\\\\nLog in / create account\\\\nNavigation\\\\nMain page\\\\nContents\\\\nFeatured content\\\\nCurrent events\\\\nRandom article\\\\nSearch\\\\nInteraction\\\\nAbout Wikipedia\\\\nCommunity portal\\\\nRecent changes\\\\nContact Wikipedia\\\\nDonate to Wikipedia\\\\nHelp\\\\nToolbox\\\\nWhat links here\\\\nRelated changes\\\\nUpload file\\\\nSpecial pages\\\\nPrintable version Permanent linkCite this page\\\\nLanguages\\\\nBahasa Indonesia\\\\nSvenska\\\\nýýýýýý\\\\nýýýýýý\\\\nThis page was last modified on 14 March 2009, at 05:50.\\\\nAll text is available under the terms of the GNU Free Documentation License. (See Copyrights for details.)\\\\nWikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a U.S. registered 501(c)(3) tax-deductible nonprofit charity.\\\\nPrivacy policy\\\\nAbout Wikipedia\\\\nDisclaimers\\n[DOC 2] Family of Barack Obama - Wikipedia, the free encyclopedia\\\\nFamily of Barack Obama\\\\nFrom Wikipedia, the free encyclopedia\\\\n(Redirected from Ruth Obama)\\\\nJump to: navigation, search\\\\nObama Family\\\\nPresident Barack Obama, First Lady Michelle, and daughters Malia and Sasha wave to the crowd after his inaugural address Jan. 20, 2009, on the west steps of the U.S. Capitol.[1]\\\\nCurrent region\\\\nWashington, DC\\\\nInformation\\\\nPlace of origin\\\\nUnited States\\\\nNotable members\\\\nBarack Obama, Michelle Obama, Ann Dunham, Barack Obama, Sr., etc.\\\\nConnected families\\\\nRobinson, Dunham, Soetoro, Ng\\\\nThis article is part of a series about\\\\nBarack Obama\\\\nBackground \\xa0ýý Illinois Senate \\xa0ýý U.S. Senate\\\\nPolitical positions\\xa0ýý Public image\\xa0ýý Family\\\\n2008 primaries\\xa0ýý ObamaýýýBiden campaign\\\\nTransition\\xa0ýý Inauguration\\xa0ýý Electoral history\\\\nPresidency (Timeline, First 100 days)\\\\nMalia, Michelle and Sasha on stage at the 2008 Democratic National Convention\\\\nThe Family of Barack Obama is an extended clan of African American, English, Indonesian, and Kenyan (Luo) heritage known through the writings and political career of Barack Obama, the President of the United States of America,[2][3][4][5] and other reports. His immediate family is the First Family of the United States. The Obamas are the first First Family of African American descent in the United States and the youngest to enter the White House since the Kennedys. One columnist wrote, ýýýObama\\'s young, energetic family harks back to days of Camelot.ýýý[6]\\\\nContents\\\\n1 Immediate family\\\\n2 Extended family - maternal relations\\\\n3 Extended family - paternal relations\\\\n4 Michelle Robinson Obama\\'s extended family\\\\n5 Genealogical charts\\\\n5.1 Obama ancestry\\\\n5.2 Family trees\\\\n6 Distant relations\\\\n7 See also\\\\n8 References\\\\n9 External links\\\\nImmediate family\\\\nMichelle Obama\\\\nMichelle Obama, nýýe Robinson, the wife of Barack Obama, was born on January 17, 1964 in Chicago, Illinois. She is a lawyer and was a University of Chicago Hospital Vice-President. She is the First Lady of the United States.\\\\nMalia Obama and Sasha Obama\\\\nBarack and Michelle Obama have two daughters: Malia Ann (pronounced /mýýýýliýýýý/), born in 1998,[7] and Natasha (known as Sasha) /ýýsýýýýýýýý/), born in 2001. Sasha is the youngest child to reside in the White House since John F. Kennedy, Jr, arrived as an infant in 1961.[8]\\\\nBefore his inauguration, President Obama published an open letter to his daughters in Parade magazine, describing what he wants for them and every child in America: \\\\\"to grow up in a world with no limits on your dreams and no achievements beyond your reach, and to grow into compassionate, committed women who will help build that world.\\\\\"[9]\\\\nWhile living in Chicago, they kept busy schedules, as the Associated Press reports: \\\\\"soccer, dance and drama for Malia, gymnastics and tap for Sasha, piano and tennis for both.\\\\\"[10][11] In July 2008, the family gave an interview to the television series Access Hollywood; Obama later said they regretted allowing the children to be included.[12]\\\\nIn his victory speech on the night of his election, President Obama repeated his promise to Sasha and Malia to get a puppy to take with them to the White House.[13] However the selection of a dog has been slow because Malia is allergic to animal dander;[14] the president subsequently said that the choice has been narrowed down to either a labradoodle or Portuguese Water Dog, and they are hoping to find a shelter animal.[15]\\\\nMalia and Sasha attend the private Sidwell Friends School in Washington, DC, the same school as attended by Chelsea Clinton, Tricia Nixon Cox, and Archibald Roosevelt, and currently the grandchildren of Vice President Joe Biden.[16] The Obama girls began classes there on January 5, 2009.[17] While in Chicago, both attended the private University of Chicago Laboratory School.\\\\nMarian Shields Robinson\\\\nMichelle Obama\\'s mother (birthname Marian Shields, born July 1937), now widowed, married Michelle\\'s father, Fraser Robinson, in 1960.[18][19] Robinson was formerly a secretary at Spiegel catalog and a bank. While Michelle and Barack Obama were campaigning in 2008, Robinson tended the Obama\\'s young children and she intends to do the same while in Washington, DC. Robinson is currently living in the White House itself as part of the First Family;[20] she is the first live-in grandmother there since Elivera M. Doud during the Eisenhower administration.[21] Some media outlets have dubbed Robinson the \\\\\"First Granny\\\\\".[22][21]\\\\nExtended family - maternal relations\\\\nRight-to-left: Barack Obama and Maya Soetoro with their mother Ann Dunham and grandfather Stanley Dunham in Hawaii (early 1970s)\\\\nWikinews has related news:\\\\nBarack Obama elected 44th President of the United States\\\\nGrandmother of Barack Obama dies at 86\\\\nMadelyn Dunham with her daughter Ann\\\\nAccording to Barack Obama\\'s Dreams from My Father, his great-grandmother Leona McCurry was part Native American, which Obama believed Leona held as a \\\\\"source of considerable shame\\\\\" and \\\\\"blanched whenever someone mentioned the subject and hoped to carry the secret to her grave\\\\\"; whereas McCurry\\'s daughter (Obama\\'s maternal grandmother) \\\\\"would turn her head in profile to show off her beaked nose, which along with a pair of jet-black eyes, was offered as proof of Cherokee blood.\\\\\"[23] To date, no concrete evidence has surfaced of Cherokee heritage. Obama\\'s maternal heritage consists mostly of English ancestry, with much smaller amounts of German, Irish, Scottish, Welsh, Swiss, and French ancestry.[2]\\\\nAnn Dunham\\\\nMother of Barack Obama born in 1942, died in 1995. Birthname Stanley Ann Dunham. Anthropologist in Hawaii and Indonesia.\\\\nMadelyn Lee Payne Dunham\\\\nBarack Obama\\'s maternal grandmother, born in 1922 and died on November 2, 2008.[24] She was a bank vice president in Hawaii. Obama said that when he was a child, his grandmother \\\\\"read me the opening lines of the Declaration of Independence and told me about the men and women who marched for equality because they believed those words put to paper two centuries ago should mean something.\\\\\"[9]\\\\nStanley Armour Dunham\\\\nGrandfather of Barack Obama, born 1918, died 1992. World War II U.S. Army sergeant, furniture salesman in Hawaii.\\\\nCharles T. Payne\\\\nGreat-uncle of Barack Obama, younger brother of Madelyn Dunham, born 1925. Served during World War II in the U.S. Army 89th Infantry Division.[25] Obama has often described Payne\\'s role in liberating Ohrdruf forced labor camp.[26] There was brief media attention when Obama mistakenly identified the camp as Auschwitz during the campaign.[27] Payne appeared in the visitor\\'s gallery at the Democratic National Convention in Denver, Colorado, when his great-nephew was nominated for President.[28] He was the assistant director of the University of Chicago\\'s Library.[26]\\\\nMaya Soetoro-Ng\\\\nHalf-sister of Barack Obama, born August 15, 1970, in Jakarta, Indonesia.[29] She is married to Konrad Ng, with whom she has a daughter, Suhaila. Maya Soetoro-Ng is a teacher in Hawaii.\\\\nKonrad Ng\\\\nBrother-in-law of Barack Obama, born 1974. A Canadian whose parents are Malaysian Chinese immigrants, he is an assistant professor at the University of Hawaii\\'s Academy of Creative Media.[30] His parents are from Kudat and Sandakan, two small towns in Sabah, Malaysia, and he was born and raised in Burlington, Ontario.[31] He married Maya Soetoro-Ng at the end of 2003 in Hawaii.[32] They have one daughter, Suhaila.[33][34][35] Konrad Ng is now a US citizen.[36]\\\\nLolo Soetoro\\\\nStepfather of Barack Obama, born in Indonesia 1936, died 1987.\\\\nExtended family - paternal relations\\\\nThe Obamas are members of the Luo, Kenya\\'s third-largest ethnic group, which is part of a larger family of ethnic groups, collectively also known as Luo. This group belongs to the Eastern Sudanic branch of the Nilo-Saharan phylum. The Obama family is largely concentrated in the western province of Nyanza.\\\\nFront row (left to right): Auma Obama (Barack\\'s half-sister), Kezia Obama (Barack\\'s step-mother), Sarah Hussein Onyango Obama (third wife of Barack\\'s paternal grandfather), Zeituni Onyango (Barack\\'s aunt)\\\\nBack row (left to right): Said Obama (Barack\\'s uncle), Barack Obama, Abongo [Roy] Obama (Barack\\'s half-brother), unidentified woman, Bernard Obama (Barack\\'s half-brother), Abo Obama (Barack\\'s half-brother).\\\\nBarack Obama, Sr.\\\\nBarack Obama\\'s father, (1936ýýý1982). Government economist in Kenya. In addition to President Obama, Barack Obama Sr. fathered six other sons and a daughter.[37]\\\\nHussein Onyango Obama\\\\nBarack Obama\\'s paternal grandfather (c. 1895ýýý1979);[38] he worked as a mission cook. He joined the British Army during World War I. (One source gives 1870ýýý1975 as his dates of birth and death based on his tombstone reading \\\\\"Mzee Hussein Onyango Obama\\\\\" in his home village.[39] The term \\\\\"mzee\\\\\" is a Kenyan honorific meaning \\\\\"old man\\\\\" or \\\\\"elder.\\\\\") According to his third wife, Sarah, he originally converted to Catholicism, but took the name Hussein when he later converted to Islam; she said he passed the name, not the religion, on to his children.[40]\\\\nHabiba Akumu Obama\\\\nBarack Obama\\'s paternal grandmother, and the second wife of Hussein Onyango Obama. A photograph of her holding her son, Barack Sr, on her lap is on the cover of her grandson\\'s memoirs titled Dreams from my Father.[41]\\\\nSarah Obama\\\\nThird wife of Obama\\'s paternal grandfather, born 1922.[39] Also known, through the addition of her late husband\\'s name, as Sarah Onyango Obama,[42] and sometimes referred to as Sarah Ogwel, Sarah Hussein Obama or Sarah Anyango Obama,[43] she lives in Nyangýýýoma Kogelo village, 30 miles west of western Kenya\\'s main town, Kisumu, on the edge of Lake Victoria.[44][45]\\\\nAlthough not a blood relation, Barack Obama calls her \\\\\"Granny Sarah\\\\\".[43][46] Sarah, who speaks Luo and only a few words of English, communicates with President Obama through an interpreter.\\\\nOn July 4, 2008, she attended the United States Independence Day celebrations in Nairobi, hosted by Michael Ranneberger, the US ambassador in Kenya.[47]\\\\nDuring the campaign she protested attempts to portray Obama as a foreigner to the United States or a Muslim, saying that while Obama\\'s grandfather had been a Muslim, \\\\\"In the world of today, children have different religions from their parents.\\\\\"[40] Sarah Obama herself is \\\\\"a strong believer of the Islamic faith,ýýý in her words.[48]\\\\nKezia Obama\\\\nFirst wife of Barack Obama\\'s father, born c. 1940. She is Barack Obama Sr.\\'s first wife, whom he married in Kenya before studying abroad in the United States. Also known as Kezia Grace Obama.[49][50] She currently lives in Bracknell, Berkshire, England.[51][52] Her sister, Jane, is the \\'Auntie Jane\\' mentioned at the very start of Dreams from My Father when she telephoned President Obama to inform him that his father had been killed in a car accident.[53]\\\\nMalik Obama\\\\nBarack Obama\\'s half-brother, also known as Abongo or Roy, born c. March, 1958,[51] son of Barack Obama, Sr. with his first wife, Kezia.[54] Malik Obama was born and raised in Nairobi, Kenya.[55] He earned a degree in accounting from the University of Nairobi.[56] He met his half-brother for the first time in 1985[55] when Barack flew from Chicago to Washington, D.C. to visit him.[57] Malik and his half-brother Barack were best men at each other\\'s weddings.[55] Barack Obama brought his wife Michelle to Kenya three years later, and they met with Malik again while Barack was introducing Michelle to many other new relatives.[58]\\\\nAlthough much of the Obama family has dispersed throughout Kenya and overseas, most, including Malik Obama, still considered their rural village on the shores of Lake Victoria to be their true home, and feel that those who have left the village have become culturally \\\\\"lost\\\\\".[59] A frequent visitor to the United States,[58] and consultant in Washington, D.C. for several months per year,[55] he nevertheless settled in the Obamas\\' ancestral home, Nyangýýýoma Kogelo, a village of several hundred people that he prefers to the city for its slow pace.[55] He runs a small electronics shop a half hour drive outside of town.[55]\\\\nDuring his brother\\'s presidential campaign, Malik Obama was a spokesman for the extended Obama family in Kenya, dealing with safety and privacy concerns arising from increased attention from the press.[60]\\\\nAbo Obama\\\\nBarack Obama\\'s half-brother, born 1968. International telephone store manager in Kenya.\\\\nAuma Obama\\\\nBarack Obama\\'s half-sister, born c. 1960.[61] As of July 2008, development worker in Kenya.[62] She studied German at the University of Heidelberg from 1981 to 1987. After her graduation in Heidelberg she went on for graduate studies at the University of Bayreuth, which awarded her a PhD in 1996. Her dissertation was about the conception of labor in Germany and its literary reflections.[62] Auma Obama lives in London, and in 1996 married an Englishman, Ian Manners. They have a daughter named Akinyi (b. 1997).[62][verification needed]\\\\nBernard Obama\\\\nBarack Obama\\'s half-brother, born 1970, son of Barack Obama, Sr. and his first wife, Kezia. He had been an auto parts supplier in Nairobi, Kenya, and has one child. Bernard converted to Islam as an adult and has said: \\\\\"Iýýým a Muslim, I donýýýt deny it. My father was raised a Muslim. But itýýýs not an issue. I donýýýt know what all the hullabaloo is about.\\\\\"[63] He currently resides in Bracknell, England, with his mother Kezia.[63]\\\\nRuth Ndesandjo\\\\nBorn Ruth Nidesand, in US c. 1940s, Barack Obama Sr.\\'s third wife and a private kindergarten director in Kenya.[64] Ruth\\'s two sons with Barack Obama, Sr., are Mark and David Ndesandjo; her third son, Joseph Ndesandjo, was born c. 1980 from a subsequent marriage to a Tanzanian.[65][66]\\\\nMark Ndesandjo\\\\nBarack Obama\\'s half-brother, son of Ruth Nidesand and Barack Obama Sr.[67] He runs an Internet company called WorldNexus that advises Chinese corporations how best to reach international customers.[68] Mark graduated from Brown University, studied physics at Stanford University, received an MBA from Emory University, and has lived in Shenzhen, China, since 2002 and is married to a Chinese woman.[69] He is also an accomplished pianist.[70]\\\\nDavid Ndesandjo\\\\nDavid Ndesandjo\\\\nBarack Obama\\'s half-brother (also known as David Opiyo Obama), son of Ruth Nidesand and Barack Obama Sr. Killed in a motorcycle accident.[71]\\\\nGeorge Hussein Onyango Obama\\\\nYoungest half-brother of Barack Obama, born c.1982, son of Barack Obama Sr.[72] and Jael (now a resident of Atlanta, Georgia).[73][74] George was six months old when his father died in an automobile accident, after which he was raised in Nairobi by his mother and a French step-father. He later lived in South Korea for two years while his mother resided there for business reasons.[73] Returning to Kenya, George Obama \\\\\"slept rough for several years,\\\\\" until his aunt gave him a six-by-eight foot corrugated metal shack in the Nairobi, Kenya, slum of Huruma Flats.[73] As of August 2008, Obama was studying to become a mechanic.[73] George received little attention until being featured in an article in the Italian language edition of Vanity Fair in August 2008, which portrayed him as living in poverty, shame, and obscurity.[75] The article quoted Obama as saying that he lived \\\\\"on less than a dollar a month\\\\\" and stated that he \\\\\"does not mention his famous half-brother in conversation\\\\\" out of shame at his poverty.[76] In later interviews George contradicted this picture. In an interview with The Times, Obama \\\\\"said that he was furious at subsequent reports that he had been abandoned by the Obama family and that he was filled with shame about living in a slum.\\\\\"[74] He told The Times, \\\\\"Life in Huruma is good.\\\\\" Obama said that he expects no favors, that he was supported by relatives, and that reports he lived on a dollar a month were \\\\\"all lies by people who donýýýt want my brother to win.ýýý[74] He told The Telegraph that he was inspired by his half-brother.[73] According to Time, George \\\\\"has repeatedly denied...that he feels abandoned by Obama.\\\\\"[77] CNN quoted him as saying, \\\\\"I was brought up well. I live well even now. The magazines, they have exaggerated everything... I think I kind of like it here. There are some challenges, but maybe it is just like where you come from, there are the same challenges.\\\\\"[75] George\\'s reported poverty was seized on by conservative critics of Barack Obama. Columnist Dinesh D\\'Souza solicited donations for George Obama from his readers,[78] while Jerome Corsi planned to give him a $1,000 check during a trip to Kenya (Corsi was expelled from the country by immigration authorities).[77]\\\\nOmar Obama\\\\nHalf-uncle of Barack Obama,[79] born on June 3, 1944 in Nyangýýýoma Kogelo. Oldest son of Onyango and Sarah Obama, resides in Boston, Massachusetts.[citation needed]\\\\nZeituni Onyango\\\\nHalf-aunt of Barack Obama,[80] born May 29, 1952, in Kenya,[81] Onyango is referred to as \\\\\"Aunti Zeituni\\\\\" in President Obama\\'s memoir, Dreams from My Father.[82]\\\\nYusuf Obama\\\\nHalf-uncle of Barack Obama,[79] born c. 1950s in Nyangýýýoma Kogelo; son of Onyango and Sarah Obama.[citation needed]\\\\nSaid Obama\\\\nHalf-uncle of Barack Obama,[79][83] born c. 1950s in Nyangýýýoma Kogelo; son of Onyango and Sarah Obama.[citation needed]\\\\nMichelle Robinson Obama\\'s extended family\\\\nFraser Robinson, Sr. (1884ýýý1936) of South Carolina, shown in an old photo along with his wife, Rosella Cohen Robinson, in its background\\\\nBarack Obama has called his wife Michelle \\\\\"the most quintessentially American woman I know.\\\\\"[3] Her family is of African American heritage, descendents of Africans of the American Colonial Era.[3] Michelle Obama\\'s family history traces back from slavery to Reconstruction to the Great Migration North. Some of Michelle\\'s relatives still reside in South Carolina.\\\\nMichelle\\'s earliest known relative is her great-great grandfather Jim Robinson, born in the 1850s, who was an American slave on the Friendfield plantation in South Carolina. The family believes that after the Civil War he remained a Friendfield plantation sharecropper for the rest of his life and that he was buried there in an unmarked grave.[3]\\\\nJim had two sons, Gabriel and Fraser, Michelle Obama\\'s great-grandfather. Fraser had an arm amputated as a result of a boyhood injury. He worked as a shoemaker, newspaper salesman and in a lumber mill and was married to Rosella Cohen.[3] Carrie Nelson, Gabriel Robinson\\'s daughter, now 80, is the oldest living Robinson and the keeper of family lore.[3]\\\\nAt least three of Michelle Obama\\'s great-uncles served in the military of the United States. One aunt moved to Princeton, New Jersey, where she worked as a maid, and cooked Southern-style meals for Michelle and her brother, Craig, when they were students at Princeton University.\\\\nCraig Robinson\\\\nMichelle Obama\\'s brother, born 1962. He is currently head coach of men\\'s basketball at Oregon State University.[84]\\\\nFraser Robinson III\\\\nMichelle Obama\\'s father, born 1935, died 1991, married Michelle\\'s mother, Marian Shields, in 1960.[85][19] Robinson was a pump worker at the City of Chicago water plant.[3]\\\\nFraser Robinson, Jr.\\\\nMichelle Obama\\'s grandfather was born on August 24, 1912 in Georgetown, South Carolina, and died on November 9, 1996, aged 84. He was a good student and orator, but moved from South Carolina to Chicago to find better work than he could find at home, eventually becoming a worker for the United States Postal Service. He was married to LaVaughn Johnson. When he retired, they moved back to South Carolina.[3]\\\\nCapers C. Funnye Jr.\\\\nMichelle Obama\\'s first cousin once removed: Funnyeýýýs mother, Verdelle Robinson Funnye (born Verdelle Robinson; August 22, 1930 ýýý April 16, 2000) and Michelle Obamaýýýs paternal grandfather, Fraser Robinson Jr., were siblings. One of America\\'s most prominent African American Jews, known for acting as a bridge between mainstream Jewry and African Americans.[86]\\\\nGenealogical charts\\\\nObama ancestry\\\\n16. Opiyo\\\\n8. Obama\\\\n4. Hussein Onyango Obama\\\\n9. Nyaoke\\\\n2. Barack Hussein Obama, Sr.\\\\n5. Habiba Akumu\\\\n1. Barack Hussein Obama II\\\\n24. Jacob William Dunham\\\\n12. Ralph Waldo Emerson Dunham, Sr.\\\\n25. Mary Ann Kearney\\\\n6. Stanley Armour Dunham\\\\n26. Harry Ellington Armour\\\\n13. Ruth Lucille Armour\\\\n27. Gabriella Clark\\\\n3. Stanley Ann Dunham\\\\n28. Charles T. Payne\\\\n14. Rolla Charles Payne\\\\n29. Della L. Wolfley\\\\n7. Madelyn Lee Payne\\\\n30. Thomas Creekmore McCurry\\\\n15. Leona Belle McCurry\\\\n31. Margaret Belle Wright\\\\nFamily trees\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nBarack Obama\\\\nStanley\\\\nDunham\\\\n1918ýýý1992\\\\nMadelyn\\\\nPayne\\\\n1922ýýý2008\\\\nHabiba\\\\nAkumu\\\\nOnyango\\\\nObama\\\\nc.\\xa01895ýýý1979\\\\nSarah\\\\nOgwel\\\\n1922ýýý\\\\nLolo\\\\nSoetoro\\\\n1936ýýý1987\\\\nAnn\\\\nDunham\\\\n1942ýýý1995\\\\nBarack\\\\nObama, Sr. *\\\\n1936ýýý1982\\\\nKezia\\\\nGrace\\\\nc. 1940ýýý\\\\nOmar\\\\nObama\\\\n1944ýýý\\\\nZeituni\\\\nOnyango\\\\n1952ýýý\\\\nYusuf\\\\nObama\\\\nc. 1950sýýý\\\\nSaid\\\\nObama\\\\nc. 1950sýýý\\\\nKonrad\\\\nNg\\\\nc. 1974ýýý\\\\nMaya\\\\nSoetoro\\\\n1970ýýý\\\\nBarack\\\\nObama\\\\n1961ýýý\\\\nMichelle\\\\nRobinson\\\\n1964ýýý\\\\nM. Abongo\\\\nObama\\\\n1958ýýý\\\\nAuma\\\\nObama\\\\nc. 1960ýýý\\\\nAbo\\\\nObama\\\\n1968ýýý\\\\nBernard\\\\nObama\\\\n1970ýýý\\\\nSuhaila\\\\nNg\\\\nc. 2005ýýý\\\\nMalia Ann\\\\nObama\\\\n1998ýýý\\\\nSasha\\\\nObama\\\\n2001ýýý\\\\n* Barack\\\\nObama, Sr.\\'s\\\\nadditional\\\\nRuth\\\\nNidesandjo\\\\nc. 1940sýýý\\\\nJael\\\\nOtieno\\\\nrelationships:\\\\nMark\\\\nNdesandjo\\\\nDavid\\\\nNdesandjo\\\\ndied\\xa0c.\\xa01987\\\\nGeorge\\\\nObama\\\\nc. 1982ýýý\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nFirst Lady Michelle Obama\\\\nFraser\\\\nRobinson, Jr.\\\\nýýý\\\\nSouth Carolina\\\\n(1912ýýý1996)\\\\nSawmill worker\\\\nLaVaughn D.\\\\nJohnson\\\\nýýý\\\\nIllinois\\\\n(1915ýýý2002)\\\\nCapers C.\\\\nFunnye, Jr.\\\\nýýý\\\\n(born c. 1952;\\\\nnephew of\\\\nFraser\\\\nRobinson, Jr.)\\\\nRabbi in Chicago\\\\nFraser\\\\nRobinson III\\\\nýýý\\\\nIllinois\\\\n(1935ýýý1991)\\\\nEnjoyed boxing\\\\nin youth;\\\\nwater plant\\\\npump operator\\\\nin Chicago\\\\nMarian\\\\nShields\\\\nýýý\\\\nIllinois\\\\n(born 1937)\\\\nSecretary at\\\\nSpiegel catalog\\\\nin Chicago;\\\\nU.S.\\'s First\\\\nGrandmother\\\\nCraig\\\\nRobinson\\\\nýýý\\\\nIllinois\\\\n(born 1962)\\\\nHead coach of\\\\nOregon State\\\\nBeavers men\\'s\\\\nbasketball\\\\nMichelle\\\\nRobinson\\\\nýýý\\\\nIllinois\\\\n(born 1964)\\\\nFirst Lady\\\\nof the United States\\\\nDistant relations\\\\nSee also: List of United States Presidents by genealogical relationship\\\\nAccording to genealogists, Barack Obama\\'s distant cousins include the multitude of descendants of his maternal ancestors from all along the early-American Atlantic seaboard as well as paternal, Kenyan relations belonging to the Luo tribe, many descending from a 17th century ancestor named Owiny.[87][88] For example, George W. Bush, the 43rd U.S. president, is the eleventh cousin of Barack Obama.[89] The New York Times science writer Nicholas Wade argues that with eleven generations leading back to their common progenitor, Samuel Hinckley, the relationship between the 43rd President and the 44th President is \\\\\"genetically meaningless\\\\\".[90]\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nSelected genealogical relationships\\\\nBill Hickok\\\\nAccording to Barack Obama\\'s family lore (and confirmed by the New England Historic Genealogical Society), the President and Hickok are sixth cousins, six-times removed.[91]\\\\nRobert Duvall\\\\nGoodnight helped inspire Pulitzer Prize-winning author Larry McMurtry to create a protagonist for his novel series Lonesome Dove: Woodrow Call. In Dove\\'s television novela, Woodrow Call\\'s partner is Gus McCrae, portrayed by Obama\\'s eighth cousin, twice removed, actor Robert Duvall.[92]\\\\nJames Madison\\\\nObama is also distantly related to U.S. Presidents James Madison, Lyndon Johnson, Gerald Ford, and U.S. Vice President Dick Cheney, British Prime Minister Sir Winston Churchill, U.S. Civil War General Robert E. Lee, and actor Brad Pitt.[93][94][95]\\\\nCharles Goodnight\\\\nAccording to Chicago Sun-Times reporter Scott Fornek, another Obama progenitor, Catherine Goodnight, was the grandniece of George Goodnight, who was in turn great-grandfather of famed cattleman Charles Goodnight.[92]\\\\nHarry S. Truman\\\\nActor Duvall is distant cousins with United States President Harry Truman, who\\'s likewise a fourth cousin, four times removed, of Obama\\'s.[92]\\\\nGeorgia O\\'Keeffe\\\\nNotable women Obama is related to include painter Georgia OýýýKeeffe.[93]\\\\nSee also\\\\nU.S. Presidential genealogical relationships\\\\nNew England Historic Genealogical Society\\\\nGary Boyd Roberts\\\\nReferences\\\\n^ Keck, Kristi (4 June 2008). \\\\\"Obama crosses historic milestone\\\\\". CNN. http://news.yahoo.com/s/ap/20081122/ap_on_go_pr_wh/obama_school_12. Retrieved on 2008-11-21.\\\\n^ a b Reitwiesner, William Addams. \\\\\"Ancestry of Barack Obama\\\\\". http://www.wargs.com/political/obama.html. Retrieved on 2008-10-09.\\\\n^ a b c d e f g h Murray, Shailagh (2 October 2008). \\\\\"A Family Tree Rooted In American Soil: Michelle Obama Learns About Her Slave Ancestors, Herself and Her Country\\\\\". The Washington Post: p.\\xa0C01. http://www.washingtonpost.com/wp-dyn/content/article/2008/10/01/AR2008100103169.html. Retrieved on 2008-10-10.\\\\n^ Sheridan, Michael (5 February 2007). \\\\\"Secrets of Obama Family Unlocked\\\\\". Muslim Observer. http://news.newamericamedia.org/news/view_article.html?article_id=ae5895fc29971b172938790be94ab107. Retrieved on 2008-11-21.\\\\n^ RTE news report March, 2007:Obamas Irish family links discovered by ancestry.co.uk\\\\n^ Noveck, Jocelyn (2008-11-07). \\\\\"Deseret News | Obama\\'s young, energetic family harks back to days of Camelot\\\\\". Secure.deseretnews.com. https://secure.deseretnews.com/article/1,5143,705261276,00.html. Retrieved on 2009-01-31.\\\\n^ Liza Mundy, Michelle: A Biography (Simon and Schuster, 2008), p. 129.\\\\n^ \\\\\"Sasha Obama\\\\\". Baltimore Sun. http://www.baltimoresun.com/topic/politics/sasha-obama-PECLB004381.topic. Retrieved on 2009-01-31.\\\\n^ a b Obama, Barack (13 January 2009). \\\\\"\\'What I Want for You--And Every Child in America\\'\\\\\". http://www.parade.com/news/2009/01/barack-obama-letter-to-my-daughters.html.\\\\n^ Sobieraj Westfall, Sandra (23 June 2008). \\\\\"Barack Obama Gives Daughter $1 Allowance a Week\\\\\". People Magazine. http://www.people.com/people/article/0,,20214569_1,00.html. Retrieved on 2008-11-21.\\\\n^ Lester, Will (July 23, 2008). \\\\\"Obama daughters keep hectic schedules of their own\\\\\". Associated Press. http://elections.apnews.com/apelect/db_6911/contentdetail.htm;jsessionid=8314A43012AB5FF1D0697247362D8752?contentguid=H95QubFb&full=true. Retrieved on 2008-08-04.\\\\n^ Hiro, Anne. \\\\\"Obama regrets letting \\\\\"Access Hollywood\\\\\" interview daughters. Won\\'t do it again. MSNBC\\'s Dan Abrams gets the story behind the story. - Lynn Sweet\\\\\". Blogs.suntimes.com. http://blogs.suntimes.com/sweet/2008/07/obama_regrets_letting_access_h.html. Retrieved on 2009-01-31.\\\\n^ Ahmed, Saeed (5 November 2008). \\\\\"Move over Barney, new dog moving into White House\\\\\". CNN. http://www.cnn.com/2008/LIVING/wayoflife/11/05/presidential.pets/index.html. Retrieved on 2008-11-21.\\\\n^ \\\\\"Obama: Getting a dog isn\\'t easy\\\\\". Associated Press. 7 November 2008. http://www.mercurynews.com/ci_10927292. Retrieved on 2008-11-21.\\\\n^ Janice Lloyd (2009-01-12). \\\\\"Obamas down to Labradoodle or Portuguese water dog\\\\\". USA Today. http://www.usatoday.com/news/washington/2009-01-11-obama-dog_N.htm. Retrieved on 2009-01-28.\\\\n^ Swarns, Rachel (21 November 2008). \\\\\"And the Winner Is ýýý Sidwell Friends\\\\\". The New York Times. http://thecaucus.blogs.nytimes.com/2008/11/21/and-the-winner-is-sidwell-friends/. Retrieved on 2008-11-21.\\\\n^ Tolin, Lisa (2009-01-05). \\\\\"Obama girls start school with photographers in tow\\\\\". The Associated Press. http://www.google.com/hostednews/ap/article/ALeqM5g6mv_lkODQMmQdpyIEnr8Zpm5mogD95H8KA80. Retrieved on 2009-01-06.\\\\n^ Taylor Marsh (2008-08-25). \\\\\"Political Analysis, National Security and Breaking News\\\\\". Taylor Marsh. http://www.taylormarsh.com/archives_view.php?id=28286. Retrieved on 2009-01-31.\\\\n^ a b Lia LoBello (2008-01-02). \\\\\"First Families: Radar introduces you to the next president\\'s relatives\\\\\". Radar Online. http://www.radaronline.com/features/2008/07/john_mccain_barack_obama_michelle_cindy_dunham_roberta_wrigh_04.php. Retrieved on 2009-01-28.\\\\n^ Rachel L. Swarns (2009-01-09). \\\\\"Obamaýýýs Mother-in-Law to Move Into the White House\\\\\". The New York Times. http://thecaucus.blogs.nytimes.com/2009/01/09/obamas-mother-in-law-to-move-into-the-white-house/?hp. Retrieved on 2009-01-09.\\\\n^ a b \\\\\"Will Obama mum-in-law make it a family affair in the White House?\\\\\". Agence France Presse. 2008-11-22. http://www.google.com/hostednews/afp/article/ALeqM5gN_i2jrCVkJQgfMbSDRRrNk8U4Sw. Retrieved on 2009-01-09.\\\\n^ Philip Sherwell (2008-2008-11-09). \\\\\"Michelle Obama persuades First Granny to join new White House team\\\\\". The Telegraph (UK). http://www.telegraph.co.uk/news/3407525/Michelle-Obama-persuades-First-Granny-to-join-new-White-House-team.html. Retrieved on 2009-01-09.\\\\n^ \\\\\"ýýýTootýýý: Obama grandmother a force that shaped him\\\\\". via Associated Press. 2008-08-25. http://www.thekansan.com/news/x1311851415/-Toot-Obama-grandmother-a-force-that-shaped-him. Retrieved on 2008-08-29.\\\\n^ \\\\\"CNN: \\\\\"Obama\\'s grandmother dies after battle with cancer\\\\\"\\\\\". http://www.cnn.com/2008/POLITICS/11/03/obama.grandma/index.html. Retrieved on 2008-11-04.\\\\n^ The 89th Infantry Division, United States Holocaust Memorial Museum\\\\n^ a b Obama\\'s great-uncle recalls liberating Nazi camp, Boston.com, July 22, 2008\\\\n^ Major Garrett (2008-05-27). \\\\\"Obama Campaign Scrambles to Correct the Record on Uncle\\'s War Service\\\\\". FOXNews.com. http://elections.foxnews.com/2008/05/27/recollection-of-obama-familys-service-missing-key-details. Retrieved on 2009-01-31.\\\\n^ \\\\\"Democrats salute Obamaýýýs great uncle\\\\\". Jewish Telegraphic Agency. August 28, 2008. http://jta.org/news/article/2008/08/28/110123/obamapayne. Retrieved on 31 January 2009.\\\\n^ Obama Family Tree dgmweb.net\\\\n^ Chicago Sun Times article with her picture\\\\n^ Obama has links to Malaysia\\\\n^ Nolan, Daniel (2008-06-11). \\\\\"Relative: Obama\\'s got \\'a good handle on Canada\\'\\\\\". The Hamilton Spectator. http://www.thespec.com/burlingtonlife/article/384475. Retrieved on 2008-07-03.\\\\n^ Nolan, Daniel (June 11, 2008). \\\\\"Obama\\'s Burlington connection\\\\\". The Hamilton Spectator. http://www.thespec.com/article/384307. Retrieved on 2008-06-21.\\\\n^ Misner, Jason (2008-06-20). \\\\\"Barack Obama was here\\\\\". Burlington Post. http://www.burlingtonpost.com/printarticle/186215. Retrieved on 2008-07-03.\\\\n^ Fornek, Scott (2007-09-09). \\\\\"\\'He helped me find my voice\\'\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545473,BSX-News-wotreehh09.article.\\\\n^ Cooper, Tom (2009-01-20). \\\\\"Keep watch for Obama\\\\\". TheSpec.com. http://www.thespec.com/Opinions/article/499161. Retrieved on 2009-01-28.\\\\n^ Ancestry of Barack Obama\\\\n^ Dreams from My Father, p. 376\\\\n^ a b Kenya: Special Report: Sleepy Little Village Where Obama Traces His Own Roots (Page 2 of 2)\\\\n^ a b \\\\\"Obama\\'s grandma slams \\'untruths\\'\\\\\". Associated Press. 2008-03-05. http://www.usatoday.com/news/world/2008-03-05-obama-kin_N.htm.\\xa0 See also this correction.\\\\n^ \\\\\"Q&A ON THE NEWS\\\\\". Atlanta Journal-Constitution. 2009-02-25. http://www.ajc.com/services/content/metro/stories/2009/02/25/questi0225.html. Retrieved on 2009-02-27.\\\\n^ In Kenya, Barack Obamaýýýs family prays for end to conflict - Times Online\\\\n^ a b Crilly, Rob (February 27, 2008). \\\\\"Dreams from Obama\\'s Grandmother\\\\\". Time Magazine, Inc.. http://www.time.com/time/world/article/0,8599,1717590,00.html?xid=rss-topstories. Retrieved on 2008-07-03.\\\\n^ Pflanz, Mike (2008-01-11). \\\\\"Barack Obama\\'s Kenyan relatives keep faith\\\\\". Daily Telegraph. http://www.telegraph.co.uk/news/main.jhtml?xml=/news/2008/01/09/wuspols1009.xml.\\\\n^ Fornek, Scott (2007-09-09). \\\\\"Sarah Obama - \\'Sparkling, laughing eyes\\'\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545459,BSX-News-wotreeu09.article.\\\\n^ \\\\\"Barack Obama in Kenya\\\\\". CNN. http://www.youtube.com/watch?v=Ikg6gj71U9k.\\\\n^ Daily Nation, July 8, 2008: Obama granny\\'s day out with envoys and top politicians\\\\n^ \\\\\"A Candidate, His Minister and the Search for Faith\\\\\". New York Times. 2007-04-30. http://www.nytimes.com/2007/04/30/us/politics/30obama.html?_r=2&pagewanted=all&oref=slogin&oref=slogin.\\\\n^ \\\\\"Kenya: All Obama kin to spend voting day in Kogelo\\\\\". afrika.no. 2008-10-27. http://www.afrika.no/Detailed/17321.html. Retrieved on 2009-01-31.\\\\n^ Cohen, Roger (2008-03-06). \\\\\"The Obamas of the World - New York Times\\\\\". Nytimes.com. http://www.nytimes.com/2008/03/06/opinion/06cohen.html. Retrieved on 2009-01-31.\\\\n^ a b Sanderson, Elizabeth (2008-01-06). \\\\\"Barack Obama\\'s stepmother living in Bracknell reveals the close bond with him ... and his mother\\\\\". Daily Mail. http://www.dailymail.co.uk/news/article-506338/Barack-Obamas-stepmother-living-Bracknell-reveals-close-bond---mother.html.\\\\n^ Lindsay, Anna (2009-01-20). \\\\\"Barack\\'s bingo-loving stepmother\\\\\". BBC News. http://news.bbc.co.uk/1/hi/england/berkshire/7834368.stm.\\\\n^ http://www.newvision.co.ug/D/8/26/666733\\\\n^ jpt (2008-06-18). \\\\\"From the Fact Check Desk: What Did Obama\\'s Half-Brother Say About Obama\\'s Background\\\\\". ABC News. http://blogs.abcnews.com/politicalpunch/2008/06/from-the-fact-c.html.\\\\n^ a b c d e f Maliti, Tom (2004-10-26). \\\\\"Obama\\'s Brother Chooses Life in Slow Lane\\\\\". The Associated Press. http://www.msnbc.msn.com/id/6333496.\\\\n^ Obama, Dreams from my Father, 2004, p. 265.\\\\n^ Obama, Dreams from my Father, 2004, p. 262.\\\\n^ a b *Oywa, John (2004-08-15). \\\\\"Sleepy Little Village Where Obama Traces His Own Roots\\\\\". The Daily Nation. http://allafrica.com/stories/200408160533.html.\\\\n^ Philip Ochieng (2004-11-01). \\\\\"From Home Squared to the US Senate: How Barack Obama Was Lost and Found\\\\\". The East African. http://www.nationmedia.com/EastAfrican/01112004/Features/PA2-11.html. Retrieved on 2008-03-23.\\\\n^ Warah, Rasna (2008-06-09). \\\\\"We cannot lay claims on Obama; he\\'s not one of us - Obama in this world\\\\\". Daily Nation. http://www.nationmedia.com. Retrieved on 2008-07-10.\\\\n^ Scott Fornek (2007-09-09). \\\\\"AUMA OBAMA: \\'Her restlessness, her independence\\'\\\\\". Chicago Sun Times. http://www.suntimes.com/news/politics/obama/familytree/545465,BSX-News-wotreew09.article. Retrieved on 2008-03-23.\\\\n^ a b c Gathmann, Florian; Gregor Peter Schmitz, Jochen Schýýnmann (July 24, 2008). \\\\\"Studentin in der Bundesrepublik: Wie Auma Obama mit Deutschland haderte\\\\\" (in German). Spiegel Online. http://www.spiegel.de/politik/ausland/0,1518,567286,00.html. Retrieved on 2008-07-24.\\\\n^ a b Harvey, Oliver (07-26 2008). \\\\\"Obama\\'s brother is in Bracknell\\\\\". The Sun. http://www.thesun.co.uk/sol/homepage/news/the_real_american_idol/article1472877.ece. Retrieved on 2008-10-06.\\\\n^ \\\\\"Madari Kindergarten\\\\\". http://www.madarikindergarten.com/.\\\\n^ \\\\\"Welcome To MedWeek San Antonio 2007\\\\\". Medweeksa.org. http://www.medweeksa.org/awardwinners/techfirm.htm. Retrieved on 2009-01-31.\\\\n^ \\\\\"PIDE - Partners for International Development & Education Inc\\\\\". Pideafrica.org. http://pideafrica.org/aboutus.htm. Retrieved on 2009-01-31.\\\\n^ Barack Obamaýýýs brother pushes Chinese imports on US - Times Online\\\\n^ Obama half-brother runs Internet company in China\\\\n^ Roger Cohen (2008-03-17). \\\\\"Obama\\'s Brother in China\\\\\". The New York Times. http://www.nytimes.com/2008/03/17/opinion/29cohen.html. Retrieved on 2008-03-23.\\\\n^ \\\\\"Youku Buzz (daily)\\xa0ýý Blog Archive\\xa0ýý Barack Obamaýýýs Half-Brother in Concert\\\\\". Buzz.youku.com. 2009-01-18. http://buzz.youku.com/2009/01/18/barack-obamas-half-brother-in-concert/. Retrieved on 2009-01-31.\\\\n^ jaketapper (2008-07-28). \\\\\"Political Punch: Barack Obama\\'s Branch-y Family Tree\\\\\". Blogs.abcnews.com. http://blogs.abcnews.com/politicalpunch/2008/07/barack-obamas-1.html. Retrieved on 2009-01-31.\\\\n^ Fornek, Scott (September 9, 2007). \\\\\"HALF-BROTHER GEORGE: \\'I would be there for him\\'\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545447,BSX-News-wotreecc09.stng. Retrieved on 2008-08-04.\\\\n^ a b c d e Pflanz, Mike (August 21, 2008). \\\\\"Barack Obama is my inspiration, says lost brother\\\\\". The Daily Telegraph. http://www.telegraph.co.uk/news/newstopics/uselection2008/barackobama/2595688/Barack-Obama-is-my-inspiration-says-lost-brother.html. Retrieved on 2008-08-23.\\\\n^ a b c Crilly, Rob (August 22, 2008). \\\\\"Life is good in my Nairobi slum, says Barack Obama\\'s younger brother\\\\\". The Times. http://www.timesonline.co.uk/tol/news/world/us_and_americas/us_elections/article4583353.ece. Retrieved on 2008-08-23.\\\\n^ a b McKenzie, David (2008-08-23). \\\\\"Behind the Scenes: Meet George Obama\\\\\". CNN. http://www.cnn.com/2008/POLITICS/08/22/bts.obama.brother/. Retrieved on 2008-10-26.\\\\n^ Pisa, Nick (August 20, 2008). \\\\\"Barack Obama\\'s \\'lost\\' brother found in Kenya\\\\\". Daily Telegraph. http://www.telegraph.co.uk/news/newstopics/uselection2008/barackobama/2590614/Barack-Obamas-lost-brother-found-in-Kenya.html. Retrieved on 2008-08-20.\\\\n^ a b Wadhams, Nick (2008-10-07). \\\\\"Corsi in Kenya: Obama\\'s Nation Boots Obama Nation Author\\\\\". TIME. http://www.time.com/time/world/article/0,8599,1847965,00.html?imw=Y. Retrieved on 2009-01-31.\\\\n^ by Dinesh D\\'Souza. \\\\\"Dinesh D\\'Souza\\xa0: George Obama, Start Packing\\\\\". Townhall.com. http://townhall.com/columnists/DineshDSouza/2008/09/22/george_obama,_start_packing. Retrieved on 2009-01-31.\\\\n^ a b c \\\\\"The Obama Family Tree\\\\\" (PDF). Chicago Sun-Times. September 9, 2007. http://www.suntimes.com/images/cds/MP3/obamatree.pdf. Retrieved on 2008-11-23.\\\\n^ First read, MSNBC\\\\n^ \\\\\"Barack Obama\\'s aunt found living in rundown public housing estate | The Australian\\\\\". Theaustralian.news.com.au. 2008-10-31. http://www.theaustralian.news.com.au/story/0,25197,24578185-5017121,00.html. Retrieved on 2009-01-31.\\\\n^ Boston Housing Authority ýýýflabbergasteredýýý Barack Obamaýýýs aunt living in Southie\\\\n^ Kilner, Derek (2008-11-05). \\\\\"Kenya Celebrates President Obama as Native Son\\\\\". Voice Of America. http://www.voanews.com/english/archive/2008-11/2008-11-05-voa45.cfm. Retrieved on 2008-12-24.\\\\n^ \\\\\"Oregon State University Beavers: Craig Robinson bio\\\\\". http://www.osubeavers.com/ViewArticle.dbml?SPSID=106239&SPID=1954&DB_OEM_ID=4700&ATCLID=1436883&Q_SEASON=2008. Retrieved on 2008-08-21.\\\\n^ \\\\\"RootsWeb\\'s WorldConnect Project: Dowling Family Genealogy\\\\\". Wc.rootsweb.ancestry.com. http://wc.rootsweb.ancestry.com/cgi-bin/igm.cgi?op=GET&db=dowfam3&id=I105855. Retrieved on 2009-01-31.\\\\n^ Weiss, Anthony (September 2, 2008). \\\\\"Michelle Obama Has a Rabbi in Her Family\\\\\". The Forward. http://www.forward.com/articles/14121/. Retrieved on 2008-10-09.\\\\n^ Gary Boyd Roberts. \\\\\"Notes on the Ancestry of Senator Barack Hussein Obama, Jr\\\\\". New England Historic Genealogical Society. http://www.newenglandancestors.org/research/services/articles_ancestry_barack_obama.asp. Retrieved on 2009-01-31.\\\\n^ Scott Fornek (2007-09-09). \\\\\"Obama Family Tree\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545463,BSX-News-wotreer09.stng. Retrieved on 2009-01-31.\\\\n^ \\\\\"Obama-Bush (family tree)\\\\\" (PDF). New England Historic Genealogical Society. http://www.newenglandancestors.org/pdfs/obama_bush.pdf. Retrieved on 2009-01-31.\\\\n^ Wade, Nicholas (2007-10-21). \\\\\"Barack Obama - Dick Cheney - Ancestry and Genealogy - Washington - New York Times\\\\\". Nytimes.com. http://www.nytimes.com/2007/10/21/weekinreview/21basic.html. Retrieved on 2009-01-31.\\\\n^ Eastman, Dick (2008-07-30). \\\\\"Barack Obama is Related to Wild Bill Hickok\\\\\". Blog.eogn.com. http://blog.eogn.com/eastmans_online_genealogy/2008/07/barack-obama-is.html. Retrieved on 2009-01-31.\\\\n^ a b c Scott Fornek (2007-09-09). \\\\\"Obama Family Tree\\\\\". Suntimes.com. http://www.suntimes.com/news/politics/obama/familytree/545441,BSX-News-wotreec09.stng. Retrieved on 2009-01-31.\\\\n^ a b \\\\\"Notes on the Ancestry of Senator Barack Hussein Obama, Jr\\\\\". New England Historic Genealogical Society. 2002-08-14. http://www.newenglandancestors.org/research/services/articles_ancestry_barack_obama.asp. Retrieved on 2009-01-31.\\\\n^ \\\\\"Obama, Clinton and McCain have some famous relations\\\\\". HeraldNet - AP. 2008-03-26. http://www.heraldnet.com/article/20080326/NEWS02/151343661. Retrieved on 2009-01-31.\\\\n^ \\\\\"Barack Obama and Joe Biden: The Change We Need\\\\\". My.barackobama.com. 2008-07-31. http://my.barackobama.com/page/community/post/williambrehm/gG5TVR. Retrieved on 2009-01-31.\\\\nExternal links\\\\nBarack Obama\\'s Family Tree - Photo Essays - TIME\\\\n\\\\\"Though Obama Had to Leave to Find Himself, It Is Hawaii That Made His Rise Possible,\\\\\" by David Maraniss\\\\nBarack Obama\\'s Branch-y Family Tree by Jake Tapper\\\\n\\\\\"Obama Family Tree\\\\\" series, by Scott Fornek\\\\n\\\\\"Six Degrees of Barack Obama\\\\\"\\\\n\\\\\"Notes on the Ancestry of Senator Barack Hussein Obama, Jr.,\\\\\" by Gary Boyd Roberts\\\\n\\\\\"Obama, Clinton and McCain have some famous relations,\\\\\" by The Associated Press\\\\n\\\\\"Obama\\'s Patriotic Family Tree,\\\\\" by Bill Brehm\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nBarack Obama\\\\nPresidency\\\\nTransition\\xa0ýý Inauguration\\xa0ýý Timeline\\xa0ýý Cabinet\\xa0ýý Judiciary\\xa0ýý Foreign policy\\xa0ýý First 100 days\\\\nEarly life and\\\\npolitical career\\\\nPublic image\\xa0ýý Illinois Senate career\\xa0ýý 2004 Democratic National Convention\\xa0ýý U.S. Senate election in Illinois\\xa0ýý U.S. Senate career\\xa0ýý Presidential primary campaign\\xa0ýý ObamaýýýBiden 2008\\xa0ýý Electoral history\\xa0ýý Political positions\\\\nBooks\\\\nDreams from My Father\\xa0ýý The Audacity of Hope\\\\nSpeeches\\\\nThe Audacity of Hope\\xa0ýý A More Perfect Union\\xa0ýý Change Has Come to America\\xa0ýý 2009 speech to joint session of Congress\\\\nFamily\\\\nMichelle Obama\\xa0ýý Barack Obama, Sr.\\xa0ýý Ann Dunham\\xa0ýý Lolo Soetoro (stepfather)\\xa0ýý Maya Soetoro-Ng (half-sister)\\xa0ýý Marian Robinson (mother-in-law)\\xa0ýý Stanley Armour Dunham (grandfather)\\xa0ýý Madelyn Dunham (grandmother)\\xa0ýý Extended family\\xa0ýý Family tree\\\\nRetrieved from \\\\\"http://en.wikipedia.org/wiki/Family_of_Barack_Obama#Ruth_Ndesandjo\\\\\"\\\\nCategories: Obama family | African American history | African American families | Luo Kenyans | People of mixed Black African-European ethnicity | African Americans | Asian Americans | Dutch Americans | English Americans | French Americans | German-Americans | Irish-Americans | Indonesian Americans | Kenyan-Americans | Scottish-Americans | Chinese Canadians | People of mixed Asian-European ethnicity | American families | First Families of the United States | Family treesHidden categories: Wikipedia semi-protected pages | Wikipedia indefinitely move-protected pages | All pages needing cleanup | Wikipedia articles needing factual verification since October 2008 | All pages needing factual verification | All articles with unsourced statements | Articles with unsourced statements since November 2008\\\\nViews\\\\nArticle\\\\nDiscussion\\\\nView source\\\\nHistory\\\\nPersonal tools\\\\nLog in / create account\\\\nNavigation\\\\nMain page\\\\nContents\\\\nFeatured content\\\\nCurrent events\\\\nRandom article\\\\nSearch\\\\nInteraction\\\\nAbout Wikipedia\\\\nCommunity portal\\\\nRecent changes\\\\nContact Wikipedia\\\\nDonate to Wikipedia\\\\nHelp\\\\nToolbox\\\\nWhat links here\\\\nRelated changes\\\\nUpload file\\\\nSpecial pages\\\\nPrintable version Permanent linkCite this page\\\\nLanguages\\\\nBahasa Indonesia\\\\nSvenska\\\\nýýýýýý\\\\nýýýýýý\\\\nThis page was last modified on 14 March 2009, at 05:50.\\\\nAll text is available under the terms of the GNU Free Documentation License. (See Copyrights for details.)\\\\nWikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a U.S. registered 501(c)(3) tax-deductible nonprofit charity.\\\\nPrivacy policy\\\\nAbout Wikipedia\\\\nDisclaimers\\n[DOC 3] Family of Barack Obama - Wikipedia, the free encyclopedia\\\\nFamily of Barack Obama\\\\nFrom Wikipedia, the free encyclopedia\\\\n(Redirected from Soetoro)\\\\nJump to: navigation, search\\\\nObama Family\\\\nPresident Barack Obama, First Lady Michelle, and daughters Malia and Sasha wave to the crowd after his inaugural address Jan. 20, 2009, on the west steps of the U.S. Capitol.[1]\\\\nCurrent region\\\\nWashington, DC\\\\nInformation\\\\nPlace of origin\\\\nUnited States\\\\nNotable members\\\\nBarack Obama, Michelle Obama, Ann Dunham, Barack Obama, Sr., etc.\\\\nConnected families\\\\nRobinson, Dunham, Soetoro, Ng\\\\nThis article is part of a series about\\\\nBarack Obama\\\\nBackground \\xa0ýý Illinois Senate \\xa0ýý U.S. Senate\\\\nPolitical positions\\xa0ýý Public image\\xa0ýý Family\\\\n2008 primaries\\xa0ýý ObamaýýýBiden campaign\\\\nTransition\\xa0ýý Inauguration\\xa0ýý Electoral history\\\\nPresidency (Timeline, First 100 days)\\\\nMalia, Michelle and Sasha on stage at the 2008 Democratic National Convention\\\\nThe Family of Barack Obama is an extended clan of African American, English, Indonesian, and Kenyan (Luo) heritage known through the writings and political career of Barack Obama, the President of the United States of America,[2][3][4][5] and other reports. His immediate family is the First Family of the United States. The Obamas are the first First Family of African American descent in the United States and the youngest to enter the White House since the Kennedys. One columnist wrote, ýýýObama\\'s young, energetic family harks back to days of Camelot.ýýý[6]\\\\nContents\\\\n1 Immediate family\\\\n2 Extended family - maternal relations\\\\n3 Extended family - paternal relations\\\\n4 Michelle Robinson Obama\\'s extended family\\\\n5 Genealogical charts\\\\n5.1 Obama ancestry\\\\n5.2 Family trees\\\\n6 Distant relations\\\\n7 See also\\\\n8 References\\\\n9 External links\\\\nImmediate family\\\\nMichelle Obama\\\\nMichelle Obama, nýýe Robinson, the wife of Barack Obama, was born on January 17, 1964 in Chicago, Illinois. She is a lawyer and was a University of Chicago Hospital Vice-President. She is the First Lady of the United States.\\\\nMalia Obama and Sasha Obama\\\\nBarack and Michelle Obama have two daughters: Malia Ann (pronounced /mýýýýliýýýý/), born in 1998,[7] and Natasha (known as Sasha) /ýýsýýýýýýýý/), born in 2001. Sasha is the youngest child to reside in the White House since John F. Kennedy, Jr, arrived as an infant in 1961.[8] Sasha is also the first White House resident born in the 21st century.\\\\nBefore his inauguration, President Obama published an open letter to his daughters in Parade magazine, describing what he wants for them and every child in America: \\\\\"to grow up in a world with no limits on your dreams and no achievements beyond your reach, and to grow into compassionate, committed women who will help build that world.\\\\\"[9]\\\\nWhile living in Chicago, they kept busy schedules, as the Associated Press reports: \\\\\"soccer, dance and drama for Malia, gymnastics and tap for Sasha, piano and tennis for both.\\\\\"[10][11] In July 2008, the family gave an interview to the television series Access Hollywood; Obama later said they regretted allowing the children to be included.[12]\\\\nIn his victory speech on the night of his election, President Obama repeated his promise to Sasha and Malia to get a puppy to take with them to the White House.[13] However the selection of a dog has been slow because Malia is allergic to animal dander;[14] the president subsequently said that the choice has been narrowed down to either a labradoodle or Portuguese Water Dog, and they are hoping to find a shelter animal.[15]\\\\nMalia and Sasha attend the private Sidwell Friends School in Washington, DC, the same school as attended by Chelsea Clinton, Tricia Nixon Cox, and Archibald Roosevelt, and currently the grandchildren of Vice President Joe Biden.[16] The Obama girls began classes there on January 5, 2009.[17] While in Chicago, both attended the private University of Chicago Laboratory School.\\\\nMarian Shields Robinson\\\\nMichelle Obama\\'s mother (birthname Marian Shields, born July 1937), now widowed, married Michelle\\'s father, Fraser Robinson, in 1960.[18][19] Robinson was formerly a secretary at Spiegel catalog and a bank. While Michelle and Barack Obama were campaigning in 2008, Robinson tended the Obama\\'s young children and she intends to do the same while in Washington, DC. Robinson is currently living in the White House itself as part of the First Family;[20] she is the first live-in grandmother there since Elivera M. Doud during the Eisenhower administration.[21] Some media outlets have dubbed Robinson the \\\\\"First Granny\\\\\".[22][21]\\\\nExtended family - maternal relations\\\\nRight-to-left: Barack Obama and Maya Soetoro with their mother Ann Dunham and grandfather Stanley Dunham in Hawaii (early 1970s)\\\\nWikinews has related news:\\\\nBarack Obama elected 44th President of the United States\\\\nGrandmother of Barack Obama dies at 86\\\\nMadelyn Dunham with her daughter Ann\\\\nAccording to Barack Obama\\'s Dreams from My Father, his great-grandmother Leona McCurry was part Native American, which Obama believed Leona held as a \\\\\"source of considerable shame\\\\\" and \\\\\"blanched whenever someone mentioned the subject and hoped to carry the secret to her grave\\\\\"; whereas McCurry\\'s daughter (Obama\\'s maternal grandmother) \\\\\"would turn her head in profile to show off her beaked nose, which along with a pair of jet-black eyes, was offered as proof of Cherokee blood.\\\\\"[23] To date, no concrete evidence has surfaced of Cherokee heritage. Obama\\'s maternal heritage consists mostly of English ancestry, with much smaller amounts of German, Irish, Scottish, Welsh, Swiss, and French ancestry.[2]\\\\nAnn Dunham\\\\nMother of Barack Obama born in 1942, died in 1995. Birthname Stanley Ann Dunham. Anthropologist in Hawaii and Indonesia.\\\\nMadelyn Lee Payne Dunham\\\\nBarack Obama\\'s maternal grandmother, born in 1922 and died on November 2, 2008.[24] She was a bank vice president in Hawaii. Obama said that when he was a child, his grandmother \\\\\"read me the opening lines of the Declaration of Independence and told me about the men and women who marched for equality because they believed those words put to paper two centuries ago should mean something.\\\\\"[9]\\\\nStanley Armour Dunham\\\\nGrandfather of Barack Obama, born 1918, died 1992. World War II U.S. Army sergeant, furniture salesman in Hawaii.\\\\nCharles T. Payne\\\\nGreat-uncle of Barack Obama, younger brother of Madelyn Dunham, born 1925. Served during World War II in the U.S. Army 89th Infantry Division.[25] Obama has often described Payne\\'s role in liberating Ohrdruf forced labor camp.[26] There was brief media attention when Obama mistakenly identified the camp as Auschwitz during the campaign.[27] Payne appeared in the visitor\\'s gallery at the Democratic National Convention in Denver, Colorado, when his great-nephew was nominated for President.[28] He was the assistant director of the University of Chicago\\'s Library.[26]\\\\nMaya Soetoro-Ng\\\\nHalf-sister of Barack Obama, born August 15, 1970, in Jakarta, Indonesia.[29] She is married to Konrad Ng, with whom she has a daughter, Suhaila. Maya Soetoro-Ng is a teacher in Hawaii.\\\\nKonrad Ng\\\\nBrother-in-law of Barack Obama, born 1974. A Canadian whose parents are Malaysian Chinese immigrants, he is an assistant professor at the University of Hawaii\\'s Academy of Creative Media.[30] His parents are from Kudat and Sandakan, two small towns in Sabah, Malaysia, and he was born and raised in Burlington, Ontario.[31] He married Maya Soetoro-Ng at the end of 2003 in Hawaii.[32] They have one daughter, Suhaila.[33][34][35] Konrad Ng is now a US citizen.[36]\\\\nLolo Soetoro\\\\nStepfather of Barack Obama, born in Indonesia 1936, died 1987.\\\\nExtended family - paternal relations\\\\nThe Obamas are members of the Luo, Kenya\\'s third-largest ethnic group, which is part of a larger family of ethnic groups, collectively also known as Luo. This group belongs to the Eastern Sudanic branch of the Nilo-Saharan phylum. The Obama family is largely concentrated in the western province of Nyanza.\\\\nFront row (left to right): Auma Obama (Barack\\'s half-sister), Kezia Obama (Barack\\'s step-mother), Sarah Hussein Onyango Obama (third wife of Barack\\'s paternal grandfather), Zeituni Onyango (Barack\\'s aunt)\\\\nBack row (left to right): Said Obama (Barack\\'s uncle), Barack Obama, Abongo [Roy] Obama (Barack\\'s half-brother), unidentified woman, Bernard Obama (Barack\\'s half-brother), Abo Obama (Barack\\'s half-brother).\\\\nBarack Obama, Sr.\\\\nBarack Obama\\'s father, (1936ýýý1982). Government economist in Kenya. In addition to President Obama, Barack Obama Sr. fathered six other sons and a daughter.[37]\\\\nHussein Onyango Obama\\\\nBarack Obama\\'s paternal grandfather (c. 1895ýýý1979);[38] he worked as a mission cook. He joined the British Army during World War I. (One source gives 1870ýýý1975 as his dates of birth and death based on his tombstone reading \\\\\"Mzee Hussein Onyango Obama\\\\\" in his home village.[39] The term \\\\\"mzee\\\\\" is a Kenyan honorific meaning \\\\\"old man\\\\\" or \\\\\"elder.\\\\\") According to his third wife, Sarah, he originally converted to Catholicism, but took the name Hussein when he later converted to Islam; she said he passed the name, not the religion, on to his children.[40]\\\\nHabiba Akumu Obama\\\\nBarack Obama\\'s paternal grandmother, and the second wife of Hussein Onyango Obama. A photograph of her holding her son, Barack Sr, on her lap is on the cover of her grandson\\'s memoirs titled Dreams from my Father.[41]\\\\nSarah Obama\\\\nThird wife of Obama\\'s paternal grandfather, born 1922.[39] Also known, through the addition of her late husband\\'s name, as Sarah Onyango Obama,[42] and sometimes referred to as Sarah Ogwel, Sarah Hussein Obama or Sarah Anyango Obama,[43] she lives in Nyangýýýoma Kogelo village, 30 miles west of western Kenya\\'s main town, Kisumu, on the edge of Lake Victoria.[44][45]\\\\nAlthough not a blood relation, Barack Obama calls her \\\\\"Granny Sarah\\\\\".[43][46] Sarah, who speaks Luo and only a few words of English, communicates with President Obama through an interpreter.\\\\nOn July 4, 2008, she attended the United States Independence Day celebrations in Nairobi, hosted by Michael Ranneberger, the US ambassador in Kenya.[47]\\\\nDuring the campaign she protested attempts to portray Obama as a foreigner to the United States or a Muslim, saying that while Obama\\'s grandfather had been a Muslim, \\\\\"In the world of today, children have different religions from their parents.\\\\\"[40] Sarah Obama herself is \\\\\"a strong believer of the Islamic faith,ýýý in her words.[48]\\\\nKezia Obama\\\\nFirst wife of Barack Obama\\'s father, born c. 1940. She is Barack Obama Sr.\\'s first wife, whom he married in Kenya before studying abroad in the United States. Also known as Kezia Grace Obama.[49][50] She currently lives in Bracknell, Berkshire, England.[51][52] Her sister, Jane, is the \\'Auntie Jane\\' mentioned at the very start of Dreams from My Father when she telephoned President Obama to inform him that his father had been killed in a car accident.[53]\\\\nMalik Obama\\\\nBarack Obama\\'s half-brother, also known as Abongo or Roy, born c. March, 1958,[51] son of Barack Obama, Sr. with his first wife, Kezia.[54] Malik Obama was born and raised in Nairobi, Kenya.[55] He earned a degree in accounting from the University of Nairobi.[56] He met his half-brother for the first time in 1985[55] when Barack flew from Chicago to Washington, D.C. to visit him.[57] Malik and his half-brother Barack were best men at each other\\'s weddings.[55] Barack Obama brought his wife Michelle to Kenya three years later, and they met with Malik again while Barack was introducing Michelle to many other new relatives.[58]\\\\nAlthough much of the Obama family has dispersed throughout Kenya and overseas, most, including Malik Obama, still considered their rural village on the shores of Lake Victoria to be their true home, and feel that those who have left the village have become culturally \\\\\"lost\\\\\".[59] A frequent visitor to the United States,[58] and consultant in Washington, D.C. for several months per year,[55] he nevertheless settled in the Obamas\\' ancestral home, Nyangýýýoma Kogelo, a village of several hundred people that he prefers to the city for its slow pace.[55] He runs a small electronics shop a half hour drive outside of town.[55]\\\\nDuring his brother\\'s presidential campaign, Malik Obama was a spokesman for the extended Obama family in Kenya, dealing with safety and privacy concerns arising from increased attention from the press.[60]\\\\nAbo Obama\\\\nBarack Obama\\'s half-brother, born 1968. International telephone store manager in Kenya.\\\\nAuma Obama\\\\nBarack Obama\\'s half-sister, born c. 1960.[61] As of July 2008, development worker in Kenya.[62] She studied German at the University of Heidelberg from 1981 to 1987. After her graduation in Heidelberg she went on for graduate studies at the University of Bayreuth, which awarded her a PhD in 1996. Her dissertation was about the conception of labor in Germany and its literary reflections.[62] Auma Obama lives in London, and in 1996 married an Englishman, Ian Manners. They have a daughter named Akinyi (b. 1997).[62][verification needed]\\\\nBernard Obama\\\\nBarack Obama\\'s half-brother, born 1970, son of Barack Obama, Sr. and his first wife, Kezia. He had been an auto parts supplier in Nairobi, Kenya, and has one child. Bernard converted to Islam as an adult and has said: \\\\\"Iýýým a Muslim, I donýýýt deny it. My father was raised a Muslim. But itýýýs not an issue. I donýýýt know what all the hullabaloo is about.\\\\\"[63] He currently resides in Bracknell, England, with his mother Kezia.[63]\\\\nRuth Ndesandjo\\\\nBorn Ruth Nidesand, in US c. 1940s, Barack Obama Sr.\\'s third wife and a private kindergarten director in Kenya.[64] Ruth\\'s two sons with Barack Obama, Sr., are Mark and David Ndesandjo; her third son, Joseph Ndesandjo, was born c. 1980 from a subsequent marriage to a Tanzanian.[65][66]\\\\nMark Ndesandjo\\\\nBarack Obama\\'s half-brother, son of Ruth Nidesand and Barack Obama Sr.[67] He runs an Internet company called WorldNexus that advises Chinese corporations how best to reach international customers.[68] Mark graduated from Brown University, studied physics at Stanford University, received an MBA from Emory University, and has lived in Shenzhen, China, since 2002 and is married to a Chinese woman.[69] He is also an accomplished pianist.[70]\\\\nDavid Ndesandjo\\\\nDavid Ndesandjo\\\\nBarack Obama\\'s half-brother (also known as David Opiyo Obama), son of Ruth Nidesand and Barack Obama Sr. Killed in a motorcycle accident.[71]\\\\nGeorge Hussein Onyango Obama\\\\nYoungest half-brother of Barack Obama, born c.1982, son of Barack Obama Sr.[72] and Jael (now a resident of Atlanta, Georgia).[73][74] George was six months old when his father died in an automobile accident, after which he was raised in Nairobi by his mother and a French step-father. He later lived in South Korea for two years while his mother resided there for business reasons.[73] Returning to Kenya, George Obama \\\\\"slept rough for several years,\\\\\" until his aunt gave him a six-by-eight foot corrugated metal shack in the Nairobi, Kenya, slum of Huruma Flats.[73] As of August 2008, Obama was studying to become a mechanic.[73] George received little attention until being featured in an article in the Italian language edition of Vanity Fair in August 2008, which portrayed him as living in poverty, shame, and obscurity.[75] The article quoted Obama as saying that he lived \\\\\"on less than a dollar a month\\\\\" and stated that he \\\\\"does not mention his famous half-brother in conversation\\\\\" out of shame at his poverty.[76] In later interviews George contradicted this picture. In an interview with The Times, Obama \\\\\"said that he was furious at subsequent reports that he had been abandoned by the Obama family and that he was filled with shame about living in a slum.\\\\\"[74] He told The Times, \\\\\"Life in Huruma is good.\\\\\" Obama said that he expects no favors, that he was supported by relatives, and that reports he lived on a dollar a month were \\\\\"all lies by people who donýýýt want my brother to win.ýýý[74] He told The Telegraph that he was inspired by his half-brother.[73] According to Time, George \\\\\"has repeatedly denied...that he feels abandoned by Obama.\\\\\"[77] CNN quoted him as saying, \\\\\"I was brought up well. I live well even now. The magazines, they have exaggerated everything... I think I kind of like it here. There are some challenges, but maybe it is just like where you come from, there are the same challenges.\\\\\"[75] George\\'s reported poverty was seized on by conservative critics of Barack Obama. Columnist Dinesh D\\'Souza solicited donations for George Obama from his readers,[78] while Jerome Corsi planned to give him a $1,000 check during a trip to Kenya (Corsi was expelled from the country by immigration authorities).[77]\\\\nOmar Obama\\\\nHalf-uncle of Barack Obama,[79] born on June 3, 1944 in Nyangýýýoma Kogelo. Oldest son of Onyango and Sarah Obama, resides in Boston, Massachusetts.[citation needed]\\\\nZeituni Onyango\\\\nHalf-aunt of Barack Obama,[80] born May 29, 1952, in Kenya,[81] Onyango is referred to as \\\\\"Aunti Zeituni\\\\\" in President Obama\\'s memoir, Dreams from My Father.[82]\\\\nYusuf Obama\\\\nHalf-uncle of Barack Obama,[79] born c. 1950s in Nyangýýýoma Kogelo; son of Onyango and Sarah Obama.[citation needed]\\\\nSaid Obama\\\\nHalf-uncle of Barack Obama,[79][83] born c. 1950s in Nyangýýýoma Kogelo; son of Onyango and Sarah Obama.[citation needed]\\\\nMichelle Robinson Obama\\'s extended family\\\\nFraser Robinson, Sr. (1884ýýý1936) of South Carolina, shown in an old photo along with his wife, Rosella Cohen Robinson, in its background\\\\nBarack Obama has called his wife Michelle \\\\\"the most quintessentially American woman I know.\\\\\"[3] Her family is of African American heritage, descendents of Africans of the American Colonial Era.[3] Michelle Obama\\'s family history traces back from slavery to Reconstruction to the Great Migration North. Some of Michelle\\'s relatives still reside in South Carolina.\\\\nMichelle\\'s earliest known relative is her great-great grandfather Jim Robinson, born in the 1850s, who was an American slave on the Friendfield plantation in South Carolina. The family believes that after the Civil War he remained a Friendfield plantation sharecropper for the rest of his life and that he was buried there in an unmarked grave.[3]\\\\nJim had two sons, Gabriel and Fraser, Michelle Obama\\'s great-grandfather. Fraser had an arm amputated as a result of a boyhood injury. He worked as a shoemaker, newspaper salesman and in a lumber mill and was married to Rosella Cohen.[3] Carrie Nelson, Gabriel Robinson\\'s daughter, now 80, is the oldest living Robinson and the keeper of family lore.[3]\\\\nAt least three of Michelle Obama\\'s great-uncles served in the military of the United States. One aunt moved to Princeton, New Jersey, where she worked as a maid, and cooked Southern-style meals for Michelle and her brother, Craig, when they were students at Princeton University.\\\\nCraig Robinson\\\\nMichelle Obama\\'s brother, born 1962. He is currently head coach of men\\'s basketball at Oregon State University.[84]\\\\nFraser Robinson III\\\\nMichelle Obama\\'s father, born 1935, died 1991, married Michelle\\'s mother, Marian Shields, in 1960.[85][19] Robinson was a pump worker at the City of Chicago water plant.[3]\\\\nFraser Robinson, Jr.\\\\nMichelle Obama\\'s grandfather was born on August 24, 1912 in Georgetown, South Carolina, and died on November 9, 1996, aged 84. He was a good student and orator, but moved from South Carolina to Chicago to find better work than he could find at home, eventually becoming a worker for the United States Postal Service. He was married to LaVaughn Johnson. When he retired, they moved back to South Carolina.[3]\\\\nCapers C. Funnye Jr.\\\\nMichelle Obama\\'s first cousin once removed: Funnyeýýýs mother, Verdelle Robinson Funnye (born Verdelle Robinson; August 22, 1930 ýýý April 16, 2000) and Michelle Obamaýýýs paternal grandfather, Fraser Robinson Jr., were siblings. One of America\\'s most prominent African American Jews, known for acting as a bridge between mainstream Jewry and African Americans.[86]\\\\nGenealogical charts\\\\nObama ancestry\\\\n16. Opiyo\\\\n8. Obama\\\\n4. Hussein Onyango Obama\\\\n9. Nyaoke\\\\n2. Barack Hussein Obama, Sr.\\\\n5. Habiba Akumu\\\\n1. Barack Hussein Obama II\\\\n24. Jacob William Dunham\\\\n12. Ralph Waldo Emerson Dunham, Sr.\\\\n25. Mary Ann Kearney\\\\n6. Stanley Armour Dunham\\\\n26. Harry Ellington Armour\\\\n13. Ruth Lucille Armour\\\\n27. Gabriella Clark\\\\n3. Stanley Ann Dunham\\\\n28. Charles T. Payne\\\\n14. Rolla Charles Payne\\\\n29. Della L. Wolfley\\\\n7. Madelyn Lee Payne\\\\n30. Thomas Creekmore McCurry\\\\n15. Leona Belle McCurry\\\\n31. Margaret Belle Wright\\\\nFamily trees\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nBarack Obama\\\\nStanley\\\\nDunham\\\\n1918ýýý1992\\\\nMadelyn\\\\nPayne\\\\n1922ýýý2008\\\\nHabiba\\\\nAkumu\\\\nOnyango\\\\nObama\\\\nc.\\xa01895ýýý1979\\\\nSarah\\\\nOgwel\\\\n1922ýýý\\\\nLolo\\\\nSoetoro\\\\n1936ýýý1987\\\\nAnn\\\\nDunham\\\\n1942ýýý1995\\\\nBarack\\\\nObama, Sr. *\\\\n1936ýýý1982\\\\nKezia\\\\nGrace\\\\nc. 1940ýýý\\\\nOmar\\\\nObama\\\\n1944ýýý\\\\nZeituni\\\\nOnyango\\\\n1952ýýý\\\\nYusuf\\\\nObama\\\\nc. 1950sýýý\\\\nSaid\\\\nObama\\\\nc. 1950sýýý\\\\nKonrad\\\\nNg\\\\nc. 1974ýýý\\\\nMaya\\\\nSoetoro\\\\n1970ýýý\\\\nBarack\\\\nObama\\\\n1961ýýý\\\\nMichelle\\\\nRobinson\\\\n1964ýýý\\\\nM. Abongo\\\\nObama\\\\n1958ýýý\\\\nAuma\\\\nObama\\\\nc. 1960ýýý\\\\nAbo\\\\nObama\\\\n1968ýýý\\\\nBernard\\\\nObama\\\\n1970ýýý\\\\nSuhaila\\\\nNg\\\\nc. 2005ýýý\\\\nMalia Ann\\\\nObama\\\\n1998ýýý\\\\nSasha\\\\nObama\\\\n2001ýýý\\\\n* Barack\\\\nObama, Sr.\\'s\\\\nadditional\\\\nRuth\\\\nNidesandjo\\\\nc. 1940sýýý\\\\nJael\\\\nOtieno\\\\nrelationships:\\\\nMark\\\\nNdesandjo\\\\nDavid\\\\nNdesandjo\\\\ndied\\xa0c.\\xa01987\\\\nGeorge\\\\nObama\\\\nc. 1982ýýý\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nMichelle Obama\\\\nFraser\\\\nRobinson, Jr.\\\\nýýý\\\\nSouth Carolina\\\\n(1912ýýý1996)\\\\nSawmill worker\\\\nLaVaughn D.\\\\nJohnson\\\\nýýý\\\\nIllinois\\\\n(1915ýýý2002)\\\\nCapers C.\\\\nFunnye, Jr.\\\\nýýý\\\\n(born c. 1952;\\\\nnephew of\\\\nFraser\\\\nRobinson, Jr.)\\\\nRabbi in Chicago\\\\nFraser\\\\nRobinson III\\\\nýýý\\\\nIllinois\\\\n(1935ýýý1991)\\\\nEnjoyed boxing\\\\nin youth;\\\\nwater plant\\\\npump operator\\\\nin Chicago\\\\nMarian\\\\nShields\\\\nýýý\\\\nIllinois\\\\n(born 1937)\\\\nSecretary at\\\\nSpiegel catalog\\\\nin Chicago;\\\\nU.S.\\'s First\\\\nGrandmother\\\\nCraig\\\\nRobinson\\\\nýýý\\\\nIllinois\\\\n(born 1962)\\\\nHead coach of\\\\nOregon State\\\\nBeavers men\\'s\\\\nbasketball\\\\nMichelle\\\\nRobinson\\\\nýýý\\\\nIllinois\\\\n(born 1964)\\\\nFirst Lady\\\\nof the United States\\\\nDistant relations\\\\nSee also: List of United States Presidents by genealogical relationship\\\\nAccording to genealogists, Barack Obama\\'s distant cousins include the multitude of descendants of his maternal ancestors from all along the early-American Atlantic seaboard as well as paternal, Kenyan relations belonging to the Luo tribe, many descending from a 17th century ancestor named Owiny.[87][88] For example, George W. Bush, the 43rd U.S. president, is the eleventh cousin of Barack Obama.[89] The New York Times science writer Nicholas Wade argues that with eleven generations leading back to their common progenitor, Samuel Hinckley, the relationship between the 43rd President and the 44th President is \\\\\"genetically meaningless\\\\\".[90]\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nSelected genealogical relationships\\\\nBill Hickok\\\\nAccording to Barack Obama\\'s family lore (and confirmed by the New England Historic Genealogical Society), the President and Hickok are sixth cousins, six-times removed.[91]\\\\nRobert Duvall\\\\nGoodnight helped inspire Pulitzer Prize-winning author Larry McMurtry to create a protagonist for his novel series Lonesome Dove: Woodrow Call. In Dove\\'s television novela, Woodrow Call\\'s partner is Gus McCrae, portrayed by Obama\\'s eighth cousin, twice removed, actor Robert Duvall.[92]\\\\nJames Madison\\\\nObama is also distantly related to U.S. Presidents James Madison, Lyndon Johnson, Gerald Ford, and U.S. Vice President Dick Cheney, British Prime Minister Sir Winston Churchill, U.S. Civil War General Robert E. Lee, and actor Brad Pitt.[93][94][95]\\\\nCharles Goodnight\\\\nAccording to Chicago Sun-Times reporter Scott Fornek, another Obama progenitor, Catherine Goodnight, was the grandniece of George Goodnight, who was in turn great-grandfather of famed cattleman Charles Goodnight.[92]\\\\nHarry S. Truman\\\\nActor Duvall is distant cousins with United States President Harry Truman, who\\'s likewise a fourth cousin, four times removed, of Obama\\'s.[92]\\\\nGeorgia O\\'Keeffe\\\\nNotable women Obama is related to include painter Georgia OýýýKeeffe.[93]\\\\nSee also\\\\nU.S. Presidential genealogical relationships\\\\nNew England Historic Genealogical Society\\\\nGary Boyd Roberts\\\\nReferences\\\\n^ Keck, Kristi (4 June 2008). \\\\\"Obama crosses historic milestone\\\\\". CNN. http://news.yahoo.com/s/ap/20081122/ap_on_go_pr_wh/obama_school_12. Retrieved on 2008-11-21.\\\\n^ a b Reitwiesner, William Addams. \\\\\"Ancestry of Barack Obama\\\\\". http://www.wargs.com/political/obama.html. Retrieved on 2008-10-09.\\\\n^ a b c d e f g h Murray, Shailagh (2 October 2008). \\\\\"A Family Tree Rooted In American Soil: Michelle Obama Learns About Her Slave Ancestors, Herself and Her Country\\\\\". The Washington Post: p.\\xa0C01. http://www.washingtonpost.com/wp-dyn/content/article/2008/10/01/AR2008100103169.html. Retrieved on 2008-10-10.\\\\n^ Sheridan, Michael (5 February 2007). \\\\\"Secrets of Obama Family Unlocked\\\\\". Muslim Observer. http://news.newamericamedia.org/news/view_article.html?article_id=ae5895fc29971b172938790be94ab107. Retrieved on 2008-11-21.\\\\n^ RTE news report March, 2007:Obamas Irish family links discovered by ancestry.co.uk\\\\n^ Noveck, Jocelyn (2008-11-07). \\\\\"Deseret News | Obama\\'s young, energetic family harks back to days of Camelot\\\\\". Secure.deseretnews.com. https://secure.deseretnews.com/article/1,5143,705261276,00.html. Retrieved on 2009-01-31.\\\\n^ Liza Mundy, Michelle: A Biography (Simon and Schuster, 2008), p. 129.\\\\n^ \\\\\"Sasha Obama\\\\\". Baltimore Sun. http://www.baltimoresun.com/topic/politics/sasha-obama-PECLB004381.topic. Retrieved on 2009-01-31.\\\\n^ a b Obama, Barack (13 January 2009). \\\\\"\\'What I Want for You--And Every Child in America\\'\\\\\". http://www.parade.com/news/2009/01/barack-obama-letter-to-my-daughters.html.\\\\n^ Sobieraj Westfall, Sandra (23 June 2008). \\\\\"Barack Obama Gives Daughter $1 Allowance a Week\\\\\". People Magazine. http://www.people.com/people/article/0,,20214569_1,00.html. Retrieved on 2008-11-21.\\\\n^ Lester, Will (July 23, 2008). \\\\\"Obama daughters keep hectic schedules of their own\\\\\". Associated Press. http://elections.apnews.com/apelect/db_6911/contentdetail.htm;jsessionid=8314A43012AB5FF1D0697247362D8752?contentguid=H95QubFb&full=true. Retrieved on 2008-08-04.\\\\n^ Hiro, Anne. \\\\\"Obama regrets letting \\\\\"Access Hollywood\\\\\" interview daughters. Won\\'t do it again. MSNBC\\'s Dan Abrams gets the story behind the story. - Lynn Sweet\\\\\". Blogs.suntimes.com. http://blogs.suntimes.com/sweet/2008/07/obama_regrets_letting_access_h.html. Retrieved on 2009-01-31.\\\\n^ Ahmed, Saeed (5 November 2008). \\\\\"Move over Barney, new dog moving into White House\\\\\". CNN. http://www.cnn.com/2008/LIVING/wayoflife/11/05/presidential.pets/index.html. Retrieved on 2008-11-21.\\\\n^ \\\\\"Obama: Getting a dog isn\\'t easy\\\\\". Associated Press. 7 November 2008. http://www.mercurynews.com/ci_10927292. Retrieved on 2008-11-21.\\\\n^ Janice Lloyd (2009-01-12). \\\\\"Obamas down to Labradoodle or Portuguese water dog\\\\\". USA Today. http://www.usatoday.com/news/washington/2009-01-11-obama-dog_N.htm. Retrieved on 2009-01-28.\\\\n^ Swarns, Rachel (21 November 2008). \\\\\"And the Winner Is ýýý Sidwell Friends\\\\\". The New York Times. http://thecaucus.blogs.nytimes.com/2008/11/21/and-the-winner-is-sidwell-friends/. Retrieved on 2008-11-21.\\\\n^ Tolin, Lisa (2009-01-05). \\\\\"Obama girls start school with photographers in tow\\\\\". The Associated Press. http://www.google.com/hostednews/ap/article/ALeqM5g6mv_lkODQMmQdpyIEnr8Zpm5mogD95H8KA80. Retrieved on 2009-01-06.\\\\n^ Taylor Marsh (2008-08-25). \\\\\"Political Analysis, National Security and Breaking News\\\\\". Taylor Marsh. http://www.taylormarsh.com/archives_view.php?id=28286. Retrieved on 2009-01-31.\\\\n^ a b Lia LoBello (2008-01-02). \\\\\"First Families: Radar introduces you to the next president\\'s relatives\\\\\". Radar Online. http://www.radaronline.com/features/2008/07/john_mccain_barack_obama_michelle_cindy_dunham_roberta_wrigh_04.php. Retrieved on 2009-01-28.\\\\n^ Rachel L. Swarns (2009-01-09). \\\\\"Obamaýýýs Mother-in-Law to Move Into the White House\\\\\". The New York Times. http://thecaucus.blogs.nytimes.com/2009/01/09/obamas-mother-in-law-to-move-into-the-white-house/?hp. Retrieved on 2009-01-09.\\\\n^ a b \\\\\"Will Obama mum-in-law make it a family affair in the White House?\\\\\". Agence France Presse. 2008-11-22. http://www.google.com/hostednews/afp/article/ALeqM5gN_i2jrCVkJQgfMbSDRRrNk8U4Sw. Retrieved on 2009-01-09.\\\\n^ Philip Sherwell (2008-2008-11-09). \\\\\"Michelle Obama persuades First Granny to join new White House team\\\\\". The Telegraph (UK). http://www.telegraph.co.uk/news/3407525/Michelle-Obama-persuades-First-Granny-to-join-new-White-House-team.html. Retrieved on 2009-01-09.\\\\n^ \\\\\"ýýýTootýýý: Obama grandmother a force that shaped him\\\\\". via Associated Press. 2008-08-25. http://www.thekansan.com/news/x1311851415/-Toot-Obama-grandmother-a-force-that-shaped-him. Retrieved on 2008-08-29.\\\\n^ \\\\\"CNN: \\\\\"Obama\\'s grandmother dies after battle with cancer\\\\\"\\\\\". http://www.cnn.com/2008/POLITICS/11/03/obama.grandma/index.html. Retrieved on 2008-11-04.\\\\n^ The 89th Infantry Division, United States Holocaust Memorial Museum\\\\n^ a b Obama\\'s great-uncle recalls liberating Nazi camp, Boston.com, July 22, 2008\\\\n^ Major Garrett (2008-05-27). \\\\\"Obama Campaign Scrambles to Correct the Record on Uncle\\'s War Service\\\\\". FOXNews.com. http://elections.foxnews.com/2008/05/27/recollection-of-obama-familys-service-missing-key-details. Retrieved on 2009-01-31.\\\\n^ \\\\\"Democrats salute Obamaýýýs great uncle\\\\\". Jewish Telegraphic Agency. August 28, 2008. http://jta.org/news/article/2008/08/28/110123/obamapayne. Retrieved on 31 January 2009.\\\\n^ Obama Family Tree dgmweb.net\\\\n^ Chicago Sun Times article with her picture\\\\n^ Obama has links to Malaysia\\\\n^ Nolan, Daniel (2008-06-11). \\\\\"Relative: Obama\\'s got \\'a good handle on Canada\\'\\\\\". The Hamilton Spectator. http://www.thespec.com/burlingtonlife/article/384475. Retrieved on 2008-07-03.\\\\n^ Nolan, Daniel (June 11, 2008). \\\\\"Obama\\'s Burlington connection\\\\\". The Hamilton Spectator. http://www.thespec.com/article/384307. Retrieved on 2008-06-21.\\\\n^ Misner, Jason (2008-06-20). \\\\\"Barack Obama was here\\\\\". Burlington Post. http://www.burlingtonpost.com/printarticle/186215. Retrieved on 2008-07-03.\\\\n^ Fornek, Scott (2007-09-09). \\\\\"\\'He helped me find my voice\\'\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545473,BSX-News-wotreehh09.article.\\\\n^ Cooper, Tom (2009-01-20). \\\\\"Keep watch for Obama\\\\\". TheSpec.com. http://www.thespec.com/Opinions/article/499161. Retrieved on 2009-01-28.\\\\n^ Ancestry of Barack Obama\\\\n^ Dreams from My Father, p. 376\\\\n^ a b Kenya: Special Report: Sleepy Little Village Where Obama Traces His Own Roots (Page 2 of 2)\\\\n^ a b \\\\\"Obama\\'s grandma slams \\'untruths\\'\\\\\". Associated Press. 2008-03-05. http://www.usatoday.com/news/world/2008-03-05-obama-kin_N.htm.\\xa0 See also this correction.\\\\n^ \\\\\"Q&A ON THE NEWS\\\\\". Atlanta Journal-Constitution. 2009-02-25. http://www.ajc.com/services/content/metro/stories/2009/02/25/questi0225.html. Retrieved on 2009-02-27.\\\\n^ In Kenya, Barack Obamaýýýs family prays for end to conflict - Times Online\\\\n^ a b Crilly, Rob (February 27, 2008). \\\\\"Dreams from Obama\\'s Grandmother\\\\\". Time Magazine, Inc.. http://www.time.com/time/world/article/0,8599,1717590,00.html?xid=rss-topstories. Retrieved on 2008-07-03.\\\\n^ Pflanz, Mike (2008-01-11). \\\\\"Barack Obama\\'s Kenyan relatives keep faith\\\\\". Daily Telegraph. http://www.telegraph.co.uk/news/main.jhtml?xml=/news/2008/01/09/wuspols1009.xml.\\\\n^ Fornek, Scott (2007-09-09). \\\\\"Sarah Obama - \\'Sparkling, laughing eyes\\'\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545459,BSX-News-wotreeu09.article.\\\\n^ \\\\\"Barack Obama in Kenya\\\\\". CNN. http://www.youtube.com/watch?v=Ikg6gj71U9k.\\\\n^ Daily Nation, July 8, 2008: Obama granny\\'s day out with envoys and top politicians\\\\n^ \\\\\"A Candidate, His Minister and the Search for Faith\\\\\". New York Times. 2007-04-30. http://www.nytimes.com/2007/04/30/us/politics/30obama.html?_r=2&pagewanted=all&oref=slogin&oref=slogin.\\\\n^ \\\\\"Kenya: All Obama kin to spend voting day in Kogelo\\\\\". afrika.no. 2008-10-27. http://www.afrika.no/Detailed/17321.html. Retrieved on 2009-01-31.\\\\n^ Cohen, Roger (2008-03-06). \\\\\"The Obamas of the World - New York Times\\\\\". Nytimes.com. http://www.nytimes.com/2008/03/06/opinion/06cohen.html. Retrieved on 2009-01-31.\\\\n^ a b Sanderson, Elizabeth (2008-01-06). \\\\\"Barack Obama\\'s stepmother living in Bracknell reveals the close bond with him ... and his mother\\\\\". Daily Mail. http://www.dailymail.co.uk/news/article-506338/Barack-Obamas-stepmother-living-Bracknell-reveals-close-bond---mother.html.\\\\n^ Lindsay, Anna (2009-01-20). \\\\\"Barack\\'s bingo-loving stepmother\\\\\". BBC News. http://news.bbc.co.uk/1/hi/england/berkshire/7834368.stm.\\\\n^ http://www.newvision.co.ug/D/8/26/666733\\\\n^ jpt (2008-06-18). \\\\\"From the Fact Check Desk: What Did Obama\\'s Half-Brother Say About Obama\\'s Background\\\\\". ABC News. http://blogs.abcnews.com/politicalpunch/2008/06/from-the-fact-c.html.\\\\n^ a b c d e f Maliti, Tom (2004-10-26). \\\\\"Obama\\'s Brother Chooses Life in Slow Lane\\\\\". The Associated Press. http://www.msnbc.msn.com/id/6333496.\\\\n^ Obama, Dreams from my Father, 2004, p. 265.\\\\n^ Obama, Dreams from my Father, 2004, p. 262.\\\\n^ a b *Oywa, John (2004-08-15). \\\\\"Sleepy Little Village Where Obama Traces His Own Roots\\\\\". The Daily Nation. http://allafrica.com/stories/200408160533.html.\\\\n^ Philip Ochieng (2004-11-01). \\\\\"From Home Squared to the US Senate: How Barack Obama Was Lost and Found\\\\\". The East African. http://www.nationmedia.com/EastAfrican/01112004/Features/PA2-11.html. Retrieved on 2008-03-23.\\\\n^ Warah, Rasna (2008-06-09). \\\\\"We cannot lay claims on Obama; he\\'s not one of us - Obama in this world\\\\\". Daily Nation. http://www.nationmedia.com. Retrieved on 2008-07-10.\\\\n^ Scott Fornek (2007-09-09). \\\\\"AUMA OBAMA: \\'Her restlessness, her independence\\'\\\\\". Chicago Sun Times. http://www.suntimes.com/news/politics/obama/familytree/545465,BSX-News-wotreew09.article. Retrieved on 2008-03-23.\\\\n^ a b c Gathmann, Florian; Gregor Peter Schmitz, Jochen Schýýnmann (July 24, 2008). \\\\\"Studentin in der Bundesrepublik: Wie Auma Obama mit Deutschland haderte\\\\\" (in German). Spiegel Online. http://www.spiegel.de/politik/ausland/0,1518,567286,00.html. Retrieved on 2008-07-24.\\\\n^ a b Harvey, Oliver (07-26 2008). \\\\\"Obama\\'s brother is in Bracknell\\\\\". The Sun. http://www.thesun.co.uk/sol/homepage/news/the_real_american_idol/article1472877.ece. Retrieved on 2008-10-06.\\\\n^ \\\\\"Madari Kindergarten\\\\\". http://www.madarikindergarten.com/.\\\\n^ \\\\\"Welcome To MedWeek San Antonio 2007\\\\\". Medweeksa.org. http://www.medweeksa.org/awardwinners/techfirm.htm. Retrieved on 2009-01-31.\\\\n^ \\\\\"PIDE - Partners for International Development & Education Inc\\\\\". Pideafrica.org. http://pideafrica.org/aboutus.htm. Retrieved on 2009-01-31.\\\\n^ Barack Obamaýýýs brother pushes Chinese imports on US - Times Online\\\\n^ Obama half-brother runs Internet company in China\\\\n^ Roger Cohen (2008-03-17). \\\\\"Obama\\'s Brother in China\\\\\". The New York Times. http://www.nytimes.com/2008/03/17/opinion/29cohen.html. Retrieved on 2008-03-23.\\\\n^ \\\\\"Youku Buzz (daily)\\xa0ýý Blog Archive\\xa0ýý Barack Obamaýýýs Half-Brother in Concert\\\\\". Buzz.youku.com. 2009-01-18. http://buzz.youku.com/2009/01/18/barack-obamas-half-brother-in-concert/. Retrieved on 2009-01-31.\\\\n^ jaketapper (2008-07-28). \\\\\"Political Punch: Barack Obama\\'s Branch-y Family Tree\\\\\". Blogs.abcnews.com. http://blogs.abcnews.com/politicalpunch/2008/07/barack-obamas-1.html. Retrieved on 2009-01-31.\\\\n^ Fornek, Scott (September 9, 2007). \\\\\"HALF-BROTHER GEORGE: \\'I would be there for him\\'\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545447,BSX-News-wotreecc09.stng. Retrieved on 2008-08-04.\\\\n^ a b c d e Pflanz, Mike (August 21, 2008). \\\\\"Barack Obama is my inspiration, says lost brother\\\\\". The Daily Telegraph. http://www.telegraph.co.uk/news/newstopics/uselection2008/barackobama/2595688/Barack-Obama-is-my-inspiration-says-lost-brother.html. Retrieved on 2008-08-23.\\\\n^ a b c Crilly, Rob (August 22, 2008). \\\\\"Life is good in my Nairobi slum, says Barack Obama\\'s younger brother\\\\\". The Times. http://www.timesonline.co.uk/tol/news/world/us_and_americas/us_elections/article4583353.ece. Retrieved on 2008-08-23.\\\\n^ a b McKenzie, David (2008-08-23). \\\\\"Behind the Scenes: Meet George Obama\\\\\". CNN. http://www.cnn.com/2008/POLITICS/08/22/bts.obama.brother/. Retrieved on 2008-10-26.\\\\n^ Pisa, Nick (August 20, 2008). \\\\\"Barack Obama\\'s \\'lost\\' brother found in Kenya\\\\\". Daily Telegraph. http://www.telegraph.co.uk/news/newstopics/uselection2008/barackobama/2590614/Barack-Obamas-lost-brother-found-in-Kenya.html. Retrieved on 2008-08-20.\\\\n^ a b Wadhams, Nick (2008-10-07). \\\\\"Corsi in Kenya: Obama\\'s Nation Boots Obama Nation Author\\\\\". TIME. http://www.time.com/time/world/article/0,8599,1847965,00.html?imw=Y. Retrieved on 2009-01-31.\\\\n^ by Dinesh D\\'Souza. \\\\\"Dinesh D\\'Souza\\xa0: George Obama, Start Packing\\\\\". Townhall.com. http://townhall.com/columnists/DineshDSouza/2008/09/22/george_obama,_start_packing. Retrieved on 2009-01-31.\\\\n^ a b c \\\\\"The Obama Family Tree\\\\\" (PDF). Chicago Sun-Times. September 9, 2007. http://www.suntimes.com/images/cds/MP3/obamatree.pdf. Retrieved on 2008-11-23.\\\\n^ First read, MSNBC\\\\n^ \\\\\"Barack Obama\\'s aunt found living in rundown public housing estate | The Australian\\\\\". Theaustralian.news.com.au. 2008-10-31. http://www.theaustralian.news.com.au/story/0,25197,24578185-5017121,00.html. Retrieved on 2009-01-31.\\\\n^ Boston Housing Authority ýýýflabbergasteredýýý Barack Obamaýýýs aunt living in Southie\\\\n^ Kilner, Derek (2008-11-05). \\\\\"Kenya Celebrates President Obama as Native Son\\\\\". Voice Of America. http://www.voanews.com/english/archive/2008-11/2008-11-05-voa45.cfm. Retrieved on 2008-12-24.\\\\n^ \\\\\"Oregon State University Beavers: Craig Robinson bio\\\\\". http://www.osubeavers.com/ViewArticle.dbml?SPSID=106239&SPID=1954&DB_OEM_ID=4700&ATCLID=1436883&Q_SEASON=2008. Retrieved on 2008-08-21.\\\\n^ \\\\\"RootsWeb\\'s WorldConnect Project: Dowling Family Genealogy\\\\\". Wc.rootsweb.ancestry.com. http://wc.rootsweb.ancestry.com/cgi-bin/igm.cgi?op=GET&db=dowfam3&id=I105855. Retrieved on 2009-01-31.\\\\n^ Weiss, Anthony (September 2, 2008). \\\\\"Michelle Obama Has a Rabbi in Her Family\\\\\". The Forward. http://www.forward.com/articles/14121/. Retrieved on 2008-10-09.\\\\n^ Gary Boyd Roberts. \\\\\"Notes on the Ancestry of Senator Barack Hussein Obama, Jr\\\\\". New England Historic Genealogical Society. http://www.newenglandancestors.org/research/services/articles_ancestry_barack_obama.asp. Retrieved on 2009-01-31.\\\\n^ Scott Fornek (2007-09-09). \\\\\"Obama Family Tree\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545463,BSX-News-wotreer09.stng. Retrieved on 2009-01-31.\\\\n^ \\\\\"Obama-Bush (family tree)\\\\\" (PDF). New England Historic Genealogical Society. http://www.newenglandancestors.org/pdfs/obama_bush.pdf. Retrieved on 2009-01-31.\\\\n^ Wade, Nicholas (2007-10-21). \\\\\"Barack Obama - Dick Cheney - Ancestry and Genealogy - Washington - New York Times\\\\\". Nytimes.com. http://www.nytimes.com/2007/10/21/weekinreview/21basic.html. Retrieved on 2009-01-31.\\\\n^ Eastman, Dick (2008-07-30). \\\\\"Barack Obama is Related to Wild Bill Hickok\\\\\". Blog.eogn.com. http://blog.eogn.com/eastmans_online_genealogy/2008/07/barack-obama-is.html. Retrieved on 2009-01-31.\\\\n^ a b c Scott Fornek (2007-09-09). \\\\\"Obama Family Tree\\\\\". Suntimes.com. http://www.suntimes.com/news/politics/obama/familytree/545441,BSX-News-wotreec09.stng. Retrieved on 2009-01-31.\\\\n^ a b \\\\\"Notes on the Ancestry of Senator Barack Hussein Obama, Jr\\\\\". New England Historic Genealogical Society. 2002-08-14. http://www.newenglandancestors.org/research/services/articles_ancestry_barack_obama.asp. Retrieved on 2009-01-31.\\\\n^ \\\\\"Obama, Clinton and McCain have some famous relations\\\\\". HeraldNet - AP. 2008-03-26. http://www.heraldnet.com/article/20080326/NEWS02/151343661. Retrieved on 2009-01-31.\\\\n^ \\\\\"Barack Obama and Joe Biden: The Change We Need\\\\\". My.barackobama.com. 2008-07-31. http://my.barackobama.com/page/community/post/williambrehm/gG5TVR. Retrieved on 2009-01-31.\\\\nExternal links\\\\nBarack Obama\\'s Family Tree - Photo Essays - TIME\\\\n\\\\\"Though Obama Had to Leave to Find Himself, It Is Hawaii That Made His Rise Possible,\\\\\" by David Maraniss\\\\nBarack Obama\\'s Branch-y Family Tree by Jake Tapper\\\\n\\\\\"Obama Family Tree\\\\\" series, by Scott Fornek\\\\n\\\\\"Six Degrees of Barack Obama\\\\\"\\\\n\\\\\"Notes on the Ancestry of Senator Barack Hussein Obama, Jr.,\\\\\" by Gary Boyd Roberts\\\\n\\\\\"Obama, Clinton and McCain have some famous relations,\\\\\" by The Associated Press\\\\n\\\\\"Obama\\'s Patriotic Family Tree,\\\\\" by Bill Brehm\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nBarack Obama\\\\nPresidency\\\\nTransition\\xa0ýý Inauguration\\xa0ýý Timeline\\xa0ýý Cabinet\\xa0ýý Judiciary\\xa0ýý Foreign policy\\xa0ýý First 100 days\\\\nEarly life and\\\\npolitical career\\\\nPublic image\\xa0ýý Illinois Senate career\\xa0ýý 2004 Democratic National Convention\\xa0ýý U.S. Senate election in Illinois\\xa0ýý U.S. Senate career\\xa0ýý Presidential primary campaign\\xa0ýý ObamaýýýBiden 2008\\xa0ýý Electoral history\\xa0ýý Political positions\\\\nBooks\\\\nDreams from My Father\\xa0ýý The Audacity of Hope\\\\nSpeeches\\\\nThe Audacity of Hope\\xa0ýý A More Perfect Union\\xa0ýý Change Has Come to America\\xa0ýý 2009 speech to joint session of Congress\\\\nFamily\\\\nMichelle Obama\\xa0ýý Barack Obama, Sr.\\xa0ýý Ann Dunham\\xa0ýý Lolo Soetoro (stepfather)\\xa0ýý Maya Soetoro-Ng (half-sister)\\xa0ýý Marian Robinson (mother-in-law)\\xa0ýý Stanley Armour Dunham (grandfather)\\xa0ýý Madelyn Dunham (grandmother)\\xa0ýý Extended family\\xa0ýý Family tree\\\\nRetrieved from \\\\\"http://en.wikipedia.org/wiki/Family_of_Barack_Obama\\\\\"\\\\nCategories: Obama family | African American history | African American families | Luo Kenyans | People of mixed Black African-European ethnicity | African Americans | Asian Americans | Dutch Americans | English Americans | French Americans | German-Americans | Irish-Americans | Indonesian Americans | Kenyan-Americans | Scottish-Americans | Chinese Canadians | People of mixed Asian-European ethnicity | American families | First Families of the United States | Family treesHidden categories: Wikipedia semi-protected pages | Wikipedia indefinitely move-protected pages | All pages needing cleanup | Wikipedia articles needing factual verification since October 2008 | All pages needing factual verification | All articles with unsourced statements | Articles with unsourced statements since November 2008\\\\nViews\\\\nArticle\\\\nDiscussion\\\\nView source\\\\nHistory\\\\nPersonal tools\\\\nLog in / create account\\\\nNavigation\\\\nMain page\\\\nContents\\\\nFeatured content\\\\nCurrent events\\\\nRandom article\\\\nSearch\\\\nInteraction\\\\nAbout Wikipedia\\\\nCommunity portal\\\\nRecent changes\\\\nContact Wikipedia\\\\nDonate to Wikipedia\\\\nHelp\\\\nToolbox\\\\nWhat links here\\\\nRelated changes\\\\nUpload file\\\\nSpecial pages\\\\nPrintable version Permanent linkCite this page\\\\nLanguages\\\\nBahasa Indonesia\\\\nSvenska\\\\nýýýýýý\\\\nýýýýýý\\\\nThis page was last modified on 16 March 2009, at 03:22.\\\\nAll text is available under the terms of the GNU Free Documentation License. (See Copyrights for details.)\\\\nWikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a U.S. registered 501(c)(3) tax-deductible nonprofit charity.\\\\nPrivacy policy\\\\nAbout Wikipedia\\\\nDisclaimers\\n[DOC 4] Family of Barack Obama - Wikipedia, the free encyclopedia\\\\nFamily of Barack Obama\\\\nFrom Wikipedia, the free encyclopedia\\\\n(Redirected from Sarah Ogwel)\\\\nJump to: navigation, search\\\\nObama Family\\\\nPresident Barack Obama, First Lady Michelle, and daughters Malia and Sasha wave to the crowd after his inaugural address Jan. 20, 2009, on the west steps of the U.S. Capitol.[1]\\\\nCurrent region\\\\nWashington, DC\\\\nInformation\\\\nPlace of origin\\\\nUnited States\\\\nNotable members\\\\nBarack Obama, Michelle Obama, Ann Dunham, Barack Obama, Sr., etc.\\\\nConnected families\\\\nRobinson, Dunham, Soetoro, Ng\\\\nThis article is part of a series about\\\\nBarack Obama\\\\nBackground \\xa0ýý Illinois Senate \\xa0ýý U.S. Senate\\\\nPolitical positions\\xa0ýý Public image\\xa0ýý Family\\\\n2008 primaries\\xa0ýý ObamaýýýBiden campaign\\\\nTransition\\xa0ýý Inauguration\\xa0ýý Electoral history\\\\nPresidency (Timeline, First 100 days)\\\\nMalia, Michelle and Sasha on stage at the 2008 Democratic National Convention\\\\nThe Family of Barack Obama is an extended clan of African American, English, Indonesian, and Kenyan (Luo) heritage known through the writings and political career of Barack Obama, the President of the United States of America,[2][3][4][5] and other reports. His immediate family is the First Family of the United States. The Obamas are the first First Family of African American descent in the United States and the youngest to enter the White House since the Kennedys. One columnist wrote, ýýýObama\\'s young, energetic family harks back to days of Camelot.ýýý[6]\\\\nContents\\\\n1 Immediate family\\\\n2 Extended family - maternal relations\\\\n3 Extended family - paternal relations\\\\n4 Michelle Robinson Obama\\'s extended family\\\\n5 Genealogical charts\\\\n5.1 Obama ancestry\\\\n5.2 Family trees\\\\n6 Distant relations\\\\n7 See also\\\\n8 References\\\\n9 External links\\\\nImmediate family\\\\nMichelle Obama\\\\nMichelle Obama, nýýe Robinson, the wife of Barack Obama, was born on January 17, 1964 in Chicago, Illinois. She is a lawyer and was a University of Chicago Hospital Vice-President. She is the First Lady of the United States.\\\\nMalia Obama and Sasha Obama\\\\nBarack and Michelle Obama have two daughters: Malia Ann (pronounced /mýýýýliýýýý/), born in 1998,[7] and Natasha (known as Sasha) /ýýsýýýýýýýý/), born in 2001. Sasha is the youngest child to reside in the White House since John F. Kennedy, Jr, arrived as an infant in 1961.[8]\\\\nBefore his inauguration, President Obama published an open letter to his daughters in Parade magazine, describing what he wants for them and every child in America: \\\\\"to grow up in a world with no limits on your dreams and no achievements beyond your reach, and to grow into compassionate, committed women who will help build that world.\\\\\"[9]\\\\nWhile living in Chicago, they kept busy schedules, as the Associated Press reports: \\\\\"soccer, dance and drama for Malia, gymnastics and tap for Sasha, piano and tennis for both.\\\\\"[10][11] In July 2008, the family gave an interview to the television series Access Hollywood; Obama later said they regretted allowing the children to be included.[12]\\\\nIn his victory speech on the night of his election, President Obama repeated his promise to Sasha and Malia to get a puppy to take with them to the White House.[13] However the selection of a dog has been slow because Malia is allergic to animal dander;[14] the president subsequently said that the choice has been narrowed down to either a labradoodle or Portuguese Water Dog, and they are hoping to find a shelter animal.[15]\\\\nMalia and Sasha attend the private Sidwell Friends School in Washington, DC, the same school as attended by Chelsea Clinton, Tricia Nixon Cox, and Archibald Roosevelt, and currently the grandchildren of Vice President Joe Biden.[16] The Obama girls began classes there on January 5, 2009.[17] While in Chicago, both attended the private University of Chicago Laboratory School.\\\\nMarian Shields Robinson\\\\nMichelle Obama\\'s mother (birthname Marian Shields, born July 1937), now widowed, married Michelle\\'s father, Fraser Robinson, in 1960.[18][19] Robinson was formerly a secretary at Spiegel catalog and a bank. While Michelle and Barack Obama were campaigning in 2008, Robinson tended the Obama\\'s young children and she intends to do the same while in Washington, DC. Robinson is currently living in the White House itself as part of the First Family;[20] she is the first live-in grandmother there since Elivera M. Doud during the Eisenhower administration.[21] Some media outlets have dubbed Robinson the \\\\\"First Granny\\\\\".[22][21]\\\\nExtended family - maternal relations\\\\nRight-to-left: Barack Obama and Maya Soetoro with their mother Ann Dunham and grandfather Stanley Dunham in Hawaii (early 1970s)\\\\nWikinews has related news:\\\\nBarack Obama elected 44th President of the United States\\\\nGrandmother of Barack Obama dies at 86\\\\nMadelyn Dunham with her daughter Ann\\\\nAccording to Barack Obama\\'s Dreams from My Father, his great-grandmother Leona McCurry was part Native American, which Obama believed Leona held as a \\\\\"source of considerable shame\\\\\" and \\\\\"blanched whenever someone mentioned the subject and hoped to carry the secret to her grave\\\\\"; whereas McCurry\\'s daughter (Obama\\'s maternal grandmother) \\\\\"would turn her head in profile to show off her beaked nose, which along with a pair of jet-black eyes, was offered as proof of Cherokee blood.\\\\\"[23] To date, no concrete evidence has surfaced of Cherokee heritage. Obama\\'s maternal heritage consists mostly of English ancestry, with much smaller amounts of German, Irish, Scottish, Welsh, Swiss, and French ancestry.[2]\\\\nAnn Dunham\\\\nMother of Barack Obama born in 1942, died in 1995. Birthname Stanley Ann Dunham. Anthropologist in Hawaii and Indonesia.\\\\nMadelyn Lee Payne Dunham\\\\nBarack Obama\\'s maternal grandmother, born in 1922 and died on November 2, 2008.[24] She was a bank vice president in Hawaii. Obama said that when he was a child, his grandmother \\\\\"read me the opening lines of the Declaration of Independence and told me about the men and women who marched for equality because they believed those words put to paper two centuries ago should mean something.\\\\\"[9]\\\\nStanley Armour Dunham\\\\nGrandfather of Barack Obama, born 1918, died 1992. World War II U.S. Army sergeant, furniture salesman in Hawaii.\\\\nCharles T. Payne\\\\nGreat-uncle of Barack Obama, younger brother of Madelyn Dunham, born 1925. Served during World War II in the U.S. Army 89th Infantry Division.[25] Obama has often described Payne\\'s role in liberating Ohrdruf forced labor camp.[26] There was brief media attention when Obama mistakenly identified the camp as Auschwitz during the campaign.[27] Payne appeared in the visitor\\'s gallery at the Democratic National Convention in Denver, Colorado, when his great-nephew was nominated for President.[28] He was the assistant director of the University of Chicago\\'s Library.[26]\\\\nMaya Soetoro-Ng\\\\nHalf-sister of Barack Obama, born August 15, 1970, in Jakarta, Indonesia.[29] She is married to Konrad Ng, with whom she has a daughter, Suhaila. Maya Soetoro-Ng is a teacher in Hawaii.\\\\nKonrad Ng\\\\nBrother-in-law of Barack Obama, born 1974. A Canadian whose parents are Malaysian Chinese immigrants, he is an assistant professor at the University of Hawaii\\'s Academy of Creative Media.[30] His parents are from Kudat and Sandakan, two small towns in Sabah, Malaysia, and he was born and raised in Burlington, Ontario.[31] He married Maya Soetoro-Ng at the end of 2003 in Hawaii.[32] They have one daughter, Suhaila.[33][34][35] Konrad Ng is now a US citizen.[36]\\\\nLolo Soetoro\\\\nStepfather of Barack Obama, born in Indonesia 1936, died 1987.\\\\nExtended family - paternal relations\\\\nThe Obamas are members of the Luo, Kenya\\'s third-largest ethnic group, which is part of a larger family of ethnic groups, collectively also known as Luo. This group belongs to the Eastern Sudanic branch of the Nilo-Saharan phylum. The Obama family is largely concentrated in the western province of Nyanza.\\\\nFront row (left to right): Auma Obama (Barack\\'s half-sister), Kezia Obama (Barack\\'s step-mother), Sarah Hussein Onyango Obama (third wife of Barack\\'s paternal grandfather), Zeituni Onyango (Barack\\'s aunt)\\\\nBack row (left to right): Said Obama (Barack\\'s uncle), Barack Obama, Abongo [Roy] Obama (Barack\\'s half-brother), unidentified woman, Bernard Obama (Barack\\'s half-brother), Abo Obama (Barack\\'s half-brother).\\\\nBarack Obama, Sr.\\\\nBarack Obama\\'s father, (1936ýýý1982). Government economist in Kenya. In addition to President Obama, Barack Obama Sr. fathered six other sons and a daughter.[37]\\\\nHussein Onyango Obama\\\\nBarack Obama\\'s paternal grandfather (c. 1895ýýý1979);[38] he worked as a mission cook. He joined the British Army during World War I. (One source gives 1870ýýý1975 as his dates of birth and death based on his tombstone reading \\\\\"Mzee Hussein Onyango Obama\\\\\" in his home village.[39] The term \\\\\"mzee\\\\\" is a Kenyan honorific meaning \\\\\"old man\\\\\" or \\\\\"elder.\\\\\") According to his third wife, Sarah, he originally converted to Catholicism, but took the name Hussein when he later converted to Islam; she said he passed the name, not the religion, on to his children.[40]\\\\nHabiba Akumu Obama\\\\nBarack Obama\\'s paternal grandmother, and the second wife of Hussein Onyango Obama. A photograph of her holding her son, Barack Sr, on her lap is on the cover of her grandson\\'s memoirs titled Dreams from my Father.[41]\\\\nSarah Obama\\\\nThird wife of Obama\\'s paternal grandfather, born 1922.[39] Also known, through the addition of her late husband\\'s name, as Sarah Onyango Obama,[42] and sometimes referred to as Sarah Ogwel, Sarah Hussein Obama or Sarah Anyango Obama,[43] she lives in Nyangýýýoma Kogelo village, 30 miles west of western Kenya\\'s main town, Kisumu, on the edge of Lake Victoria.[44][45]\\\\nAlthough not a blood relation, Barack Obama calls her \\\\\"Granny Sarah\\\\\".[43][46] Sarah, who speaks Luo and only a few words of English, communicates with President Obama through an interpreter.\\\\nOn July 4, 2008, she attended the United States Independence Day celebrations in Nairobi, hosted by Michael Ranneberger, the US ambassador in Kenya.[47]\\\\nDuring the campaign she protested attempts to portray Obama as a foreigner to the United States or a Muslim, saying that while Obama\\'s grandfather had been a Muslim, \\\\\"In the world of today, children have different religions from their parents.\\\\\"[40] Sarah Obama herself is \\\\\"a strong believer of the Islamic faith,ýýý in her words.[48]\\\\nKezia Obama\\\\nFirst wife of Barack Obama\\'s father, born c. 1940. She is Barack Obama Sr.\\'s first wife, whom he married in Kenya before studying abroad in the United States. Also known as Kezia Grace Obama.[49][50] She currently lives in Bracknell, Berkshire, England.[51][52] Her sister, Jane, is the \\'Auntie Jane\\' mentioned at the very start of Dreams from My Father when she telephoned President Obama to inform him that his father had been killed in a car accident.[53]\\\\nMalik Obama\\\\nBarack Obama\\'s half-brother, also known as Abongo or Roy, born c. March, 1958,[51] son of Barack Obama, Sr. with his first wife, Kezia.[54] Malik Obama was born and raised in Nairobi, Kenya.[55] He earned a degree in accounting from the University of Nairobi.[56] He met his half-brother for the first time in 1985[55] when Barack flew from Chicago to Washington, D.C. to visit him.[57] Malik and his half-brother Barack were best men at each other\\'s weddings.[55] Barack Obama brought his wife Michelle to Kenya three years later, and they met with Malik again while Barack was introducing Michelle to many other new relatives.[58]\\\\nAlthough much of the Obama family has dispersed throughout Kenya and overseas, most, including Malik Obama, still considered their rural village on the shores of Lake Victoria to be their true home, and feel that those who have left the village have become culturally \\\\\"lost\\\\\".[59] A frequent visitor to the United States,[58] and consultant in Washington, D.C. for several months per year,[55] he nevertheless settled in the Obamas\\' ancestral home, Nyangýýýoma Kogelo, a village of several hundred people that he prefers to the city for its slow pace.[55] He runs a small electronics shop a half hour drive outside of town.[55]\\\\nDuring his brother\\'s presidential campaign, Malik Obama was a spokesman for the extended Obama family in Kenya, dealing with safety and privacy concerns arising from increased attention from the press.[60]\\\\nAbo Obama\\\\nBarack Obama\\'s half-brother, born 1968. International telephone store manager in Kenya.\\\\nAuma Obama\\\\nBarack Obama\\'s half-sister, born c. 1960.[61] As of July 2008, development worker in Kenya.[62] She studied German at the University of Heidelberg from 1981 to 1987. After her graduation in Heidelberg she went on for graduate studies at the University of Bayreuth, which awarded her a PhD in 1996. Her dissertation was about the conception of labor in Germany and its literary reflections.[62] Auma Obama lives in London, and in 1996 married an Englishman, Ian Manners. They have a daughter named Akinyi (b. 1997).[62][verification needed]\\\\nBernard Obama\\\\nBarack Obama\\'s half-brother, born 1970, son of Barack Obama, Sr. and his first wife, Kezia. He had been an auto parts supplier in Nairobi, Kenya, and has one child. Bernard converted to Islam as an adult and has said: \\\\\"Iýýým a Muslim, I donýýýt deny it. My father was raised a Muslim. But itýýýs not an issue. I donýýýt know what all the hullabaloo is about.\\\\\"[63] He currently resides in Bracknell, England, with his mother Kezia.[63]\\\\nRuth Ndesandjo\\\\nBorn Ruth Nidesand, in US c. 1940s, Barack Obama Sr.\\'s third wife and a private kindergarten director in Kenya.[64] Ruth\\'s two sons with Barack Obama, Sr., are Mark and David Ndesandjo; her third son, Joseph Ndesandjo, was born c. 1980 from a subsequent marriage to a Tanzanian.[65][66]\\\\nMark Ndesandjo\\\\nBarack Obama\\'s half-brother, son of Ruth Nidesand and Barack Obama Sr.[67] He runs an Internet company called WorldNexus that advises Chinese corporations how best to reach international customers.[68] Mark graduated from Brown University, studied physics at Stanford University, received an MBA from Emory University, and has lived in Shenzhen, China, since 2002 and is married to a Chinese woman.[69] He is also an accomplished pianist.[70]\\\\nDavid Ndesandjo\\\\nDavid Ndesandjo\\\\nBarack Obama\\'s half-brother (also known as David Opiyo Obama), son of Ruth Nidesand and Barack Obama Sr. Killed in a motorcycle accident.[71]\\\\nGeorge Hussein Onyango Obama\\\\nYoungest half-brother of Barack Obama, born c.1982, son of Barack Obama Sr.[72] and Jael (now a resident of Atlanta, Georgia).[73][74] George was six months old when his father died in an automobile accident, after which he was raised in Nairobi by his mother and a French step-father. He later lived in South Korea for two years while his mother resided there for business reasons.[73] Returning to Kenya, George Obama \\\\\"slept rough for several years,\\\\\" until his aunt gave him a six-by-eight foot corrugated metal shack in the Nairobi, Kenya, slum of Huruma Flats.[73] As of August 2008, Obama was studying to become a mechanic.[73] George received little attention until being featured in an article in the Italian language edition of Vanity Fair in August 2008, which portrayed him as living in poverty, shame, and obscurity.[75] The article quoted Obama as saying that he lived \\\\\"on less than a dollar a month\\\\\" and stated that he \\\\\"does not mention his famous half-brother in conversation\\\\\" out of shame at his poverty.[76] In later interviews George contradicted this picture. In an interview with The Times, Obama \\\\\"said that he was furious at subsequent reports that he had been abandoned by the Obama family and that he was filled with shame about living in a slum.\\\\\"[74] He told The Times, \\\\\"Life in Huruma is good.\\\\\" Obama said that he expects no favors, that he was supported by relatives, and that reports he lived on a dollar a month were \\\\\"all lies by people who donýýýt want my brother to win.ýýý[74] He told The Telegraph that he was inspired by his half-brother.[73] According to Time, George \\\\\"has repeatedly denied...that he feels abandoned by Obama.\\\\\"[77] CNN quoted him as saying, \\\\\"I was brought up well. I live well even now. The magazines, they have exaggerated everything... I think I kind of like it here. There are some challenges, but maybe it is just like where you come from, there are the same challenges.\\\\\"[75] George\\'s reported poverty was seized on by conservative critics of Barack Obama. Columnist Dinesh D\\'Souza solicited donations for George Obama from his readers,[78] while Jerome Corsi planned to give him a $1,000 check during a trip to Kenya (Corsi was expelled from the country by immigration authorities).[77]\\\\nOmar Obama\\\\nHalf-uncle of Barack Obama,[79] born on June 3, 1944 in Nyangýýýoma Kogelo. Oldest son of Onyango and Sarah Obama, resides in Boston, Massachusetts.[citation needed]\\\\nZeituni Onyango\\\\nHalf-aunt of Barack Obama,[80] born May 29, 1952, in Kenya,[81] Onyango is referred to as \\\\\"Aunti Zeituni\\\\\" in President Obama\\'s memoir, Dreams from My Father.[82]\\\\nYusuf Obama\\\\nHalf-uncle of Barack Obama,[79] born c. 1950s in Nyangýýýoma Kogelo; son of Onyango and Sarah Obama.[citation needed]\\\\nSaid Obama\\\\nHalf-uncle of Barack Obama,[79][83] born c. 1950s in Nyangýýýoma Kogelo; son of Onyango and Sarah Obama.[citation needed]\\\\nMichelle Robinson Obama\\'s extended family\\\\nFraser Robinson, Sr. (1884ýýý1936) of South Carolina, shown in an old photo along with his wife, Rosella Cohen Robinson, in its background\\\\nBarack Obama has called his wife Michelle \\\\\"the most quintessentially American woman I know.\\\\\"[3] Her family is of African American heritage, descendents of Africans of the American Colonial Era.[3] Michelle Obama\\'s family history traces back from slavery to Reconstruction to the Great Migration North. Some of Michelle\\'s relatives still reside in South Carolina.\\\\nMichelle\\'s earliest known relative is her great-great grandfather Jim Robinson, born in the 1850s, who was an American slave on the Friendfield plantation in South Carolina. The family believes that after the Civil War he remained a Friendfield plantation sharecropper for the rest of his life and that he was buried there in an unmarked grave.[3]\\\\nJim had two sons, Gabriel and Fraser, Michelle Obama\\'s great-grandfather. Fraser had an arm amputated as a result of a boyhood injury. He worked as a shoemaker, newspaper salesman and in a lumber mill and was married to Rosella Cohen.[3] Carrie Nelson, Gabriel Robinson\\'s daughter, now 80, is the oldest living Robinson and the keeper of family lore.[3]\\\\nAt least three of Michelle Obama\\'s great-uncles served in the military of the United States. One aunt moved to Princeton, New Jersey, where she worked as a maid, and cooked Southern-style meals for Michelle and her brother, Craig, when they were students at Princeton University.\\\\nCraig Robinson\\\\nMichelle Obama\\'s brother, born 1962. He is currently head coach of men\\'s basketball at Oregon State University.[84]\\\\nFraser Robinson III\\\\nMichelle Obama\\'s father, born 1935, died 1991, married Michelle\\'s mother, Marian Shields, in 1960.[85][19] Robinson was a pump worker at the City of Chicago water plant.[3]\\\\nFraser Robinson, Jr.\\\\nMichelle Obama\\'s grandfather was born on August 24, 1912 in Georgetown, South Carolina, and died on November 9, 1996, aged 84. He was a good student and orator, but moved from South Carolina to Chicago to find better work than he could find at home, eventually becoming a worker for the United States Postal Service. He was married to LaVaughn Johnson. When he retired, they moved back to South Carolina.[3]\\\\nCapers C. Funnye Jr.\\\\nMichelle Obama\\'s first cousin once removed: Funnyeýýýs mother, Verdelle Robinson Funnye (born Verdelle Robinson; August 22, 1930 ýýý April 16, 2000) and Michelle Obamaýýýs paternal grandfather, Fraser Robinson Jr., were siblings. One of America\\'s most prominent African American Jews, known for acting as a bridge between mainstream Jewry and African Americans.[86]\\\\nGenealogical charts\\\\nObama ancestry\\\\n16. Opiyo\\\\n8. Obama\\\\n4. Hussein Onyango Obama\\\\n9. Nyaoke\\\\n2. Barack Hussein Obama, Sr.\\\\n5. Habiba Akumu\\\\n1. Barack Hussein Obama II\\\\n24. Jacob William Dunham\\\\n12. Ralph Waldo Emerson Dunham, Sr.\\\\n25. Mary Ann Kearney\\\\n6. Stanley Armour Dunham\\\\n26. Harry Ellington Armour\\\\n13. Ruth Lucille Armour\\\\n27. Gabriella Clark\\\\n3. Stanley Ann Dunham\\\\n28. Charles T. Payne\\\\n14. Rolla Charles Payne\\\\n29. Della L. Wolfley\\\\n7. Madelyn Lee Payne\\\\n30. Thomas Creekmore McCurry\\\\n15. Leona Belle McCurry\\\\n31. Margaret Belle Wright\\\\nFamily trees\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nBarack Obama\\\\nStanley\\\\nDunham\\\\n1918ýýý1992\\\\nMadelyn\\\\nPayne\\\\n1922ýýý2008\\\\nHabiba\\\\nAkumu\\\\nOnyango\\\\nObama\\\\nc.\\xa01895ýýý1979\\\\nSarah\\\\nOgwel\\\\n1922ýýý\\\\nLolo\\\\nSoetoro\\\\n1936ýýý1987\\\\nAnn\\\\nDunham\\\\n1942ýýý1995\\\\nBarack\\\\nObama, Sr. *\\\\n1936ýýý1982\\\\nKezia\\\\nGrace\\\\nc. 1940ýýý\\\\nOmar\\\\nObama\\\\n1944ýýý\\\\nZeituni\\\\nOnyango\\\\n1952ýýý\\\\nYusuf\\\\nObama\\\\nc. 1950sýýý\\\\nSaid\\\\nObama\\\\nc. 1950sýýý\\\\nKonrad\\\\nNg\\\\nc. 1974ýýý\\\\nMaya\\\\nSoetoro\\\\n1970ýýý\\\\nBarack\\\\nObama\\\\n1961ýýý\\\\nMichelle\\\\nRobinson\\\\n1964ýýý\\\\nM. Abongo\\\\nObama\\\\n1958ýýý\\\\nAuma\\\\nObama\\\\nc. 1960ýýý\\\\nAbo\\\\nObama\\\\n1968ýýý\\\\nBernard\\\\nObama\\\\n1970ýýý\\\\nSuhaila\\\\nNg\\\\nc. 2005ýýý\\\\nMalia Ann\\\\nObama\\\\n1998ýýý\\\\nSasha\\\\nObama\\\\n2001ýýý\\\\n* Barack\\\\nObama, Sr.\\'s\\\\nadditional\\\\nRuth\\\\nNidesandjo\\\\nc. 1940sýýý\\\\nJael\\\\nOtieno\\\\nrelationships:\\\\nMark\\\\nNdesandjo\\\\nDavid\\\\nNdesandjo\\\\ndied\\xa0c.\\xa01987\\\\nGeorge\\\\nObama\\\\nc. 1982ýýý\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nMichelle Obama\\\\nFraser\\\\nRobinson, Jr.\\\\nýýý\\\\nSouth Carolina\\\\n(1912ýýý1996)\\\\nSawmill worker\\\\nLaVaughn D.\\\\nJohnson\\\\nýýý\\\\nIllinois\\\\n(1915ýýý2002)\\\\nCapers C.\\\\nFunnye, Jr.\\\\nýýý\\\\n(born c. 1952;\\\\nnephew of\\\\nFraser\\\\nRobinson, Jr.)\\\\nRabbi in Chicago\\\\nFraser\\\\nRobinson III\\\\nýýý\\\\nIllinois\\\\n(1935ýýý1991)\\\\nEnjoyed boxing\\\\nin youth;\\\\nwater plant\\\\npump operator\\\\nin Chicago\\\\nMarian\\\\nShields\\\\nýýý\\\\nIllinois\\\\n(born 1937)\\\\nSecretary at\\\\nSpiegel catalog\\\\nin Chicago;\\\\nU.S.\\'s First\\\\nGrandmother\\\\nCraig\\\\nRobinson\\\\nýýý\\\\nIllinois\\\\n(born 1962)\\\\nHead coach of\\\\nOregon State\\\\nBeavers men\\'s\\\\nbasketball\\\\nMichelle\\\\nRobinson\\\\nýýý\\\\nIllinois\\\\n(born 1964)\\\\nFirst Lady\\\\nof the United States\\\\nDistant relations\\\\nSee also: List of United States Presidents by genealogical relationship\\\\nAccording to genealogists, Barack Obama\\'s distant cousins include the multitude of descendants of his maternal ancestors from all along the early-American Atlantic seaboard as well as paternal, Kenyan relations belonging to the Luo tribe, many descending from a 17th century ancestor named Owiny.[87][88] For example, George W. Bush, the 43rd U.S. president, is the eleventh cousin of Barack Obama.[89] The New York Times science writer Nicholas Wade argues that with eleven generations leading back to their common progenitor, Samuel Hinckley, the relationship between the 43rd President and the 44th President is \\\\\"genetically meaningless\\\\\".[90]\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nSelected genealogical relationships\\\\nBill Hickok\\\\nAccording to Barack Obama\\'s family lore (and confirmed by the New England Historic Genealogical Society), the President and Hickok are sixth cousins, six-times removed.[91]\\\\nRobert Duvall\\\\nGoodnight helped inspire Pulitzer Prize-winning author Larry McMurtry to create a protagonist for his novel series Lonesome Dove: Woodrow Call. In Dove\\'s television novela, Woodrow Call\\'s partner is Gus McCrae, portrayed by Obama\\'s eighth cousin, twice removed, actor Robert Duvall.[92]\\\\nJames Madison\\\\nObama is also distantly related to U.S. Presidents James Madison, Lyndon Johnson, Gerald Ford, and U.S. Vice President Dick Cheney, British Prime Minister Sir Winston Churchill, U.S. Civil War General Robert E. Lee, and actor Brad Pitt.[93][94][95]\\\\nCharles Goodnight\\\\nAccording to Chicago Sun-Times reporter Scott Fornek, another Obama progenitor, Catherine Goodnight, was the grandniece of George Goodnight, who was in turn great-grandfather of famed cattleman Charles Goodnight.[92]\\\\nHarry S. Truman\\\\nActor Duvall is distant cousins with United States President Harry Truman, who\\'s likewise a fourth cousin, four times removed, of Obama\\'s.[92]\\\\nGeorgia O\\'Keeffe\\\\nNotable women Obama is related to include painter Georgia OýýýKeeffe.[93]\\\\nSee also\\\\nU.S. Presidential genealogical relationships\\\\nNew England Historic Genealogical Society\\\\nGary Boyd Roberts\\\\nReferences\\\\n^ Keck, Kristi (4 June 2008). \\\\\"Obama crosses historic milestone\\\\\". CNN. http://news.yahoo.com/s/ap/20081122/ap_on_go_pr_wh/obama_school_12. Retrieved on 2008-11-21.\\\\n^ a b Reitwiesner, William Addams. \\\\\"Ancestry of Barack Obama\\\\\". http://www.wargs.com/political/obama.html. Retrieved on 2008-10-09.\\\\n^ a b c d e f g h Murray, Shailagh (2 October 2008). \\\\\"A Family Tree Rooted In American Soil: Michelle Obama Learns About Her Slave Ancestors, Herself and Her Country\\\\\". The Washington Post: p.\\xa0C01. http://www.washingtonpost.com/wp-dyn/content/article/2008/10/01/AR2008100103169.html. Retrieved on 2008-10-10.\\\\n^ Sheridan, Michael (5 February 2007). \\\\\"Secrets of Obama Family Unlocked\\\\\". Muslim Observer. http://news.newamericamedia.org/news/view_article.html?article_id=ae5895fc29971b172938790be94ab107. Retrieved on 2008-11-21.\\\\n^ RTE news report March, 2007:Obamas Irish family links discovered by ancestry.co.uk\\\\n^ Noveck, Jocelyn (2008-11-07). \\\\\"Deseret News | Obama\\'s young, energetic family harks back to days of Camelot\\\\\". Secure.deseretnews.com. https://secure.deseretnews.com/article/1,5143,705261276,00.html. Retrieved on 2009-01-31.\\\\n^ Liza Mundy, Michelle: A Biography (Simon and Schuster, 2008), p. 129.\\\\n^ \\\\\"Sasha Obama\\\\\". Baltimore Sun. http://www.baltimoresun.com/topic/politics/sasha-obama-PECLB004381.topic. Retrieved on 2009-01-31.\\\\n^ a b Obama, Barack (13 January 2009). \\\\\"\\'What I Want for You--And Every Child in America\\'\\\\\". http://www.parade.com/news/2009/01/barack-obama-letter-to-my-daughters.html.\\\\n^ Sobieraj Westfall, Sandra (23 June 2008). \\\\\"Barack Obama Gives Daughter $1 Allowance a Week\\\\\". People Magazine. http://www.people.com/people/article/0,,20214569_1,00.html. Retrieved on 2008-11-21.\\\\n^ Lester, Will (July 23, 2008). \\\\\"Obama daughters keep hectic schedules of their own\\\\\". Associated Press. http://elections.apnews.com/apelect/db_6911/contentdetail.htm;jsessionid=8314A43012AB5FF1D0697247362D8752?contentguid=H95QubFb&full=true. Retrieved on 2008-08-04.\\\\n^ Hiro, Anne. \\\\\"Obama regrets letting \\\\\"Access Hollywood\\\\\" interview daughters. Won\\'t do it again. MSNBC\\'s Dan Abrams gets the story behind the story. - Lynn Sweet\\\\\". Blogs.suntimes.com. http://blogs.suntimes.com/sweet/2008/07/obama_regrets_letting_access_h.html. Retrieved on 2009-01-31.\\\\n^ Ahmed, Saeed (5 November 2008). \\\\\"Move over Barney, new dog moving into White House\\\\\". CNN. http://www.cnn.com/2008/LIVING/wayoflife/11/05/presidential.pets/index.html. Retrieved on 2008-11-21.\\\\n^ \\\\\"Obama: Getting a dog isn\\'t easy\\\\\". Associated Press. 7 November 2008. http://www.mercurynews.com/ci_10927292. Retrieved on 2008-11-21.\\\\n^ Janice Lloyd (2009-01-12). \\\\\"Obamas down to Labradoodle or Portuguese water dog\\\\\". USA Today. http://www.usatoday.com/news/washington/2009-01-11-obama-dog_N.htm. Retrieved on 2009-01-28.\\\\n^ Swarns, Rachel (21 November 2008). \\\\\"And the Winner Is ýýý Sidwell Friends\\\\\". The New York Times. http://thecaucus.blogs.nytimes.com/2008/11/21/and-the-winner-is-sidwell-friends/. Retrieved on 2008-11-21.\\\\n^ Tolin, Lisa (2009-01-05). \\\\\"Obama girls start school with photographers in tow\\\\\". The Associated Press. http://www.google.com/hostednews/ap/article/ALeqM5g6mv_lkODQMmQdpyIEnr8Zpm5mogD95H8KA80. Retrieved on 2009-01-06.\\\\n^ Taylor Marsh (2008-08-25). \\\\\"Political Analysis, National Security and Breaking News\\\\\". Taylor Marsh. http://www.taylormarsh.com/archives_view.php?id=28286. Retrieved on 2009-01-31.\\\\n^ a b Lia LoBello (2008-01-02). \\\\\"First Families: Radar introduces you to the next president\\'s relatives\\\\\". Radar Online. http://www.radaronline.com/features/2008/07/john_mccain_barack_obama_michelle_cindy_dunham_roberta_wrigh_04.php. Retrieved on 2009-01-28.\\\\n^ Rachel L. Swarns (2009-01-09). \\\\\"Obamaýýýs Mother-in-Law to Move Into the White House\\\\\". The New York Times. http://thecaucus.blogs.nytimes.com/2009/01/09/obamas-mother-in-law-to-move-into-the-white-house/?hp. Retrieved on 2009-01-09.\\\\n^ a b \\\\\"Will Obama mum-in-law make it a family affair in the White House?\\\\\". Agence France Presse. 2008-11-22. http://www.google.com/hostednews/afp/article/ALeqM5gN_i2jrCVkJQgfMbSDRRrNk8U4Sw. Retrieved on 2009-01-09.\\\\n^ Philip Sherwell (2008-2008-11-09). \\\\\"Michelle Obama persuades First Granny to join new White House team\\\\\". The Telegraph (UK). http://www.telegraph.co.uk/news/3407525/Michelle-Obama-persuades-First-Granny-to-join-new-White-House-team.html. Retrieved on 2009-01-09.\\\\n^ \\\\\"ýýýTootýýý: Obama grandmother a force that shaped him\\\\\". via Associated Press. 2008-08-25. http://www.thekansan.com/news/x1311851415/-Toot-Obama-grandmother-a-force-that-shaped-him. Retrieved on 2008-08-29.\\\\n^ \\\\\"CNN: \\\\\"Obama\\'s grandmother dies after battle with cancer\\\\\"\\\\\". http://www.cnn.com/2008/POLITICS/11/03/obama.grandma/index.html. Retrieved on 2008-11-04.\\\\n^ The 89th Infantry Division, United States Holocaust Memorial Museum\\\\n^ a b Obama\\'s great-uncle recalls liberating Nazi camp, Boston.com, July 22, 2008\\\\n^ Major Garrett (2008-05-27). \\\\\"Obama Campaign Scrambles to Correct the Record on Uncle\\'s War Service\\\\\". FOXNews.com. http://elections.foxnews.com/2008/05/27/recollection-of-obama-familys-service-missing-key-details. Retrieved on 2009-01-31.\\\\n^ \\\\\"Democrats salute Obamaýýýs great uncle\\\\\". Jewish Telegraphic Agency. August 28, 2008. http://jta.org/news/article/2008/08/28/110123/obamapayne. Retrieved on 31 January 2009.\\\\n^ Obama Family Tree dgmweb.net\\\\n^ Chicago Sun Times article with her picture\\\\n^ Obama has links to Malaysia\\\\n^ Nolan, Daniel (2008-06-11). \\\\\"Relative: Obama\\'s got \\'a good handle on Canada\\'\\\\\". The Hamilton Spectator. http://www.thespec.com/burlingtonlife/article/384475. Retrieved on 2008-07-03.\\\\n^ Nolan, Daniel (June 11, 2008). \\\\\"Obama\\'s Burlington connection\\\\\". The Hamilton Spectator. http://www.thespec.com/article/384307. Retrieved on 2008-06-21.\\\\n^ Misner, Jason (2008-06-20). \\\\\"Barack Obama was here\\\\\". Burlington Post. http://www.burlingtonpost.com/printarticle/186215. Retrieved on 2008-07-03.\\\\n^ Fornek, Scott (2007-09-09). \\\\\"\\'He helped me find my voice\\'\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545473,BSX-News-wotreehh09.article.\\\\n^ Cooper, Tom (2009-01-20). \\\\\"Keep watch for Obama\\\\\". TheSpec.com. http://www.thespec.com/Opinions/article/499161. Retrieved on 2009-01-28.\\\\n^ Ancestry of Barack Obama\\\\n^ Dreams from My Father, p. 376\\\\n^ a b Kenya: Special Report: Sleepy Little Village Where Obama Traces His Own Roots (Page 2 of 2)\\\\n^ a b \\\\\"Obama\\'s grandma slams \\'untruths\\'\\\\\". Associated Press. 2008-03-05. http://www.usatoday.com/news/world/2008-03-05-obama-kin_N.htm.\\xa0 See also this correction.\\\\n^ \\\\\"Q&A ON THE NEWS\\\\\". Atlanta Journal-Constitution. 2009-02-25. http://www.ajc.com/services/content/metro/stories/2009/02/25/questi0225.html. Retrieved on 2009-02-27.\\\\n^ In Kenya, Barack Obamaýýýs family prays for end to conflict - Times Online\\\\n^ a b Crilly, Rob (February 27, 2008). \\\\\"Dreams from Obama\\'s Grandmother\\\\\". Time Magazine, Inc.. http://www.time.com/time/world/article/0,8599,1717590,00.html?xid=rss-topstories. Retrieved on 2008-07-03.\\\\n^ Pflanz, Mike (2008-01-11). \\\\\"Barack Obama\\'s Kenyan relatives keep faith\\\\\". Daily Telegraph. http://www.telegraph.co.uk/news/main.jhtml?xml=/news/2008/01/09/wuspols1009.xml.\\\\n^ Fornek, Scott (2007-09-09). \\\\\"Sarah Obama - \\'Sparkling, laughing eyes\\'\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545459,BSX-News-wotreeu09.article.\\\\n^ \\\\\"Barack Obama in Kenya\\\\\". CNN. http://www.youtube.com/watch?v=Ikg6gj71U9k.\\\\n^ Daily Nation, July 8, 2008: Obama granny\\'s day out with envoys and top politicians\\\\n^ \\\\\"A Candidate, His Minister and the Search for Faith\\\\\". New York Times. 2007-04-30. http://www.nytimes.com/2007/04/30/us/politics/30obama.html?_r=2&pagewanted=all&oref=slogin&oref=slogin.\\\\n^ \\\\\"Kenya: All Obama kin to spend voting day in Kogelo\\\\\". afrika.no. 2008-10-27. http://www.afrika.no/Detailed/17321.html. Retrieved on 2009-01-31.\\\\n^ Cohen, Roger (2008-03-06). \\\\\"The Obamas of the World - New York Times\\\\\". Nytimes.com. http://www.nytimes.com/2008/03/06/opinion/06cohen.html. Retrieved on 2009-01-31.\\\\n^ a b Sanderson, Elizabeth (2008-01-06). \\\\\"Barack Obama\\'s stepmother living in Bracknell reveals the close bond with him ... and his mother\\\\\". Daily Mail. http://www.dailymail.co.uk/news/article-506338/Barack-Obamas-stepmother-living-Bracknell-reveals-close-bond---mother.html.\\\\n^ Lindsay, Anna (2009-01-20). \\\\\"Barack\\'s bingo-loving stepmother\\\\\". BBC News. http://news.bbc.co.uk/1/hi/england/berkshire/7834368.stm.\\\\n^ http://www.newvision.co.ug/D/8/26/666733\\\\n^ jpt (2008-06-18). \\\\\"From the Fact Check Desk: What Did Obama\\'s Half-Brother Say About Obama\\'s Background\\\\\". ABC News. http://blogs.abcnews.com/politicalpunch/2008/06/from-the-fact-c.html.\\\\n^ a b c d e f Maliti, Tom (2004-10-26). \\\\\"Obama\\'s Brother Chooses Life in Slow Lane\\\\\". The Associated Press. http://www.msnbc.msn.com/id/6333496.\\\\n^ Obama, Dreams from my Father, 2004, p. 265.\\\\n^ Obama, Dreams from my Father, 2004, p. 262.\\\\n^ a b *Oywa, John (2004-08-15). \\\\\"Sleepy Little Village Where Obama Traces His Own Roots\\\\\". The Daily Nation. http://allafrica.com/stories/200408160533.html.\\\\n^ Philip Ochieng (2004-11-01). \\\\\"From Home Squared to the US Senate: How Barack Obama Was Lost and Found\\\\\". The East African. http://www.nationmedia.com/EastAfrican/01112004/Features/PA2-11.html. Retrieved on 2008-03-23.\\\\n^ Warah, Rasna (2008-06-09). \\\\\"We cannot lay claims on Obama; he\\'s not one of us - Obama in this world\\\\\". Daily Nation. http://www.nationmedia.com. Retrieved on 2008-07-10.\\\\n^ Scott Fornek (2007-09-09). \\\\\"AUMA OBAMA: \\'Her restlessness, her independence\\'\\\\\". Chicago Sun Times. http://www.suntimes.com/news/politics/obama/familytree/545465,BSX-News-wotreew09.article. Retrieved on 2008-03-23.\\\\n^ a b c Gathmann, Florian; Gregor Peter Schmitz, Jochen Schýýnmann (July 24, 2008). \\\\\"Studentin in der Bundesrepublik: Wie Auma Obama mit Deutschland haderte\\\\\" (in German). Spiegel Online. http://www.spiegel.de/politik/ausland/0,1518,567286,00.html. Retrieved on 2008-07-24.\\\\n^ a b Harvey, Oliver (07-26 2008). \\\\\"Obama\\'s brother is in Bracknell\\\\\". The Sun. http://www.thesun.co.uk/sol/homepage/news/the_real_american_idol/article1472877.ece. Retrieved on 2008-10-06.\\\\n^ \\\\\"Madari Kindergarten\\\\\". http://www.madarikindergarten.com/.\\\\n^ \\\\\"Welcome To MedWeek San Antonio 2007\\\\\". Medweeksa.org. http://www.medweeksa.org/awardwinners/techfirm.htm. Retrieved on 2009-01-31.\\\\n^ \\\\\"PIDE - Partners for International Development & Education Inc\\\\\". Pideafrica.org. http://pideafrica.org/aboutus.htm. Retrieved on 2009-01-31.\\\\n^ Barack Obamaýýýs brother pushes Chinese imports on US - Times Online\\\\n^ Obama half-brother runs Internet company in China\\\\n^ Roger Cohen (2008-03-17). \\\\\"Obama\\'s Brother in China\\\\\". The New York Times. http://www.nytimes.com/2008/03/17/opinion/29cohen.html. Retrieved on 2008-03-23.\\\\n^ \\\\\"Youku Buzz (daily)\\xa0ýý Blog Archive\\xa0ýý Barack Obamaýýýs Half-Brother in Concert\\\\\". Buzz.youku.com. 2009-01-18. http://buzz.youku.com/2009/01/18/barack-obamas-half-brother-in-concert/. Retrieved on 2009-01-31.\\\\n^ jaketapper (2008-07-28). \\\\\"Political Punch: Barack Obama\\'s Branch-y Family Tree\\\\\". Blogs.abcnews.com. http://blogs.abcnews.com/politicalpunch/2008/07/barack-obamas-1.html. Retrieved on 2009-01-31.\\\\n^ Fornek, Scott (September 9, 2007). \\\\\"HALF-BROTHER GEORGE: \\'I would be there for him\\'\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545447,BSX-News-wotreecc09.stng. Retrieved on 2008-08-04.\\\\n^ a b c d e Pflanz, Mike (August 21, 2008). \\\\\"Barack Obama is my inspiration, says lost brother\\\\\". The Daily Telegraph. http://www.telegraph.co.uk/news/newstopics/uselection2008/barackobama/2595688/Barack-Obama-is-my-inspiration-says-lost-brother.html. Retrieved on 2008-08-23.\\\\n^ a b c Crilly, Rob (August 22, 2008). \\\\\"Life is good in my Nairobi slum, says Barack Obama\\'s younger brother\\\\\". The Times. http://www.timesonline.co.uk/tol/news/world/us_and_americas/us_elections/article4583353.ece. Retrieved on 2008-08-23.\\\\n^ a b McKenzie, David (2008-08-23). \\\\\"Behind the Scenes: Meet George Obama\\\\\". CNN. http://www.cnn.com/2008/POLITICS/08/22/bts.obama.brother/. Retrieved on 2008-10-26.\\\\n^ Pisa, Nick (August 20, 2008). \\\\\"Barack Obama\\'s \\'lost\\' brother found in Kenya\\\\\". Daily Telegraph. http://www.telegraph.co.uk/news/newstopics/uselection2008/barackobama/2590614/Barack-Obamas-lost-brother-found-in-Kenya.html. Retrieved on 2008-08-20.\\\\n^ a b Wadhams, Nick (2008-10-07). \\\\\"Corsi in Kenya: Obama\\'s Nation Boots Obama Nation Author\\\\\". TIME. http://www.time.com/time/world/article/0,8599,1847965,00.html?imw=Y. Retrieved on 2009-01-31.\\\\n^ by Dinesh D\\'Souza. \\\\\"Dinesh D\\'Souza\\xa0: George Obama, Start Packing\\\\\". Townhall.com. http://townhall.com/columnists/DineshDSouza/2008/09/22/george_obama,_start_packing. Retrieved on 2009-01-31.\\\\n^ a b c \\\\\"The Obama Family Tree\\\\\" (PDF). Chicago Sun-Times. September 9, 2007. http://www.suntimes.com/images/cds/MP3/obamatree.pdf. Retrieved on 2008-11-23.\\\\n^ First read, MSNBC\\\\n^ \\\\\"Barack Obama\\'s aunt found living in rundown public housing estate | The Australian\\\\\". Theaustralian.news.com.au. 2008-10-31. http://www.theaustralian.news.com.au/story/0,25197,24578185-5017121,00.html. Retrieved on 2009-01-31.\\\\n^ Boston Housing Authority ýýýflabbergasteredýýý Barack Obamaýýýs aunt living in Southie\\\\n^ Kilner, Derek (2008-11-05). \\\\\"Kenya Celebrates President Obama as Native Son\\\\\". Voice Of America. http://www.voanews.com/english/archive/2008-11/2008-11-05-voa45.cfm. Retrieved on 2008-12-24.\\\\n^ \\\\\"Oregon State University Beavers: Craig Robinson bio\\\\\". http://www.osubeavers.com/ViewArticle.dbml?SPSID=106239&SPID=1954&DB_OEM_ID=4700&ATCLID=1436883&Q_SEASON=2008. Retrieved on 2008-08-21.\\\\n^ \\\\\"RootsWeb\\'s WorldConnect Project: Dowling Family Genealogy\\\\\". Wc.rootsweb.ancestry.com. http://wc.rootsweb.ancestry.com/cgi-bin/igm.cgi?op=GET&db=dowfam3&id=I105855. Retrieved on 2009-01-31.\\\\n^ Weiss, Anthony (September 2, 2008). \\\\\"Michelle Obama Has a Rabbi in Her Family\\\\\". The Forward. http://www.forward.com/articles/14121/. Retrieved on 2008-10-09.\\\\n^ Gary Boyd Roberts. \\\\\"Notes on the Ancestry of Senator Barack Hussein Obama, Jr\\\\\". New England Historic Genealogical Society. http://www.newenglandancestors.org/research/services/articles_ancestry_barack_obama.asp. Retrieved on 2009-01-31.\\\\n^ Scott Fornek (2007-09-09). \\\\\"Obama Family Tree\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545463,BSX-News-wotreer09.stng. Retrieved on 2009-01-31.\\\\n^ \\\\\"Obama-Bush (family tree)\\\\\" (PDF). New England Historic Genealogical Society. http://www.newenglandancestors.org/pdfs/obama_bush.pdf. Retrieved on 2009-01-31.\\\\n^ Wade, Nicholas (2007-10-21). \\\\\"Barack Obama - Dick Cheney - Ancestry and Genealogy - Washington - New York Times\\\\\". Nytimes.com. http://www.nytimes.com/2007/10/21/weekinreview/21basic.html. Retrieved on 2009-01-31.\\\\n^ Eastman, Dick (2008-07-30). \\\\\"Barack Obama is Related to Wild Bill Hickok\\\\\". Blog.eogn.com. http://blog.eogn.com/eastmans_online_genealogy/2008/07/barack-obama-is.html. Retrieved on 2009-01-31.\\\\n^ a b c Scott Fornek (2007-09-09). \\\\\"Obama Family Tree\\\\\". Suntimes.com. http://www.suntimes.com/news/politics/obama/familytree/545441,BSX-News-wotreec09.stng. Retrieved on 2009-01-31.\\\\n^ a b \\\\\"Notes on the Ancestry of Senator Barack Hussein Obama, Jr\\\\\". New England Historic Genealogical Society. 2002-08-14. http://www.newenglandancestors.org/research/services/articles_ancestry_barack_obama.asp. Retrieved on 2009-01-31.\\\\n^ \\\\\"Obama, Clinton and McCain have some famous relations\\\\\". HeraldNet - AP. 2008-03-26. http://www.heraldnet.com/article/20080326/NEWS02/151343661. Retrieved on 2009-01-31.\\\\n^ \\\\\"Barack Obama and Joe Biden: The Change We Need\\\\\". My.barackobama.com. 2008-07-31. http://my.barackobama.com/page/community/post/williambrehm/gG5TVR. Retrieved on 2009-01-31.\\\\nExternal links\\\\nBarack Obama\\'s Family Tree - Photo Essays - TIME\\\\n\\\\\"Though Obama Had to Leave to Find Himself, It Is Hawaii That Made His Rise Possible,\\\\\" by David Maraniss\\\\nBarack Obama\\'s Branch-y Family Tree by Jake Tapper\\\\n\\\\\"Obama Family Tree\\\\\" series, by Scott Fornek\\\\n\\\\\"Six Degrees of Barack Obama\\\\\"\\\\n\\\\\"Notes on the Ancestry of Senator Barack Hussein Obama, Jr.,\\\\\" by Gary Boyd Roberts\\\\n\\\\\"Obama, Clinton and McCain have some famous relations,\\\\\" by The Associated Press\\\\n\\\\\"Obama\\'s Patriotic Family Tree,\\\\\" by Bill Brehm\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nBarack Obama\\\\nPresidency\\\\nTransition\\xa0ýý Inauguration\\xa0ýý Timeline\\xa0ýý Cabinet\\xa0ýý Judiciary\\xa0ýý Foreign policy\\xa0ýý First 100 days\\\\nEarly life and\\\\npolitical career\\\\nPublic image\\xa0ýý Illinois Senate career\\xa0ýý 2004 Democratic National Convention\\xa0ýý U.S. Senate election in Illinois\\xa0ýý U.S. Senate career\\xa0ýý Presidential primary campaign\\xa0ýý ObamaýýýBiden 2008\\xa0ýý Electoral history\\xa0ýý Political positions\\\\nBooks\\\\nDreams from My Father\\xa0ýý The Audacity of Hope\\\\nSpeeches\\\\nThe Audacity of Hope\\xa0ýý A More Perfect Union\\xa0ýý Change Has Come to America\\xa0ýý 2009 speech to joint session of Congress\\\\nFamily\\\\nMichelle Obama\\xa0ýý Barack Obama, Sr.\\xa0ýý Ann Dunham\\xa0ýý Lolo Soetoro (stepfather)\\xa0ýý Maya Soetoro-Ng (half-sister)\\xa0ýý Marian Robinson (mother-in-law)\\xa0ýý Stanley Armour Dunham (grandfather)\\xa0ýý Madelyn Dunham (grandmother)\\xa0ýý Extended family\\xa0ýý Family tree\\\\nRetrieved from \\\\\"http://en.wikipedia.org/wiki/Family_of_Barack_Obama#Sarah_Obama\\\\\"\\\\nCategories: Obama family | African American history | African American families | Luo Kenyans | People of mixed Black African-European ethnicity | African Americans | Asian Americans | Dutch Americans | English Americans | French Americans | German-Americans | Irish-Americans | Indonesian Americans | Kenyan-Americans | Scottish-Americans | Chinese Canadians | People of mixed Asian-European ethnicity | American families | First Families of the United States | Family treesHidden categories: Wikipedia semi-protected pages | Wikipedia indefinitely move-protected pages | All pages needing cleanup | Wikipedia articles needing factual verification since October 2008 | All pages needing factual verification | All articles with unsourced statements | Articles with unsourced statements since November 2008\\\\nViews\\\\nArticle\\\\nDiscussion\\\\nView source\\\\nHistory\\\\nPersonal tools\\\\nLog in / create account\\\\nNavigation\\\\nMain page\\\\nContents\\\\nFeatured content\\\\nCurrent events\\\\nRandom article\\\\nSearch\\\\nInteraction\\\\nAbout Wikipedia\\\\nCommunity portal\\\\nRecent changes\\\\nContact Wikipedia\\\\nDonate to Wikipedia\\\\nHelp\\\\nToolbox\\\\nWhat links here\\\\nRelated changes\\\\nUpload file\\\\nSpecial pages\\\\nPrintable version Permanent linkCite this page\\\\nLanguages\\\\nBahasa Indonesia\\\\nSvenska\\\\nýýýýýý\\\\nýýýýýý\\\\nThis page was last modified on 14 March 2009, at 05:50.\\\\nAll text is available under the terms of the GNU Free Documentation License. (See Copyrights for details.)\\\\nWikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a U.S. registered 501(c)(3) tax-deductible nonprofit charity.\\\\nPrivacy policy\\\\nAbout Wikipedia\\\\nDisclaimers', 'output': \"Where did Barack Obama's parents and grandparents come from?\", 'input_ids': [784, 15367, 11824, 908, 2588, 251, 30, 1661, 20653, 4534, 31, 7, 384, 892, 6, 379, 31960, 6, 1157, 5233, 7, 6, 1747, 11, 5128, 13, 3879, 6, 672, 5, 784, 9857, 5211, 11810, 134, 908, 784, 26472, 209, 908, 3712, 13, 20653, 4534, 3, 18, 16885, 6, 8, 339, 3, 4392, 3903, 24477, 2, 29, 371, 3690, 120, 13, 20653, 4534, 2, 29, 22674, 16885, 6, 8, 339, 3, 4392, 3903, 24477, 2, 29, 599, 1649, 22955, 45, 180, 3198, 9, 4534, 61, 2, 29, 683, 440, 102, 12, 10, 8789, 6, 960, 2, 29, 667, 115, 265, 9, 3712, 2, 29, 345, 15704, 20653, 4534, 6, 1485, 8571, 15275, 6, 11, 16649, 283, 5434, 11, 180, 3198, 9, 6772, 12, 8, 4374, 227, 112, 22145, 1115, 3049, 5, 16047, 2464, 6, 30, 8, 4653, 2245, 13, 8, 412, 5, 134, 5, 18808, 5, 6306, 536, 908, 2, 29, 254, 450, 5320, 1719, 2, 29, 518, 3198, 6029, 6, 5795, 2, 29, 1570, 14678, 2, 29, 345, 11706, 13, 5233, 2, 29, 5110, 23, 1054, 1323, 2, 29, 10358, 179, 724, 2, 29, 14851, 4365, 4534, 6, 15275, 4534, 6, 6206, 6393, 1483, 6, 20653, 4534, 6, 180, 52, 5, 6, 672, 5, 2, 29, 25772, 15, 26, 1791, 2, 29, 24372, 77, 739, 6, 6393, 1483, 6, 264, 15, 17, 127, 32, 6, 445, 122, 2, 29, 3713, 1108, 19, 294, 13, 3, 9, 939, 81, 2, 29, 14851, 4365, 4534, 2, 29, 21106, 9232, 3, 2, 7659, 7819, 3, 2, 412, 5, 134, 5, 7819, 2, 29, 8931, 155, 1950, 4655, 3, 2, 2575, 1023, 3, 2, 3712, 2, 29, 16128, 3778, 2593, 3, 2, 4534, 2, 279, 23, 537, 2066, 2, 29, 18474, 4749, 3, 2, 86, 402, 7840, 257, 3, 2, 3, 21543, 8563, 892, 2, 29, 10572, 1583, 11298, 41, 13368, 747, 6, 1485, 910, 477, 61, 2, 29, 329, 5434, 6, 15275, 11, 180, 3198, 9, 30, 1726, 44, 8, 2628, 10021, 868, 11347, 2, 29, 634, 3712, 13, 20653, 4534, 19, 46, 4760, 18297, 13, 3850, 797, 6, 1566, 6, 9995, 29, 6, 11, 12605, 29, 41, 434, 76, 32, 61, 8681, 801, 190, 8, 913, 7, 11, 1827, 1415, 13, 20653, 4534, 6, 8, 1661, 13, 8, 907, 1323, 13, 1371, 6, 6306, 357, 908, 6306, 519, 908, 6306, 591, 908, 6306, 755, 908, 11, 119, 2279, 5, 978, 5299, 384, 19, 8, 1485, 3712, 13, 8, 907, 1323, 5, 37, 4534, 7, 33, 8, 166, 1485, 3712, 13, 3850, 797, 19991, 16, 8, 907, 1323, 11, 8, 19147, 12, 2058, 8, 1945, 1384, 437, 8, 14532, 7, 5, 555, 6710, 343, 2832, 6, 3, 2, 667, 115, 265, 9, 31, 7, 1021, 6, 11273, 384, 3, 3272, 157, 7, 223, 12, 477, 13, 5184, 15, 3171, 5, 2, 6306, 948, 908, 2, 29, 4302, 4669, 7, 2, 29, 536, 1318, 5700, 342, 384, 2, 29, 357, 27944, 384, 3, 18, 28574, 5836, 2, 29, 519, 27944, 384, 3, 18, 2576, 2947, 138, 5836, 2, 29, 591, 15275, 17461, 4534, 31, 7, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [2840, 410, 20653, 4534, 31, 7, 1362, 11, 22229, 369, 45, 58, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "tokenizer,tokenized_dataset= tokenize_and_split_dataset(\"training_top5_qulac_PREPROCESSED_FOR_MODEL.json\",\"google/t5-efficient-tiny\")\n",
    "print(tokenized_dataset[0])  # Optional: check one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9a89c9bd-4c58-4238-bd9e-4ee7894f1c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds= generate_folds(tokenized_dataset, n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4ed35c88-3bad-4303-8e5a-42f0eaba6064",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/users/nfs/Etu0/21402600/wandb/run-20250509_012913-dlqr2mzo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/dlqr2mzo' target=\"_blank\">T5_Tiny_TOP_5_DOCS</a></strong> to <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/dlqr2mzo' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/dlqr2mzo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1981337/3761292519.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/450 00:49 < 02:19, 2.37 it/s, Epoch 8/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>19.895900</td>\n",
       "      <td>12.165541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041692</td>\n",
       "      <td>0.006561</td>\n",
       "      <td>0.041580</td>\n",
       "      <td>0.017722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>16.623700</td>\n",
       "      <td>7.866652</td>\n",
       "      <td>0.010369</td>\n",
       "      <td>0.157073</td>\n",
       "      <td>0.034147</td>\n",
       "      <td>0.155178</td>\n",
       "      <td>0.077415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>11.413700</td>\n",
       "      <td>4.419832</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.205159</td>\n",
       "      <td>0.028687</td>\n",
       "      <td>0.198033</td>\n",
       "      <td>0.099604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7.553300</td>\n",
       "      <td>1.940492</td>\n",
       "      <td>0.016824</td>\n",
       "      <td>0.227160</td>\n",
       "      <td>0.029911</td>\n",
       "      <td>0.223101</td>\n",
       "      <td>0.125728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>7.553300</td>\n",
       "      <td>1.239293</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.212471</td>\n",
       "      <td>0.025821</td>\n",
       "      <td>0.206655</td>\n",
       "      <td>0.115816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.606600</td>\n",
       "      <td>1.288048</td>\n",
       "      <td>0.017448</td>\n",
       "      <td>0.208856</td>\n",
       "      <td>0.032854</td>\n",
       "      <td>0.206730</td>\n",
       "      <td>0.112172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.826900</td>\n",
       "      <td>1.344526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.202133</td>\n",
       "      <td>0.026542</td>\n",
       "      <td>0.194540</td>\n",
       "      <td>0.119747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.909600</td>\n",
       "      <td>1.305108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.195121</td>\n",
       "      <td>0.024606</td>\n",
       "      <td>0.191752</td>\n",
       "      <td>0.113181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 results: {'eval_loss': 1.2392934560775757, 'eval_bleu': 0.0, 'eval_rouge1': 0.21247113604066634, 'eval_rouge2': 0.025821114272201226, 'eval_rougeL': 0.2066552314766033, 'eval_meteor': 0.11581556071151769, 'eval_runtime': 1.3604, 'eval_samples_per_second': 88.208, 'eval_steps_per_second': 2.94, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 44759.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 1 saved to crossval_T5_TINY_TOP5DOCS_fold_1.jsonl\n",
      "Processing Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1981337/3761292519.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/450 00:49 < 02:18, 2.38 it/s, Epoch 8/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>19.964700</td>\n",
       "      <td>11.826748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060564</td>\n",
       "      <td>0.008606</td>\n",
       "      <td>0.058518</td>\n",
       "      <td>0.020487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>16.553600</td>\n",
       "      <td>7.603567</td>\n",
       "      <td>0.014782</td>\n",
       "      <td>0.162806</td>\n",
       "      <td>0.034315</td>\n",
       "      <td>0.154842</td>\n",
       "      <td>0.073018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>11.316400</td>\n",
       "      <td>4.272438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.228909</td>\n",
       "      <td>0.038069</td>\n",
       "      <td>0.216459</td>\n",
       "      <td>0.105510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7.483400</td>\n",
       "      <td>1.850949</td>\n",
       "      <td>0.012644</td>\n",
       "      <td>0.234726</td>\n",
       "      <td>0.035315</td>\n",
       "      <td>0.222595</td>\n",
       "      <td>0.140218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>7.483400</td>\n",
       "      <td>1.233391</td>\n",
       "      <td>0.013446</td>\n",
       "      <td>0.235854</td>\n",
       "      <td>0.035110</td>\n",
       "      <td>0.225608</td>\n",
       "      <td>0.144731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.588600</td>\n",
       "      <td>1.289651</td>\n",
       "      <td>0.013694</td>\n",
       "      <td>0.221556</td>\n",
       "      <td>0.036779</td>\n",
       "      <td>0.216412</td>\n",
       "      <td>0.136736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.834100</td>\n",
       "      <td>1.334804</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.197467</td>\n",
       "      <td>0.023828</td>\n",
       "      <td>0.192150</td>\n",
       "      <td>0.125321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.923600</td>\n",
       "      <td>1.310598</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.185503</td>\n",
       "      <td>0.031210</td>\n",
       "      <td>0.180361</td>\n",
       "      <td>0.123104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 results: {'eval_loss': 1.233391284942627, 'eval_bleu': 0.013446231426163632, 'eval_rouge1': 0.23585385568050726, 'eval_rouge2': 0.03510989231552663, 'eval_rougeL': 0.2256084331921131, 'eval_meteor': 0.14473132278042222, 'eval_runtime': 1.4237, 'eval_samples_per_second': 84.285, 'eval_steps_per_second': 2.809, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 44384.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 2 saved to crossval_T5_TINY_TOP5DOCS_fold_2.jsonl\n",
      "Processing Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1981337/3761292519.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/450 00:49 < 02:17, 2.39 it/s, Epoch 8/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>19.898700</td>\n",
       "      <td>12.029180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049670</td>\n",
       "      <td>0.002778</td>\n",
       "      <td>0.049746</td>\n",
       "      <td>0.018328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>16.569400</td>\n",
       "      <td>7.849791</td>\n",
       "      <td>0.013185</td>\n",
       "      <td>0.161026</td>\n",
       "      <td>0.027185</td>\n",
       "      <td>0.158716</td>\n",
       "      <td>0.072013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>11.328300</td>\n",
       "      <td>4.454855</td>\n",
       "      <td>0.019262</td>\n",
       "      <td>0.205261</td>\n",
       "      <td>0.030170</td>\n",
       "      <td>0.203289</td>\n",
       "      <td>0.103368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7.497800</td>\n",
       "      <td>1.919009</td>\n",
       "      <td>0.018909</td>\n",
       "      <td>0.219176</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>0.212656</td>\n",
       "      <td>0.135543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>7.497800</td>\n",
       "      <td>1.178231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.220435</td>\n",
       "      <td>0.025388</td>\n",
       "      <td>0.211518</td>\n",
       "      <td>0.131590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.572100</td>\n",
       "      <td>1.229147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188174</td>\n",
       "      <td>0.020669</td>\n",
       "      <td>0.179022</td>\n",
       "      <td>0.110542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.790400</td>\n",
       "      <td>1.276459</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.182209</td>\n",
       "      <td>0.030943</td>\n",
       "      <td>0.175188</td>\n",
       "      <td>0.113311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.890600</td>\n",
       "      <td>1.216539</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188961</td>\n",
       "      <td>0.033931</td>\n",
       "      <td>0.181967</td>\n",
       "      <td>0.120242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 results: {'eval_loss': 1.1782305240631104, 'eval_bleu': 0.0, 'eval_rouge1': 0.22043543707674962, 'eval_rouge2': 0.025388208635625853, 'eval_rougeL': 0.21151775731237926, 'eval_meteor': 0.13158964879999843, 'eval_runtime': 1.3359, 'eval_samples_per_second': 89.828, 'eval_steps_per_second': 2.994, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 48479.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 3 saved to crossval_T5_TINY_TOP5DOCS_fold_3.jsonl\n",
      "Processing Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1981337/3761292519.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/450 00:48 < 02:16, 2.41 it/s, Epoch 8/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>19.847500</td>\n",
       "      <td>11.957872</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045639</td>\n",
       "      <td>0.008380</td>\n",
       "      <td>0.045242</td>\n",
       "      <td>0.016167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>16.579800</td>\n",
       "      <td>7.687338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.165631</td>\n",
       "      <td>0.028639</td>\n",
       "      <td>0.155512</td>\n",
       "      <td>0.075194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>11.386200</td>\n",
       "      <td>4.303197</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.204192</td>\n",
       "      <td>0.028711</td>\n",
       "      <td>0.196798</td>\n",
       "      <td>0.100012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7.523100</td>\n",
       "      <td>1.883443</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.202111</td>\n",
       "      <td>0.020839</td>\n",
       "      <td>0.197393</td>\n",
       "      <td>0.111244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>7.523100</td>\n",
       "      <td>1.199907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.220169</td>\n",
       "      <td>0.029197</td>\n",
       "      <td>0.211159</td>\n",
       "      <td>0.132375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.629000</td>\n",
       "      <td>1.254744</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.201221</td>\n",
       "      <td>0.022607</td>\n",
       "      <td>0.194075</td>\n",
       "      <td>0.124993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.858500</td>\n",
       "      <td>1.284096</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.190848</td>\n",
       "      <td>0.028864</td>\n",
       "      <td>0.183436</td>\n",
       "      <td>0.129394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.918600</td>\n",
       "      <td>1.280519</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.192258</td>\n",
       "      <td>0.028709</td>\n",
       "      <td>0.183094</td>\n",
       "      <td>0.131700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 results: {'eval_loss': 1.1999067068099976, 'eval_bleu': 0.0, 'eval_rouge1': 0.22016912649084702, 'eval_rouge2': 0.029196867946867947, 'eval_rougeL': 0.2111585978916288, 'eval_meteor': 0.13237481195767542, 'eval_runtime': 1.321, 'eval_samples_per_second': 90.839, 'eval_steps_per_second': 3.028, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 49636.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 4 saved to crossval_T5_TINY_TOP5DOCS_fold_4.jsonl\n",
      "Processing Fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1981337/3761292519.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/450 00:50 < 02:20, 2.35 it/s, Epoch 8/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>19.925500</td>\n",
       "      <td>11.946588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067314</td>\n",
       "      <td>0.005502</td>\n",
       "      <td>0.062529</td>\n",
       "      <td>0.028497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>16.630700</td>\n",
       "      <td>7.844331</td>\n",
       "      <td>0.013857</td>\n",
       "      <td>0.147934</td>\n",
       "      <td>0.024655</td>\n",
       "      <td>0.146009</td>\n",
       "      <td>0.060015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>11.421700</td>\n",
       "      <td>4.331576</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.221424</td>\n",
       "      <td>0.035767</td>\n",
       "      <td>0.211850</td>\n",
       "      <td>0.099773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7.473800</td>\n",
       "      <td>1.828266</td>\n",
       "      <td>0.015047</td>\n",
       "      <td>0.239918</td>\n",
       "      <td>0.039189</td>\n",
       "      <td>0.228590</td>\n",
       "      <td>0.137183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>7.473800</td>\n",
       "      <td>1.188184</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.244032</td>\n",
       "      <td>0.037217</td>\n",
       "      <td>0.232855</td>\n",
       "      <td>0.146628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.568100</td>\n",
       "      <td>1.260487</td>\n",
       "      <td>0.015872</td>\n",
       "      <td>0.224353</td>\n",
       "      <td>0.042238</td>\n",
       "      <td>0.219690</td>\n",
       "      <td>0.142709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.785300</td>\n",
       "      <td>1.305366</td>\n",
       "      <td>0.014277</td>\n",
       "      <td>0.203024</td>\n",
       "      <td>0.033262</td>\n",
       "      <td>0.198880</td>\n",
       "      <td>0.129840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.899600</td>\n",
       "      <td>1.246084</td>\n",
       "      <td>0.011784</td>\n",
       "      <td>0.205642</td>\n",
       "      <td>0.039399</td>\n",
       "      <td>0.204519</td>\n",
       "      <td>0.132901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 results: {'eval_loss': 1.1881837844848633, 'eval_bleu': 0.0, 'eval_rouge1': 0.2440319997795114, 'eval_rouge2': 0.03721688181648636, 'eval_rougeL': 0.2328550349254946, 'eval_meteor': 0.14662765593001656, 'eval_runtime': 1.3179, 'eval_samples_per_second': 90.297, 'eval_steps_per_second': 3.035, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:00<00:00, 46879.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 5 saved to crossval_T5_TINY_TOP5DOCS_fold_5.jsonl\n",
      "Average metrics over all folds: {'eval_loss': 1.2078011512756348, 'eval_bleu': 0.0026892462852327265, 'eval_rouge1': 0.2265923110136563, 'eval_rouge2': 0.030546592997341603, 'eval_rougeL': 0.21755901095964383, 'eval_meteor': 0.13422780003592608, 'eval_runtime': 1.35178, 'eval_samples_per_second': 88.6914, 'eval_steps_per_second': 2.9612000000000003, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Averages/epoch</td><td>▁</td></tr><tr><td>Averages/eval_bleu</td><td>▁</td></tr><tr><td>Averages/eval_loss</td><td>▁</td></tr><tr><td>Averages/eval_meteor</td><td>▁</td></tr><tr><td>Averages/eval_rouge1</td><td>▁</td></tr><tr><td>Averages/eval_rouge2</td><td>▁</td></tr><tr><td>Averages/eval_rougeL</td><td>▁</td></tr><tr><td>Averages/eval_runtime</td><td>▁</td></tr><tr><td>Averages/eval_samples_per_second</td><td>▁</td></tr><tr><td>Averages/eval_steps_per_second</td><td>▁</td></tr><tr><td>Fold_1/eval/epoch</td><td>▁▂▃▄▅▆▇██</td></tr><tr><td>Fold_1/eval/eval_bleu</td><td>▁▅██▁█▁▁▁</td></tr><tr><td>Fold_1/eval/eval_loss</td><td>█▅▃▁▁▁▁▁▁</td></tr><tr><td>Fold_1/eval/eval_meteor</td><td>▁▅▆█▇▇█▇▇</td></tr><tr><td>Fold_1/eval/eval_rouge1</td><td>▁▅▇█▇▇▇▇▇</td></tr><tr><td>Fold_1/eval/eval_rouge2</td><td>▁█▇▇▆█▆▆▆</td></tr><tr><td>Fold_1/eval/eval_rougeL</td><td>▁▅▇█▇▇▇▇▇</td></tr><tr><td>Fold_1/eval/eval_runtime</td><td>█▆▄▄▆▁▂▂▅</td></tr><tr><td>Fold_1/eval/eval_samples_per_second</td><td>▁▃▅▅▃█▇▇▄</td></tr><tr><td>Fold_1/eval/eval_steps_per_second</td><td>▁▃▅▅▃█▇▇▄</td></tr><tr><td>Fold_2/eval/epoch</td><td>▁▂▃▄▅▆▇██</td></tr><tr><td>Fold_2/eval/eval_bleu</td><td>▁█▁▇▇▇▁▁▇</td></tr><tr><td>Fold_2/eval/eval_loss</td><td>█▅▃▁▁▁▁▁▁</td></tr><tr><td>Fold_2/eval/eval_meteor</td><td>▁▄▆███▇▇█</td></tr><tr><td>Fold_2/eval/eval_rouge1</td><td>▁▅███▇▆▆█</td></tr><tr><td>Fold_2/eval/eval_rouge2</td><td>▁▇█▇▇█▅▆▇</td></tr><tr><td>Fold_2/eval/eval_rougeL</td><td>▁▅████▇▆█</td></tr><tr><td>Fold_2/eval/eval_runtime</td><td>▃▄▁▄▄▆▇██</td></tr><tr><td>Fold_2/eval/eval_samples_per_second</td><td>▆▄█▄▅▃▂▁▁</td></tr><tr><td>Fold_2/eval/eval_steps_per_second</td><td>▆▄█▄▅▃▂▁▁</td></tr><tr><td>Fold_3/eval/epoch</td><td>▁▂▃▄▅▆▇██</td></tr><tr><td>Fold_3/eval/eval_bleu</td><td>▁▆██▁▁▁▁▁</td></tr><tr><td>Fold_3/eval/eval_loss</td><td>█▅▃▁▁▁▁▁▁</td></tr><tr><td>Fold_3/eval/eval_meteor</td><td>▁▄▆██▇▇▇█</td></tr><tr><td>Fold_3/eval/eval_rouge1</td><td>▁▆▇██▇▆▇█</td></tr><tr><td>Fold_3/eval/eval_rouge2</td><td>▁▆▇█▆▅▇█▆</td></tr><tr><td>Fold_3/eval/eval_rougeL</td><td>▁▆███▇▆▇█</td></tr><tr><td>Fold_3/eval/eval_runtime</td><td>▁▃▁▄▅█▇▁▄</td></tr><tr><td>Fold_3/eval/eval_samples_per_second</td><td>█▆█▅▄▁▂█▅</td></tr><tr><td>Fold_3/eval/eval_steps_per_second</td><td>█▆█▅▄▁▂█▅</td></tr><tr><td>Fold_4/eval/epoch</td><td>▁▂▃▄▅▆▇██</td></tr><tr><td>Fold_4/eval/eval_bleu</td><td>▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_4/eval/eval_loss</td><td>█▅▃▁▁▁▁▁▁</td></tr><tr><td>Fold_4/eval/eval_meteor</td><td>▁▅▆▇█████</td></tr><tr><td>Fold_4/eval/eval_rouge1</td><td>▁▆▇▇█▇▇▇█</td></tr><tr><td>Fold_4/eval/eval_rouge2</td><td>▁██▅█▆███</td></tr><tr><td>Fold_4/eval/eval_rougeL</td><td>▁▆▇▇█▇▇▇█</td></tr><tr><td>Fold_4/eval/eval_runtime</td><td>▄██▃▃▁▅▆▃</td></tr><tr><td>Fold_4/eval/eval_samples_per_second</td><td>▅▁▁▆▆█▄▃▆</td></tr><tr><td>Fold_4/eval/eval_steps_per_second</td><td>▅▁▁▆▆█▄▃▆</td></tr><tr><td>Fold_5/eval/epoch</td><td>▁▂▃▄▅▆▇██</td></tr><tr><td>Fold_5/eval/eval_bleu</td><td>▁▇▁█▁█▇▆▁</td></tr><tr><td>Fold_5/eval/eval_loss</td><td>█▅▃▁▁▁▁▁▁</td></tr><tr><td>Fold_5/eval/eval_meteor</td><td>▁▃▅▇██▇▇█</td></tr><tr><td>Fold_5/eval/eval_rouge1</td><td>▁▄▇██▇▆▆█</td></tr><tr><td>Fold_5/eval/eval_rouge2</td><td>▁▅▇▇▇█▆▇▇</td></tr><tr><td>Fold_5/eval/eval_rougeL</td><td>▁▄▇██▇▇▇█</td></tr><tr><td>Fold_5/eval/eval_runtime</td><td>▂▂▁▁▂▁█▂▁</td></tr><tr><td>Fold_5/eval/eval_samples_per_second</td><td>▇▇██▆▇▁▇█</td></tr><tr><td>Fold_5/eval/eval_steps_per_second</td><td>▇▇██▆▇▁▇█</td></tr><tr><td>eval/bleu</td><td>▁▅▇▇▁▇▁▁▁▆▁▆▆▆▁▁▁▆██▁▁▁▁▁▁▁▁▁▁▁▁▁▆▁▆▁▇▆▁</td></tr><tr><td>eval/loss</td><td>█▅▃▁▁▁▁▁█▅▃▁▁▁▁▁█▅▃▁▁▁▁▁█▅▃▁▁▁▁▁█▅▃▁▁▁▁▁</td></tr><tr><td>eval/meteor</td><td>▁▄▅▇▆▆▇▆▁▄▆██▇▇▇▁▄▆▇▇▆▆▇▁▄▅▆▇▇▇▇▂▃▅▇██▇█</td></tr><tr><td>eval/rouge1</td><td>▁▅▇▇▇▇▇▆▂▅▇██▇▆▆▁▅▇▇▇▆▆▆▁▅▇▇▇▇▆▆▂▅▇██▇▇█</td></tr><tr><td>eval/rouge2</td><td>▂▇▆▆▅▆▅▅▂▇▇▇▇▇▅▆▁▅▆▇▅▄▆▇▂▆▆▄▆▅▆▆▁▅▇▇▇█▆▇</td></tr><tr><td>eval/rougeL</td><td>▁▅▇█▇▇▇▆▂▅▇██▇▇▆▁▅▇▇▇▆▆▆▁▅▇▇▇▇▆▆▂▅▇███▇█</td></tr><tr><td>eval/runtime</td><td>▂▂▂▂▂▁▂▂▁▂▁▂▂▂▂▂▁▁▁▂▂▂▂▁▁▂▂▁▁▁▂▂▂▂▁▁▂▂█▁</td></tr><tr><td>eval/samples_per_second</td><td>▆▇▇▇▇▇▇▇▇▇█▇▇▆▆▆█▇█▇▇▇▇█▇▇▇█▇█▇▇▇▇▇█▆▇▁▇</td></tr><tr><td>eval/steps_per_second</td><td>▆▇▇▇▇▇▇▇▇▇█▇▇▆▆▆█▇█▇▇▇▇█▇▇▇█▇█▇▇▇▇██▆▇▁█</td></tr><tr><td>test/bleu</td><td>▁█▁▁▁</td></tr><tr><td>test/loss</td><td>█▇▁▃▂</td></tr><tr><td>test/meteor</td><td>▁█▅▅█</td></tr><tr><td>test/rouge1</td><td>▁▆▃▃█</td></tr><tr><td>test/rouge2</td><td>▁▇▁▃█</td></tr><tr><td>test/rougeL</td><td>▁▆▂▂█</td></tr><tr><td>test/runtime</td><td>▁█▃▃▁</td></tr><tr><td>test/samples_per_second</td><td>█▁▅▆▇</td></tr><tr><td>test/steps_per_second</td><td>█▁▅▆█</td></tr><tr><td>train/epoch</td><td>▁▃▄▆▇██▁▂▃▅▆▆▇█▂▃▄▄▅▇▇███▂▃▄▄▄▆█▂▂▃▄▅▆▆█</td></tr><tr><td>train/global_step</td><td>▁▂▃▄▅▆▇██▄▇▇▇██▁▃▄▅▅██▁▂▃▄▅▇██▁▁▃▄▅▆████</td></tr><tr><td>train/grad_norm</td><td>█▄▃▃▂▁▁█▄▄▃▂▂▁█▄▄▃▂▁▁▇▄▄▃▂▁▁█▄▄▃▂▁▁</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▃▂▁█▇▆▅▃▂▁█▇▆▅▃▂▁█▇▆▅▃▂▁█▇▆▅▃▂▁</td></tr><tr><td>train/loss</td><td>█▇▅▃▂▁▁█▇▅▃▂▁▁█▇▅▃▂▁▁█▇▅▃▂▁▁█▇▅▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Averages/epoch</td><td>8</td></tr><tr><td>Averages/eval_bleu</td><td>0.00269</td></tr><tr><td>Averages/eval_loss</td><td>1.2078</td></tr><tr><td>Averages/eval_meteor</td><td>0.13423</td></tr><tr><td>Averages/eval_rouge1</td><td>0.22659</td></tr><tr><td>Averages/eval_rouge2</td><td>0.03055</td></tr><tr><td>Averages/eval_rougeL</td><td>0.21756</td></tr><tr><td>Averages/eval_runtime</td><td>1.35178</td></tr><tr><td>Averages/eval_samples_per_second</td><td>88.6914</td></tr><tr><td>Averages/eval_steps_per_second</td><td>2.9612</td></tr><tr><td>Fold_1/eval/epoch</td><td>8</td></tr><tr><td>Fold_1/eval/eval_bleu</td><td>0</td></tr><tr><td>Fold_1/eval/eval_loss</td><td>1.23929</td></tr><tr><td>Fold_1/eval/eval_meteor</td><td>0.11582</td></tr><tr><td>Fold_1/eval/eval_rouge1</td><td>0.21247</td></tr><tr><td>Fold_1/eval/eval_rouge2</td><td>0.02582</td></tr><tr><td>Fold_1/eval/eval_rougeL</td><td>0.20666</td></tr><tr><td>Fold_1/eval/eval_runtime</td><td>1.3604</td></tr><tr><td>Fold_1/eval/eval_samples_per_second</td><td>88.208</td></tr><tr><td>Fold_1/eval/eval_steps_per_second</td><td>2.94</td></tr><tr><td>Fold_2/eval/epoch</td><td>8</td></tr><tr><td>Fold_2/eval/eval_bleu</td><td>0.01345</td></tr><tr><td>Fold_2/eval/eval_loss</td><td>1.23339</td></tr><tr><td>Fold_2/eval/eval_meteor</td><td>0.14473</td></tr><tr><td>Fold_2/eval/eval_rouge1</td><td>0.23585</td></tr><tr><td>Fold_2/eval/eval_rouge2</td><td>0.03511</td></tr><tr><td>Fold_2/eval/eval_rougeL</td><td>0.22561</td></tr><tr><td>Fold_2/eval/eval_runtime</td><td>1.4237</td></tr><tr><td>Fold_2/eval/eval_samples_per_second</td><td>84.285</td></tr><tr><td>Fold_2/eval/eval_steps_per_second</td><td>2.809</td></tr><tr><td>Fold_3/eval/epoch</td><td>8</td></tr><tr><td>Fold_3/eval/eval_bleu</td><td>0</td></tr><tr><td>Fold_3/eval/eval_loss</td><td>1.17823</td></tr><tr><td>Fold_3/eval/eval_meteor</td><td>0.13159</td></tr><tr><td>Fold_3/eval/eval_rouge1</td><td>0.22044</td></tr><tr><td>Fold_3/eval/eval_rouge2</td><td>0.02539</td></tr><tr><td>Fold_3/eval/eval_rougeL</td><td>0.21152</td></tr><tr><td>Fold_3/eval/eval_runtime</td><td>1.3359</td></tr><tr><td>Fold_3/eval/eval_samples_per_second</td><td>89.828</td></tr><tr><td>Fold_3/eval/eval_steps_per_second</td><td>2.994</td></tr><tr><td>Fold_4/eval/epoch</td><td>8</td></tr><tr><td>Fold_4/eval/eval_bleu</td><td>0</td></tr><tr><td>Fold_4/eval/eval_loss</td><td>1.19991</td></tr><tr><td>Fold_4/eval/eval_meteor</td><td>0.13237</td></tr><tr><td>Fold_4/eval/eval_rouge1</td><td>0.22017</td></tr><tr><td>Fold_4/eval/eval_rouge2</td><td>0.0292</td></tr><tr><td>Fold_4/eval/eval_rougeL</td><td>0.21116</td></tr><tr><td>Fold_4/eval/eval_runtime</td><td>1.321</td></tr><tr><td>Fold_4/eval/eval_samples_per_second</td><td>90.839</td></tr><tr><td>Fold_4/eval/eval_steps_per_second</td><td>3.028</td></tr><tr><td>Fold_5/eval/epoch</td><td>8</td></tr><tr><td>Fold_5/eval/eval_bleu</td><td>0</td></tr><tr><td>Fold_5/eval/eval_loss</td><td>1.18818</td></tr><tr><td>Fold_5/eval/eval_meteor</td><td>0.14663</td></tr><tr><td>Fold_5/eval/eval_rouge1</td><td>0.24403</td></tr><tr><td>Fold_5/eval/eval_rouge2</td><td>0.03722</td></tr><tr><td>Fold_5/eval/eval_rougeL</td><td>0.23286</td></tr><tr><td>Fold_5/eval/eval_runtime</td><td>1.3179</td></tr><tr><td>Fold_5/eval/eval_samples_per_second</td><td>90.297</td></tr><tr><td>Fold_5/eval/eval_steps_per_second</td><td>3.035</td></tr><tr><td>eval/bleu</td><td>0</td></tr><tr><td>eval/loss</td><td>1.18818</td></tr><tr><td>eval/meteor</td><td>0.14663</td></tr><tr><td>eval/rouge1</td><td>0.24403</td></tr><tr><td>eval/rouge2</td><td>0.03722</td></tr><tr><td>eval/rougeL</td><td>0.23286</td></tr><tr><td>eval/runtime</td><td>1.3179</td></tr><tr><td>eval/samples_per_second</td><td>90.297</td></tr><tr><td>eval/steps_per_second</td><td>3.035</td></tr><tr><td>test/bleu</td><td>0</td></tr><tr><td>test/loss</td><td>1.18818</td></tr><tr><td>test/meteor</td><td>0.14663</td></tr><tr><td>test/rouge1</td><td>0.24403</td></tr><tr><td>test/rouge2</td><td>0.03722</td></tr><tr><td>test/rougeL</td><td>0.23286</td></tr><tr><td>test/runtime</td><td>1.2838</td></tr><tr><td>test/samples_per_second</td><td>92.697</td></tr><tr><td>test/steps_per_second</td><td>3.116</td></tr><tr><td>total_flos</td><td>86652978462720.0</td></tr><tr><td>train/epoch</td><td>8</td></tr><tr><td>train/global_step</td><td>120</td></tr><tr><td>train/grad_norm</td><td>1.67125</td></tr><tr><td>train/learning_rate</td><td>4e-05</td></tr><tr><td>train/loss</td><td>1.8996</td></tr><tr><td>train_loss</td><td>7.49066</td></tr><tr><td>train_runtime</td><td>50.4819</td></tr><tr><td>train_samples_per_second</td><td>285.251</td></tr><tr><td>train_steps_per_second</td><td>8.914</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">T5_Tiny_TOP_5_DOCS</strong> at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/dlqr2mzo' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/dlqr2mzo</a><br> View project at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250509_012913-dlqr2mzo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final predicitons for all folds foudn in crossval_T5_TINY_TOP5DOCS.jsonl\n"
     ]
    }
   ],
   "source": [
    "cross_val_train(modelname=\"google/t5-efficient-tiny\", run_name=\"T5_Tiny_TOP_5_DOCS\",filename=\"crossval_T5_TINY_TOP5DOCS.jsonl\", \n",
    "                tokenizer=tokenizer, tokenized_dataset=tokenized_dataset, \n",
    "                folds=folds, nb_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ac6395-d9cb-4f1d-8483-50c68ff46e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORKING VERSION OF CV WITHOUT PROPER LOGGING\n",
    "\n",
    "# import wandb\n",
    "# from transformers import AutoModelForSeq2SeqLM, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "# from datasets import Dataset\n",
    "# import os\n",
    "# from tqdm import tqdm\n",
    "# from sklearn.model_selection import KFold\n",
    "# import numpy as np\n",
    "\n",
    "# def cross_val_train(modelname, filename, run_name, tokenizer, tokenized_dataset, folds, nb_epochs=30):\n",
    "#     # Start a single WandB run for all folds\n",
    "#     wandb.init(project=\"cross_val_Test\", name=run_name, config={\"epochs\": nb_epochs})\n",
    "\n",
    "#     fold_metrics = []\n",
    "\n",
    "#     for fold_num, (train_idx, test_idx) in enumerate(folds):\n",
    "#         print(f\"Processing Fold {fold_num + 1}\")\n",
    "\n",
    "#         # Split the dataset\n",
    "#         train_dataset = tokenized_dataset.select(train_idx)\n",
    "#         test_dataset = tokenized_dataset.select(test_idx)\n",
    "\n",
    "#         # Initialize the model for each fold\n",
    "#         model = AutoModelForSeq2SeqLM.from_pretrained(modelname, from_tf=True)\n",
    "\n",
    "#         # Training arguments\n",
    "#         training_args = TrainingArguments(\n",
    "#             output_dir=f\"./results/fold_{fold_num+1}\",  # Different folder per fold\n",
    "#             eval_strategy=\"epoch\",\n",
    "#             save_strategy=\"epoch\",\n",
    "#             learning_rate=5e-5,\n",
    "#             per_device_train_batch_size=8,\n",
    "#             per_device_eval_batch_size=8,\n",
    "#             num_train_epochs=nb_epochs,\n",
    "#             logging_dir=f\"./results/logs/fold_{fold_num+1}\",\n",
    "#             load_best_model_at_end=True,\n",
    "#             metric_for_best_model=\"eval_loss\",\n",
    "#             greater_is_better=False,\n",
    "#             seed=42,  # Seed for reproducibility\n",
    "#         )\n",
    "\n",
    "#         # Initialize Trainer\n",
    "#         trainer = Trainer(\n",
    "#             model=model,\n",
    "#             args=training_args,\n",
    "#             train_dataset=train_dataset,\n",
    "#             eval_dataset=test_dataset,\n",
    "#             tokenizer=tokenizer,\n",
    "#             compute_metrics=compute_metrics,\n",
    "#         )\n",
    "\n",
    "#         # Train the model\n",
    "#         trainer.train()\n",
    "\n",
    "#         # Evaluate the model on this fold\n",
    "#         eval_results = trainer.evaluate()\n",
    "#         print(f\"Fold {fold_num + 1} results: {eval_results}\")\n",
    "\n",
    "#         # Store the fold metrics\n",
    "#         fold_metrics.append(eval_results)\n",
    "\n",
    "#     # Aggregate results over all folds\n",
    "#     avg_metrics = {key: np.mean([fold[key] for fold in fold_metrics]) for key in fold_metrics[0]}\n",
    "#     print(\"Average metrics over all folds:\", avg_metrics)\n",
    "\n",
    "#     # Log average metrics to WandB\n",
    "\n",
    "#     # End the WandB run\n",
    "#     wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e84200f6-9236-4dcb-8d78-e54b0a153f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# from transformers import AutoModelForSeq2SeqLM, Trainer, TrainingArguments, EarlyStoppingCallback, TrainerCallback\n",
    "# from datasets import Dataset\n",
    "# import numpy as np\n",
    "\n",
    "# # Custom callback to log metrics after each evaluation step\n",
    "# class WandbLoggingCallback(TrainerCallback):\n",
    "#     def __init__(self, fold_num):\n",
    "#         self.fold_num = fold_num\n",
    "\n",
    "#     def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "#         if state.is_world_process_zero:\n",
    "#             # Log metrics with fold-specific keys\n",
    "#             log_data = {f\"Fold_{self.fold_num}/eval/{key}\": value for key, value in metrics.items()}\n",
    "#             wandb.log(log_data)\n",
    "\n",
    "# def cross_val_train(modelname, filename, run_name, tokenizer, tokenized_dataset, folds, nb_epochs=30):\n",
    "#     # Start a single WandB run for all folds\n",
    "#     wandb.init(project=\"cross_val_T5\", name=run_name, config={\"epochs\": nb_epochs})\n",
    "\n",
    "#     fold_metrics = []\n",
    "\n",
    "#     for fold_num, (train_idx, test_idx) in enumerate(folds):\n",
    "#         print(f\"Processing Fold {fold_num + 1}\")\n",
    "\n",
    "#         # Split the dataset\n",
    "#         train_dataset = tokenized_dataset.select(train_idx)\n",
    "#         test_dataset = tokenized_dataset.select(test_idx)\n",
    "\n",
    "#         # Initialize the model\n",
    "#         model = AutoModelForSeq2SeqLM.from_pretrained(modelname, from_tf=True)\n",
    "\n",
    "#         # Training arguments with early stopping\n",
    "#         training_args = TrainingArguments(\n",
    "#             output_dir=f\"./results/fold_{fold_num+1}\",\n",
    "#             eval_strategy=\"epoch\",\n",
    "#             save_strategy=\"epoch\",\n",
    "#             learning_rate=5e-5,\n",
    "#             per_device_train_batch_size=8,\n",
    "#             per_device_eval_batch_size=8,\n",
    "#             num_train_epochs=nb_epochs,\n",
    "#             logging_dir=f\"./results/logs/fold_{fold_num+1}\",\n",
    "#             load_best_model_at_end=True,\n",
    "#             logging_steps=20,\n",
    "#             logging_first_step=True,\n",
    "#             metric_for_best_model=\"eval_loss\",\n",
    "#             greater_is_better=False,\n",
    "#             seed=42,\n",
    "#         )\n",
    "\n",
    "#         # Initialize Trainer with the logging callback and early stopping\n",
    "#         trainer = Trainer(\n",
    "#             model=model,\n",
    "#             args=training_args,\n",
    "#             train_dataset=train_dataset,\n",
    "#             eval_dataset=test_dataset,\n",
    "#             tokenizer=tokenizer,\n",
    "#             compute_metrics=compute_metrics,\n",
    "#             callbacks=[\n",
    "#                 WandbLoggingCallback(fold_num + 1),  # Log metrics for each fold\n",
    "#                 EarlyStoppingCallback(early_stopping_patience=3)  # Early stopping after 3 epochs of no improvement\n",
    "#             ],\n",
    "#         )\n",
    "\n",
    "#         # Train the model\n",
    "#         trainer.train()\n",
    "\n",
    "#         # Final evaluation for the fold\n",
    "#         eval_results = trainer.evaluate()\n",
    "#         print(f\"Fold {fold_num + 1} results: {eval_results}\")\n",
    "\n",
    "#         # Store the final evaluation results for this fold\n",
    "#         fold_metrics.append(eval_results)\n",
    "\n",
    "#     # Calculate and log average metrics\n",
    "#     avg_metrics = {key: np.mean([fold[key] for fold in fold_metrics]) for key in fold_metrics[0]}\n",
    "#     wandb.log({f\"Averages/{key}\": value for key, value in avg_metrics.items()})\n",
    "#     print(\"Average metrics over all folds:\", avg_metrics)\n",
    "\n",
    "#     wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55287291-74c3-4494-8a34-bb2bc8d5572f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# T5 SMALL ------------------- TO5 RAW DOCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e617d9-2b97-40ea-af98-77cb2d5dc9d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "19172d91-d925-4171-ad8c-3ed0b74a3eb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5ea4cd15aad4b678ede1bfc7e05b3a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/Etu0/21402600/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': '[QUERY] Find information on President Barack Obama\\'s family history, including genealogy, national origins, places and dates of birth, etc.\\n[DOCUMENTS]\\n[DOC 1] Family of Barack Obama - Wikipedia, the free encyclopedia\\\\nFamily of Barack Obama\\\\nFrom Wikipedia, the free encyclopedia\\\\n(Redirected from Sasha Obama)\\\\nJump to: navigation, search\\\\nObama Family\\\\nPresident Barack Obama, First Lady Michelle, and daughters Malia and Sasha wave to the crowd after his inaugural address Jan. 20, 2009, on the west steps of the U.S. Capitol.[1]\\\\nCurrent region\\\\nWashington, DC\\\\nInformation\\\\nPlace of origin\\\\nUnited States\\\\nNotable members\\\\nBarack Obama, Michelle Obama, Ann Dunham, Barack Obama, Sr., etc.\\\\nConnected families\\\\nRobinson, Dunham, Soetoro, Ng\\\\nThis article is part of a series about\\\\nBarack Obama\\\\nBackground \\xa0ýý Illinois Senate \\xa0ýý U.S. Senate\\\\nPolitical positions\\xa0ýý Public image\\xa0ýý Family\\\\n2008 primaries\\xa0ýý ObamaýýýBiden campaign\\\\nTransition\\xa0ýý Inauguration\\xa0ýý Electoral history\\\\nPresidency (Timeline, First 100 days)\\\\nMalia, Michelle and Sasha on stage at the 2008 Democratic National Convention\\\\nThe Family of Barack Obama is an extended clan of African American, English, Indonesian, and Kenyan (Luo) heritage known through the writings and political career of Barack Obama, the President of the United States of America,[2][3][4][5] and other reports. His immediate family is the First Family of the United States. The Obamas are the first First Family of African American descent in the United States and the youngest to enter the White House since the Kennedys. One columnist wrote, ýýýObama\\'s young, energetic family harks back to days of Camelot.ýýý[6]\\\\nContents\\\\n1 Immediate family\\\\n2 Extended family - maternal relations\\\\n3 Extended family - paternal relations\\\\n4 Michelle Robinson Obama\\'s extended family\\\\n5 Genealogical charts\\\\n5.1 Obama ancestry\\\\n5.2 Family trees\\\\n6 Distant relations\\\\n7 See also\\\\n8 References\\\\n9 External links\\\\nImmediate family\\\\nMichelle Obama\\\\nMichelle Obama, nýýe Robinson, the wife of Barack Obama, was born on January 17, 1964 in Chicago, Illinois. She is a lawyer and was a University of Chicago Hospital Vice-President. She is the First Lady of the United States.\\\\nMalia Obama and Sasha Obama\\\\nBarack and Michelle Obama have two daughters: Malia Ann (pronounced /mýýýýliýýýý/), born in 1998,[7] and Natasha (known as Sasha) /ýýsýýýýýýýý/), born in 2001. Sasha is the youngest child to reside in the White House since John F. Kennedy, Jr, arrived as an infant in 1961.[8]\\\\nBefore his inauguration, President Obama published an open letter to his daughters in Parade magazine, describing what he wants for them and every child in America: \\\\\"to grow up in a world with no limits on your dreams and no achievements beyond your reach, and to grow into compassionate, committed women who will help build that world.\\\\\"[9]\\\\nWhile living in Chicago, they kept busy schedules, as the Associated Press reports: \\\\\"soccer, dance and drama for Malia, gymnastics and tap for Sasha, piano and tennis for both.\\\\\"[10][11] In July 2008, the family gave an interview to the television series Access Hollywood; Obama later said they regretted allowing the children to be included.[12]\\\\nIn his victory speech on the night of his election, President Obama repeated his promise to Sasha and Malia to get a puppy to take with them to the White House.[13] However the selection of a dog has been slow because Malia is allergic to animal dander;[14] the president subsequently said that the choice has been narrowed down to either a labradoodle or Portuguese Water Dog, and they are hoping to find a shelter animal.[15]\\\\nMalia and Sasha attend the private Sidwell Friends School in Washington, DC, the same school as attended by Chelsea Clinton, Tricia Nixon Cox, and Archibald Roosevelt, and currently the grandchildren of Vice President Joe Biden.[16] The Obama girls began classes there on January 5, 2009.[17] While in Chicago, both attended the private University of Chicago Laboratory School.\\\\nMarian Shields Robinson\\\\nMichelle Obama\\'s mother (birthname Marian Shields, born July 1937), now widowed, married Michelle\\'s father, Fraser Robinson, in 1960.[18][19] Robinson was formerly a secretary at Spiegel catalog and a bank. While Michelle and Barack Obama were campaigning in 2008, Robinson tended the Obama\\'s young children and she intends to do the same while in Washington, DC. Robinson is currently living in the White House itself as part of the First Family;[20] she is the first live-in grandmother there since Elivera M. Doud during the Eisenhower administration.[21] Some media outlets have dubbed Robinson the \\\\\"First Granny\\\\\".[22][21]\\\\nExtended family - maternal relations\\\\nRight-to-left: Barack Obama and Maya Soetoro with their mother Ann Dunham and grandfather Stanley Dunham in Hawaii (early 1970s)\\\\nWikinews has related news:\\\\nBarack Obama elected 44th President of the United States\\\\nGrandmother of Barack Obama dies at 86\\\\nMadelyn Dunham with her daughter Ann\\\\nAccording to Barack Obama\\'s Dreams from My Father, his great-grandmother Leona McCurry was part Native American, which Obama believed Leona held as a \\\\\"source of considerable shame\\\\\" and \\\\\"blanched whenever someone mentioned the subject and hoped to carry the secret to her grave\\\\\"; whereas McCurry\\'s daughter (Obama\\'s maternal grandmother) \\\\\"would turn her head in profile to show off her beaked nose, which along with a pair of jet-black eyes, was offered as proof of Cherokee blood.\\\\\"[23] To date, no concrete evidence has surfaced of Cherokee heritage. Obama\\'s maternal heritage consists mostly of English ancestry, with much smaller amounts of German, Irish, Scottish, Welsh, Swiss, and French ancestry.[2]\\\\nAnn Dunham\\\\nMother of Barack Obama born in 1942, died in 1995. Birthname Stanley Ann Dunham. Anthropologist in Hawaii and Indonesia.\\\\nMadelyn Lee Payne Dunham\\\\nBarack Obama\\'s maternal grandmother, born in 1922 and died on November 2, 2008.[24] She was a bank vice president in Hawaii. Obama said that when he was a child, his grandmother \\\\\"read me the opening lines of the Declaration of Independence and told me about the men and women who marched for equality because they believed those words put to paper two centuries ago should mean something.\\\\\"[9]\\\\nStanley Armour Dunham\\\\nGrandfather of Barack Obama, born 1918, died 1992. World War II U.S. Army sergeant, furniture salesman in Hawaii.\\\\nCharles T. Payne\\\\nGreat-uncle of Barack Obama, younger brother of Madelyn Dunham, born 1925. Served during World War II in the U.S. Army 89th Infantry Division.[25] Obama has often described Payne\\'s role in liberating Ohrdruf forced labor camp.[26] There was brief media attention when Obama mistakenly identified the camp as Auschwitz during the campaign.[27] Payne appeared in the visitor\\'s gallery at the Democratic National Convention in Denver, Colorado, when his great-nephew was nominated for President.[28] He was the assistant director of the University of Chicago\\'s Library.[26]\\\\nMaya Soetoro-Ng\\\\nHalf-sister of Barack Obama, born August 15, 1970, in Jakarta, Indonesia.[29] She is married to Konrad Ng, with whom she has a daughter, Suhaila. Maya Soetoro-Ng is a teacher in Hawaii.\\\\nKonrad Ng\\\\nBrother-in-law of Barack Obama, born 1974. A Canadian whose parents are Malaysian Chinese immigrants, he is an assistant professor at the University of Hawaii\\'s Academy of Creative Media.[30] His parents are from Kudat and Sandakan, two small towns in Sabah, Malaysia, and he was born and raised in Burlington, Ontario.[31] He married Maya Soetoro-Ng at the end of 2003 in Hawaii.[32] They have one daughter, Suhaila.[33][34][35] Konrad Ng is now a US citizen.[36]\\\\nLolo Soetoro\\\\nStepfather of Barack Obama, born in Indonesia 1936, died 1987.\\\\nExtended family - paternal relations\\\\nThe Obamas are members of the Luo, Kenya\\'s third-largest ethnic group, which is part of a larger family of ethnic groups, collectively also known as Luo. This group belongs to the Eastern Sudanic branch of the Nilo-Saharan phylum. The Obama family is largely concentrated in the western province of Nyanza.\\\\nFront row (left to right): Auma Obama (Barack\\'s half-sister), Kezia Obama (Barack\\'s step-mother), Sarah Hussein Onyango Obama (third wife of Barack\\'s paternal grandfather), Zeituni Onyango (Barack\\'s aunt)\\\\nBack row (left to right): Said Obama (Barack\\'s uncle), Barack Obama, Abongo [Roy] Obama (Barack\\'s half-brother), unidentified woman, Bernard Obama (Barack\\'s half-brother), Abo Obama (Barack\\'s half-brother).\\\\nBarack Obama, Sr.\\\\nBarack Obama\\'s father, (1936ýýý1982). Government economist in Kenya. In addition to President Obama, Barack Obama Sr. fathered six other sons and a daughter.[37]\\\\nHussein Onyango Obama\\\\nBarack Obama\\'s paternal grandfather (c. 1895ýýý1979);[38] he worked as a mission cook. He joined the British Army during World War I. (One source gives 1870ýýý1975 as his dates of birth and death based on his tombstone reading \\\\\"Mzee Hussein Onyango Obama\\\\\" in his home village.[39] The term \\\\\"mzee\\\\\" is a Kenyan honorific meaning \\\\\"old man\\\\\" or \\\\\"elder.\\\\\") According to his third wife, Sarah, he originally converted to Catholicism, but took the name Hussein when he later converted to Islam; she said he passed the name, not the religion, on to his children.[40]\\\\nHabiba Akumu Obama\\\\nBarack Obama\\'s paternal grandmother, and the second wife of Hussein Onyango Obama. A photograph of her holding her son, Barack Sr, on her lap is on the cover of her grandson\\'s memoirs titled Dreams from my Father.[41]\\\\nSarah Obama\\\\nThird wife of Obama\\'s paternal grandfather, born 1922.[39] Also known, through the addition of her late husband\\'s name, as Sarah Onyango Obama,[42] and sometimes referred to as Sarah Ogwel, Sarah Hussein Obama or Sarah Anyango Obama,[43] she lives in Nyangýýýoma Kogelo village, 30 miles west of western Kenya\\'s main town, Kisumu, on the edge of Lake Victoria.[44][45]\\\\nAlthough not a blood relation, Barack Obama calls her \\\\\"Granny Sarah\\\\\".[43][46] Sarah, who speaks Luo and only a few words of English, communicates with President Obama through an interpreter.\\\\nOn July 4, 2008, she attended the United States Independence Day celebrations in Nairobi, hosted by Michael Ranneberger, the US ambassador in Kenya.[47]\\\\nDuring the campaign she protested attempts to portray Obama as a foreigner to the United States or a Muslim, saying that while Obama\\'s grandfather had been a Muslim, \\\\\"In the world of today, children have different religions from their parents.\\\\\"[40] Sarah Obama herself is \\\\\"a strong believer of the Islamic faith,ýýý in her words.[48]\\\\nKezia Obama\\\\nFirst wife of Barack Obama\\'s father, born c. 1940. She is Barack Obama Sr.\\'s first wife, whom he married in Kenya before studying abroad in the United States. Also known as Kezia Grace Obama.[49][50] She currently lives in Bracknell, Berkshire, England.[51][52] Her sister, Jane, is the \\'Auntie Jane\\' mentioned at the very start of Dreams from My Father when she telephoned President Obama to inform him that his father had been killed in a car accident.[53]\\\\nMalik Obama\\\\nBarack Obama\\'s half-brother, also known as Abongo or Roy, born c. March, 1958,[51] son of Barack Obama, Sr. with his first wife, Kezia.[54] Malik Obama was born and raised in Nairobi, Kenya.[55] He earned a degree in accounting from the University of Nairobi.[56] He met his half-brother for the first time in 1985[55] when Barack flew from Chicago to Washington, D.C. to visit him.[57] Malik and his half-brother Barack were best men at each other\\'s weddings.[55] Barack Obama brought his wife Michelle to Kenya three years later, and they met with Malik again while Barack was introducing Michelle to many other new relatives.[58]\\\\nAlthough much of the Obama family has dispersed throughout Kenya and overseas, most, including Malik Obama, still considered their rural village on the shores of Lake Victoria to be their true home, and feel that those who have left the village have become culturally \\\\\"lost\\\\\".[59] A frequent visitor to the United States,[58] and consultant in Washington, D.C. for several months per year,[55] he nevertheless settled in the Obamas\\' ancestral home, Nyangýýýoma Kogelo, a village of several hundred people that he prefers to the city for its slow pace.[55] He runs a small electronics shop a half hour drive outside of town.[55]\\\\nDuring his brother\\'s presidential campaign, Malik Obama was a spokesman for the extended Obama family in Kenya, dealing with safety and privacy concerns arising from increased attention from the press.[60]\\\\nAbo Obama\\\\nBarack Obama\\'s half-brother, born 1968. International telephone store manager in Kenya.\\\\nAuma Obama\\\\nBarack Obama\\'s half-sister, born c. 1960.[61] As of July 2008, development worker in Kenya.[62] She studied German at the University of Heidelberg from 1981 to 1987. After her graduation in Heidelberg she went on for graduate studies at the University of Bayreuth, which awarded her a PhD in 1996. Her dissertation was about the conception of labor in Germany and its literary reflections.[62] Auma Obama lives in London, and in 1996 married an Englishman, Ian Manners. They have a daughter named Akinyi (b. 1997).[62][verification needed]\\\\nBernard Obama\\\\nBarack Obama\\'s half-brother, born 1970, son of Barack Obama, Sr. and his first wife, Kezia. He had been an auto parts supplier in Nairobi, Kenya, and has one child. Bernard converted to Islam as an adult and has said: \\\\\"Iýýým a Muslim, I donýýýt deny it. My father was raised a Muslim. But itýýýs not an issue. I donýýýt know what all the hullabaloo is about.\\\\\"[63] He currently resides in Bracknell, England, with his mother Kezia.[63]\\\\nRuth Ndesandjo\\\\nBorn Ruth Nidesand, in US c. 1940s, Barack Obama Sr.\\'s third wife and a private kindergarten director in Kenya.[64] Ruth\\'s two sons with Barack Obama, Sr., are Mark and David Ndesandjo; her third son, Joseph Ndesandjo, was born c. 1980 from a subsequent marriage to a Tanzanian.[65][66]\\\\nMark Ndesandjo\\\\nBarack Obama\\'s half-brother, son of Ruth Nidesand and Barack Obama Sr.[67] He runs an Internet company called WorldNexus that advises Chinese corporations how best to reach international customers.[68] Mark graduated from Brown University, studied physics at Stanford University, received an MBA from Emory University, and has lived in Shenzhen, China, since 2002 and is married to a Chinese woman.[69] He is also an accomplished pianist.[70]\\\\nDavid Ndesandjo\\\\nDavid Ndesandjo\\\\nBarack Obama\\'s half-brother (also known as David Opiyo Obama), son of Ruth Nidesand and Barack Obama Sr. Killed in a motorcycle accident.[71]\\\\nGeorge Hussein Onyango Obama\\\\nYoungest half-brother of Barack Obama, born c.1982, son of Barack Obama Sr.[72] and Jael (now a resident of Atlanta, Georgia).[73][74] George was six months old when his father died in an automobile accident, after which he was raised in Nairobi by his mother and a French step-father. He later lived in South Korea for two years while his mother resided there for business reasons.[73] Returning to Kenya, George Obama \\\\\"slept rough for several years,\\\\\" until his aunt gave him a six-by-eight foot corrugated metal shack in the Nairobi, Kenya, slum of Huruma Flats.[73] As of August 2008, Obama was studying to become a mechanic.[73] George received little attention until being featured in an article in the Italian language edition of Vanity Fair in August 2008, which portrayed him as living in poverty, shame, and obscurity.[75] The article quoted Obama as saying that he lived \\\\\"on less than a dollar a month\\\\\" and stated that he \\\\\"does not mention his famous half-brother in conversation\\\\\" out of shame at his poverty.[76] In later interviews George contradicted this picture. In an interview with The Times, Obama \\\\\"said that he was furious at subsequent reports that he had been abandoned by the Obama family and that he was filled with shame about living in a slum.\\\\\"[74] He told The Times, \\\\\"Life in Huruma is good.\\\\\" Obama said that he expects no favors, that he was supported by relatives, and that reports he lived on a dollar a month were \\\\\"all lies by people who donýýýt want my brother to win.ýýý[74] He told The Telegraph that he was inspired by his half-brother.[73] According to Time, George \\\\\"has repeatedly denied...that he feels abandoned by Obama.\\\\\"[77] CNN quoted him as saying, \\\\\"I was brought up well. I live well even now. The magazines, they have exaggerated everything... I think I kind of like it here. There are some challenges, but maybe it is just like where you come from, there are the same challenges.\\\\\"[75] George\\'s reported poverty was seized on by conservative critics of Barack Obama. Columnist Dinesh D\\'Souza solicited donations for George Obama from his readers,[78] while Jerome Corsi planned to give him a $1,000 check during a trip to Kenya (Corsi was expelled from the country by immigration authorities).[77]\\\\nOmar Obama\\\\nHalf-uncle of Barack Obama,[79] born on June 3, 1944 in Nyangýýýoma Kogelo. Oldest son of Onyango and Sarah Obama, resides in Boston, Massachusetts.[citation needed]\\\\nZeituni Onyango\\\\nHalf-aunt of Barack Obama,[80] born May 29, 1952, in Kenya,[81] Onyango is referred to as \\\\\"Aunti Zeituni\\\\\" in President Obama\\'s memoir, Dreams from My Father.[82]\\\\nYusuf Obama\\\\nHalf-uncle of Barack Obama,[79] born c. 1950s in Nyangýýýoma Kogelo; son of Onyango and Sarah Obama.[citation needed]\\\\nSaid Obama\\\\nHalf-uncle of Barack Obama,[79][83] born c. 1950s in Nyangýýýoma Kogelo; son of Onyango and Sarah Obama.[citation needed]\\\\nMichelle Robinson Obama\\'s extended family\\\\nFraser Robinson, Sr. (1884ýýý1936) of South Carolina, shown in an old photo along with his wife, Rosella Cohen Robinson, in its background\\\\nBarack Obama has called his wife Michelle \\\\\"the most quintessentially American woman I know.\\\\\"[3] Her family is of African American heritage, descendents of Africans of the American Colonial Era.[3] Michelle Obama\\'s family history traces back from slavery to Reconstruction to the Great Migration North. Some of Michelle\\'s relatives still reside in South Carolina.\\\\nMichelle\\'s earliest known relative is her great-great grandfather Jim Robinson, born in the 1850s, who was an American slave on the Friendfield plantation in South Carolina. The family believes that after the Civil War he remained a Friendfield plantation sharecropper for the rest of his life and that he was buried there in an unmarked grave.[3]\\\\nJim had two sons, Gabriel and Fraser, Michelle Obama\\'s great-grandfather. Fraser had an arm amputated as a result of a boyhood injury. He worked as a shoemaker, newspaper salesman and in a lumber mill and was married to Rosella Cohen.[3] Carrie Nelson, Gabriel Robinson\\'s daughter, now 80, is the oldest living Robinson and the keeper of family lore.[3]\\\\nAt least three of Michelle Obama\\'s great-uncles served in the military of the United States. One aunt moved to Princeton, New Jersey, where she worked as a maid, and cooked Southern-style meals for Michelle and her brother, Craig, when they were students at Princeton University.\\\\nCraig Robinson\\\\nMichelle Obama\\'s brother, born 1962. He is currently head coach of men\\'s basketball at Oregon State University.[84]\\\\nFraser Robinson III\\\\nMichelle Obama\\'s father, born 1935, died 1991, married Michelle\\'s mother, Marian Shields, in 1960.[85][19] Robinson was a pump worker at the City of Chicago water plant.[3]\\\\nFraser Robinson, Jr.\\\\nMichelle Obama\\'s grandfather was born on August 24, 1912 in Georgetown, South Carolina, and died on November 9, 1996, aged 84. He was a good student and orator, but moved from South Carolina to Chicago to find better work than he could find at home, eventually becoming a worker for the United States Postal Service. He was married to LaVaughn Johnson. When he retired, they moved back to South Carolina.[3]\\\\nCapers C. Funnye Jr.\\\\nMichelle Obama\\'s first cousin once removed: Funnyeýýýs mother, Verdelle Robinson Funnye (born Verdelle Robinson; August 22, 1930 ýýý April 16, 2000) and Michelle Obamaýýýs paternal grandfather, Fraser Robinson Jr., were siblings. One of America\\'s most prominent African American Jews, known for acting as a bridge between mainstream Jewry and African Americans.[86]\\\\nGenealogical charts\\\\nObama ancestry\\\\n16. Opiyo\\\\n8. Obama\\\\n4. Hussein Onyango Obama\\\\n9. Nyaoke\\\\n2. Barack Hussein Obama, Sr.\\\\n5. Habiba Akumu\\\\n1. Barack Hussein Obama II\\\\n24. Jacob William Dunham\\\\n12. Ralph Waldo Emerson Dunham, Sr.\\\\n25. Mary Ann Kearney\\\\n6. Stanley Armour Dunham\\\\n26. Harry Ellington Armour\\\\n13. Ruth Lucille Armour\\\\n27. Gabriella Clark\\\\n3. Stanley Ann Dunham\\\\n28. Charles T. Payne\\\\n14. Rolla Charles Payne\\\\n29. Della L. Wolfley\\\\n7. Madelyn Lee Payne\\\\n30. Thomas Creekmore McCurry\\\\n15. Leona Belle McCurry\\\\n31. Margaret Belle Wright\\\\nFamily trees\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nBarack Obama\\\\nStanley\\\\nDunham\\\\n1918ýýý1992\\\\nMadelyn\\\\nPayne\\\\n1922ýýý2008\\\\nHabiba\\\\nAkumu\\\\nOnyango\\\\nObama\\\\nc.\\xa01895ýýý1979\\\\nSarah\\\\nOgwel\\\\n1922ýýý\\\\nLolo\\\\nSoetoro\\\\n1936ýýý1987\\\\nAnn\\\\nDunham\\\\n1942ýýý1995\\\\nBarack\\\\nObama, Sr. *\\\\n1936ýýý1982\\\\nKezia\\\\nGrace\\\\nc. 1940ýýý\\\\nOmar\\\\nObama\\\\n1944ýýý\\\\nZeituni\\\\nOnyango\\\\n1952ýýý\\\\nYusuf\\\\nObama\\\\nc. 1950sýýý\\\\nSaid\\\\nObama\\\\nc. 1950sýýý\\\\nKonrad\\\\nNg\\\\nc. 1974ýýý\\\\nMaya\\\\nSoetoro\\\\n1970ýýý\\\\nBarack\\\\nObama\\\\n1961ýýý\\\\nMichelle\\\\nRobinson\\\\n1964ýýý\\\\nM. Abongo\\\\nObama\\\\n1958ýýý\\\\nAuma\\\\nObama\\\\nc. 1960ýýý\\\\nAbo\\\\nObama\\\\n1968ýýý\\\\nBernard\\\\nObama\\\\n1970ýýý\\\\nSuhaila\\\\nNg\\\\nc. 2005ýýý\\\\nMalia Ann\\\\nObama\\\\n1998ýýý\\\\nSasha\\\\nObama\\\\n2001ýýý\\\\n* Barack\\\\nObama, Sr.\\'s\\\\nadditional\\\\nRuth\\\\nNidesandjo\\\\nc. 1940sýýý\\\\nJael\\\\nOtieno\\\\nrelationships:\\\\nMark\\\\nNdesandjo\\\\nDavid\\\\nNdesandjo\\\\ndied\\xa0c.\\xa01987\\\\nGeorge\\\\nObama\\\\nc. 1982ýýý\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nFirst Lady Michelle Obama\\\\nFraser\\\\nRobinson, Jr.\\\\nýýý\\\\nSouth Carolina\\\\n(1912ýýý1996)\\\\nSawmill worker\\\\nLaVaughn D.\\\\nJohnson\\\\nýýý\\\\nIllinois\\\\n(1915ýýý2002)\\\\nCapers C.\\\\nFunnye, Jr.\\\\nýýý\\\\n(born c. 1952;\\\\nnephew of\\\\nFraser\\\\nRobinson, Jr.)\\\\nRabbi in Chicago\\\\nFraser\\\\nRobinson III\\\\nýýý\\\\nIllinois\\\\n(1935ýýý1991)\\\\nEnjoyed boxing\\\\nin youth;\\\\nwater plant\\\\npump operator\\\\nin Chicago\\\\nMarian\\\\nShields\\\\nýýý\\\\nIllinois\\\\n(born 1937)\\\\nSecretary at\\\\nSpiegel catalog\\\\nin Chicago;\\\\nU.S.\\'s First\\\\nGrandmother\\\\nCraig\\\\nRobinson\\\\nýýý\\\\nIllinois\\\\n(born 1962)\\\\nHead coach of\\\\nOregon State\\\\nBeavers men\\'s\\\\nbasketball\\\\nMichelle\\\\nRobinson\\\\nýýý\\\\nIllinois\\\\n(born 1964)\\\\nFirst Lady\\\\nof the United States\\\\nDistant relations\\\\nSee also: List of United States Presidents by genealogical relationship\\\\nAccording to genealogists, Barack Obama\\'s distant cousins include the multitude of descendants of his maternal ancestors from all along the early-American Atlantic seaboard as well as paternal, Kenyan relations belonging to the Luo tribe, many descending from a 17th century ancestor named Owiny.[87][88] For example, George W. Bush, the 43rd U.S. president, is the eleventh cousin of Barack Obama.[89] The New York Times science writer Nicholas Wade argues that with eleven generations leading back to their common progenitor, Samuel Hinckley, the relationship between the 43rd President and the 44th President is \\\\\"genetically meaningless\\\\\".[90]\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nSelected genealogical relationships\\\\nBill Hickok\\\\nAccording to Barack Obama\\'s family lore (and confirmed by the New England Historic Genealogical Society), the President and Hickok are sixth cousins, six-times removed.[91]\\\\nRobert Duvall\\\\nGoodnight helped inspire Pulitzer Prize-winning author Larry McMurtry to create a protagonist for his novel series Lonesome Dove: Woodrow Call. In Dove\\'s television novela, Woodrow Call\\'s partner is Gus McCrae, portrayed by Obama\\'s eighth cousin, twice removed, actor Robert Duvall.[92]\\\\nJames Madison\\\\nObama is also distantly related to U.S. Presidents James Madison, Lyndon Johnson, Gerald Ford, and U.S. Vice President Dick Cheney, British Prime Minister Sir Winston Churchill, U.S. Civil War General Robert E. Lee, and actor Brad Pitt.[93][94][95]\\\\nCharles Goodnight\\\\nAccording to Chicago Sun-Times reporter Scott Fornek, another Obama progenitor, Catherine Goodnight, was the grandniece of George Goodnight, who was in turn great-grandfather of famed cattleman Charles Goodnight.[92]\\\\nHarry S. Truman\\\\nActor Duvall is distant cousins with United States President Harry Truman, who\\'s likewise a fourth cousin, four times removed, of Obama\\'s.[92]\\\\nGeorgia O\\'Keeffe\\\\nNotable women Obama is related to include painter Georgia OýýýKeeffe.[93]\\\\nSee also\\\\nU.S. Presidential genealogical relationships\\\\nNew England Historic Genealogical Society\\\\nGary Boyd Roberts\\\\nReferences\\\\n^ Keck, Kristi (4 June 2008). \\\\\"Obama crosses historic milestone\\\\\". CNN. http://news.yahoo.com/s/ap/20081122/ap_on_go_pr_wh/obama_school_12. Retrieved on 2008-11-21.\\\\n^ a b Reitwiesner, William Addams. \\\\\"Ancestry of Barack Obama\\\\\". http://www.wargs.com/political/obama.html. Retrieved on 2008-10-09.\\\\n^ a b c d e f g h Murray, Shailagh (2 October 2008). \\\\\"A Family Tree Rooted In American Soil: Michelle Obama Learns About Her Slave Ancestors, Herself and Her Country\\\\\". The Washington Post: p.\\xa0C01. http://www.washingtonpost.com/wp-dyn/content/article/2008/10/01/AR2008100103169.html. Retrieved on 2008-10-10.\\\\n^ Sheridan, Michael (5 February 2007). \\\\\"Secrets of Obama Family Unlocked\\\\\". Muslim Observer. http://news.newamericamedia.org/news/view_article.html?article_id=ae5895fc29971b172938790be94ab107. Retrieved on 2008-11-21.\\\\n^ RTE news report March, 2007:Obamas Irish family links discovered by ancestry.co.uk\\\\n^ Noveck, Jocelyn (2008-11-07). \\\\\"Deseret News | Obama\\'s young, energetic family harks back to days of Camelot\\\\\". Secure.deseretnews.com. https://secure.deseretnews.com/article/1,5143,705261276,00.html. Retrieved on 2009-01-31.\\\\n^ Liza Mundy, Michelle: A Biography (Simon and Schuster, 2008), p. 129.\\\\n^ \\\\\"Sasha Obama\\\\\". Baltimore Sun. http://www.baltimoresun.com/topic/politics/sasha-obama-PECLB004381.topic. Retrieved on 2009-01-31.\\\\n^ a b Obama, Barack (13 January 2009). \\\\\"\\'What I Want for You--And Every Child in America\\'\\\\\". http://www.parade.com/news/2009/01/barack-obama-letter-to-my-daughters.html.\\\\n^ Sobieraj Westfall, Sandra (23 June 2008). \\\\\"Barack Obama Gives Daughter $1 Allowance a Week\\\\\". People Magazine. http://www.people.com/people/article/0,,20214569_1,00.html. Retrieved on 2008-11-21.\\\\n^ Lester, Will (July 23, 2008). \\\\\"Obama daughters keep hectic schedules of their own\\\\\". Associated Press. http://elections.apnews.com/apelect/db_6911/contentdetail.htm;jsessionid=8314A43012AB5FF1D0697247362D8752?contentguid=H95QubFb&full=true. Retrieved on 2008-08-04.\\\\n^ Hiro, Anne. \\\\\"Obama regrets letting \\\\\"Access Hollywood\\\\\" interview daughters. Won\\'t do it again. MSNBC\\'s Dan Abrams gets the story behind the story. - Lynn Sweet\\\\\". Blogs.suntimes.com. http://blogs.suntimes.com/sweet/2008/07/obama_regrets_letting_access_h.html. Retrieved on 2009-01-31.\\\\n^ Ahmed, Saeed (5 November 2008). \\\\\"Move over Barney, new dog moving into White House\\\\\". CNN. http://www.cnn.com/2008/LIVING/wayoflife/11/05/presidential.pets/index.html. Retrieved on 2008-11-21.\\\\n^ \\\\\"Obama: Getting a dog isn\\'t easy\\\\\". Associated Press. 7 November 2008. http://www.mercurynews.com/ci_10927292. Retrieved on 2008-11-21.\\\\n^ Janice Lloyd (2009-01-12). \\\\\"Obamas down to Labradoodle or Portuguese water dog\\\\\". USA Today. http://www.usatoday.com/news/washington/2009-01-11-obama-dog_N.htm. Retrieved on 2009-01-28.\\\\n^ Swarns, Rachel (21 November 2008). \\\\\"And the Winner Is ýýý Sidwell Friends\\\\\". The New York Times. http://thecaucus.blogs.nytimes.com/2008/11/21/and-the-winner-is-sidwell-friends/. Retrieved on 2008-11-21.\\\\n^ Tolin, Lisa (2009-01-05). \\\\\"Obama girls start school with photographers in tow\\\\\". The Associated Press. http://www.google.com/hostednews/ap/article/ALeqM5g6mv_lkODQMmQdpyIEnr8Zpm5mogD95H8KA80. Retrieved on 2009-01-06.\\\\n^ Taylor Marsh (2008-08-25). \\\\\"Political Analysis, National Security and Breaking News\\\\\". Taylor Marsh. http://www.taylormarsh.com/archives_view.php?id=28286. Retrieved on 2009-01-31.\\\\n^ a b Lia LoBello (2008-01-02). \\\\\"First Families: Radar introduces you to the next president\\'s relatives\\\\\". Radar Online. http://www.radaronline.com/features/2008/07/john_mccain_barack_obama_michelle_cindy_dunham_roberta_wrigh_04.php. Retrieved on 2009-01-28.\\\\n^ Rachel L. Swarns (2009-01-09). \\\\\"Obamaýýýs Mother-in-Law to Move Into the White House\\\\\". The New York Times. http://thecaucus.blogs.nytimes.com/2009/01/09/obamas-mother-in-law-to-move-into-the-white-house/?hp. Retrieved on 2009-01-09.\\\\n^ a b \\\\\"Will Obama mum-in-law make it a family affair in the White House?\\\\\". Agence France Presse. 2008-11-22. http://www.google.com/hostednews/afp/article/ALeqM5gN_i2jrCVkJQgfMbSDRRrNk8U4Sw. Retrieved on 2009-01-09.\\\\n^ Philip Sherwell (2008-2008-11-09). \\\\\"Michelle Obama persuades First Granny to join new White House team\\\\\". The Telegraph (UK). http://www.telegraph.co.uk/news/3407525/Michelle-Obama-persuades-First-Granny-to-join-new-White-House-team.html. Retrieved on 2009-01-09.\\\\n^ \\\\\"ýýýTootýýý: Obama grandmother a force that shaped him\\\\\". via Associated Press. 2008-08-25. http://www.thekansan.com/news/x1311851415/-Toot-Obama-grandmother-a-force-that-shaped-him. Retrieved on 2008-08-29.\\\\n^ \\\\\"CNN: \\\\\"Obama\\'s grandmother dies after battle with cancer\\\\\"\\\\\". http://www.cnn.com/2008/POLITICS/11/03/obama.grandma/index.html. Retrieved on 2008-11-04.\\\\n^ The 89th Infantry Division, United States Holocaust Memorial Museum\\\\n^ a b Obama\\'s great-uncle recalls liberating Nazi camp, Boston.com, July 22, 2008\\\\n^ Major Garrett (2008-05-27). \\\\\"Obama Campaign Scrambles to Correct the Record on Uncle\\'s War Service\\\\\". FOXNews.com. http://elections.foxnews.com/2008/05/27/recollection-of-obama-familys-service-missing-key-details. Retrieved on 2009-01-31.\\\\n^ \\\\\"Democrats salute Obamaýýýs great uncle\\\\\". Jewish Telegraphic Agency. August 28, 2008. http://jta.org/news/article/2008/08/28/110123/obamapayne. Retrieved on 31 January 2009.\\\\n^ Obama Family Tree dgmweb.net\\\\n^ Chicago Sun Times article with her picture\\\\n^ Obama has links to Malaysia\\\\n^ Nolan, Daniel (2008-06-11). \\\\\"Relative: Obama\\'s got \\'a good handle on Canada\\'\\\\\". The Hamilton Spectator. http://www.thespec.com/burlingtonlife/article/384475. Retrieved on 2008-07-03.\\\\n^ Nolan, Daniel (June 11, 2008). \\\\\"Obama\\'s Burlington connection\\\\\". The Hamilton Spectator. http://www.thespec.com/article/384307. Retrieved on 2008-06-21.\\\\n^ Misner, Jason (2008-06-20). \\\\\"Barack Obama was here\\\\\". Burlington Post. http://www.burlingtonpost.com/printarticle/186215. Retrieved on 2008-07-03.\\\\n^ Fornek, Scott (2007-09-09). \\\\\"\\'He helped me find my voice\\'\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545473,BSX-News-wotreehh09.article.\\\\n^ Cooper, Tom (2009-01-20). \\\\\"Keep watch for Obama\\\\\". TheSpec.com. http://www.thespec.com/Opinions/article/499161. Retrieved on 2009-01-28.\\\\n^ Ancestry of Barack Obama\\\\n^ Dreams from My Father, p. 376\\\\n^ a b Kenya: Special Report: Sleepy Little Village Where Obama Traces His Own Roots (Page 2 of 2)\\\\n^ a b \\\\\"Obama\\'s grandma slams \\'untruths\\'\\\\\". Associated Press. 2008-03-05. http://www.usatoday.com/news/world/2008-03-05-obama-kin_N.htm.\\xa0 See also this correction.\\\\n^ \\\\\"Q&A ON THE NEWS\\\\\". Atlanta Journal-Constitution. 2009-02-25. http://www.ajc.com/services/content/metro/stories/2009/02/25/questi0225.html. Retrieved on 2009-02-27.\\\\n^ In Kenya, Barack Obamaýýýs family prays for end to conflict - Times Online\\\\n^ a b Crilly, Rob (February 27, 2008). \\\\\"Dreams from Obama\\'s Grandmother\\\\\". Time Magazine, Inc.. http://www.time.com/time/world/article/0,8599,1717590,00.html?xid=rss-topstories. Retrieved on 2008-07-03.\\\\n^ Pflanz, Mike (2008-01-11). \\\\\"Barack Obama\\'s Kenyan relatives keep faith\\\\\". Daily Telegraph. http://www.telegraph.co.uk/news/main.jhtml?xml=/news/2008/01/09/wuspols1009.xml.\\\\n^ Fornek, Scott (2007-09-09). \\\\\"Sarah Obama - \\'Sparkling, laughing eyes\\'\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545459,BSX-News-wotreeu09.article.\\\\n^ \\\\\"Barack Obama in Kenya\\\\\". CNN. http://www.youtube.com/watch?v=Ikg6gj71U9k.\\\\n^ Daily Nation, July 8, 2008: Obama granny\\'s day out with envoys and top politicians\\\\n^ \\\\\"A Candidate, His Minister and the Search for Faith\\\\\". New York Times. 2007-04-30. http://www.nytimes.com/2007/04/30/us/politics/30obama.html?_r=2&pagewanted=all&oref=slogin&oref=slogin.\\\\n^ \\\\\"Kenya: All Obama kin to spend voting day in Kogelo\\\\\". afrika.no. 2008-10-27. http://www.afrika.no/Detailed/17321.html. Retrieved on 2009-01-31.\\\\n^ Cohen, Roger (2008-03-06). \\\\\"The Obamas of the World - New York Times\\\\\". Nytimes.com. http://www.nytimes.com/2008/03/06/opinion/06cohen.html. Retrieved on 2009-01-31.\\\\n^ a b Sanderson, Elizabeth (2008-01-06). \\\\\"Barack Obama\\'s stepmother living in Bracknell reveals the close bond with him ... and his mother\\\\\". Daily Mail. http://www.dailymail.co.uk/news/article-506338/Barack-Obamas-stepmother-living-Bracknell-reveals-close-bond---mother.html.\\\\n^ Lindsay, Anna (2009-01-20). \\\\\"Barack\\'s bingo-loving stepmother\\\\\". BBC News. http://news.bbc.co.uk/1/hi/england/berkshire/7834368.stm.\\\\n^ http://www.newvision.co.ug/D/8/26/666733\\\\n^ jpt (2008-06-18). \\\\\"From the Fact Check Desk: What Did Obama\\'s Half-Brother Say About Obama\\'s Background\\\\\". ABC News. http://blogs.abcnews.com/politicalpunch/2008/06/from-the-fact-c.html.\\\\n^ a b c d e f Maliti, Tom (2004-10-26). \\\\\"Obama\\'s Brother Chooses Life in Slow Lane\\\\\". The Associated Press. http://www.msnbc.msn.com/id/6333496.\\\\n^ Obama, Dreams from my Father, 2004, p. 265.\\\\n^ Obama, Dreams from my Father, 2004, p. 262.\\\\n^ a b *Oywa, John (2004-08-15). \\\\\"Sleepy Little Village Where Obama Traces His Own Roots\\\\\". The Daily Nation. http://allafrica.com/stories/200408160533.html.\\\\n^ Philip Ochieng (2004-11-01). \\\\\"From Home Squared to the US Senate: How Barack Obama Was Lost and Found\\\\\". The East African. http://www.nationmedia.com/EastAfrican/01112004/Features/PA2-11.html. Retrieved on 2008-03-23.\\\\n^ Warah, Rasna (2008-06-09). \\\\\"We cannot lay claims on Obama; he\\'s not one of us - Obama in this world\\\\\". Daily Nation. http://www.nationmedia.com. Retrieved on 2008-07-10.\\\\n^ Scott Fornek (2007-09-09). \\\\\"AUMA OBAMA: \\'Her restlessness, her independence\\'\\\\\". Chicago Sun Times. http://www.suntimes.com/news/politics/obama/familytree/545465,BSX-News-wotreew09.article. Retrieved on 2008-03-23.\\\\n^ a b c Gathmann, Florian; Gregor Peter Schmitz, Jochen Schýýnmann (July 24, 2008). \\\\\"Studentin in der Bundesrepublik: Wie Auma Obama mit Deutschland haderte\\\\\" (in German). Spiegel Online. http://www.spiegel.de/politik/ausland/0,1518,567286,00.html. Retrieved on 2008-07-24.\\\\n^ a b Harvey, Oliver (07-26 2008). \\\\\"Obama\\'s brother is in Bracknell\\\\\". The Sun. http://www.thesun.co.uk/sol/homepage/news/the_real_american_idol/article1472877.ece. Retrieved on 2008-10-06.\\\\n^ \\\\\"Madari Kindergarten\\\\\". http://www.madarikindergarten.com/.\\\\n^ \\\\\"Welcome To MedWeek San Antonio 2007\\\\\". Medweeksa.org. http://www.medweeksa.org/awardwinners/techfirm.htm. Retrieved on 2009-01-31.\\\\n^ \\\\\"PIDE - Partners for International Development & Education Inc\\\\\". Pideafrica.org. http://pideafrica.org/aboutus.htm. Retrieved on 2009-01-31.\\\\n^ Barack Obamaýýýs brother pushes Chinese imports on US - Times Online\\\\n^ Obama half-brother runs Internet company in China\\\\n^ Roger Cohen (2008-03-17). \\\\\"Obama\\'s Brother in China\\\\\". The New York Times. http://www.nytimes.com/2008/03/17/opinion/29cohen.html. Retrieved on 2008-03-23.\\\\n^ \\\\\"Youku Buzz (daily)\\xa0ýý Blog Archive\\xa0ýý Barack Obamaýýýs Half-Brother in Concert\\\\\". Buzz.youku.com. 2009-01-18. http://buzz.youku.com/2009/01/18/barack-obamas-half-brother-in-concert/. Retrieved on 2009-01-31.\\\\n^ jaketapper (2008-07-28). \\\\\"Political Punch: Barack Obama\\'s Branch-y Family Tree\\\\\". Blogs.abcnews.com. http://blogs.abcnews.com/politicalpunch/2008/07/barack-obamas-1.html. Retrieved on 2009-01-31.\\\\n^ Fornek, Scott (September 9, 2007). \\\\\"HALF-BROTHER GEORGE: \\'I would be there for him\\'\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545447,BSX-News-wotreecc09.stng. Retrieved on 2008-08-04.\\\\n^ a b c d e Pflanz, Mike (August 21, 2008). \\\\\"Barack Obama is my inspiration, says lost brother\\\\\". The Daily Telegraph. http://www.telegraph.co.uk/news/newstopics/uselection2008/barackobama/2595688/Barack-Obama-is-my-inspiration-says-lost-brother.html. Retrieved on 2008-08-23.\\\\n^ a b c Crilly, Rob (August 22, 2008). \\\\\"Life is good in my Nairobi slum, says Barack Obama\\'s younger brother\\\\\". The Times. http://www.timesonline.co.uk/tol/news/world/us_and_americas/us_elections/article4583353.ece. Retrieved on 2008-08-23.\\\\n^ a b McKenzie, David (2008-08-23). \\\\\"Behind the Scenes: Meet George Obama\\\\\". CNN. http://www.cnn.com/2008/POLITICS/08/22/bts.obama.brother/. Retrieved on 2008-10-26.\\\\n^ Pisa, Nick (August 20, 2008). \\\\\"Barack Obama\\'s \\'lost\\' brother found in Kenya\\\\\". Daily Telegraph. http://www.telegraph.co.uk/news/newstopics/uselection2008/barackobama/2590614/Barack-Obamas-lost-brother-found-in-Kenya.html. Retrieved on 2008-08-20.\\\\n^ a b Wadhams, Nick (2008-10-07). \\\\\"Corsi in Kenya: Obama\\'s Nation Boots Obama Nation Author\\\\\". TIME. http://www.time.com/time/world/article/0,8599,1847965,00.html?imw=Y. Retrieved on 2009-01-31.\\\\n^ by Dinesh D\\'Souza. \\\\\"Dinesh D\\'Souza\\xa0: George Obama, Start Packing\\\\\". Townhall.com. http://townhall.com/columnists/DineshDSouza/2008/09/22/george_obama,_start_packing. Retrieved on 2009-01-31.\\\\n^ a b c \\\\\"The Obama Family Tree\\\\\" (PDF). Chicago Sun-Times. September 9, 2007. http://www.suntimes.com/images/cds/MP3/obamatree.pdf. Retrieved on 2008-11-23.\\\\n^ First read, MSNBC\\\\n^ \\\\\"Barack Obama\\'s aunt found living in rundown public housing estate | The Australian\\\\\". Theaustralian.news.com.au. 2008-10-31. http://www.theaustralian.news.com.au/story/0,25197,24578185-5017121,00.html. Retrieved on 2009-01-31.\\\\n^ Boston Housing Authority ýýýflabbergasteredýýý Barack Obamaýýýs aunt living in Southie\\\\n^ Kilner, Derek (2008-11-05). \\\\\"Kenya Celebrates President Obama as Native Son\\\\\". Voice Of America. http://www.voanews.com/english/archive/2008-11/2008-11-05-voa45.cfm. Retrieved on 2008-12-24.\\\\n^ \\\\\"Oregon State University Beavers: Craig Robinson bio\\\\\". http://www.osubeavers.com/ViewArticle.dbml?SPSID=106239&SPID=1954&DB_OEM_ID=4700&ATCLID=1436883&Q_SEASON=2008. Retrieved on 2008-08-21.\\\\n^ \\\\\"RootsWeb\\'s WorldConnect Project: Dowling Family Genealogy\\\\\". Wc.rootsweb.ancestry.com. http://wc.rootsweb.ancestry.com/cgi-bin/igm.cgi?op=GET&db=dowfam3&id=I105855. Retrieved on 2009-01-31.\\\\n^ Weiss, Anthony (September 2, 2008). \\\\\"Michelle Obama Has a Rabbi in Her Family\\\\\". The Forward. http://www.forward.com/articles/14121/. Retrieved on 2008-10-09.\\\\n^ Gary Boyd Roberts. \\\\\"Notes on the Ancestry of Senator Barack Hussein Obama, Jr\\\\\". New England Historic Genealogical Society. http://www.newenglandancestors.org/research/services/articles_ancestry_barack_obama.asp. Retrieved on 2009-01-31.\\\\n^ Scott Fornek (2007-09-09). \\\\\"Obama Family Tree\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545463,BSX-News-wotreer09.stng. Retrieved on 2009-01-31.\\\\n^ \\\\\"Obama-Bush (family tree)\\\\\" (PDF). New England Historic Genealogical Society. http://www.newenglandancestors.org/pdfs/obama_bush.pdf. Retrieved on 2009-01-31.\\\\n^ Wade, Nicholas (2007-10-21). \\\\\"Barack Obama - Dick Cheney - Ancestry and Genealogy - Washington - New York Times\\\\\". Nytimes.com. http://www.nytimes.com/2007/10/21/weekinreview/21basic.html. Retrieved on 2009-01-31.\\\\n^ Eastman, Dick (2008-07-30). \\\\\"Barack Obama is Related to Wild Bill Hickok\\\\\". Blog.eogn.com. http://blog.eogn.com/eastmans_online_genealogy/2008/07/barack-obama-is.html. Retrieved on 2009-01-31.\\\\n^ a b c Scott Fornek (2007-09-09). \\\\\"Obama Family Tree\\\\\". Suntimes.com. http://www.suntimes.com/news/politics/obama/familytree/545441,BSX-News-wotreec09.stng. Retrieved on 2009-01-31.\\\\n^ a b \\\\\"Notes on the Ancestry of Senator Barack Hussein Obama, Jr\\\\\". New England Historic Genealogical Society. 2002-08-14. http://www.newenglandancestors.org/research/services/articles_ancestry_barack_obama.asp. Retrieved on 2009-01-31.\\\\n^ \\\\\"Obama, Clinton and McCain have some famous relations\\\\\". HeraldNet - AP. 2008-03-26. http://www.heraldnet.com/article/20080326/NEWS02/151343661. Retrieved on 2009-01-31.\\\\n^ \\\\\"Barack Obama and Joe Biden: The Change We Need\\\\\". My.barackobama.com. 2008-07-31. http://my.barackobama.com/page/community/post/williambrehm/gG5TVR. Retrieved on 2009-01-31.\\\\nExternal links\\\\nBarack Obama\\'s Family Tree - Photo Essays - TIME\\\\n\\\\\"Though Obama Had to Leave to Find Himself, It Is Hawaii That Made His Rise Possible,\\\\\" by David Maraniss\\\\nBarack Obama\\'s Branch-y Family Tree by Jake Tapper\\\\n\\\\\"Obama Family Tree\\\\\" series, by Scott Fornek\\\\n\\\\\"Six Degrees of Barack Obama\\\\\"\\\\n\\\\\"Notes on the Ancestry of Senator Barack Hussein Obama, Jr.,\\\\\" by Gary Boyd Roberts\\\\n\\\\\"Obama, Clinton and McCain have some famous relations,\\\\\" by The Associated Press\\\\n\\\\\"Obama\\'s Patriotic Family Tree,\\\\\" by Bill Brehm\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nBarack Obama\\\\nPresidency\\\\nTransition\\xa0ýý Inauguration\\xa0ýý Timeline\\xa0ýý Cabinet\\xa0ýý Judiciary\\xa0ýý Foreign policy\\xa0ýý First 100 days\\\\nEarly life and\\\\npolitical career\\\\nPublic image\\xa0ýý Illinois Senate career\\xa0ýý 2004 Democratic National Convention\\xa0ýý U.S. Senate election in Illinois\\xa0ýý U.S. Senate career\\xa0ýý Presidential primary campaign\\xa0ýý ObamaýýýBiden 2008\\xa0ýý Electoral history\\xa0ýý Political positions\\\\nBooks\\\\nDreams from My Father\\xa0ýý The Audacity of Hope\\\\nSpeeches\\\\nThe Audacity of Hope\\xa0ýý A More Perfect Union\\xa0ýý Change Has Come to America\\xa0ýý 2009 speech to joint session of Congress\\\\nFamily\\\\nMichelle Obama\\xa0ýý Barack Obama, Sr.\\xa0ýý Ann Dunham\\xa0ýý Lolo Soetoro (stepfather)\\xa0ýý Maya Soetoro-Ng (half-sister)\\xa0ýý Marian Robinson (mother-in-law)\\xa0ýý Stanley Armour Dunham (grandfather)\\xa0ýý Madelyn Dunham (grandmother)\\xa0ýý Extended family\\xa0ýý Family tree\\\\nRetrieved from \\\\\"http://en.wikipedia.org/wiki/Family_of_Barack_Obama#Malia_and_Sasha_Obama\\\\\"\\\\nCategories: Obama family | African American history | African American families | Luo Kenyans | People of mixed Black African-European ethnicity | African Americans | Asian Americans | Dutch Americans | English Americans | French Americans | German-Americans | Irish-Americans | Indonesian Americans | Kenyan-Americans | Scottish-Americans | Chinese Canadians | People of mixed Asian-European ethnicity | American families | First Families of the United States | Family treesHidden categories: Wikipedia semi-protected pages | Wikipedia indefinitely move-protected pages | All pages needing cleanup | Wikipedia articles needing factual verification since October 2008 | All pages needing factual verification | All articles with unsourced statements | Articles with unsourced statements since November 2008\\\\nViews\\\\nArticle\\\\nDiscussion\\\\nView source\\\\nHistory\\\\nPersonal tools\\\\nLog in / create account\\\\nNavigation\\\\nMain page\\\\nContents\\\\nFeatured content\\\\nCurrent events\\\\nRandom article\\\\nSearch\\\\nInteraction\\\\nAbout Wikipedia\\\\nCommunity portal\\\\nRecent changes\\\\nContact Wikipedia\\\\nDonate to Wikipedia\\\\nHelp\\\\nToolbox\\\\nWhat links here\\\\nRelated changes\\\\nUpload file\\\\nSpecial pages\\\\nPrintable version Permanent linkCite this page\\\\nLanguages\\\\nBahasa Indonesia\\\\nSvenska\\\\nýýýýýý\\\\nýýýýýý\\\\nThis page was last modified on 14 March 2009, at 05:50.\\\\nAll text is available under the terms of the GNU Free Documentation License. (See Copyrights for details.)\\\\nWikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a U.S. registered 501(c)(3) tax-deductible nonprofit charity.\\\\nPrivacy policy\\\\nAbout Wikipedia\\\\nDisclaimers\\n[DOC 2] Family of Barack Obama - Wikipedia, the free encyclopedia\\\\nFamily of Barack Obama\\\\nFrom Wikipedia, the free encyclopedia\\\\n(Redirected from Ruth Obama)\\\\nJump to: navigation, search\\\\nObama Family\\\\nPresident Barack Obama, First Lady Michelle, and daughters Malia and Sasha wave to the crowd after his inaugural address Jan. 20, 2009, on the west steps of the U.S. Capitol.[1]\\\\nCurrent region\\\\nWashington, DC\\\\nInformation\\\\nPlace of origin\\\\nUnited States\\\\nNotable members\\\\nBarack Obama, Michelle Obama, Ann Dunham, Barack Obama, Sr., etc.\\\\nConnected families\\\\nRobinson, Dunham, Soetoro, Ng\\\\nThis article is part of a series about\\\\nBarack Obama\\\\nBackground \\xa0ýý Illinois Senate \\xa0ýý U.S. Senate\\\\nPolitical positions\\xa0ýý Public image\\xa0ýý Family\\\\n2008 primaries\\xa0ýý ObamaýýýBiden campaign\\\\nTransition\\xa0ýý Inauguration\\xa0ýý Electoral history\\\\nPresidency (Timeline, First 100 days)\\\\nMalia, Michelle and Sasha on stage at the 2008 Democratic National Convention\\\\nThe Family of Barack Obama is an extended clan of African American, English, Indonesian, and Kenyan (Luo) heritage known through the writings and political career of Barack Obama, the President of the United States of America,[2][3][4][5] and other reports. His immediate family is the First Family of the United States. The Obamas are the first First Family of African American descent in the United States and the youngest to enter the White House since the Kennedys. One columnist wrote, ýýýObama\\'s young, energetic family harks back to days of Camelot.ýýý[6]\\\\nContents\\\\n1 Immediate family\\\\n2 Extended family - maternal relations\\\\n3 Extended family - paternal relations\\\\n4 Michelle Robinson Obama\\'s extended family\\\\n5 Genealogical charts\\\\n5.1 Obama ancestry\\\\n5.2 Family trees\\\\n6 Distant relations\\\\n7 See also\\\\n8 References\\\\n9 External links\\\\nImmediate family\\\\nMichelle Obama\\\\nMichelle Obama, nýýe Robinson, the wife of Barack Obama, was born on January 17, 1964 in Chicago, Illinois. She is a lawyer and was a University of Chicago Hospital Vice-President. She is the First Lady of the United States.\\\\nMalia Obama and Sasha Obama\\\\nBarack and Michelle Obama have two daughters: Malia Ann (pronounced /mýýýýliýýýý/), born in 1998,[7] and Natasha (known as Sasha) /ýýsýýýýýýýý/), born in 2001. Sasha is the youngest child to reside in the White House since John F. Kennedy, Jr, arrived as an infant in 1961.[8]\\\\nBefore his inauguration, President Obama published an open letter to his daughters in Parade magazine, describing what he wants for them and every child in America: \\\\\"to grow up in a world with no limits on your dreams and no achievements beyond your reach, and to grow into compassionate, committed women who will help build that world.\\\\\"[9]\\\\nWhile living in Chicago, they kept busy schedules, as the Associated Press reports: \\\\\"soccer, dance and drama for Malia, gymnastics and tap for Sasha, piano and tennis for both.\\\\\"[10][11] In July 2008, the family gave an interview to the television series Access Hollywood; Obama later said they regretted allowing the children to be included.[12]\\\\nIn his victory speech on the night of his election, President Obama repeated his promise to Sasha and Malia to get a puppy to take with them to the White House.[13] However the selection of a dog has been slow because Malia is allergic to animal dander;[14] the president subsequently said that the choice has been narrowed down to either a labradoodle or Portuguese Water Dog, and they are hoping to find a shelter animal.[15]\\\\nMalia and Sasha attend the private Sidwell Friends School in Washington, DC, the same school as attended by Chelsea Clinton, Tricia Nixon Cox, and Archibald Roosevelt, and currently the grandchildren of Vice President Joe Biden.[16] The Obama girls began classes there on January 5, 2009.[17] While in Chicago, both attended the private University of Chicago Laboratory School.\\\\nMarian Shields Robinson\\\\nMichelle Obama\\'s mother (birthname Marian Shields, born July 1937), now widowed, married Michelle\\'s father, Fraser Robinson, in 1960.[18][19] Robinson was formerly a secretary at Spiegel catalog and a bank. While Michelle and Barack Obama were campaigning in 2008, Robinson tended the Obama\\'s young children and she intends to do the same while in Washington, DC. Robinson is currently living in the White House itself as part of the First Family;[20] she is the first live-in grandmother there since Elivera M. Doud during the Eisenhower administration.[21] Some media outlets have dubbed Robinson the \\\\\"First Granny\\\\\".[22][21]\\\\nExtended family - maternal relations\\\\nRight-to-left: Barack Obama and Maya Soetoro with their mother Ann Dunham and grandfather Stanley Dunham in Hawaii (early 1970s)\\\\nWikinews has related news:\\\\nBarack Obama elected 44th President of the United States\\\\nGrandmother of Barack Obama dies at 86\\\\nMadelyn Dunham with her daughter Ann\\\\nAccording to Barack Obama\\'s Dreams from My Father, his great-grandmother Leona McCurry was part Native American, which Obama believed Leona held as a \\\\\"source of considerable shame\\\\\" and \\\\\"blanched whenever someone mentioned the subject and hoped to carry the secret to her grave\\\\\"; whereas McCurry\\'s daughter (Obama\\'s maternal grandmother) \\\\\"would turn her head in profile to show off her beaked nose, which along with a pair of jet-black eyes, was offered as proof of Cherokee blood.\\\\\"[23] To date, no concrete evidence has surfaced of Cherokee heritage. Obama\\'s maternal heritage consists mostly of English ancestry, with much smaller amounts of German, Irish, Scottish, Welsh, Swiss, and French ancestry.[2]\\\\nAnn Dunham\\\\nMother of Barack Obama born in 1942, died in 1995. Birthname Stanley Ann Dunham. Anthropologist in Hawaii and Indonesia.\\\\nMadelyn Lee Payne Dunham\\\\nBarack Obama\\'s maternal grandmother, born in 1922 and died on November 2, 2008.[24] She was a bank vice president in Hawaii. Obama said that when he was a child, his grandmother \\\\\"read me the opening lines of the Declaration of Independence and told me about the men and women who marched for equality because they believed those words put to paper two centuries ago should mean something.\\\\\"[9]\\\\nStanley Armour Dunham\\\\nGrandfather of Barack Obama, born 1918, died 1992. World War II U.S. Army sergeant, furniture salesman in Hawaii.\\\\nCharles T. Payne\\\\nGreat-uncle of Barack Obama, younger brother of Madelyn Dunham, born 1925. Served during World War II in the U.S. Army 89th Infantry Division.[25] Obama has often described Payne\\'s role in liberating Ohrdruf forced labor camp.[26] There was brief media attention when Obama mistakenly identified the camp as Auschwitz during the campaign.[27] Payne appeared in the visitor\\'s gallery at the Democratic National Convention in Denver, Colorado, when his great-nephew was nominated for President.[28] He was the assistant director of the University of Chicago\\'s Library.[26]\\\\nMaya Soetoro-Ng\\\\nHalf-sister of Barack Obama, born August 15, 1970, in Jakarta, Indonesia.[29] She is married to Konrad Ng, with whom she has a daughter, Suhaila. Maya Soetoro-Ng is a teacher in Hawaii.\\\\nKonrad Ng\\\\nBrother-in-law of Barack Obama, born 1974. A Canadian whose parents are Malaysian Chinese immigrants, he is an assistant professor at the University of Hawaii\\'s Academy of Creative Media.[30] His parents are from Kudat and Sandakan, two small towns in Sabah, Malaysia, and he was born and raised in Burlington, Ontario.[31] He married Maya Soetoro-Ng at the end of 2003 in Hawaii.[32] They have one daughter, Suhaila.[33][34][35] Konrad Ng is now a US citizen.[36]\\\\nLolo Soetoro\\\\nStepfather of Barack Obama, born in Indonesia 1936, died 1987.\\\\nExtended family - paternal relations\\\\nThe Obamas are members of the Luo, Kenya\\'s third-largest ethnic group, which is part of a larger family of ethnic groups, collectively also known as Luo. This group belongs to the Eastern Sudanic branch of the Nilo-Saharan phylum. The Obama family is largely concentrated in the western province of Nyanza.\\\\nFront row (left to right): Auma Obama (Barack\\'s half-sister), Kezia Obama (Barack\\'s step-mother), Sarah Hussein Onyango Obama (third wife of Barack\\'s paternal grandfather), Zeituni Onyango (Barack\\'s aunt)\\\\nBack row (left to right): Said Obama (Barack\\'s uncle), Barack Obama, Abongo [Roy] Obama (Barack\\'s half-brother), unidentified woman, Bernard Obama (Barack\\'s half-brother), Abo Obama (Barack\\'s half-brother).\\\\nBarack Obama, Sr.\\\\nBarack Obama\\'s father, (1936ýýý1982). Government economist in Kenya. In addition to President Obama, Barack Obama Sr. fathered six other sons and a daughter.[37]\\\\nHussein Onyango Obama\\\\nBarack Obama\\'s paternal grandfather (c. 1895ýýý1979);[38] he worked as a mission cook. He joined the British Army during World War I. (One source gives 1870ýýý1975 as his dates of birth and death based on his tombstone reading \\\\\"Mzee Hussein Onyango Obama\\\\\" in his home village.[39] The term \\\\\"mzee\\\\\" is a Kenyan honorific meaning \\\\\"old man\\\\\" or \\\\\"elder.\\\\\") According to his third wife, Sarah, he originally converted to Catholicism, but took the name Hussein when he later converted to Islam; she said he passed the name, not the religion, on to his children.[40]\\\\nHabiba Akumu Obama\\\\nBarack Obama\\'s paternal grandmother, and the second wife of Hussein Onyango Obama. A photograph of her holding her son, Barack Sr, on her lap is on the cover of her grandson\\'s memoirs titled Dreams from my Father.[41]\\\\nSarah Obama\\\\nThird wife of Obama\\'s paternal grandfather, born 1922.[39] Also known, through the addition of her late husband\\'s name, as Sarah Onyango Obama,[42] and sometimes referred to as Sarah Ogwel, Sarah Hussein Obama or Sarah Anyango Obama,[43] she lives in Nyangýýýoma Kogelo village, 30 miles west of western Kenya\\'s main town, Kisumu, on the edge of Lake Victoria.[44][45]\\\\nAlthough not a blood relation, Barack Obama calls her \\\\\"Granny Sarah\\\\\".[43][46] Sarah, who speaks Luo and only a few words of English, communicates with President Obama through an interpreter.\\\\nOn July 4, 2008, she attended the United States Independence Day celebrations in Nairobi, hosted by Michael Ranneberger, the US ambassador in Kenya.[47]\\\\nDuring the campaign she protested attempts to portray Obama as a foreigner to the United States or a Muslim, saying that while Obama\\'s grandfather had been a Muslim, \\\\\"In the world of today, children have different religions from their parents.\\\\\"[40] Sarah Obama herself is \\\\\"a strong believer of the Islamic faith,ýýý in her words.[48]\\\\nKezia Obama\\\\nFirst wife of Barack Obama\\'s father, born c. 1940. She is Barack Obama Sr.\\'s first wife, whom he married in Kenya before studying abroad in the United States. Also known as Kezia Grace Obama.[49][50] She currently lives in Bracknell, Berkshire, England.[51][52] Her sister, Jane, is the \\'Auntie Jane\\' mentioned at the very start of Dreams from My Father when she telephoned President Obama to inform him that his father had been killed in a car accident.[53]\\\\nMalik Obama\\\\nBarack Obama\\'s half-brother, also known as Abongo or Roy, born c. March, 1958,[51] son of Barack Obama, Sr. with his first wife, Kezia.[54] Malik Obama was born and raised in Nairobi, Kenya.[55] He earned a degree in accounting from the University of Nairobi.[56] He met his half-brother for the first time in 1985[55] when Barack flew from Chicago to Washington, D.C. to visit him.[57] Malik and his half-brother Barack were best men at each other\\'s weddings.[55] Barack Obama brought his wife Michelle to Kenya three years later, and they met with Malik again while Barack was introducing Michelle to many other new relatives.[58]\\\\nAlthough much of the Obama family has dispersed throughout Kenya and overseas, most, including Malik Obama, still considered their rural village on the shores of Lake Victoria to be their true home, and feel that those who have left the village have become culturally \\\\\"lost\\\\\".[59] A frequent visitor to the United States,[58] and consultant in Washington, D.C. for several months per year,[55] he nevertheless settled in the Obamas\\' ancestral home, Nyangýýýoma Kogelo, a village of several hundred people that he prefers to the city for its slow pace.[55] He runs a small electronics shop a half hour drive outside of town.[55]\\\\nDuring his brother\\'s presidential campaign, Malik Obama was a spokesman for the extended Obama family in Kenya, dealing with safety and privacy concerns arising from increased attention from the press.[60]\\\\nAbo Obama\\\\nBarack Obama\\'s half-brother, born 1968. International telephone store manager in Kenya.\\\\nAuma Obama\\\\nBarack Obama\\'s half-sister, born c. 1960.[61] As of July 2008, development worker in Kenya.[62] She studied German at the University of Heidelberg from 1981 to 1987. After her graduation in Heidelberg she went on for graduate studies at the University of Bayreuth, which awarded her a PhD in 1996. Her dissertation was about the conception of labor in Germany and its literary reflections.[62] Auma Obama lives in London, and in 1996 married an Englishman, Ian Manners. They have a daughter named Akinyi (b. 1997).[62][verification needed]\\\\nBernard Obama\\\\nBarack Obama\\'s half-brother, born 1970, son of Barack Obama, Sr. and his first wife, Kezia. He had been an auto parts supplier in Nairobi, Kenya, and has one child. Bernard converted to Islam as an adult and has said: \\\\\"Iýýým a Muslim, I donýýýt deny it. My father was raised a Muslim. But itýýýs not an issue. I donýýýt know what all the hullabaloo is about.\\\\\"[63] He currently resides in Bracknell, England, with his mother Kezia.[63]\\\\nRuth Ndesandjo\\\\nBorn Ruth Nidesand, in US c. 1940s, Barack Obama Sr.\\'s third wife and a private kindergarten director in Kenya.[64] Ruth\\'s two sons with Barack Obama, Sr., are Mark and David Ndesandjo; her third son, Joseph Ndesandjo, was born c. 1980 from a subsequent marriage to a Tanzanian.[65][66]\\\\nMark Ndesandjo\\\\nBarack Obama\\'s half-brother, son of Ruth Nidesand and Barack Obama Sr.[67] He runs an Internet company called WorldNexus that advises Chinese corporations how best to reach international customers.[68] Mark graduated from Brown University, studied physics at Stanford University, received an MBA from Emory University, and has lived in Shenzhen, China, since 2002 and is married to a Chinese woman.[69] He is also an accomplished pianist.[70]\\\\nDavid Ndesandjo\\\\nDavid Ndesandjo\\\\nBarack Obama\\'s half-brother (also known as David Opiyo Obama), son of Ruth Nidesand and Barack Obama Sr. Killed in a motorcycle accident.[71]\\\\nGeorge Hussein Onyango Obama\\\\nYoungest half-brother of Barack Obama, born c.1982, son of Barack Obama Sr.[72] and Jael (now a resident of Atlanta, Georgia).[73][74] George was six months old when his father died in an automobile accident, after which he was raised in Nairobi by his mother and a French step-father. He later lived in South Korea for two years while his mother resided there for business reasons.[73] Returning to Kenya, George Obama \\\\\"slept rough for several years,\\\\\" until his aunt gave him a six-by-eight foot corrugated metal shack in the Nairobi, Kenya, slum of Huruma Flats.[73] As of August 2008, Obama was studying to become a mechanic.[73] George received little attention until being featured in an article in the Italian language edition of Vanity Fair in August 2008, which portrayed him as living in poverty, shame, and obscurity.[75] The article quoted Obama as saying that he lived \\\\\"on less than a dollar a month\\\\\" and stated that he \\\\\"does not mention his famous half-brother in conversation\\\\\" out of shame at his poverty.[76] In later interviews George contradicted this picture. In an interview with The Times, Obama \\\\\"said that he was furious at subsequent reports that he had been abandoned by the Obama family and that he was filled with shame about living in a slum.\\\\\"[74] He told The Times, \\\\\"Life in Huruma is good.\\\\\" Obama said that he expects no favors, that he was supported by relatives, and that reports he lived on a dollar a month were \\\\\"all lies by people who donýýýt want my brother to win.ýýý[74] He told The Telegraph that he was inspired by his half-brother.[73] According to Time, George \\\\\"has repeatedly denied...that he feels abandoned by Obama.\\\\\"[77] CNN quoted him as saying, \\\\\"I was brought up well. I live well even now. The magazines, they have exaggerated everything... I think I kind of like it here. There are some challenges, but maybe it is just like where you come from, there are the same challenges.\\\\\"[75] George\\'s reported poverty was seized on by conservative critics of Barack Obama. Columnist Dinesh D\\'Souza solicited donations for George Obama from his readers,[78] while Jerome Corsi planned to give him a $1,000 check during a trip to Kenya (Corsi was expelled from the country by immigration authorities).[77]\\\\nOmar Obama\\\\nHalf-uncle of Barack Obama,[79] born on June 3, 1944 in Nyangýýýoma Kogelo. Oldest son of Onyango and Sarah Obama, resides in Boston, Massachusetts.[citation needed]\\\\nZeituni Onyango\\\\nHalf-aunt of Barack Obama,[80] born May 29, 1952, in Kenya,[81] Onyango is referred to as \\\\\"Aunti Zeituni\\\\\" in President Obama\\'s memoir, Dreams from My Father.[82]\\\\nYusuf Obama\\\\nHalf-uncle of Barack Obama,[79] born c. 1950s in Nyangýýýoma Kogelo; son of Onyango and Sarah Obama.[citation needed]\\\\nSaid Obama\\\\nHalf-uncle of Barack Obama,[79][83] born c. 1950s in Nyangýýýoma Kogelo; son of Onyango and Sarah Obama.[citation needed]\\\\nMichelle Robinson Obama\\'s extended family\\\\nFraser Robinson, Sr. (1884ýýý1936) of South Carolina, shown in an old photo along with his wife, Rosella Cohen Robinson, in its background\\\\nBarack Obama has called his wife Michelle \\\\\"the most quintessentially American woman I know.\\\\\"[3] Her family is of African American heritage, descendents of Africans of the American Colonial Era.[3] Michelle Obama\\'s family history traces back from slavery to Reconstruction to the Great Migration North. Some of Michelle\\'s relatives still reside in South Carolina.\\\\nMichelle\\'s earliest known relative is her great-great grandfather Jim Robinson, born in the 1850s, who was an American slave on the Friendfield plantation in South Carolina. The family believes that after the Civil War he remained a Friendfield plantation sharecropper for the rest of his life and that he was buried there in an unmarked grave.[3]\\\\nJim had two sons, Gabriel and Fraser, Michelle Obama\\'s great-grandfather. Fraser had an arm amputated as a result of a boyhood injury. He worked as a shoemaker, newspaper salesman and in a lumber mill and was married to Rosella Cohen.[3] Carrie Nelson, Gabriel Robinson\\'s daughter, now 80, is the oldest living Robinson and the keeper of family lore.[3]\\\\nAt least three of Michelle Obama\\'s great-uncles served in the military of the United States. One aunt moved to Princeton, New Jersey, where she worked as a maid, and cooked Southern-style meals for Michelle and her brother, Craig, when they were students at Princeton University.\\\\nCraig Robinson\\\\nMichelle Obama\\'s brother, born 1962. He is currently head coach of men\\'s basketball at Oregon State University.[84]\\\\nFraser Robinson III\\\\nMichelle Obama\\'s father, born 1935, died 1991, married Michelle\\'s mother, Marian Shields, in 1960.[85][19] Robinson was a pump worker at the City of Chicago water plant.[3]\\\\nFraser Robinson, Jr.\\\\nMichelle Obama\\'s grandfather was born on August 24, 1912 in Georgetown, South Carolina, and died on November 9, 1996, aged 84. He was a good student and orator, but moved from South Carolina to Chicago to find better work than he could find at home, eventually becoming a worker for the United States Postal Service. He was married to LaVaughn Johnson. When he retired, they moved back to South Carolina.[3]\\\\nCapers C. Funnye Jr.\\\\nMichelle Obama\\'s first cousin once removed: Funnyeýýýs mother, Verdelle Robinson Funnye (born Verdelle Robinson; August 22, 1930 ýýý April 16, 2000) and Michelle Obamaýýýs paternal grandfather, Fraser Robinson Jr., were siblings. One of America\\'s most prominent African American Jews, known for acting as a bridge between mainstream Jewry and African Americans.[86]\\\\nGenealogical charts\\\\nObama ancestry\\\\n16. Opiyo\\\\n8. Obama\\\\n4. Hussein Onyango Obama\\\\n9. Nyaoke\\\\n2. Barack Hussein Obama, Sr.\\\\n5. Habiba Akumu\\\\n1. Barack Hussein Obama II\\\\n24. Jacob William Dunham\\\\n12. Ralph Waldo Emerson Dunham, Sr.\\\\n25. Mary Ann Kearney\\\\n6. Stanley Armour Dunham\\\\n26. Harry Ellington Armour\\\\n13. Ruth Lucille Armour\\\\n27. Gabriella Clark\\\\n3. Stanley Ann Dunham\\\\n28. Charles T. Payne\\\\n14. Rolla Charles Payne\\\\n29. Della L. Wolfley\\\\n7. Madelyn Lee Payne\\\\n30. Thomas Creekmore McCurry\\\\n15. Leona Belle McCurry\\\\n31. Margaret Belle Wright\\\\nFamily trees\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nBarack Obama\\\\nStanley\\\\nDunham\\\\n1918ýýý1992\\\\nMadelyn\\\\nPayne\\\\n1922ýýý2008\\\\nHabiba\\\\nAkumu\\\\nOnyango\\\\nObama\\\\nc.\\xa01895ýýý1979\\\\nSarah\\\\nOgwel\\\\n1922ýýý\\\\nLolo\\\\nSoetoro\\\\n1936ýýý1987\\\\nAnn\\\\nDunham\\\\n1942ýýý1995\\\\nBarack\\\\nObama, Sr. *\\\\n1936ýýý1982\\\\nKezia\\\\nGrace\\\\nc. 1940ýýý\\\\nOmar\\\\nObama\\\\n1944ýýý\\\\nZeituni\\\\nOnyango\\\\n1952ýýý\\\\nYusuf\\\\nObama\\\\nc. 1950sýýý\\\\nSaid\\\\nObama\\\\nc. 1950sýýý\\\\nKonrad\\\\nNg\\\\nc. 1974ýýý\\\\nMaya\\\\nSoetoro\\\\n1970ýýý\\\\nBarack\\\\nObama\\\\n1961ýýý\\\\nMichelle\\\\nRobinson\\\\n1964ýýý\\\\nM. Abongo\\\\nObama\\\\n1958ýýý\\\\nAuma\\\\nObama\\\\nc. 1960ýýý\\\\nAbo\\\\nObama\\\\n1968ýýý\\\\nBernard\\\\nObama\\\\n1970ýýý\\\\nSuhaila\\\\nNg\\\\nc. 2005ýýý\\\\nMalia Ann\\\\nObama\\\\n1998ýýý\\\\nSasha\\\\nObama\\\\n2001ýýý\\\\n* Barack\\\\nObama, Sr.\\'s\\\\nadditional\\\\nRuth\\\\nNidesandjo\\\\nc. 1940sýýý\\\\nJael\\\\nOtieno\\\\nrelationships:\\\\nMark\\\\nNdesandjo\\\\nDavid\\\\nNdesandjo\\\\ndied\\xa0c.\\xa01987\\\\nGeorge\\\\nObama\\\\nc. 1982ýýý\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nFirst Lady Michelle Obama\\\\nFraser\\\\nRobinson, Jr.\\\\nýýý\\\\nSouth Carolina\\\\n(1912ýýý1996)\\\\nSawmill worker\\\\nLaVaughn D.\\\\nJohnson\\\\nýýý\\\\nIllinois\\\\n(1915ýýý2002)\\\\nCapers C.\\\\nFunnye, Jr.\\\\nýýý\\\\n(born c. 1952;\\\\nnephew of\\\\nFraser\\\\nRobinson, Jr.)\\\\nRabbi in Chicago\\\\nFraser\\\\nRobinson III\\\\nýýý\\\\nIllinois\\\\n(1935ýýý1991)\\\\nEnjoyed boxing\\\\nin youth;\\\\nwater plant\\\\npump operator\\\\nin Chicago\\\\nMarian\\\\nShields\\\\nýýý\\\\nIllinois\\\\n(born 1937)\\\\nSecretary at\\\\nSpiegel catalog\\\\nin Chicago;\\\\nU.S.\\'s First\\\\nGrandmother\\\\nCraig\\\\nRobinson\\\\nýýý\\\\nIllinois\\\\n(born 1962)\\\\nHead coach of\\\\nOregon State\\\\nBeavers men\\'s\\\\nbasketball\\\\nMichelle\\\\nRobinson\\\\nýýý\\\\nIllinois\\\\n(born 1964)\\\\nFirst Lady\\\\nof the United States\\\\nDistant relations\\\\nSee also: List of United States Presidents by genealogical relationship\\\\nAccording to genealogists, Barack Obama\\'s distant cousins include the multitude of descendants of his maternal ancestors from all along the early-American Atlantic seaboard as well as paternal, Kenyan relations belonging to the Luo tribe, many descending from a 17th century ancestor named Owiny.[87][88] For example, George W. Bush, the 43rd U.S. president, is the eleventh cousin of Barack Obama.[89] The New York Times science writer Nicholas Wade argues that with eleven generations leading back to their common progenitor, Samuel Hinckley, the relationship between the 43rd President and the 44th President is \\\\\"genetically meaningless\\\\\".[90]\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nSelected genealogical relationships\\\\nBill Hickok\\\\nAccording to Barack Obama\\'s family lore (and confirmed by the New England Historic Genealogical Society), the President and Hickok are sixth cousins, six-times removed.[91]\\\\nRobert Duvall\\\\nGoodnight helped inspire Pulitzer Prize-winning author Larry McMurtry to create a protagonist for his novel series Lonesome Dove: Woodrow Call. In Dove\\'s television novela, Woodrow Call\\'s partner is Gus McCrae, portrayed by Obama\\'s eighth cousin, twice removed, actor Robert Duvall.[92]\\\\nJames Madison\\\\nObama is also distantly related to U.S. Presidents James Madison, Lyndon Johnson, Gerald Ford, and U.S. Vice President Dick Cheney, British Prime Minister Sir Winston Churchill, U.S. Civil War General Robert E. Lee, and actor Brad Pitt.[93][94][95]\\\\nCharles Goodnight\\\\nAccording to Chicago Sun-Times reporter Scott Fornek, another Obama progenitor, Catherine Goodnight, was the grandniece of George Goodnight, who was in turn great-grandfather of famed cattleman Charles Goodnight.[92]\\\\nHarry S. Truman\\\\nActor Duvall is distant cousins with United States President Harry Truman, who\\'s likewise a fourth cousin, four times removed, of Obama\\'s.[92]\\\\nGeorgia O\\'Keeffe\\\\nNotable women Obama is related to include painter Georgia OýýýKeeffe.[93]\\\\nSee also\\\\nU.S. Presidential genealogical relationships\\\\nNew England Historic Genealogical Society\\\\nGary Boyd Roberts\\\\nReferences\\\\n^ Keck, Kristi (4 June 2008). \\\\\"Obama crosses historic milestone\\\\\". CNN. http://news.yahoo.com/s/ap/20081122/ap_on_go_pr_wh/obama_school_12. Retrieved on 2008-11-21.\\\\n^ a b Reitwiesner, William Addams. \\\\\"Ancestry of Barack Obama\\\\\". http://www.wargs.com/political/obama.html. Retrieved on 2008-10-09.\\\\n^ a b c d e f g h Murray, Shailagh (2 October 2008). \\\\\"A Family Tree Rooted In American Soil: Michelle Obama Learns About Her Slave Ancestors, Herself and Her Country\\\\\". The Washington Post: p.\\xa0C01. http://www.washingtonpost.com/wp-dyn/content/article/2008/10/01/AR2008100103169.html. Retrieved on 2008-10-10.\\\\n^ Sheridan, Michael (5 February 2007). \\\\\"Secrets of Obama Family Unlocked\\\\\". Muslim Observer. http://news.newamericamedia.org/news/view_article.html?article_id=ae5895fc29971b172938790be94ab107. Retrieved on 2008-11-21.\\\\n^ RTE news report March, 2007:Obamas Irish family links discovered by ancestry.co.uk\\\\n^ Noveck, Jocelyn (2008-11-07). \\\\\"Deseret News | Obama\\'s young, energetic family harks back to days of Camelot\\\\\". Secure.deseretnews.com. https://secure.deseretnews.com/article/1,5143,705261276,00.html. Retrieved on 2009-01-31.\\\\n^ Liza Mundy, Michelle: A Biography (Simon and Schuster, 2008), p. 129.\\\\n^ \\\\\"Sasha Obama\\\\\". Baltimore Sun. http://www.baltimoresun.com/topic/politics/sasha-obama-PECLB004381.topic. Retrieved on 2009-01-31.\\\\n^ a b Obama, Barack (13 January 2009). \\\\\"\\'What I Want for You--And Every Child in America\\'\\\\\". http://www.parade.com/news/2009/01/barack-obama-letter-to-my-daughters.html.\\\\n^ Sobieraj Westfall, Sandra (23 June 2008). \\\\\"Barack Obama Gives Daughter $1 Allowance a Week\\\\\". People Magazine. http://www.people.com/people/article/0,,20214569_1,00.html. Retrieved on 2008-11-21.\\\\n^ Lester, Will (July 23, 2008). \\\\\"Obama daughters keep hectic schedules of their own\\\\\". Associated Press. http://elections.apnews.com/apelect/db_6911/contentdetail.htm;jsessionid=8314A43012AB5FF1D0697247362D8752?contentguid=H95QubFb&full=true. Retrieved on 2008-08-04.\\\\n^ Hiro, Anne. \\\\\"Obama regrets letting \\\\\"Access Hollywood\\\\\" interview daughters. Won\\'t do it again. MSNBC\\'s Dan Abrams gets the story behind the story. - Lynn Sweet\\\\\". Blogs.suntimes.com. http://blogs.suntimes.com/sweet/2008/07/obama_regrets_letting_access_h.html. Retrieved on 2009-01-31.\\\\n^ Ahmed, Saeed (5 November 2008). \\\\\"Move over Barney, new dog moving into White House\\\\\". CNN. http://www.cnn.com/2008/LIVING/wayoflife/11/05/presidential.pets/index.html. Retrieved on 2008-11-21.\\\\n^ \\\\\"Obama: Getting a dog isn\\'t easy\\\\\". Associated Press. 7 November 2008. http://www.mercurynews.com/ci_10927292. Retrieved on 2008-11-21.\\\\n^ Janice Lloyd (2009-01-12). \\\\\"Obamas down to Labradoodle or Portuguese water dog\\\\\". USA Today. http://www.usatoday.com/news/washington/2009-01-11-obama-dog_N.htm. Retrieved on 2009-01-28.\\\\n^ Swarns, Rachel (21 November 2008). \\\\\"And the Winner Is ýýý Sidwell Friends\\\\\". The New York Times. http://thecaucus.blogs.nytimes.com/2008/11/21/and-the-winner-is-sidwell-friends/. Retrieved on 2008-11-21.\\\\n^ Tolin, Lisa (2009-01-05). \\\\\"Obama girls start school with photographers in tow\\\\\". The Associated Press. http://www.google.com/hostednews/ap/article/ALeqM5g6mv_lkODQMmQdpyIEnr8Zpm5mogD95H8KA80. Retrieved on 2009-01-06.\\\\n^ Taylor Marsh (2008-08-25). \\\\\"Political Analysis, National Security and Breaking News\\\\\". Taylor Marsh. http://www.taylormarsh.com/archives_view.php?id=28286. Retrieved on 2009-01-31.\\\\n^ a b Lia LoBello (2008-01-02). \\\\\"First Families: Radar introduces you to the next president\\'s relatives\\\\\". Radar Online. http://www.radaronline.com/features/2008/07/john_mccain_barack_obama_michelle_cindy_dunham_roberta_wrigh_04.php. Retrieved on 2009-01-28.\\\\n^ Rachel L. Swarns (2009-01-09). \\\\\"Obamaýýýs Mother-in-Law to Move Into the White House\\\\\". The New York Times. http://thecaucus.blogs.nytimes.com/2009/01/09/obamas-mother-in-law-to-move-into-the-white-house/?hp. Retrieved on 2009-01-09.\\\\n^ a b \\\\\"Will Obama mum-in-law make it a family affair in the White House?\\\\\". Agence France Presse. 2008-11-22. http://www.google.com/hostednews/afp/article/ALeqM5gN_i2jrCVkJQgfMbSDRRrNk8U4Sw. Retrieved on 2009-01-09.\\\\n^ Philip Sherwell (2008-2008-11-09). \\\\\"Michelle Obama persuades First Granny to join new White House team\\\\\". The Telegraph (UK). http://www.telegraph.co.uk/news/3407525/Michelle-Obama-persuades-First-Granny-to-join-new-White-House-team.html. Retrieved on 2009-01-09.\\\\n^ \\\\\"ýýýTootýýý: Obama grandmother a force that shaped him\\\\\". via Associated Press. 2008-08-25. http://www.thekansan.com/news/x1311851415/-Toot-Obama-grandmother-a-force-that-shaped-him. Retrieved on 2008-08-29.\\\\n^ \\\\\"CNN: \\\\\"Obama\\'s grandmother dies after battle with cancer\\\\\"\\\\\". http://www.cnn.com/2008/POLITICS/11/03/obama.grandma/index.html. Retrieved on 2008-11-04.\\\\n^ The 89th Infantry Division, United States Holocaust Memorial Museum\\\\n^ a b Obama\\'s great-uncle recalls liberating Nazi camp, Boston.com, July 22, 2008\\\\n^ Major Garrett (2008-05-27). \\\\\"Obama Campaign Scrambles to Correct the Record on Uncle\\'s War Service\\\\\". FOXNews.com. http://elections.foxnews.com/2008/05/27/recollection-of-obama-familys-service-missing-key-details. Retrieved on 2009-01-31.\\\\n^ \\\\\"Democrats salute Obamaýýýs great uncle\\\\\". Jewish Telegraphic Agency. August 28, 2008. http://jta.org/news/article/2008/08/28/110123/obamapayne. Retrieved on 31 January 2009.\\\\n^ Obama Family Tree dgmweb.net\\\\n^ Chicago Sun Times article with her picture\\\\n^ Obama has links to Malaysia\\\\n^ Nolan, Daniel (2008-06-11). \\\\\"Relative: Obama\\'s got \\'a good handle on Canada\\'\\\\\". The Hamilton Spectator. http://www.thespec.com/burlingtonlife/article/384475. Retrieved on 2008-07-03.\\\\n^ Nolan, Daniel (June 11, 2008). \\\\\"Obama\\'s Burlington connection\\\\\". The Hamilton Spectator. http://www.thespec.com/article/384307. Retrieved on 2008-06-21.\\\\n^ Misner, Jason (2008-06-20). \\\\\"Barack Obama was here\\\\\". Burlington Post. http://www.burlingtonpost.com/printarticle/186215. Retrieved on 2008-07-03.\\\\n^ Fornek, Scott (2007-09-09). \\\\\"\\'He helped me find my voice\\'\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545473,BSX-News-wotreehh09.article.\\\\n^ Cooper, Tom (2009-01-20). \\\\\"Keep watch for Obama\\\\\". TheSpec.com. http://www.thespec.com/Opinions/article/499161. Retrieved on 2009-01-28.\\\\n^ Ancestry of Barack Obama\\\\n^ Dreams from My Father, p. 376\\\\n^ a b Kenya: Special Report: Sleepy Little Village Where Obama Traces His Own Roots (Page 2 of 2)\\\\n^ a b \\\\\"Obama\\'s grandma slams \\'untruths\\'\\\\\". Associated Press. 2008-03-05. http://www.usatoday.com/news/world/2008-03-05-obama-kin_N.htm.\\xa0 See also this correction.\\\\n^ \\\\\"Q&A ON THE NEWS\\\\\". Atlanta Journal-Constitution. 2009-02-25. http://www.ajc.com/services/content/metro/stories/2009/02/25/questi0225.html. Retrieved on 2009-02-27.\\\\n^ In Kenya, Barack Obamaýýýs family prays for end to conflict - Times Online\\\\n^ a b Crilly, Rob (February 27, 2008). \\\\\"Dreams from Obama\\'s Grandmother\\\\\". Time Magazine, Inc.. http://www.time.com/time/world/article/0,8599,1717590,00.html?xid=rss-topstories. Retrieved on 2008-07-03.\\\\n^ Pflanz, Mike (2008-01-11). \\\\\"Barack Obama\\'s Kenyan relatives keep faith\\\\\". Daily Telegraph. http://www.telegraph.co.uk/news/main.jhtml?xml=/news/2008/01/09/wuspols1009.xml.\\\\n^ Fornek, Scott (2007-09-09). \\\\\"Sarah Obama - \\'Sparkling, laughing eyes\\'\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545459,BSX-News-wotreeu09.article.\\\\n^ \\\\\"Barack Obama in Kenya\\\\\". CNN. http://www.youtube.com/watch?v=Ikg6gj71U9k.\\\\n^ Daily Nation, July 8, 2008: Obama granny\\'s day out with envoys and top politicians\\\\n^ \\\\\"A Candidate, His Minister and the Search for Faith\\\\\". New York Times. 2007-04-30. http://www.nytimes.com/2007/04/30/us/politics/30obama.html?_r=2&pagewanted=all&oref=slogin&oref=slogin.\\\\n^ \\\\\"Kenya: All Obama kin to spend voting day in Kogelo\\\\\". afrika.no. 2008-10-27. http://www.afrika.no/Detailed/17321.html. Retrieved on 2009-01-31.\\\\n^ Cohen, Roger (2008-03-06). \\\\\"The Obamas of the World - New York Times\\\\\". Nytimes.com. http://www.nytimes.com/2008/03/06/opinion/06cohen.html. Retrieved on 2009-01-31.\\\\n^ a b Sanderson, Elizabeth (2008-01-06). \\\\\"Barack Obama\\'s stepmother living in Bracknell reveals the close bond with him ... and his mother\\\\\". Daily Mail. http://www.dailymail.co.uk/news/article-506338/Barack-Obamas-stepmother-living-Bracknell-reveals-close-bond---mother.html.\\\\n^ Lindsay, Anna (2009-01-20). \\\\\"Barack\\'s bingo-loving stepmother\\\\\". BBC News. http://news.bbc.co.uk/1/hi/england/berkshire/7834368.stm.\\\\n^ http://www.newvision.co.ug/D/8/26/666733\\\\n^ jpt (2008-06-18). \\\\\"From the Fact Check Desk: What Did Obama\\'s Half-Brother Say About Obama\\'s Background\\\\\". ABC News. http://blogs.abcnews.com/politicalpunch/2008/06/from-the-fact-c.html.\\\\n^ a b c d e f Maliti, Tom (2004-10-26). \\\\\"Obama\\'s Brother Chooses Life in Slow Lane\\\\\". The Associated Press. http://www.msnbc.msn.com/id/6333496.\\\\n^ Obama, Dreams from my Father, 2004, p. 265.\\\\n^ Obama, Dreams from my Father, 2004, p. 262.\\\\n^ a b *Oywa, John (2004-08-15). \\\\\"Sleepy Little Village Where Obama Traces His Own Roots\\\\\". The Daily Nation. http://allafrica.com/stories/200408160533.html.\\\\n^ Philip Ochieng (2004-11-01). \\\\\"From Home Squared to the US Senate: How Barack Obama Was Lost and Found\\\\\". The East African. http://www.nationmedia.com/EastAfrican/01112004/Features/PA2-11.html. Retrieved on 2008-03-23.\\\\n^ Warah, Rasna (2008-06-09). \\\\\"We cannot lay claims on Obama; he\\'s not one of us - Obama in this world\\\\\". Daily Nation. http://www.nationmedia.com. Retrieved on 2008-07-10.\\\\n^ Scott Fornek (2007-09-09). \\\\\"AUMA OBAMA: \\'Her restlessness, her independence\\'\\\\\". Chicago Sun Times. http://www.suntimes.com/news/politics/obama/familytree/545465,BSX-News-wotreew09.article. Retrieved on 2008-03-23.\\\\n^ a b c Gathmann, Florian; Gregor Peter Schmitz, Jochen Schýýnmann (July 24, 2008). \\\\\"Studentin in der Bundesrepublik: Wie Auma Obama mit Deutschland haderte\\\\\" (in German). Spiegel Online. http://www.spiegel.de/politik/ausland/0,1518,567286,00.html. Retrieved on 2008-07-24.\\\\n^ a b Harvey, Oliver (07-26 2008). \\\\\"Obama\\'s brother is in Bracknell\\\\\". The Sun. http://www.thesun.co.uk/sol/homepage/news/the_real_american_idol/article1472877.ece. Retrieved on 2008-10-06.\\\\n^ \\\\\"Madari Kindergarten\\\\\". http://www.madarikindergarten.com/.\\\\n^ \\\\\"Welcome To MedWeek San Antonio 2007\\\\\". Medweeksa.org. http://www.medweeksa.org/awardwinners/techfirm.htm. Retrieved on 2009-01-31.\\\\n^ \\\\\"PIDE - Partners for International Development & Education Inc\\\\\". Pideafrica.org. http://pideafrica.org/aboutus.htm. Retrieved on 2009-01-31.\\\\n^ Barack Obamaýýýs brother pushes Chinese imports on US - Times Online\\\\n^ Obama half-brother runs Internet company in China\\\\n^ Roger Cohen (2008-03-17). \\\\\"Obama\\'s Brother in China\\\\\". The New York Times. http://www.nytimes.com/2008/03/17/opinion/29cohen.html. Retrieved on 2008-03-23.\\\\n^ \\\\\"Youku Buzz (daily)\\xa0ýý Blog Archive\\xa0ýý Barack Obamaýýýs Half-Brother in Concert\\\\\". Buzz.youku.com. 2009-01-18. http://buzz.youku.com/2009/01/18/barack-obamas-half-brother-in-concert/. Retrieved on 2009-01-31.\\\\n^ jaketapper (2008-07-28). \\\\\"Political Punch: Barack Obama\\'s Branch-y Family Tree\\\\\". Blogs.abcnews.com. http://blogs.abcnews.com/politicalpunch/2008/07/barack-obamas-1.html. Retrieved on 2009-01-31.\\\\n^ Fornek, Scott (September 9, 2007). \\\\\"HALF-BROTHER GEORGE: \\'I would be there for him\\'\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545447,BSX-News-wotreecc09.stng. Retrieved on 2008-08-04.\\\\n^ a b c d e Pflanz, Mike (August 21, 2008). \\\\\"Barack Obama is my inspiration, says lost brother\\\\\". The Daily Telegraph. http://www.telegraph.co.uk/news/newstopics/uselection2008/barackobama/2595688/Barack-Obama-is-my-inspiration-says-lost-brother.html. Retrieved on 2008-08-23.\\\\n^ a b c Crilly, Rob (August 22, 2008). \\\\\"Life is good in my Nairobi slum, says Barack Obama\\'s younger brother\\\\\". The Times. http://www.timesonline.co.uk/tol/news/world/us_and_americas/us_elections/article4583353.ece. Retrieved on 2008-08-23.\\\\n^ a b McKenzie, David (2008-08-23). \\\\\"Behind the Scenes: Meet George Obama\\\\\". CNN. http://www.cnn.com/2008/POLITICS/08/22/bts.obama.brother/. Retrieved on 2008-10-26.\\\\n^ Pisa, Nick (August 20, 2008). \\\\\"Barack Obama\\'s \\'lost\\' brother found in Kenya\\\\\". Daily Telegraph. http://www.telegraph.co.uk/news/newstopics/uselection2008/barackobama/2590614/Barack-Obamas-lost-brother-found-in-Kenya.html. Retrieved on 2008-08-20.\\\\n^ a b Wadhams, Nick (2008-10-07). \\\\\"Corsi in Kenya: Obama\\'s Nation Boots Obama Nation Author\\\\\". TIME. http://www.time.com/time/world/article/0,8599,1847965,00.html?imw=Y. Retrieved on 2009-01-31.\\\\n^ by Dinesh D\\'Souza. \\\\\"Dinesh D\\'Souza\\xa0: George Obama, Start Packing\\\\\". Townhall.com. http://townhall.com/columnists/DineshDSouza/2008/09/22/george_obama,_start_packing. Retrieved on 2009-01-31.\\\\n^ a b c \\\\\"The Obama Family Tree\\\\\" (PDF). Chicago Sun-Times. September 9, 2007. http://www.suntimes.com/images/cds/MP3/obamatree.pdf. Retrieved on 2008-11-23.\\\\n^ First read, MSNBC\\\\n^ \\\\\"Barack Obama\\'s aunt found living in rundown public housing estate | The Australian\\\\\". Theaustralian.news.com.au. 2008-10-31. http://www.theaustralian.news.com.au/story/0,25197,24578185-5017121,00.html. Retrieved on 2009-01-31.\\\\n^ Boston Housing Authority ýýýflabbergasteredýýý Barack Obamaýýýs aunt living in Southie\\\\n^ Kilner, Derek (2008-11-05). \\\\\"Kenya Celebrates President Obama as Native Son\\\\\". Voice Of America. http://www.voanews.com/english/archive/2008-11/2008-11-05-voa45.cfm. Retrieved on 2008-12-24.\\\\n^ \\\\\"Oregon State University Beavers: Craig Robinson bio\\\\\". http://www.osubeavers.com/ViewArticle.dbml?SPSID=106239&SPID=1954&DB_OEM_ID=4700&ATCLID=1436883&Q_SEASON=2008. Retrieved on 2008-08-21.\\\\n^ \\\\\"RootsWeb\\'s WorldConnect Project: Dowling Family Genealogy\\\\\". Wc.rootsweb.ancestry.com. http://wc.rootsweb.ancestry.com/cgi-bin/igm.cgi?op=GET&db=dowfam3&id=I105855. Retrieved on 2009-01-31.\\\\n^ Weiss, Anthony (September 2, 2008). \\\\\"Michelle Obama Has a Rabbi in Her Family\\\\\". The Forward. http://www.forward.com/articles/14121/. Retrieved on 2008-10-09.\\\\n^ Gary Boyd Roberts. \\\\\"Notes on the Ancestry of Senator Barack Hussein Obama, Jr\\\\\". New England Historic Genealogical Society. http://www.newenglandancestors.org/research/services/articles_ancestry_barack_obama.asp. Retrieved on 2009-01-31.\\\\n^ Scott Fornek (2007-09-09). \\\\\"Obama Family Tree\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545463,BSX-News-wotreer09.stng. Retrieved on 2009-01-31.\\\\n^ \\\\\"Obama-Bush (family tree)\\\\\" (PDF). New England Historic Genealogical Society. http://www.newenglandancestors.org/pdfs/obama_bush.pdf. Retrieved on 2009-01-31.\\\\n^ Wade, Nicholas (2007-10-21). \\\\\"Barack Obama - Dick Cheney - Ancestry and Genealogy - Washington - New York Times\\\\\". Nytimes.com. http://www.nytimes.com/2007/10/21/weekinreview/21basic.html. Retrieved on 2009-01-31.\\\\n^ Eastman, Dick (2008-07-30). \\\\\"Barack Obama is Related to Wild Bill Hickok\\\\\". Blog.eogn.com. http://blog.eogn.com/eastmans_online_genealogy/2008/07/barack-obama-is.html. Retrieved on 2009-01-31.\\\\n^ a b c Scott Fornek (2007-09-09). \\\\\"Obama Family Tree\\\\\". Suntimes.com. http://www.suntimes.com/news/politics/obama/familytree/545441,BSX-News-wotreec09.stng. Retrieved on 2009-01-31.\\\\n^ a b \\\\\"Notes on the Ancestry of Senator Barack Hussein Obama, Jr\\\\\". New England Historic Genealogical Society. 2002-08-14. http://www.newenglandancestors.org/research/services/articles_ancestry_barack_obama.asp. Retrieved on 2009-01-31.\\\\n^ \\\\\"Obama, Clinton and McCain have some famous relations\\\\\". HeraldNet - AP. 2008-03-26. http://www.heraldnet.com/article/20080326/NEWS02/151343661. Retrieved on 2009-01-31.\\\\n^ \\\\\"Barack Obama and Joe Biden: The Change We Need\\\\\". My.barackobama.com. 2008-07-31. http://my.barackobama.com/page/community/post/williambrehm/gG5TVR. Retrieved on 2009-01-31.\\\\nExternal links\\\\nBarack Obama\\'s Family Tree - Photo Essays - TIME\\\\n\\\\\"Though Obama Had to Leave to Find Himself, It Is Hawaii That Made His Rise Possible,\\\\\" by David Maraniss\\\\nBarack Obama\\'s Branch-y Family Tree by Jake Tapper\\\\n\\\\\"Obama Family Tree\\\\\" series, by Scott Fornek\\\\n\\\\\"Six Degrees of Barack Obama\\\\\"\\\\n\\\\\"Notes on the Ancestry of Senator Barack Hussein Obama, Jr.,\\\\\" by Gary Boyd Roberts\\\\n\\\\\"Obama, Clinton and McCain have some famous relations,\\\\\" by The Associated Press\\\\n\\\\\"Obama\\'s Patriotic Family Tree,\\\\\" by Bill Brehm\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nBarack Obama\\\\nPresidency\\\\nTransition\\xa0ýý Inauguration\\xa0ýý Timeline\\xa0ýý Cabinet\\xa0ýý Judiciary\\xa0ýý Foreign policy\\xa0ýý First 100 days\\\\nEarly life and\\\\npolitical career\\\\nPublic image\\xa0ýý Illinois Senate career\\xa0ýý 2004 Democratic National Convention\\xa0ýý U.S. Senate election in Illinois\\xa0ýý U.S. Senate career\\xa0ýý Presidential primary campaign\\xa0ýý ObamaýýýBiden 2008\\xa0ýý Electoral history\\xa0ýý Political positions\\\\nBooks\\\\nDreams from My Father\\xa0ýý The Audacity of Hope\\\\nSpeeches\\\\nThe Audacity of Hope\\xa0ýý A More Perfect Union\\xa0ýý Change Has Come to America\\xa0ýý 2009 speech to joint session of Congress\\\\nFamily\\\\nMichelle Obama\\xa0ýý Barack Obama, Sr.\\xa0ýý Ann Dunham\\xa0ýý Lolo Soetoro (stepfather)\\xa0ýý Maya Soetoro-Ng (half-sister)\\xa0ýý Marian Robinson (mother-in-law)\\xa0ýý Stanley Armour Dunham (grandfather)\\xa0ýý Madelyn Dunham (grandmother)\\xa0ýý Extended family\\xa0ýý Family tree\\\\nRetrieved from \\\\\"http://en.wikipedia.org/wiki/Family_of_Barack_Obama#Ruth_Ndesandjo\\\\\"\\\\nCategories: Obama family | African American history | African American families | Luo Kenyans | People of mixed Black African-European ethnicity | African Americans | Asian Americans | Dutch Americans | English Americans | French Americans | German-Americans | Irish-Americans | Indonesian Americans | Kenyan-Americans | Scottish-Americans | Chinese Canadians | People of mixed Asian-European ethnicity | American families | First Families of the United States | Family treesHidden categories: Wikipedia semi-protected pages | Wikipedia indefinitely move-protected pages | All pages needing cleanup | Wikipedia articles needing factual verification since October 2008 | All pages needing factual verification | All articles with unsourced statements | Articles with unsourced statements since November 2008\\\\nViews\\\\nArticle\\\\nDiscussion\\\\nView source\\\\nHistory\\\\nPersonal tools\\\\nLog in / create account\\\\nNavigation\\\\nMain page\\\\nContents\\\\nFeatured content\\\\nCurrent events\\\\nRandom article\\\\nSearch\\\\nInteraction\\\\nAbout Wikipedia\\\\nCommunity portal\\\\nRecent changes\\\\nContact Wikipedia\\\\nDonate to Wikipedia\\\\nHelp\\\\nToolbox\\\\nWhat links here\\\\nRelated changes\\\\nUpload file\\\\nSpecial pages\\\\nPrintable version Permanent linkCite this page\\\\nLanguages\\\\nBahasa Indonesia\\\\nSvenska\\\\nýýýýýý\\\\nýýýýýý\\\\nThis page was last modified on 14 March 2009, at 05:50.\\\\nAll text is available under the terms of the GNU Free Documentation License. (See Copyrights for details.)\\\\nWikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a U.S. registered 501(c)(3) tax-deductible nonprofit charity.\\\\nPrivacy policy\\\\nAbout Wikipedia\\\\nDisclaimers\\n[DOC 3] Family of Barack Obama - Wikipedia, the free encyclopedia\\\\nFamily of Barack Obama\\\\nFrom Wikipedia, the free encyclopedia\\\\n(Redirected from Soetoro)\\\\nJump to: navigation, search\\\\nObama Family\\\\nPresident Barack Obama, First Lady Michelle, and daughters Malia and Sasha wave to the crowd after his inaugural address Jan. 20, 2009, on the west steps of the U.S. Capitol.[1]\\\\nCurrent region\\\\nWashington, DC\\\\nInformation\\\\nPlace of origin\\\\nUnited States\\\\nNotable members\\\\nBarack Obama, Michelle Obama, Ann Dunham, Barack Obama, Sr., etc.\\\\nConnected families\\\\nRobinson, Dunham, Soetoro, Ng\\\\nThis article is part of a series about\\\\nBarack Obama\\\\nBackground \\xa0ýý Illinois Senate \\xa0ýý U.S. Senate\\\\nPolitical positions\\xa0ýý Public image\\xa0ýý Family\\\\n2008 primaries\\xa0ýý ObamaýýýBiden campaign\\\\nTransition\\xa0ýý Inauguration\\xa0ýý Electoral history\\\\nPresidency (Timeline, First 100 days)\\\\nMalia, Michelle and Sasha on stage at the 2008 Democratic National Convention\\\\nThe Family of Barack Obama is an extended clan of African American, English, Indonesian, and Kenyan (Luo) heritage known through the writings and political career of Barack Obama, the President of the United States of America,[2][3][4][5] and other reports. His immediate family is the First Family of the United States. The Obamas are the first First Family of African American descent in the United States and the youngest to enter the White House since the Kennedys. One columnist wrote, ýýýObama\\'s young, energetic family harks back to days of Camelot.ýýý[6]\\\\nContents\\\\n1 Immediate family\\\\n2 Extended family - maternal relations\\\\n3 Extended family - paternal relations\\\\n4 Michelle Robinson Obama\\'s extended family\\\\n5 Genealogical charts\\\\n5.1 Obama ancestry\\\\n5.2 Family trees\\\\n6 Distant relations\\\\n7 See also\\\\n8 References\\\\n9 External links\\\\nImmediate family\\\\nMichelle Obama\\\\nMichelle Obama, nýýe Robinson, the wife of Barack Obama, was born on January 17, 1964 in Chicago, Illinois. She is a lawyer and was a University of Chicago Hospital Vice-President. She is the First Lady of the United States.\\\\nMalia Obama and Sasha Obama\\\\nBarack and Michelle Obama have two daughters: Malia Ann (pronounced /mýýýýliýýýý/), born in 1998,[7] and Natasha (known as Sasha) /ýýsýýýýýýýý/), born in 2001. Sasha is the youngest child to reside in the White House since John F. Kennedy, Jr, arrived as an infant in 1961.[8] Sasha is also the first White House resident born in the 21st century.\\\\nBefore his inauguration, President Obama published an open letter to his daughters in Parade magazine, describing what he wants for them and every child in America: \\\\\"to grow up in a world with no limits on your dreams and no achievements beyond your reach, and to grow into compassionate, committed women who will help build that world.\\\\\"[9]\\\\nWhile living in Chicago, they kept busy schedules, as the Associated Press reports: \\\\\"soccer, dance and drama for Malia, gymnastics and tap for Sasha, piano and tennis for both.\\\\\"[10][11] In July 2008, the family gave an interview to the television series Access Hollywood; Obama later said they regretted allowing the children to be included.[12]\\\\nIn his victory speech on the night of his election, President Obama repeated his promise to Sasha and Malia to get a puppy to take with them to the White House.[13] However the selection of a dog has been slow because Malia is allergic to animal dander;[14] the president subsequently said that the choice has been narrowed down to either a labradoodle or Portuguese Water Dog, and they are hoping to find a shelter animal.[15]\\\\nMalia and Sasha attend the private Sidwell Friends School in Washington, DC, the same school as attended by Chelsea Clinton, Tricia Nixon Cox, and Archibald Roosevelt, and currently the grandchildren of Vice President Joe Biden.[16] The Obama girls began classes there on January 5, 2009.[17] While in Chicago, both attended the private University of Chicago Laboratory School.\\\\nMarian Shields Robinson\\\\nMichelle Obama\\'s mother (birthname Marian Shields, born July 1937), now widowed, married Michelle\\'s father, Fraser Robinson, in 1960.[18][19] Robinson was formerly a secretary at Spiegel catalog and a bank. While Michelle and Barack Obama were campaigning in 2008, Robinson tended the Obama\\'s young children and she intends to do the same while in Washington, DC. Robinson is currently living in the White House itself as part of the First Family;[20] she is the first live-in grandmother there since Elivera M. Doud during the Eisenhower administration.[21] Some media outlets have dubbed Robinson the \\\\\"First Granny\\\\\".[22][21]\\\\nExtended family - maternal relations\\\\nRight-to-left: Barack Obama and Maya Soetoro with their mother Ann Dunham and grandfather Stanley Dunham in Hawaii (early 1970s)\\\\nWikinews has related news:\\\\nBarack Obama elected 44th President of the United States\\\\nGrandmother of Barack Obama dies at 86\\\\nMadelyn Dunham with her daughter Ann\\\\nAccording to Barack Obama\\'s Dreams from My Father, his great-grandmother Leona McCurry was part Native American, which Obama believed Leona held as a \\\\\"source of considerable shame\\\\\" and \\\\\"blanched whenever someone mentioned the subject and hoped to carry the secret to her grave\\\\\"; whereas McCurry\\'s daughter (Obama\\'s maternal grandmother) \\\\\"would turn her head in profile to show off her beaked nose, which along with a pair of jet-black eyes, was offered as proof of Cherokee blood.\\\\\"[23] To date, no concrete evidence has surfaced of Cherokee heritage. Obama\\'s maternal heritage consists mostly of English ancestry, with much smaller amounts of German, Irish, Scottish, Welsh, Swiss, and French ancestry.[2]\\\\nAnn Dunham\\\\nMother of Barack Obama born in 1942, died in 1995. Birthname Stanley Ann Dunham. Anthropologist in Hawaii and Indonesia.\\\\nMadelyn Lee Payne Dunham\\\\nBarack Obama\\'s maternal grandmother, born in 1922 and died on November 2, 2008.[24] She was a bank vice president in Hawaii. Obama said that when he was a child, his grandmother \\\\\"read me the opening lines of the Declaration of Independence and told me about the men and women who marched for equality because they believed those words put to paper two centuries ago should mean something.\\\\\"[9]\\\\nStanley Armour Dunham\\\\nGrandfather of Barack Obama, born 1918, died 1992. World War II U.S. Army sergeant, furniture salesman in Hawaii.\\\\nCharles T. Payne\\\\nGreat-uncle of Barack Obama, younger brother of Madelyn Dunham, born 1925. Served during World War II in the U.S. Army 89th Infantry Division.[25] Obama has often described Payne\\'s role in liberating Ohrdruf forced labor camp.[26] There was brief media attention when Obama mistakenly identified the camp as Auschwitz during the campaign.[27] Payne appeared in the visitor\\'s gallery at the Democratic National Convention in Denver, Colorado, when his great-nephew was nominated for President.[28] He was the assistant director of the University of Chicago\\'s Library.[26]\\\\nMaya Soetoro-Ng\\\\nHalf-sister of Barack Obama, born August 15, 1970, in Jakarta, Indonesia.[29] She is married to Konrad Ng, with whom she has a daughter, Suhaila. Maya Soetoro-Ng is a teacher in Hawaii.\\\\nKonrad Ng\\\\nBrother-in-law of Barack Obama, born 1974. A Canadian whose parents are Malaysian Chinese immigrants, he is an assistant professor at the University of Hawaii\\'s Academy of Creative Media.[30] His parents are from Kudat and Sandakan, two small towns in Sabah, Malaysia, and he was born and raised in Burlington, Ontario.[31] He married Maya Soetoro-Ng at the end of 2003 in Hawaii.[32] They have one daughter, Suhaila.[33][34][35] Konrad Ng is now a US citizen.[36]\\\\nLolo Soetoro\\\\nStepfather of Barack Obama, born in Indonesia 1936, died 1987.\\\\nExtended family - paternal relations\\\\nThe Obamas are members of the Luo, Kenya\\'s third-largest ethnic group, which is part of a larger family of ethnic groups, collectively also known as Luo. This group belongs to the Eastern Sudanic branch of the Nilo-Saharan phylum. The Obama family is largely concentrated in the western province of Nyanza.\\\\nFront row (left to right): Auma Obama (Barack\\'s half-sister), Kezia Obama (Barack\\'s step-mother), Sarah Hussein Onyango Obama (third wife of Barack\\'s paternal grandfather), Zeituni Onyango (Barack\\'s aunt)\\\\nBack row (left to right): Said Obama (Barack\\'s uncle), Barack Obama, Abongo [Roy] Obama (Barack\\'s half-brother), unidentified woman, Bernard Obama (Barack\\'s half-brother), Abo Obama (Barack\\'s half-brother).\\\\nBarack Obama, Sr.\\\\nBarack Obama\\'s father, (1936ýýý1982). Government economist in Kenya. In addition to President Obama, Barack Obama Sr. fathered six other sons and a daughter.[37]\\\\nHussein Onyango Obama\\\\nBarack Obama\\'s paternal grandfather (c. 1895ýýý1979);[38] he worked as a mission cook. He joined the British Army during World War I. (One source gives 1870ýýý1975 as his dates of birth and death based on his tombstone reading \\\\\"Mzee Hussein Onyango Obama\\\\\" in his home village.[39] The term \\\\\"mzee\\\\\" is a Kenyan honorific meaning \\\\\"old man\\\\\" or \\\\\"elder.\\\\\") According to his third wife, Sarah, he originally converted to Catholicism, but took the name Hussein when he later converted to Islam; she said he passed the name, not the religion, on to his children.[40]\\\\nHabiba Akumu Obama\\\\nBarack Obama\\'s paternal grandmother, and the second wife of Hussein Onyango Obama. A photograph of her holding her son, Barack Sr, on her lap is on the cover of her grandson\\'s memoirs titled Dreams from my Father.[41]\\\\nSarah Obama\\\\nThird wife of Obama\\'s paternal grandfather, born 1922.[39] Also known, through the addition of her late husband\\'s name, as Sarah Onyango Obama,[42] and sometimes referred to as Sarah Ogwel, Sarah Hussein Obama or Sarah Anyango Obama,[43] she lives in Nyangýýýoma Kogelo village, 30 miles west of western Kenya\\'s main town, Kisumu, on the edge of Lake Victoria.[44][45]\\\\nAlthough not a blood relation, Barack Obama calls her \\\\\"Granny Sarah\\\\\".[43][46] Sarah, who speaks Luo and only a few words of English, communicates with President Obama through an interpreter.\\\\nOn July 4, 2008, she attended the United States Independence Day celebrations in Nairobi, hosted by Michael Ranneberger, the US ambassador in Kenya.[47]\\\\nDuring the campaign she protested attempts to portray Obama as a foreigner to the United States or a Muslim, saying that while Obama\\'s grandfather had been a Muslim, \\\\\"In the world of today, children have different religions from their parents.\\\\\"[40] Sarah Obama herself is \\\\\"a strong believer of the Islamic faith,ýýý in her words.[48]\\\\nKezia Obama\\\\nFirst wife of Barack Obama\\'s father, born c. 1940. She is Barack Obama Sr.\\'s first wife, whom he married in Kenya before studying abroad in the United States. Also known as Kezia Grace Obama.[49][50] She currently lives in Bracknell, Berkshire, England.[51][52] Her sister, Jane, is the \\'Auntie Jane\\' mentioned at the very start of Dreams from My Father when she telephoned President Obama to inform him that his father had been killed in a car accident.[53]\\\\nMalik Obama\\\\nBarack Obama\\'s half-brother, also known as Abongo or Roy, born c. March, 1958,[51] son of Barack Obama, Sr. with his first wife, Kezia.[54] Malik Obama was born and raised in Nairobi, Kenya.[55] He earned a degree in accounting from the University of Nairobi.[56] He met his half-brother for the first time in 1985[55] when Barack flew from Chicago to Washington, D.C. to visit him.[57] Malik and his half-brother Barack were best men at each other\\'s weddings.[55] Barack Obama brought his wife Michelle to Kenya three years later, and they met with Malik again while Barack was introducing Michelle to many other new relatives.[58]\\\\nAlthough much of the Obama family has dispersed throughout Kenya and overseas, most, including Malik Obama, still considered their rural village on the shores of Lake Victoria to be their true home, and feel that those who have left the village have become culturally \\\\\"lost\\\\\".[59] A frequent visitor to the United States,[58] and consultant in Washington, D.C. for several months per year,[55] he nevertheless settled in the Obamas\\' ancestral home, Nyangýýýoma Kogelo, a village of several hundred people that he prefers to the city for its slow pace.[55] He runs a small electronics shop a half hour drive outside of town.[55]\\\\nDuring his brother\\'s presidential campaign, Malik Obama was a spokesman for the extended Obama family in Kenya, dealing with safety and privacy concerns arising from increased attention from the press.[60]\\\\nAbo Obama\\\\nBarack Obama\\'s half-brother, born 1968. International telephone store manager in Kenya.\\\\nAuma Obama\\\\nBarack Obama\\'s half-sister, born c. 1960.[61] As of July 2008, development worker in Kenya.[62] She studied German at the University of Heidelberg from 1981 to 1987. After her graduation in Heidelberg she went on for graduate studies at the University of Bayreuth, which awarded her a PhD in 1996. Her dissertation was about the conception of labor in Germany and its literary reflections.[62] Auma Obama lives in London, and in 1996 married an Englishman, Ian Manners. They have a daughter named Akinyi (b. 1997).[62][verification needed]\\\\nBernard Obama\\\\nBarack Obama\\'s half-brother, born 1970, son of Barack Obama, Sr. and his first wife, Kezia. He had been an auto parts supplier in Nairobi, Kenya, and has one child. Bernard converted to Islam as an adult and has said: \\\\\"Iýýým a Muslim, I donýýýt deny it. My father was raised a Muslim. But itýýýs not an issue. I donýýýt know what all the hullabaloo is about.\\\\\"[63] He currently resides in Bracknell, England, with his mother Kezia.[63]\\\\nRuth Ndesandjo\\\\nBorn Ruth Nidesand, in US c. 1940s, Barack Obama Sr.\\'s third wife and a private kindergarten director in Kenya.[64] Ruth\\'s two sons with Barack Obama, Sr., are Mark and David Ndesandjo; her third son, Joseph Ndesandjo, was born c. 1980 from a subsequent marriage to a Tanzanian.[65][66]\\\\nMark Ndesandjo\\\\nBarack Obama\\'s half-brother, son of Ruth Nidesand and Barack Obama Sr.[67] He runs an Internet company called WorldNexus that advises Chinese corporations how best to reach international customers.[68] Mark graduated from Brown University, studied physics at Stanford University, received an MBA from Emory University, and has lived in Shenzhen, China, since 2002 and is married to a Chinese woman.[69] He is also an accomplished pianist.[70]\\\\nDavid Ndesandjo\\\\nDavid Ndesandjo\\\\nBarack Obama\\'s half-brother (also known as David Opiyo Obama), son of Ruth Nidesand and Barack Obama Sr. Killed in a motorcycle accident.[71]\\\\nGeorge Hussein Onyango Obama\\\\nYoungest half-brother of Barack Obama, born c.1982, son of Barack Obama Sr.[72] and Jael (now a resident of Atlanta, Georgia).[73][74] George was six months old when his father died in an automobile accident, after which he was raised in Nairobi by his mother and a French step-father. He later lived in South Korea for two years while his mother resided there for business reasons.[73] Returning to Kenya, George Obama \\\\\"slept rough for several years,\\\\\" until his aunt gave him a six-by-eight foot corrugated metal shack in the Nairobi, Kenya, slum of Huruma Flats.[73] As of August 2008, Obama was studying to become a mechanic.[73] George received little attention until being featured in an article in the Italian language edition of Vanity Fair in August 2008, which portrayed him as living in poverty, shame, and obscurity.[75] The article quoted Obama as saying that he lived \\\\\"on less than a dollar a month\\\\\" and stated that he \\\\\"does not mention his famous half-brother in conversation\\\\\" out of shame at his poverty.[76] In later interviews George contradicted this picture. In an interview with The Times, Obama \\\\\"said that he was furious at subsequent reports that he had been abandoned by the Obama family and that he was filled with shame about living in a slum.\\\\\"[74] He told The Times, \\\\\"Life in Huruma is good.\\\\\" Obama said that he expects no favors, that he was supported by relatives, and that reports he lived on a dollar a month were \\\\\"all lies by people who donýýýt want my brother to win.ýýý[74] He told The Telegraph that he was inspired by his half-brother.[73] According to Time, George \\\\\"has repeatedly denied...that he feels abandoned by Obama.\\\\\"[77] CNN quoted him as saying, \\\\\"I was brought up well. I live well even now. The magazines, they have exaggerated everything... I think I kind of like it here. There are some challenges, but maybe it is just like where you come from, there are the same challenges.\\\\\"[75] George\\'s reported poverty was seized on by conservative critics of Barack Obama. Columnist Dinesh D\\'Souza solicited donations for George Obama from his readers,[78] while Jerome Corsi planned to give him a $1,000 check during a trip to Kenya (Corsi was expelled from the country by immigration authorities).[77]\\\\nOmar Obama\\\\nHalf-uncle of Barack Obama,[79] born on June 3, 1944 in Nyangýýýoma Kogelo. Oldest son of Onyango and Sarah Obama, resides in Boston, Massachusetts.[citation needed]\\\\nZeituni Onyango\\\\nHalf-aunt of Barack Obama,[80] born May 29, 1952, in Kenya,[81] Onyango is referred to as \\\\\"Aunti Zeituni\\\\\" in President Obama\\'s memoir, Dreams from My Father.[82]\\\\nYusuf Obama\\\\nHalf-uncle of Barack Obama,[79] born c. 1950s in Nyangýýýoma Kogelo; son of Onyango and Sarah Obama.[citation needed]\\\\nSaid Obama\\\\nHalf-uncle of Barack Obama,[79][83] born c. 1950s in Nyangýýýoma Kogelo; son of Onyango and Sarah Obama.[citation needed]\\\\nMichelle Robinson Obama\\'s extended family\\\\nFraser Robinson, Sr. (1884ýýý1936) of South Carolina, shown in an old photo along with his wife, Rosella Cohen Robinson, in its background\\\\nBarack Obama has called his wife Michelle \\\\\"the most quintessentially American woman I know.\\\\\"[3] Her family is of African American heritage, descendents of Africans of the American Colonial Era.[3] Michelle Obama\\'s family history traces back from slavery to Reconstruction to the Great Migration North. Some of Michelle\\'s relatives still reside in South Carolina.\\\\nMichelle\\'s earliest known relative is her great-great grandfather Jim Robinson, born in the 1850s, who was an American slave on the Friendfield plantation in South Carolina. The family believes that after the Civil War he remained a Friendfield plantation sharecropper for the rest of his life and that he was buried there in an unmarked grave.[3]\\\\nJim had two sons, Gabriel and Fraser, Michelle Obama\\'s great-grandfather. Fraser had an arm amputated as a result of a boyhood injury. He worked as a shoemaker, newspaper salesman and in a lumber mill and was married to Rosella Cohen.[3] Carrie Nelson, Gabriel Robinson\\'s daughter, now 80, is the oldest living Robinson and the keeper of family lore.[3]\\\\nAt least three of Michelle Obama\\'s great-uncles served in the military of the United States. One aunt moved to Princeton, New Jersey, where she worked as a maid, and cooked Southern-style meals for Michelle and her brother, Craig, when they were students at Princeton University.\\\\nCraig Robinson\\\\nMichelle Obama\\'s brother, born 1962. He is currently head coach of men\\'s basketball at Oregon State University.[84]\\\\nFraser Robinson III\\\\nMichelle Obama\\'s father, born 1935, died 1991, married Michelle\\'s mother, Marian Shields, in 1960.[85][19] Robinson was a pump worker at the City of Chicago water plant.[3]\\\\nFraser Robinson, Jr.\\\\nMichelle Obama\\'s grandfather was born on August 24, 1912 in Georgetown, South Carolina, and died on November 9, 1996, aged 84. He was a good student and orator, but moved from South Carolina to Chicago to find better work than he could find at home, eventually becoming a worker for the United States Postal Service. He was married to LaVaughn Johnson. When he retired, they moved back to South Carolina.[3]\\\\nCapers C. Funnye Jr.\\\\nMichelle Obama\\'s first cousin once removed: Funnyeýýýs mother, Verdelle Robinson Funnye (born Verdelle Robinson; August 22, 1930 ýýý April 16, 2000) and Michelle Obamaýýýs paternal grandfather, Fraser Robinson Jr., were siblings. One of America\\'s most prominent African American Jews, known for acting as a bridge between mainstream Jewry and African Americans.[86]\\\\nGenealogical charts\\\\nObama ancestry\\\\n16. Opiyo\\\\n8. Obama\\\\n4. Hussein Onyango Obama\\\\n9. Nyaoke\\\\n2. Barack Hussein Obama, Sr.\\\\n5. Habiba Akumu\\\\n1. Barack Hussein Obama II\\\\n24. Jacob William Dunham\\\\n12. Ralph Waldo Emerson Dunham, Sr.\\\\n25. Mary Ann Kearney\\\\n6. Stanley Armour Dunham\\\\n26. Harry Ellington Armour\\\\n13. Ruth Lucille Armour\\\\n27. Gabriella Clark\\\\n3. Stanley Ann Dunham\\\\n28. Charles T. Payne\\\\n14. Rolla Charles Payne\\\\n29. Della L. Wolfley\\\\n7. Madelyn Lee Payne\\\\n30. Thomas Creekmore McCurry\\\\n15. Leona Belle McCurry\\\\n31. Margaret Belle Wright\\\\nFamily trees\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nBarack Obama\\\\nStanley\\\\nDunham\\\\n1918ýýý1992\\\\nMadelyn\\\\nPayne\\\\n1922ýýý2008\\\\nHabiba\\\\nAkumu\\\\nOnyango\\\\nObama\\\\nc.\\xa01895ýýý1979\\\\nSarah\\\\nOgwel\\\\n1922ýýý\\\\nLolo\\\\nSoetoro\\\\n1936ýýý1987\\\\nAnn\\\\nDunham\\\\n1942ýýý1995\\\\nBarack\\\\nObama, Sr. *\\\\n1936ýýý1982\\\\nKezia\\\\nGrace\\\\nc. 1940ýýý\\\\nOmar\\\\nObama\\\\n1944ýýý\\\\nZeituni\\\\nOnyango\\\\n1952ýýý\\\\nYusuf\\\\nObama\\\\nc. 1950sýýý\\\\nSaid\\\\nObama\\\\nc. 1950sýýý\\\\nKonrad\\\\nNg\\\\nc. 1974ýýý\\\\nMaya\\\\nSoetoro\\\\n1970ýýý\\\\nBarack\\\\nObama\\\\n1961ýýý\\\\nMichelle\\\\nRobinson\\\\n1964ýýý\\\\nM. Abongo\\\\nObama\\\\n1958ýýý\\\\nAuma\\\\nObama\\\\nc. 1960ýýý\\\\nAbo\\\\nObama\\\\n1968ýýý\\\\nBernard\\\\nObama\\\\n1970ýýý\\\\nSuhaila\\\\nNg\\\\nc. 2005ýýý\\\\nMalia Ann\\\\nObama\\\\n1998ýýý\\\\nSasha\\\\nObama\\\\n2001ýýý\\\\n* Barack\\\\nObama, Sr.\\'s\\\\nadditional\\\\nRuth\\\\nNidesandjo\\\\nc. 1940sýýý\\\\nJael\\\\nOtieno\\\\nrelationships:\\\\nMark\\\\nNdesandjo\\\\nDavid\\\\nNdesandjo\\\\ndied\\xa0c.\\xa01987\\\\nGeorge\\\\nObama\\\\nc. 1982ýýý\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nMichelle Obama\\\\nFraser\\\\nRobinson, Jr.\\\\nýýý\\\\nSouth Carolina\\\\n(1912ýýý1996)\\\\nSawmill worker\\\\nLaVaughn D.\\\\nJohnson\\\\nýýý\\\\nIllinois\\\\n(1915ýýý2002)\\\\nCapers C.\\\\nFunnye, Jr.\\\\nýýý\\\\n(born c. 1952;\\\\nnephew of\\\\nFraser\\\\nRobinson, Jr.)\\\\nRabbi in Chicago\\\\nFraser\\\\nRobinson III\\\\nýýý\\\\nIllinois\\\\n(1935ýýý1991)\\\\nEnjoyed boxing\\\\nin youth;\\\\nwater plant\\\\npump operator\\\\nin Chicago\\\\nMarian\\\\nShields\\\\nýýý\\\\nIllinois\\\\n(born 1937)\\\\nSecretary at\\\\nSpiegel catalog\\\\nin Chicago;\\\\nU.S.\\'s First\\\\nGrandmother\\\\nCraig\\\\nRobinson\\\\nýýý\\\\nIllinois\\\\n(born 1962)\\\\nHead coach of\\\\nOregon State\\\\nBeavers men\\'s\\\\nbasketball\\\\nMichelle\\\\nRobinson\\\\nýýý\\\\nIllinois\\\\n(born 1964)\\\\nFirst Lady\\\\nof the United States\\\\nDistant relations\\\\nSee also: List of United States Presidents by genealogical relationship\\\\nAccording to genealogists, Barack Obama\\'s distant cousins include the multitude of descendants of his maternal ancestors from all along the early-American Atlantic seaboard as well as paternal, Kenyan relations belonging to the Luo tribe, many descending from a 17th century ancestor named Owiny.[87][88] For example, George W. Bush, the 43rd U.S. president, is the eleventh cousin of Barack Obama.[89] The New York Times science writer Nicholas Wade argues that with eleven generations leading back to their common progenitor, Samuel Hinckley, the relationship between the 43rd President and the 44th President is \\\\\"genetically meaningless\\\\\".[90]\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nSelected genealogical relationships\\\\nBill Hickok\\\\nAccording to Barack Obama\\'s family lore (and confirmed by the New England Historic Genealogical Society), the President and Hickok are sixth cousins, six-times removed.[91]\\\\nRobert Duvall\\\\nGoodnight helped inspire Pulitzer Prize-winning author Larry McMurtry to create a protagonist for his novel series Lonesome Dove: Woodrow Call. In Dove\\'s television novela, Woodrow Call\\'s partner is Gus McCrae, portrayed by Obama\\'s eighth cousin, twice removed, actor Robert Duvall.[92]\\\\nJames Madison\\\\nObama is also distantly related to U.S. Presidents James Madison, Lyndon Johnson, Gerald Ford, and U.S. Vice President Dick Cheney, British Prime Minister Sir Winston Churchill, U.S. Civil War General Robert E. Lee, and actor Brad Pitt.[93][94][95]\\\\nCharles Goodnight\\\\nAccording to Chicago Sun-Times reporter Scott Fornek, another Obama progenitor, Catherine Goodnight, was the grandniece of George Goodnight, who was in turn great-grandfather of famed cattleman Charles Goodnight.[92]\\\\nHarry S. Truman\\\\nActor Duvall is distant cousins with United States President Harry Truman, who\\'s likewise a fourth cousin, four times removed, of Obama\\'s.[92]\\\\nGeorgia O\\'Keeffe\\\\nNotable women Obama is related to include painter Georgia OýýýKeeffe.[93]\\\\nSee also\\\\nU.S. Presidential genealogical relationships\\\\nNew England Historic Genealogical Society\\\\nGary Boyd Roberts\\\\nReferences\\\\n^ Keck, Kristi (4 June 2008). \\\\\"Obama crosses historic milestone\\\\\". CNN. http://news.yahoo.com/s/ap/20081122/ap_on_go_pr_wh/obama_school_12. Retrieved on 2008-11-21.\\\\n^ a b Reitwiesner, William Addams. \\\\\"Ancestry of Barack Obama\\\\\". http://www.wargs.com/political/obama.html. Retrieved on 2008-10-09.\\\\n^ a b c d e f g h Murray, Shailagh (2 October 2008). \\\\\"A Family Tree Rooted In American Soil: Michelle Obama Learns About Her Slave Ancestors, Herself and Her Country\\\\\". The Washington Post: p.\\xa0C01. http://www.washingtonpost.com/wp-dyn/content/article/2008/10/01/AR2008100103169.html. Retrieved on 2008-10-10.\\\\n^ Sheridan, Michael (5 February 2007). \\\\\"Secrets of Obama Family Unlocked\\\\\". Muslim Observer. http://news.newamericamedia.org/news/view_article.html?article_id=ae5895fc29971b172938790be94ab107. Retrieved on 2008-11-21.\\\\n^ RTE news report March, 2007:Obamas Irish family links discovered by ancestry.co.uk\\\\n^ Noveck, Jocelyn (2008-11-07). \\\\\"Deseret News | Obama\\'s young, energetic family harks back to days of Camelot\\\\\". Secure.deseretnews.com. https://secure.deseretnews.com/article/1,5143,705261276,00.html. Retrieved on 2009-01-31.\\\\n^ Liza Mundy, Michelle: A Biography (Simon and Schuster, 2008), p. 129.\\\\n^ \\\\\"Sasha Obama\\\\\". Baltimore Sun. http://www.baltimoresun.com/topic/politics/sasha-obama-PECLB004381.topic. Retrieved on 2009-01-31.\\\\n^ a b Obama, Barack (13 January 2009). \\\\\"\\'What I Want for You--And Every Child in America\\'\\\\\". http://www.parade.com/news/2009/01/barack-obama-letter-to-my-daughters.html.\\\\n^ Sobieraj Westfall, Sandra (23 June 2008). \\\\\"Barack Obama Gives Daughter $1 Allowance a Week\\\\\". People Magazine. http://www.people.com/people/article/0,,20214569_1,00.html. Retrieved on 2008-11-21.\\\\n^ Lester, Will (July 23, 2008). \\\\\"Obama daughters keep hectic schedules of their own\\\\\". Associated Press. http://elections.apnews.com/apelect/db_6911/contentdetail.htm;jsessionid=8314A43012AB5FF1D0697247362D8752?contentguid=H95QubFb&full=true. Retrieved on 2008-08-04.\\\\n^ Hiro, Anne. \\\\\"Obama regrets letting \\\\\"Access Hollywood\\\\\" interview daughters. Won\\'t do it again. MSNBC\\'s Dan Abrams gets the story behind the story. - Lynn Sweet\\\\\". Blogs.suntimes.com. http://blogs.suntimes.com/sweet/2008/07/obama_regrets_letting_access_h.html. Retrieved on 2009-01-31.\\\\n^ Ahmed, Saeed (5 November 2008). \\\\\"Move over Barney, new dog moving into White House\\\\\". CNN. http://www.cnn.com/2008/LIVING/wayoflife/11/05/presidential.pets/index.html. Retrieved on 2008-11-21.\\\\n^ \\\\\"Obama: Getting a dog isn\\'t easy\\\\\". Associated Press. 7 November 2008. http://www.mercurynews.com/ci_10927292. Retrieved on 2008-11-21.\\\\n^ Janice Lloyd (2009-01-12). \\\\\"Obamas down to Labradoodle or Portuguese water dog\\\\\". USA Today. http://www.usatoday.com/news/washington/2009-01-11-obama-dog_N.htm. Retrieved on 2009-01-28.\\\\n^ Swarns, Rachel (21 November 2008). \\\\\"And the Winner Is ýýý Sidwell Friends\\\\\". The New York Times. http://thecaucus.blogs.nytimes.com/2008/11/21/and-the-winner-is-sidwell-friends/. Retrieved on 2008-11-21.\\\\n^ Tolin, Lisa (2009-01-05). \\\\\"Obama girls start school with photographers in tow\\\\\". The Associated Press. http://www.google.com/hostednews/ap/article/ALeqM5g6mv_lkODQMmQdpyIEnr8Zpm5mogD95H8KA80. Retrieved on 2009-01-06.\\\\n^ Taylor Marsh (2008-08-25). \\\\\"Political Analysis, National Security and Breaking News\\\\\". Taylor Marsh. http://www.taylormarsh.com/archives_view.php?id=28286. Retrieved on 2009-01-31.\\\\n^ a b Lia LoBello (2008-01-02). \\\\\"First Families: Radar introduces you to the next president\\'s relatives\\\\\". Radar Online. http://www.radaronline.com/features/2008/07/john_mccain_barack_obama_michelle_cindy_dunham_roberta_wrigh_04.php. Retrieved on 2009-01-28.\\\\n^ Rachel L. Swarns (2009-01-09). \\\\\"Obamaýýýs Mother-in-Law to Move Into the White House\\\\\". The New York Times. http://thecaucus.blogs.nytimes.com/2009/01/09/obamas-mother-in-law-to-move-into-the-white-house/?hp. Retrieved on 2009-01-09.\\\\n^ a b \\\\\"Will Obama mum-in-law make it a family affair in the White House?\\\\\". Agence France Presse. 2008-11-22. http://www.google.com/hostednews/afp/article/ALeqM5gN_i2jrCVkJQgfMbSDRRrNk8U4Sw. Retrieved on 2009-01-09.\\\\n^ Philip Sherwell (2008-2008-11-09). \\\\\"Michelle Obama persuades First Granny to join new White House team\\\\\". The Telegraph (UK). http://www.telegraph.co.uk/news/3407525/Michelle-Obama-persuades-First-Granny-to-join-new-White-House-team.html. Retrieved on 2009-01-09.\\\\n^ \\\\\"ýýýTootýýý: Obama grandmother a force that shaped him\\\\\". via Associated Press. 2008-08-25. http://www.thekansan.com/news/x1311851415/-Toot-Obama-grandmother-a-force-that-shaped-him. Retrieved on 2008-08-29.\\\\n^ \\\\\"CNN: \\\\\"Obama\\'s grandmother dies after battle with cancer\\\\\"\\\\\". http://www.cnn.com/2008/POLITICS/11/03/obama.grandma/index.html. Retrieved on 2008-11-04.\\\\n^ The 89th Infantry Division, United States Holocaust Memorial Museum\\\\n^ a b Obama\\'s great-uncle recalls liberating Nazi camp, Boston.com, July 22, 2008\\\\n^ Major Garrett (2008-05-27). \\\\\"Obama Campaign Scrambles to Correct the Record on Uncle\\'s War Service\\\\\". FOXNews.com. http://elections.foxnews.com/2008/05/27/recollection-of-obama-familys-service-missing-key-details. Retrieved on 2009-01-31.\\\\n^ \\\\\"Democrats salute Obamaýýýs great uncle\\\\\". Jewish Telegraphic Agency. August 28, 2008. http://jta.org/news/article/2008/08/28/110123/obamapayne. Retrieved on 31 January 2009.\\\\n^ Obama Family Tree dgmweb.net\\\\n^ Chicago Sun Times article with her picture\\\\n^ Obama has links to Malaysia\\\\n^ Nolan, Daniel (2008-06-11). \\\\\"Relative: Obama\\'s got \\'a good handle on Canada\\'\\\\\". The Hamilton Spectator. http://www.thespec.com/burlingtonlife/article/384475. Retrieved on 2008-07-03.\\\\n^ Nolan, Daniel (June 11, 2008). \\\\\"Obama\\'s Burlington connection\\\\\". The Hamilton Spectator. http://www.thespec.com/article/384307. Retrieved on 2008-06-21.\\\\n^ Misner, Jason (2008-06-20). \\\\\"Barack Obama was here\\\\\". Burlington Post. http://www.burlingtonpost.com/printarticle/186215. Retrieved on 2008-07-03.\\\\n^ Fornek, Scott (2007-09-09). \\\\\"\\'He helped me find my voice\\'\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545473,BSX-News-wotreehh09.article.\\\\n^ Cooper, Tom (2009-01-20). \\\\\"Keep watch for Obama\\\\\". TheSpec.com. http://www.thespec.com/Opinions/article/499161. Retrieved on 2009-01-28.\\\\n^ Ancestry of Barack Obama\\\\n^ Dreams from My Father, p. 376\\\\n^ a b Kenya: Special Report: Sleepy Little Village Where Obama Traces His Own Roots (Page 2 of 2)\\\\n^ a b \\\\\"Obama\\'s grandma slams \\'untruths\\'\\\\\". Associated Press. 2008-03-05. http://www.usatoday.com/news/world/2008-03-05-obama-kin_N.htm.\\xa0 See also this correction.\\\\n^ \\\\\"Q&A ON THE NEWS\\\\\". Atlanta Journal-Constitution. 2009-02-25. http://www.ajc.com/services/content/metro/stories/2009/02/25/questi0225.html. Retrieved on 2009-02-27.\\\\n^ In Kenya, Barack Obamaýýýs family prays for end to conflict - Times Online\\\\n^ a b Crilly, Rob (February 27, 2008). \\\\\"Dreams from Obama\\'s Grandmother\\\\\". Time Magazine, Inc.. http://www.time.com/time/world/article/0,8599,1717590,00.html?xid=rss-topstories. Retrieved on 2008-07-03.\\\\n^ Pflanz, Mike (2008-01-11). \\\\\"Barack Obama\\'s Kenyan relatives keep faith\\\\\". Daily Telegraph. http://www.telegraph.co.uk/news/main.jhtml?xml=/news/2008/01/09/wuspols1009.xml.\\\\n^ Fornek, Scott (2007-09-09). \\\\\"Sarah Obama - \\'Sparkling, laughing eyes\\'\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545459,BSX-News-wotreeu09.article.\\\\n^ \\\\\"Barack Obama in Kenya\\\\\". CNN. http://www.youtube.com/watch?v=Ikg6gj71U9k.\\\\n^ Daily Nation, July 8, 2008: Obama granny\\'s day out with envoys and top politicians\\\\n^ \\\\\"A Candidate, His Minister and the Search for Faith\\\\\". New York Times. 2007-04-30. http://www.nytimes.com/2007/04/30/us/politics/30obama.html?_r=2&pagewanted=all&oref=slogin&oref=slogin.\\\\n^ \\\\\"Kenya: All Obama kin to spend voting day in Kogelo\\\\\". afrika.no. 2008-10-27. http://www.afrika.no/Detailed/17321.html. Retrieved on 2009-01-31.\\\\n^ Cohen, Roger (2008-03-06). \\\\\"The Obamas of the World - New York Times\\\\\". Nytimes.com. http://www.nytimes.com/2008/03/06/opinion/06cohen.html. Retrieved on 2009-01-31.\\\\n^ a b Sanderson, Elizabeth (2008-01-06). \\\\\"Barack Obama\\'s stepmother living in Bracknell reveals the close bond with him ... and his mother\\\\\". Daily Mail. http://www.dailymail.co.uk/news/article-506338/Barack-Obamas-stepmother-living-Bracknell-reveals-close-bond---mother.html.\\\\n^ Lindsay, Anna (2009-01-20). \\\\\"Barack\\'s bingo-loving stepmother\\\\\". BBC News. http://news.bbc.co.uk/1/hi/england/berkshire/7834368.stm.\\\\n^ http://www.newvision.co.ug/D/8/26/666733\\\\n^ jpt (2008-06-18). \\\\\"From the Fact Check Desk: What Did Obama\\'s Half-Brother Say About Obama\\'s Background\\\\\". ABC News. http://blogs.abcnews.com/politicalpunch/2008/06/from-the-fact-c.html.\\\\n^ a b c d e f Maliti, Tom (2004-10-26). \\\\\"Obama\\'s Brother Chooses Life in Slow Lane\\\\\". The Associated Press. http://www.msnbc.msn.com/id/6333496.\\\\n^ Obama, Dreams from my Father, 2004, p. 265.\\\\n^ Obama, Dreams from my Father, 2004, p. 262.\\\\n^ a b *Oywa, John (2004-08-15). \\\\\"Sleepy Little Village Where Obama Traces His Own Roots\\\\\". The Daily Nation. http://allafrica.com/stories/200408160533.html.\\\\n^ Philip Ochieng (2004-11-01). \\\\\"From Home Squared to the US Senate: How Barack Obama Was Lost and Found\\\\\". The East African. http://www.nationmedia.com/EastAfrican/01112004/Features/PA2-11.html. Retrieved on 2008-03-23.\\\\n^ Warah, Rasna (2008-06-09). \\\\\"We cannot lay claims on Obama; he\\'s not one of us - Obama in this world\\\\\". Daily Nation. http://www.nationmedia.com. Retrieved on 2008-07-10.\\\\n^ Scott Fornek (2007-09-09). \\\\\"AUMA OBAMA: \\'Her restlessness, her independence\\'\\\\\". Chicago Sun Times. http://www.suntimes.com/news/politics/obama/familytree/545465,BSX-News-wotreew09.article. Retrieved on 2008-03-23.\\\\n^ a b c Gathmann, Florian; Gregor Peter Schmitz, Jochen Schýýnmann (July 24, 2008). \\\\\"Studentin in der Bundesrepublik: Wie Auma Obama mit Deutschland haderte\\\\\" (in German). Spiegel Online. http://www.spiegel.de/politik/ausland/0,1518,567286,00.html. Retrieved on 2008-07-24.\\\\n^ a b Harvey, Oliver (07-26 2008). \\\\\"Obama\\'s brother is in Bracknell\\\\\". The Sun. http://www.thesun.co.uk/sol/homepage/news/the_real_american_idol/article1472877.ece. Retrieved on 2008-10-06.\\\\n^ \\\\\"Madari Kindergarten\\\\\". http://www.madarikindergarten.com/.\\\\n^ \\\\\"Welcome To MedWeek San Antonio 2007\\\\\". Medweeksa.org. http://www.medweeksa.org/awardwinners/techfirm.htm. Retrieved on 2009-01-31.\\\\n^ \\\\\"PIDE - Partners for International Development & Education Inc\\\\\". Pideafrica.org. http://pideafrica.org/aboutus.htm. Retrieved on 2009-01-31.\\\\n^ Barack Obamaýýýs brother pushes Chinese imports on US - Times Online\\\\n^ Obama half-brother runs Internet company in China\\\\n^ Roger Cohen (2008-03-17). \\\\\"Obama\\'s Brother in China\\\\\". The New York Times. http://www.nytimes.com/2008/03/17/opinion/29cohen.html. Retrieved on 2008-03-23.\\\\n^ \\\\\"Youku Buzz (daily)\\xa0ýý Blog Archive\\xa0ýý Barack Obamaýýýs Half-Brother in Concert\\\\\". Buzz.youku.com. 2009-01-18. http://buzz.youku.com/2009/01/18/barack-obamas-half-brother-in-concert/. Retrieved on 2009-01-31.\\\\n^ jaketapper (2008-07-28). \\\\\"Political Punch: Barack Obama\\'s Branch-y Family Tree\\\\\". Blogs.abcnews.com. http://blogs.abcnews.com/politicalpunch/2008/07/barack-obamas-1.html. Retrieved on 2009-01-31.\\\\n^ Fornek, Scott (September 9, 2007). \\\\\"HALF-BROTHER GEORGE: \\'I would be there for him\\'\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545447,BSX-News-wotreecc09.stng. Retrieved on 2008-08-04.\\\\n^ a b c d e Pflanz, Mike (August 21, 2008). \\\\\"Barack Obama is my inspiration, says lost brother\\\\\". The Daily Telegraph. http://www.telegraph.co.uk/news/newstopics/uselection2008/barackobama/2595688/Barack-Obama-is-my-inspiration-says-lost-brother.html. Retrieved on 2008-08-23.\\\\n^ a b c Crilly, Rob (August 22, 2008). \\\\\"Life is good in my Nairobi slum, says Barack Obama\\'s younger brother\\\\\". The Times. http://www.timesonline.co.uk/tol/news/world/us_and_americas/us_elections/article4583353.ece. Retrieved on 2008-08-23.\\\\n^ a b McKenzie, David (2008-08-23). \\\\\"Behind the Scenes: Meet George Obama\\\\\". CNN. http://www.cnn.com/2008/POLITICS/08/22/bts.obama.brother/. Retrieved on 2008-10-26.\\\\n^ Pisa, Nick (August 20, 2008). \\\\\"Barack Obama\\'s \\'lost\\' brother found in Kenya\\\\\". Daily Telegraph. http://www.telegraph.co.uk/news/newstopics/uselection2008/barackobama/2590614/Barack-Obamas-lost-brother-found-in-Kenya.html. Retrieved on 2008-08-20.\\\\n^ a b Wadhams, Nick (2008-10-07). \\\\\"Corsi in Kenya: Obama\\'s Nation Boots Obama Nation Author\\\\\". TIME. http://www.time.com/time/world/article/0,8599,1847965,00.html?imw=Y. Retrieved on 2009-01-31.\\\\n^ by Dinesh D\\'Souza. \\\\\"Dinesh D\\'Souza\\xa0: George Obama, Start Packing\\\\\". Townhall.com. http://townhall.com/columnists/DineshDSouza/2008/09/22/george_obama,_start_packing. Retrieved on 2009-01-31.\\\\n^ a b c \\\\\"The Obama Family Tree\\\\\" (PDF). Chicago Sun-Times. September 9, 2007. http://www.suntimes.com/images/cds/MP3/obamatree.pdf. Retrieved on 2008-11-23.\\\\n^ First read, MSNBC\\\\n^ \\\\\"Barack Obama\\'s aunt found living in rundown public housing estate | The Australian\\\\\". Theaustralian.news.com.au. 2008-10-31. http://www.theaustralian.news.com.au/story/0,25197,24578185-5017121,00.html. Retrieved on 2009-01-31.\\\\n^ Boston Housing Authority ýýýflabbergasteredýýý Barack Obamaýýýs aunt living in Southie\\\\n^ Kilner, Derek (2008-11-05). \\\\\"Kenya Celebrates President Obama as Native Son\\\\\". Voice Of America. http://www.voanews.com/english/archive/2008-11/2008-11-05-voa45.cfm. Retrieved on 2008-12-24.\\\\n^ \\\\\"Oregon State University Beavers: Craig Robinson bio\\\\\". http://www.osubeavers.com/ViewArticle.dbml?SPSID=106239&SPID=1954&DB_OEM_ID=4700&ATCLID=1436883&Q_SEASON=2008. Retrieved on 2008-08-21.\\\\n^ \\\\\"RootsWeb\\'s WorldConnect Project: Dowling Family Genealogy\\\\\". Wc.rootsweb.ancestry.com. http://wc.rootsweb.ancestry.com/cgi-bin/igm.cgi?op=GET&db=dowfam3&id=I105855. Retrieved on 2009-01-31.\\\\n^ Weiss, Anthony (September 2, 2008). \\\\\"Michelle Obama Has a Rabbi in Her Family\\\\\". The Forward. http://www.forward.com/articles/14121/. Retrieved on 2008-10-09.\\\\n^ Gary Boyd Roberts. \\\\\"Notes on the Ancestry of Senator Barack Hussein Obama, Jr\\\\\". New England Historic Genealogical Society. http://www.newenglandancestors.org/research/services/articles_ancestry_barack_obama.asp. Retrieved on 2009-01-31.\\\\n^ Scott Fornek (2007-09-09). \\\\\"Obama Family Tree\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545463,BSX-News-wotreer09.stng. Retrieved on 2009-01-31.\\\\n^ \\\\\"Obama-Bush (family tree)\\\\\" (PDF). New England Historic Genealogical Society. http://www.newenglandancestors.org/pdfs/obama_bush.pdf. Retrieved on 2009-01-31.\\\\n^ Wade, Nicholas (2007-10-21). \\\\\"Barack Obama - Dick Cheney - Ancestry and Genealogy - Washington - New York Times\\\\\". Nytimes.com. http://www.nytimes.com/2007/10/21/weekinreview/21basic.html. Retrieved on 2009-01-31.\\\\n^ Eastman, Dick (2008-07-30). \\\\\"Barack Obama is Related to Wild Bill Hickok\\\\\". Blog.eogn.com. http://blog.eogn.com/eastmans_online_genealogy/2008/07/barack-obama-is.html. Retrieved on 2009-01-31.\\\\n^ a b c Scott Fornek (2007-09-09). \\\\\"Obama Family Tree\\\\\". Suntimes.com. http://www.suntimes.com/news/politics/obama/familytree/545441,BSX-News-wotreec09.stng. Retrieved on 2009-01-31.\\\\n^ a b \\\\\"Notes on the Ancestry of Senator Barack Hussein Obama, Jr\\\\\". New England Historic Genealogical Society. 2002-08-14. http://www.newenglandancestors.org/research/services/articles_ancestry_barack_obama.asp. Retrieved on 2009-01-31.\\\\n^ \\\\\"Obama, Clinton and McCain have some famous relations\\\\\". HeraldNet - AP. 2008-03-26. http://www.heraldnet.com/article/20080326/NEWS02/151343661. Retrieved on 2009-01-31.\\\\n^ \\\\\"Barack Obama and Joe Biden: The Change We Need\\\\\". My.barackobama.com. 2008-07-31. http://my.barackobama.com/page/community/post/williambrehm/gG5TVR. Retrieved on 2009-01-31.\\\\nExternal links\\\\nBarack Obama\\'s Family Tree - Photo Essays - TIME\\\\n\\\\\"Though Obama Had to Leave to Find Himself, It Is Hawaii That Made His Rise Possible,\\\\\" by David Maraniss\\\\nBarack Obama\\'s Branch-y Family Tree by Jake Tapper\\\\n\\\\\"Obama Family Tree\\\\\" series, by Scott Fornek\\\\n\\\\\"Six Degrees of Barack Obama\\\\\"\\\\n\\\\\"Notes on the Ancestry of Senator Barack Hussein Obama, Jr.,\\\\\" by Gary Boyd Roberts\\\\n\\\\\"Obama, Clinton and McCain have some famous relations,\\\\\" by The Associated Press\\\\n\\\\\"Obama\\'s Patriotic Family Tree,\\\\\" by Bill Brehm\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nBarack Obama\\\\nPresidency\\\\nTransition\\xa0ýý Inauguration\\xa0ýý Timeline\\xa0ýý Cabinet\\xa0ýý Judiciary\\xa0ýý Foreign policy\\xa0ýý First 100 days\\\\nEarly life and\\\\npolitical career\\\\nPublic image\\xa0ýý Illinois Senate career\\xa0ýý 2004 Democratic National Convention\\xa0ýý U.S. Senate election in Illinois\\xa0ýý U.S. Senate career\\xa0ýý Presidential primary campaign\\xa0ýý ObamaýýýBiden 2008\\xa0ýý Electoral history\\xa0ýý Political positions\\\\nBooks\\\\nDreams from My Father\\xa0ýý The Audacity of Hope\\\\nSpeeches\\\\nThe Audacity of Hope\\xa0ýý A More Perfect Union\\xa0ýý Change Has Come to America\\xa0ýý 2009 speech to joint session of Congress\\\\nFamily\\\\nMichelle Obama\\xa0ýý Barack Obama, Sr.\\xa0ýý Ann Dunham\\xa0ýý Lolo Soetoro (stepfather)\\xa0ýý Maya Soetoro-Ng (half-sister)\\xa0ýý Marian Robinson (mother-in-law)\\xa0ýý Stanley Armour Dunham (grandfather)\\xa0ýý Madelyn Dunham (grandmother)\\xa0ýý Extended family\\xa0ýý Family tree\\\\nRetrieved from \\\\\"http://en.wikipedia.org/wiki/Family_of_Barack_Obama\\\\\"\\\\nCategories: Obama family | African American history | African American families | Luo Kenyans | People of mixed Black African-European ethnicity | African Americans | Asian Americans | Dutch Americans | English Americans | French Americans | German-Americans | Irish-Americans | Indonesian Americans | Kenyan-Americans | Scottish-Americans | Chinese Canadians | People of mixed Asian-European ethnicity | American families | First Families of the United States | Family treesHidden categories: Wikipedia semi-protected pages | Wikipedia indefinitely move-protected pages | All pages needing cleanup | Wikipedia articles needing factual verification since October 2008 | All pages needing factual verification | All articles with unsourced statements | Articles with unsourced statements since November 2008\\\\nViews\\\\nArticle\\\\nDiscussion\\\\nView source\\\\nHistory\\\\nPersonal tools\\\\nLog in / create account\\\\nNavigation\\\\nMain page\\\\nContents\\\\nFeatured content\\\\nCurrent events\\\\nRandom article\\\\nSearch\\\\nInteraction\\\\nAbout Wikipedia\\\\nCommunity portal\\\\nRecent changes\\\\nContact Wikipedia\\\\nDonate to Wikipedia\\\\nHelp\\\\nToolbox\\\\nWhat links here\\\\nRelated changes\\\\nUpload file\\\\nSpecial pages\\\\nPrintable version Permanent linkCite this page\\\\nLanguages\\\\nBahasa Indonesia\\\\nSvenska\\\\nýýýýýý\\\\nýýýýýý\\\\nThis page was last modified on 16 March 2009, at 03:22.\\\\nAll text is available under the terms of the GNU Free Documentation License. (See Copyrights for details.)\\\\nWikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a U.S. registered 501(c)(3) tax-deductible nonprofit charity.\\\\nPrivacy policy\\\\nAbout Wikipedia\\\\nDisclaimers\\n[DOC 4] Family of Barack Obama - Wikipedia, the free encyclopedia\\\\nFamily of Barack Obama\\\\nFrom Wikipedia, the free encyclopedia\\\\n(Redirected from Sarah Ogwel)\\\\nJump to: navigation, search\\\\nObama Family\\\\nPresident Barack Obama, First Lady Michelle, and daughters Malia and Sasha wave to the crowd after his inaugural address Jan. 20, 2009, on the west steps of the U.S. Capitol.[1]\\\\nCurrent region\\\\nWashington, DC\\\\nInformation\\\\nPlace of origin\\\\nUnited States\\\\nNotable members\\\\nBarack Obama, Michelle Obama, Ann Dunham, Barack Obama, Sr., etc.\\\\nConnected families\\\\nRobinson, Dunham, Soetoro, Ng\\\\nThis article is part of a series about\\\\nBarack Obama\\\\nBackground \\xa0ýý Illinois Senate \\xa0ýý U.S. Senate\\\\nPolitical positions\\xa0ýý Public image\\xa0ýý Family\\\\n2008 primaries\\xa0ýý ObamaýýýBiden campaign\\\\nTransition\\xa0ýý Inauguration\\xa0ýý Electoral history\\\\nPresidency (Timeline, First 100 days)\\\\nMalia, Michelle and Sasha on stage at the 2008 Democratic National Convention\\\\nThe Family of Barack Obama is an extended clan of African American, English, Indonesian, and Kenyan (Luo) heritage known through the writings and political career of Barack Obama, the President of the United States of America,[2][3][4][5] and other reports. His immediate family is the First Family of the United States. The Obamas are the first First Family of African American descent in the United States and the youngest to enter the White House since the Kennedys. One columnist wrote, ýýýObama\\'s young, energetic family harks back to days of Camelot.ýýý[6]\\\\nContents\\\\n1 Immediate family\\\\n2 Extended family - maternal relations\\\\n3 Extended family - paternal relations\\\\n4 Michelle Robinson Obama\\'s extended family\\\\n5 Genealogical charts\\\\n5.1 Obama ancestry\\\\n5.2 Family trees\\\\n6 Distant relations\\\\n7 See also\\\\n8 References\\\\n9 External links\\\\nImmediate family\\\\nMichelle Obama\\\\nMichelle Obama, nýýe Robinson, the wife of Barack Obama, was born on January 17, 1964 in Chicago, Illinois. She is a lawyer and was a University of Chicago Hospital Vice-President. She is the First Lady of the United States.\\\\nMalia Obama and Sasha Obama\\\\nBarack and Michelle Obama have two daughters: Malia Ann (pronounced /mýýýýliýýýý/), born in 1998,[7] and Natasha (known as Sasha) /ýýsýýýýýýýý/), born in 2001. Sasha is the youngest child to reside in the White House since John F. Kennedy, Jr, arrived as an infant in 1961.[8]\\\\nBefore his inauguration, President Obama published an open letter to his daughters in Parade magazine, describing what he wants for them and every child in America: \\\\\"to grow up in a world with no limits on your dreams and no achievements beyond your reach, and to grow into compassionate, committed women who will help build that world.\\\\\"[9]\\\\nWhile living in Chicago, they kept busy schedules, as the Associated Press reports: \\\\\"soccer, dance and drama for Malia, gymnastics and tap for Sasha, piano and tennis for both.\\\\\"[10][11] In July 2008, the family gave an interview to the television series Access Hollywood; Obama later said they regretted allowing the children to be included.[12]\\\\nIn his victory speech on the night of his election, President Obama repeated his promise to Sasha and Malia to get a puppy to take with them to the White House.[13] However the selection of a dog has been slow because Malia is allergic to animal dander;[14] the president subsequently said that the choice has been narrowed down to either a labradoodle or Portuguese Water Dog, and they are hoping to find a shelter animal.[15]\\\\nMalia and Sasha attend the private Sidwell Friends School in Washington, DC, the same school as attended by Chelsea Clinton, Tricia Nixon Cox, and Archibald Roosevelt, and currently the grandchildren of Vice President Joe Biden.[16] The Obama girls began classes there on January 5, 2009.[17] While in Chicago, both attended the private University of Chicago Laboratory School.\\\\nMarian Shields Robinson\\\\nMichelle Obama\\'s mother (birthname Marian Shields, born July 1937), now widowed, married Michelle\\'s father, Fraser Robinson, in 1960.[18][19] Robinson was formerly a secretary at Spiegel catalog and a bank. While Michelle and Barack Obama were campaigning in 2008, Robinson tended the Obama\\'s young children and she intends to do the same while in Washington, DC. Robinson is currently living in the White House itself as part of the First Family;[20] she is the first live-in grandmother there since Elivera M. Doud during the Eisenhower administration.[21] Some media outlets have dubbed Robinson the \\\\\"First Granny\\\\\".[22][21]\\\\nExtended family - maternal relations\\\\nRight-to-left: Barack Obama and Maya Soetoro with their mother Ann Dunham and grandfather Stanley Dunham in Hawaii (early 1970s)\\\\nWikinews has related news:\\\\nBarack Obama elected 44th President of the United States\\\\nGrandmother of Barack Obama dies at 86\\\\nMadelyn Dunham with her daughter Ann\\\\nAccording to Barack Obama\\'s Dreams from My Father, his great-grandmother Leona McCurry was part Native American, which Obama believed Leona held as a \\\\\"source of considerable shame\\\\\" and \\\\\"blanched whenever someone mentioned the subject and hoped to carry the secret to her grave\\\\\"; whereas McCurry\\'s daughter (Obama\\'s maternal grandmother) \\\\\"would turn her head in profile to show off her beaked nose, which along with a pair of jet-black eyes, was offered as proof of Cherokee blood.\\\\\"[23] To date, no concrete evidence has surfaced of Cherokee heritage. Obama\\'s maternal heritage consists mostly of English ancestry, with much smaller amounts of German, Irish, Scottish, Welsh, Swiss, and French ancestry.[2]\\\\nAnn Dunham\\\\nMother of Barack Obama born in 1942, died in 1995. Birthname Stanley Ann Dunham. Anthropologist in Hawaii and Indonesia.\\\\nMadelyn Lee Payne Dunham\\\\nBarack Obama\\'s maternal grandmother, born in 1922 and died on November 2, 2008.[24] She was a bank vice president in Hawaii. Obama said that when he was a child, his grandmother \\\\\"read me the opening lines of the Declaration of Independence and told me about the men and women who marched for equality because they believed those words put to paper two centuries ago should mean something.\\\\\"[9]\\\\nStanley Armour Dunham\\\\nGrandfather of Barack Obama, born 1918, died 1992. World War II U.S. Army sergeant, furniture salesman in Hawaii.\\\\nCharles T. Payne\\\\nGreat-uncle of Barack Obama, younger brother of Madelyn Dunham, born 1925. Served during World War II in the U.S. Army 89th Infantry Division.[25] Obama has often described Payne\\'s role in liberating Ohrdruf forced labor camp.[26] There was brief media attention when Obama mistakenly identified the camp as Auschwitz during the campaign.[27] Payne appeared in the visitor\\'s gallery at the Democratic National Convention in Denver, Colorado, when his great-nephew was nominated for President.[28] He was the assistant director of the University of Chicago\\'s Library.[26]\\\\nMaya Soetoro-Ng\\\\nHalf-sister of Barack Obama, born August 15, 1970, in Jakarta, Indonesia.[29] She is married to Konrad Ng, with whom she has a daughter, Suhaila. Maya Soetoro-Ng is a teacher in Hawaii.\\\\nKonrad Ng\\\\nBrother-in-law of Barack Obama, born 1974. A Canadian whose parents are Malaysian Chinese immigrants, he is an assistant professor at the University of Hawaii\\'s Academy of Creative Media.[30] His parents are from Kudat and Sandakan, two small towns in Sabah, Malaysia, and he was born and raised in Burlington, Ontario.[31] He married Maya Soetoro-Ng at the end of 2003 in Hawaii.[32] They have one daughter, Suhaila.[33][34][35] Konrad Ng is now a US citizen.[36]\\\\nLolo Soetoro\\\\nStepfather of Barack Obama, born in Indonesia 1936, died 1987.\\\\nExtended family - paternal relations\\\\nThe Obamas are members of the Luo, Kenya\\'s third-largest ethnic group, which is part of a larger family of ethnic groups, collectively also known as Luo. This group belongs to the Eastern Sudanic branch of the Nilo-Saharan phylum. The Obama family is largely concentrated in the western province of Nyanza.\\\\nFront row (left to right): Auma Obama (Barack\\'s half-sister), Kezia Obama (Barack\\'s step-mother), Sarah Hussein Onyango Obama (third wife of Barack\\'s paternal grandfather), Zeituni Onyango (Barack\\'s aunt)\\\\nBack row (left to right): Said Obama (Barack\\'s uncle), Barack Obama, Abongo [Roy] Obama (Barack\\'s half-brother), unidentified woman, Bernard Obama (Barack\\'s half-brother), Abo Obama (Barack\\'s half-brother).\\\\nBarack Obama, Sr.\\\\nBarack Obama\\'s father, (1936ýýý1982). Government economist in Kenya. In addition to President Obama, Barack Obama Sr. fathered six other sons and a daughter.[37]\\\\nHussein Onyango Obama\\\\nBarack Obama\\'s paternal grandfather (c. 1895ýýý1979);[38] he worked as a mission cook. He joined the British Army during World War I. (One source gives 1870ýýý1975 as his dates of birth and death based on his tombstone reading \\\\\"Mzee Hussein Onyango Obama\\\\\" in his home village.[39] The term \\\\\"mzee\\\\\" is a Kenyan honorific meaning \\\\\"old man\\\\\" or \\\\\"elder.\\\\\") According to his third wife, Sarah, he originally converted to Catholicism, but took the name Hussein when he later converted to Islam; she said he passed the name, not the religion, on to his children.[40]\\\\nHabiba Akumu Obama\\\\nBarack Obama\\'s paternal grandmother, and the second wife of Hussein Onyango Obama. A photograph of her holding her son, Barack Sr, on her lap is on the cover of her grandson\\'s memoirs titled Dreams from my Father.[41]\\\\nSarah Obama\\\\nThird wife of Obama\\'s paternal grandfather, born 1922.[39] Also known, through the addition of her late husband\\'s name, as Sarah Onyango Obama,[42] and sometimes referred to as Sarah Ogwel, Sarah Hussein Obama or Sarah Anyango Obama,[43] she lives in Nyangýýýoma Kogelo village, 30 miles west of western Kenya\\'s main town, Kisumu, on the edge of Lake Victoria.[44][45]\\\\nAlthough not a blood relation, Barack Obama calls her \\\\\"Granny Sarah\\\\\".[43][46] Sarah, who speaks Luo and only a few words of English, communicates with President Obama through an interpreter.\\\\nOn July 4, 2008, she attended the United States Independence Day celebrations in Nairobi, hosted by Michael Ranneberger, the US ambassador in Kenya.[47]\\\\nDuring the campaign she protested attempts to portray Obama as a foreigner to the United States or a Muslim, saying that while Obama\\'s grandfather had been a Muslim, \\\\\"In the world of today, children have different religions from their parents.\\\\\"[40] Sarah Obama herself is \\\\\"a strong believer of the Islamic faith,ýýý in her words.[48]\\\\nKezia Obama\\\\nFirst wife of Barack Obama\\'s father, born c. 1940. She is Barack Obama Sr.\\'s first wife, whom he married in Kenya before studying abroad in the United States. Also known as Kezia Grace Obama.[49][50] She currently lives in Bracknell, Berkshire, England.[51][52] Her sister, Jane, is the \\'Auntie Jane\\' mentioned at the very start of Dreams from My Father when she telephoned President Obama to inform him that his father had been killed in a car accident.[53]\\\\nMalik Obama\\\\nBarack Obama\\'s half-brother, also known as Abongo or Roy, born c. March, 1958,[51] son of Barack Obama, Sr. with his first wife, Kezia.[54] Malik Obama was born and raised in Nairobi, Kenya.[55] He earned a degree in accounting from the University of Nairobi.[56] He met his half-brother for the first time in 1985[55] when Barack flew from Chicago to Washington, D.C. to visit him.[57] Malik and his half-brother Barack were best men at each other\\'s weddings.[55] Barack Obama brought his wife Michelle to Kenya three years later, and they met with Malik again while Barack was introducing Michelle to many other new relatives.[58]\\\\nAlthough much of the Obama family has dispersed throughout Kenya and overseas, most, including Malik Obama, still considered their rural village on the shores of Lake Victoria to be their true home, and feel that those who have left the village have become culturally \\\\\"lost\\\\\".[59] A frequent visitor to the United States,[58] and consultant in Washington, D.C. for several months per year,[55] he nevertheless settled in the Obamas\\' ancestral home, Nyangýýýoma Kogelo, a village of several hundred people that he prefers to the city for its slow pace.[55] He runs a small electronics shop a half hour drive outside of town.[55]\\\\nDuring his brother\\'s presidential campaign, Malik Obama was a spokesman for the extended Obama family in Kenya, dealing with safety and privacy concerns arising from increased attention from the press.[60]\\\\nAbo Obama\\\\nBarack Obama\\'s half-brother, born 1968. International telephone store manager in Kenya.\\\\nAuma Obama\\\\nBarack Obama\\'s half-sister, born c. 1960.[61] As of July 2008, development worker in Kenya.[62] She studied German at the University of Heidelberg from 1981 to 1987. After her graduation in Heidelberg she went on for graduate studies at the University of Bayreuth, which awarded her a PhD in 1996. Her dissertation was about the conception of labor in Germany and its literary reflections.[62] Auma Obama lives in London, and in 1996 married an Englishman, Ian Manners. They have a daughter named Akinyi (b. 1997).[62][verification needed]\\\\nBernard Obama\\\\nBarack Obama\\'s half-brother, born 1970, son of Barack Obama, Sr. and his first wife, Kezia. He had been an auto parts supplier in Nairobi, Kenya, and has one child. Bernard converted to Islam as an adult and has said: \\\\\"Iýýým a Muslim, I donýýýt deny it. My father was raised a Muslim. But itýýýs not an issue. I donýýýt know what all the hullabaloo is about.\\\\\"[63] He currently resides in Bracknell, England, with his mother Kezia.[63]\\\\nRuth Ndesandjo\\\\nBorn Ruth Nidesand, in US c. 1940s, Barack Obama Sr.\\'s third wife and a private kindergarten director in Kenya.[64] Ruth\\'s two sons with Barack Obama, Sr., are Mark and David Ndesandjo; her third son, Joseph Ndesandjo, was born c. 1980 from a subsequent marriage to a Tanzanian.[65][66]\\\\nMark Ndesandjo\\\\nBarack Obama\\'s half-brother, son of Ruth Nidesand and Barack Obama Sr.[67] He runs an Internet company called WorldNexus that advises Chinese corporations how best to reach international customers.[68] Mark graduated from Brown University, studied physics at Stanford University, received an MBA from Emory University, and has lived in Shenzhen, China, since 2002 and is married to a Chinese woman.[69] He is also an accomplished pianist.[70]\\\\nDavid Ndesandjo\\\\nDavid Ndesandjo\\\\nBarack Obama\\'s half-brother (also known as David Opiyo Obama), son of Ruth Nidesand and Barack Obama Sr. Killed in a motorcycle accident.[71]\\\\nGeorge Hussein Onyango Obama\\\\nYoungest half-brother of Barack Obama, born c.1982, son of Barack Obama Sr.[72] and Jael (now a resident of Atlanta, Georgia).[73][74] George was six months old when his father died in an automobile accident, after which he was raised in Nairobi by his mother and a French step-father. He later lived in South Korea for two years while his mother resided there for business reasons.[73] Returning to Kenya, George Obama \\\\\"slept rough for several years,\\\\\" until his aunt gave him a six-by-eight foot corrugated metal shack in the Nairobi, Kenya, slum of Huruma Flats.[73] As of August 2008, Obama was studying to become a mechanic.[73] George received little attention until being featured in an article in the Italian language edition of Vanity Fair in August 2008, which portrayed him as living in poverty, shame, and obscurity.[75] The article quoted Obama as saying that he lived \\\\\"on less than a dollar a month\\\\\" and stated that he \\\\\"does not mention his famous half-brother in conversation\\\\\" out of shame at his poverty.[76] In later interviews George contradicted this picture. In an interview with The Times, Obama \\\\\"said that he was furious at subsequent reports that he had been abandoned by the Obama family and that he was filled with shame about living in a slum.\\\\\"[74] He told The Times, \\\\\"Life in Huruma is good.\\\\\" Obama said that he expects no favors, that he was supported by relatives, and that reports he lived on a dollar a month were \\\\\"all lies by people who donýýýt want my brother to win.ýýý[74] He told The Telegraph that he was inspired by his half-brother.[73] According to Time, George \\\\\"has repeatedly denied...that he feels abandoned by Obama.\\\\\"[77] CNN quoted him as saying, \\\\\"I was brought up well. I live well even now. The magazines, they have exaggerated everything... I think I kind of like it here. There are some challenges, but maybe it is just like where you come from, there are the same challenges.\\\\\"[75] George\\'s reported poverty was seized on by conservative critics of Barack Obama. Columnist Dinesh D\\'Souza solicited donations for George Obama from his readers,[78] while Jerome Corsi planned to give him a $1,000 check during a trip to Kenya (Corsi was expelled from the country by immigration authorities).[77]\\\\nOmar Obama\\\\nHalf-uncle of Barack Obama,[79] born on June 3, 1944 in Nyangýýýoma Kogelo. Oldest son of Onyango and Sarah Obama, resides in Boston, Massachusetts.[citation needed]\\\\nZeituni Onyango\\\\nHalf-aunt of Barack Obama,[80] born May 29, 1952, in Kenya,[81] Onyango is referred to as \\\\\"Aunti Zeituni\\\\\" in President Obama\\'s memoir, Dreams from My Father.[82]\\\\nYusuf Obama\\\\nHalf-uncle of Barack Obama,[79] born c. 1950s in Nyangýýýoma Kogelo; son of Onyango and Sarah Obama.[citation needed]\\\\nSaid Obama\\\\nHalf-uncle of Barack Obama,[79][83] born c. 1950s in Nyangýýýoma Kogelo; son of Onyango and Sarah Obama.[citation needed]\\\\nMichelle Robinson Obama\\'s extended family\\\\nFraser Robinson, Sr. (1884ýýý1936) of South Carolina, shown in an old photo along with his wife, Rosella Cohen Robinson, in its background\\\\nBarack Obama has called his wife Michelle \\\\\"the most quintessentially American woman I know.\\\\\"[3] Her family is of African American heritage, descendents of Africans of the American Colonial Era.[3] Michelle Obama\\'s family history traces back from slavery to Reconstruction to the Great Migration North. Some of Michelle\\'s relatives still reside in South Carolina.\\\\nMichelle\\'s earliest known relative is her great-great grandfather Jim Robinson, born in the 1850s, who was an American slave on the Friendfield plantation in South Carolina. The family believes that after the Civil War he remained a Friendfield plantation sharecropper for the rest of his life and that he was buried there in an unmarked grave.[3]\\\\nJim had two sons, Gabriel and Fraser, Michelle Obama\\'s great-grandfather. Fraser had an arm amputated as a result of a boyhood injury. He worked as a shoemaker, newspaper salesman and in a lumber mill and was married to Rosella Cohen.[3] Carrie Nelson, Gabriel Robinson\\'s daughter, now 80, is the oldest living Robinson and the keeper of family lore.[3]\\\\nAt least three of Michelle Obama\\'s great-uncles served in the military of the United States. One aunt moved to Princeton, New Jersey, where she worked as a maid, and cooked Southern-style meals for Michelle and her brother, Craig, when they were students at Princeton University.\\\\nCraig Robinson\\\\nMichelle Obama\\'s brother, born 1962. He is currently head coach of men\\'s basketball at Oregon State University.[84]\\\\nFraser Robinson III\\\\nMichelle Obama\\'s father, born 1935, died 1991, married Michelle\\'s mother, Marian Shields, in 1960.[85][19] Robinson was a pump worker at the City of Chicago water plant.[3]\\\\nFraser Robinson, Jr.\\\\nMichelle Obama\\'s grandfather was born on August 24, 1912 in Georgetown, South Carolina, and died on November 9, 1996, aged 84. He was a good student and orator, but moved from South Carolina to Chicago to find better work than he could find at home, eventually becoming a worker for the United States Postal Service. He was married to LaVaughn Johnson. When he retired, they moved back to South Carolina.[3]\\\\nCapers C. Funnye Jr.\\\\nMichelle Obama\\'s first cousin once removed: Funnyeýýýs mother, Verdelle Robinson Funnye (born Verdelle Robinson; August 22, 1930 ýýý April 16, 2000) and Michelle Obamaýýýs paternal grandfather, Fraser Robinson Jr., were siblings. One of America\\'s most prominent African American Jews, known for acting as a bridge between mainstream Jewry and African Americans.[86]\\\\nGenealogical charts\\\\nObama ancestry\\\\n16. Opiyo\\\\n8. Obama\\\\n4. Hussein Onyango Obama\\\\n9. Nyaoke\\\\n2. Barack Hussein Obama, Sr.\\\\n5. Habiba Akumu\\\\n1. Barack Hussein Obama II\\\\n24. Jacob William Dunham\\\\n12. Ralph Waldo Emerson Dunham, Sr.\\\\n25. Mary Ann Kearney\\\\n6. Stanley Armour Dunham\\\\n26. Harry Ellington Armour\\\\n13. Ruth Lucille Armour\\\\n27. Gabriella Clark\\\\n3. Stanley Ann Dunham\\\\n28. Charles T. Payne\\\\n14. Rolla Charles Payne\\\\n29. Della L. Wolfley\\\\n7. Madelyn Lee Payne\\\\n30. Thomas Creekmore McCurry\\\\n15. Leona Belle McCurry\\\\n31. Margaret Belle Wright\\\\nFamily trees\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nBarack Obama\\\\nStanley\\\\nDunham\\\\n1918ýýý1992\\\\nMadelyn\\\\nPayne\\\\n1922ýýý2008\\\\nHabiba\\\\nAkumu\\\\nOnyango\\\\nObama\\\\nc.\\xa01895ýýý1979\\\\nSarah\\\\nOgwel\\\\n1922ýýý\\\\nLolo\\\\nSoetoro\\\\n1936ýýý1987\\\\nAnn\\\\nDunham\\\\n1942ýýý1995\\\\nBarack\\\\nObama, Sr. *\\\\n1936ýýý1982\\\\nKezia\\\\nGrace\\\\nc. 1940ýýý\\\\nOmar\\\\nObama\\\\n1944ýýý\\\\nZeituni\\\\nOnyango\\\\n1952ýýý\\\\nYusuf\\\\nObama\\\\nc. 1950sýýý\\\\nSaid\\\\nObama\\\\nc. 1950sýýý\\\\nKonrad\\\\nNg\\\\nc. 1974ýýý\\\\nMaya\\\\nSoetoro\\\\n1970ýýý\\\\nBarack\\\\nObama\\\\n1961ýýý\\\\nMichelle\\\\nRobinson\\\\n1964ýýý\\\\nM. Abongo\\\\nObama\\\\n1958ýýý\\\\nAuma\\\\nObama\\\\nc. 1960ýýý\\\\nAbo\\\\nObama\\\\n1968ýýý\\\\nBernard\\\\nObama\\\\n1970ýýý\\\\nSuhaila\\\\nNg\\\\nc. 2005ýýý\\\\nMalia Ann\\\\nObama\\\\n1998ýýý\\\\nSasha\\\\nObama\\\\n2001ýýý\\\\n* Barack\\\\nObama, Sr.\\'s\\\\nadditional\\\\nRuth\\\\nNidesandjo\\\\nc. 1940sýýý\\\\nJael\\\\nOtieno\\\\nrelationships:\\\\nMark\\\\nNdesandjo\\\\nDavid\\\\nNdesandjo\\\\ndied\\xa0c.\\xa01987\\\\nGeorge\\\\nObama\\\\nc. 1982ýýý\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nMichelle Obama\\\\nFraser\\\\nRobinson, Jr.\\\\nýýý\\\\nSouth Carolina\\\\n(1912ýýý1996)\\\\nSawmill worker\\\\nLaVaughn D.\\\\nJohnson\\\\nýýý\\\\nIllinois\\\\n(1915ýýý2002)\\\\nCapers C.\\\\nFunnye, Jr.\\\\nýýý\\\\n(born c. 1952;\\\\nnephew of\\\\nFraser\\\\nRobinson, Jr.)\\\\nRabbi in Chicago\\\\nFraser\\\\nRobinson III\\\\nýýý\\\\nIllinois\\\\n(1935ýýý1991)\\\\nEnjoyed boxing\\\\nin youth;\\\\nwater plant\\\\npump operator\\\\nin Chicago\\\\nMarian\\\\nShields\\\\nýýý\\\\nIllinois\\\\n(born 1937)\\\\nSecretary at\\\\nSpiegel catalog\\\\nin Chicago;\\\\nU.S.\\'s First\\\\nGrandmother\\\\nCraig\\\\nRobinson\\\\nýýý\\\\nIllinois\\\\n(born 1962)\\\\nHead coach of\\\\nOregon State\\\\nBeavers men\\'s\\\\nbasketball\\\\nMichelle\\\\nRobinson\\\\nýýý\\\\nIllinois\\\\n(born 1964)\\\\nFirst Lady\\\\nof the United States\\\\nDistant relations\\\\nSee also: List of United States Presidents by genealogical relationship\\\\nAccording to genealogists, Barack Obama\\'s distant cousins include the multitude of descendants of his maternal ancestors from all along the early-American Atlantic seaboard as well as paternal, Kenyan relations belonging to the Luo tribe, many descending from a 17th century ancestor named Owiny.[87][88] For example, George W. Bush, the 43rd U.S. president, is the eleventh cousin of Barack Obama.[89] The New York Times science writer Nicholas Wade argues that with eleven generations leading back to their common progenitor, Samuel Hinckley, the relationship between the 43rd President and the 44th President is \\\\\"genetically meaningless\\\\\".[90]\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nSelected genealogical relationships\\\\nBill Hickok\\\\nAccording to Barack Obama\\'s family lore (and confirmed by the New England Historic Genealogical Society), the President and Hickok are sixth cousins, six-times removed.[91]\\\\nRobert Duvall\\\\nGoodnight helped inspire Pulitzer Prize-winning author Larry McMurtry to create a protagonist for his novel series Lonesome Dove: Woodrow Call. In Dove\\'s television novela, Woodrow Call\\'s partner is Gus McCrae, portrayed by Obama\\'s eighth cousin, twice removed, actor Robert Duvall.[92]\\\\nJames Madison\\\\nObama is also distantly related to U.S. Presidents James Madison, Lyndon Johnson, Gerald Ford, and U.S. Vice President Dick Cheney, British Prime Minister Sir Winston Churchill, U.S. Civil War General Robert E. Lee, and actor Brad Pitt.[93][94][95]\\\\nCharles Goodnight\\\\nAccording to Chicago Sun-Times reporter Scott Fornek, another Obama progenitor, Catherine Goodnight, was the grandniece of George Goodnight, who was in turn great-grandfather of famed cattleman Charles Goodnight.[92]\\\\nHarry S. Truman\\\\nActor Duvall is distant cousins with United States President Harry Truman, who\\'s likewise a fourth cousin, four times removed, of Obama\\'s.[92]\\\\nGeorgia O\\'Keeffe\\\\nNotable women Obama is related to include painter Georgia OýýýKeeffe.[93]\\\\nSee also\\\\nU.S. Presidential genealogical relationships\\\\nNew England Historic Genealogical Society\\\\nGary Boyd Roberts\\\\nReferences\\\\n^ Keck, Kristi (4 June 2008). \\\\\"Obama crosses historic milestone\\\\\". CNN. http://news.yahoo.com/s/ap/20081122/ap_on_go_pr_wh/obama_school_12. Retrieved on 2008-11-21.\\\\n^ a b Reitwiesner, William Addams. \\\\\"Ancestry of Barack Obama\\\\\". http://www.wargs.com/political/obama.html. Retrieved on 2008-10-09.\\\\n^ a b c d e f g h Murray, Shailagh (2 October 2008). \\\\\"A Family Tree Rooted In American Soil: Michelle Obama Learns About Her Slave Ancestors, Herself and Her Country\\\\\". The Washington Post: p.\\xa0C01. http://www.washingtonpost.com/wp-dyn/content/article/2008/10/01/AR2008100103169.html. Retrieved on 2008-10-10.\\\\n^ Sheridan, Michael (5 February 2007). \\\\\"Secrets of Obama Family Unlocked\\\\\". Muslim Observer. http://news.newamericamedia.org/news/view_article.html?article_id=ae5895fc29971b172938790be94ab107. Retrieved on 2008-11-21.\\\\n^ RTE news report March, 2007:Obamas Irish family links discovered by ancestry.co.uk\\\\n^ Noveck, Jocelyn (2008-11-07). \\\\\"Deseret News | Obama\\'s young, energetic family harks back to days of Camelot\\\\\". Secure.deseretnews.com. https://secure.deseretnews.com/article/1,5143,705261276,00.html. Retrieved on 2009-01-31.\\\\n^ Liza Mundy, Michelle: A Biography (Simon and Schuster, 2008), p. 129.\\\\n^ \\\\\"Sasha Obama\\\\\". Baltimore Sun. http://www.baltimoresun.com/topic/politics/sasha-obama-PECLB004381.topic. Retrieved on 2009-01-31.\\\\n^ a b Obama, Barack (13 January 2009). \\\\\"\\'What I Want for You--And Every Child in America\\'\\\\\". http://www.parade.com/news/2009/01/barack-obama-letter-to-my-daughters.html.\\\\n^ Sobieraj Westfall, Sandra (23 June 2008). \\\\\"Barack Obama Gives Daughter $1 Allowance a Week\\\\\". People Magazine. http://www.people.com/people/article/0,,20214569_1,00.html. Retrieved on 2008-11-21.\\\\n^ Lester, Will (July 23, 2008). \\\\\"Obama daughters keep hectic schedules of their own\\\\\". Associated Press. http://elections.apnews.com/apelect/db_6911/contentdetail.htm;jsessionid=8314A43012AB5FF1D0697247362D8752?contentguid=H95QubFb&full=true. Retrieved on 2008-08-04.\\\\n^ Hiro, Anne. \\\\\"Obama regrets letting \\\\\"Access Hollywood\\\\\" interview daughters. Won\\'t do it again. MSNBC\\'s Dan Abrams gets the story behind the story. - Lynn Sweet\\\\\". Blogs.suntimes.com. http://blogs.suntimes.com/sweet/2008/07/obama_regrets_letting_access_h.html. Retrieved on 2009-01-31.\\\\n^ Ahmed, Saeed (5 November 2008). \\\\\"Move over Barney, new dog moving into White House\\\\\". CNN. http://www.cnn.com/2008/LIVING/wayoflife/11/05/presidential.pets/index.html. Retrieved on 2008-11-21.\\\\n^ \\\\\"Obama: Getting a dog isn\\'t easy\\\\\". Associated Press. 7 November 2008. http://www.mercurynews.com/ci_10927292. Retrieved on 2008-11-21.\\\\n^ Janice Lloyd (2009-01-12). \\\\\"Obamas down to Labradoodle or Portuguese water dog\\\\\". USA Today. http://www.usatoday.com/news/washington/2009-01-11-obama-dog_N.htm. Retrieved on 2009-01-28.\\\\n^ Swarns, Rachel (21 November 2008). \\\\\"And the Winner Is ýýý Sidwell Friends\\\\\". The New York Times. http://thecaucus.blogs.nytimes.com/2008/11/21/and-the-winner-is-sidwell-friends/. Retrieved on 2008-11-21.\\\\n^ Tolin, Lisa (2009-01-05). \\\\\"Obama girls start school with photographers in tow\\\\\". The Associated Press. http://www.google.com/hostednews/ap/article/ALeqM5g6mv_lkODQMmQdpyIEnr8Zpm5mogD95H8KA80. Retrieved on 2009-01-06.\\\\n^ Taylor Marsh (2008-08-25). \\\\\"Political Analysis, National Security and Breaking News\\\\\". Taylor Marsh. http://www.taylormarsh.com/archives_view.php?id=28286. Retrieved on 2009-01-31.\\\\n^ a b Lia LoBello (2008-01-02). \\\\\"First Families: Radar introduces you to the next president\\'s relatives\\\\\". Radar Online. http://www.radaronline.com/features/2008/07/john_mccain_barack_obama_michelle_cindy_dunham_roberta_wrigh_04.php. Retrieved on 2009-01-28.\\\\n^ Rachel L. Swarns (2009-01-09). \\\\\"Obamaýýýs Mother-in-Law to Move Into the White House\\\\\". The New York Times. http://thecaucus.blogs.nytimes.com/2009/01/09/obamas-mother-in-law-to-move-into-the-white-house/?hp. Retrieved on 2009-01-09.\\\\n^ a b \\\\\"Will Obama mum-in-law make it a family affair in the White House?\\\\\". Agence France Presse. 2008-11-22. http://www.google.com/hostednews/afp/article/ALeqM5gN_i2jrCVkJQgfMbSDRRrNk8U4Sw. Retrieved on 2009-01-09.\\\\n^ Philip Sherwell (2008-2008-11-09). \\\\\"Michelle Obama persuades First Granny to join new White House team\\\\\". The Telegraph (UK). http://www.telegraph.co.uk/news/3407525/Michelle-Obama-persuades-First-Granny-to-join-new-White-House-team.html. Retrieved on 2009-01-09.\\\\n^ \\\\\"ýýýTootýýý: Obama grandmother a force that shaped him\\\\\". via Associated Press. 2008-08-25. http://www.thekansan.com/news/x1311851415/-Toot-Obama-grandmother-a-force-that-shaped-him. Retrieved on 2008-08-29.\\\\n^ \\\\\"CNN: \\\\\"Obama\\'s grandmother dies after battle with cancer\\\\\"\\\\\". http://www.cnn.com/2008/POLITICS/11/03/obama.grandma/index.html. Retrieved on 2008-11-04.\\\\n^ The 89th Infantry Division, United States Holocaust Memorial Museum\\\\n^ a b Obama\\'s great-uncle recalls liberating Nazi camp, Boston.com, July 22, 2008\\\\n^ Major Garrett (2008-05-27). \\\\\"Obama Campaign Scrambles to Correct the Record on Uncle\\'s War Service\\\\\". FOXNews.com. http://elections.foxnews.com/2008/05/27/recollection-of-obama-familys-service-missing-key-details. Retrieved on 2009-01-31.\\\\n^ \\\\\"Democrats salute Obamaýýýs great uncle\\\\\". Jewish Telegraphic Agency. August 28, 2008. http://jta.org/news/article/2008/08/28/110123/obamapayne. Retrieved on 31 January 2009.\\\\n^ Obama Family Tree dgmweb.net\\\\n^ Chicago Sun Times article with her picture\\\\n^ Obama has links to Malaysia\\\\n^ Nolan, Daniel (2008-06-11). \\\\\"Relative: Obama\\'s got \\'a good handle on Canada\\'\\\\\". The Hamilton Spectator. http://www.thespec.com/burlingtonlife/article/384475. Retrieved on 2008-07-03.\\\\n^ Nolan, Daniel (June 11, 2008). \\\\\"Obama\\'s Burlington connection\\\\\". The Hamilton Spectator. http://www.thespec.com/article/384307. Retrieved on 2008-06-21.\\\\n^ Misner, Jason (2008-06-20). \\\\\"Barack Obama was here\\\\\". Burlington Post. http://www.burlingtonpost.com/printarticle/186215. Retrieved on 2008-07-03.\\\\n^ Fornek, Scott (2007-09-09). \\\\\"\\'He helped me find my voice\\'\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545473,BSX-News-wotreehh09.article.\\\\n^ Cooper, Tom (2009-01-20). \\\\\"Keep watch for Obama\\\\\". TheSpec.com. http://www.thespec.com/Opinions/article/499161. Retrieved on 2009-01-28.\\\\n^ Ancestry of Barack Obama\\\\n^ Dreams from My Father, p. 376\\\\n^ a b Kenya: Special Report: Sleepy Little Village Where Obama Traces His Own Roots (Page 2 of 2)\\\\n^ a b \\\\\"Obama\\'s grandma slams \\'untruths\\'\\\\\". Associated Press. 2008-03-05. http://www.usatoday.com/news/world/2008-03-05-obama-kin_N.htm.\\xa0 See also this correction.\\\\n^ \\\\\"Q&A ON THE NEWS\\\\\". Atlanta Journal-Constitution. 2009-02-25. http://www.ajc.com/services/content/metro/stories/2009/02/25/questi0225.html. Retrieved on 2009-02-27.\\\\n^ In Kenya, Barack Obamaýýýs family prays for end to conflict - Times Online\\\\n^ a b Crilly, Rob (February 27, 2008). \\\\\"Dreams from Obama\\'s Grandmother\\\\\". Time Magazine, Inc.. http://www.time.com/time/world/article/0,8599,1717590,00.html?xid=rss-topstories. Retrieved on 2008-07-03.\\\\n^ Pflanz, Mike (2008-01-11). \\\\\"Barack Obama\\'s Kenyan relatives keep faith\\\\\". Daily Telegraph. http://www.telegraph.co.uk/news/main.jhtml?xml=/news/2008/01/09/wuspols1009.xml.\\\\n^ Fornek, Scott (2007-09-09). \\\\\"Sarah Obama - \\'Sparkling, laughing eyes\\'\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545459,BSX-News-wotreeu09.article.\\\\n^ \\\\\"Barack Obama in Kenya\\\\\". CNN. http://www.youtube.com/watch?v=Ikg6gj71U9k.\\\\n^ Daily Nation, July 8, 2008: Obama granny\\'s day out with envoys and top politicians\\\\n^ \\\\\"A Candidate, His Minister and the Search for Faith\\\\\". New York Times. 2007-04-30. http://www.nytimes.com/2007/04/30/us/politics/30obama.html?_r=2&pagewanted=all&oref=slogin&oref=slogin.\\\\n^ \\\\\"Kenya: All Obama kin to spend voting day in Kogelo\\\\\". afrika.no. 2008-10-27. http://www.afrika.no/Detailed/17321.html. Retrieved on 2009-01-31.\\\\n^ Cohen, Roger (2008-03-06). \\\\\"The Obamas of the World - New York Times\\\\\". Nytimes.com. http://www.nytimes.com/2008/03/06/opinion/06cohen.html. Retrieved on 2009-01-31.\\\\n^ a b Sanderson, Elizabeth (2008-01-06). \\\\\"Barack Obama\\'s stepmother living in Bracknell reveals the close bond with him ... and his mother\\\\\". Daily Mail. http://www.dailymail.co.uk/news/article-506338/Barack-Obamas-stepmother-living-Bracknell-reveals-close-bond---mother.html.\\\\n^ Lindsay, Anna (2009-01-20). \\\\\"Barack\\'s bingo-loving stepmother\\\\\". BBC News. http://news.bbc.co.uk/1/hi/england/berkshire/7834368.stm.\\\\n^ http://www.newvision.co.ug/D/8/26/666733\\\\n^ jpt (2008-06-18). \\\\\"From the Fact Check Desk: What Did Obama\\'s Half-Brother Say About Obama\\'s Background\\\\\". ABC News. http://blogs.abcnews.com/politicalpunch/2008/06/from-the-fact-c.html.\\\\n^ a b c d e f Maliti, Tom (2004-10-26). \\\\\"Obama\\'s Brother Chooses Life in Slow Lane\\\\\". The Associated Press. http://www.msnbc.msn.com/id/6333496.\\\\n^ Obama, Dreams from my Father, 2004, p. 265.\\\\n^ Obama, Dreams from my Father, 2004, p. 262.\\\\n^ a b *Oywa, John (2004-08-15). \\\\\"Sleepy Little Village Where Obama Traces His Own Roots\\\\\". The Daily Nation. http://allafrica.com/stories/200408160533.html.\\\\n^ Philip Ochieng (2004-11-01). \\\\\"From Home Squared to the US Senate: How Barack Obama Was Lost and Found\\\\\". The East African. http://www.nationmedia.com/EastAfrican/01112004/Features/PA2-11.html. Retrieved on 2008-03-23.\\\\n^ Warah, Rasna (2008-06-09). \\\\\"We cannot lay claims on Obama; he\\'s not one of us - Obama in this world\\\\\". Daily Nation. http://www.nationmedia.com. Retrieved on 2008-07-10.\\\\n^ Scott Fornek (2007-09-09). \\\\\"AUMA OBAMA: \\'Her restlessness, her independence\\'\\\\\". Chicago Sun Times. http://www.suntimes.com/news/politics/obama/familytree/545465,BSX-News-wotreew09.article. Retrieved on 2008-03-23.\\\\n^ a b c Gathmann, Florian; Gregor Peter Schmitz, Jochen Schýýnmann (July 24, 2008). \\\\\"Studentin in der Bundesrepublik: Wie Auma Obama mit Deutschland haderte\\\\\" (in German). Spiegel Online. http://www.spiegel.de/politik/ausland/0,1518,567286,00.html. Retrieved on 2008-07-24.\\\\n^ a b Harvey, Oliver (07-26 2008). \\\\\"Obama\\'s brother is in Bracknell\\\\\". The Sun. http://www.thesun.co.uk/sol/homepage/news/the_real_american_idol/article1472877.ece. Retrieved on 2008-10-06.\\\\n^ \\\\\"Madari Kindergarten\\\\\". http://www.madarikindergarten.com/.\\\\n^ \\\\\"Welcome To MedWeek San Antonio 2007\\\\\". Medweeksa.org. http://www.medweeksa.org/awardwinners/techfirm.htm. Retrieved on 2009-01-31.\\\\n^ \\\\\"PIDE - Partners for International Development & Education Inc\\\\\". Pideafrica.org. http://pideafrica.org/aboutus.htm. Retrieved on 2009-01-31.\\\\n^ Barack Obamaýýýs brother pushes Chinese imports on US - Times Online\\\\n^ Obama half-brother runs Internet company in China\\\\n^ Roger Cohen (2008-03-17). \\\\\"Obama\\'s Brother in China\\\\\". The New York Times. http://www.nytimes.com/2008/03/17/opinion/29cohen.html. Retrieved on 2008-03-23.\\\\n^ \\\\\"Youku Buzz (daily)\\xa0ýý Blog Archive\\xa0ýý Barack Obamaýýýs Half-Brother in Concert\\\\\". Buzz.youku.com. 2009-01-18. http://buzz.youku.com/2009/01/18/barack-obamas-half-brother-in-concert/. Retrieved on 2009-01-31.\\\\n^ jaketapper (2008-07-28). \\\\\"Political Punch: Barack Obama\\'s Branch-y Family Tree\\\\\". Blogs.abcnews.com. http://blogs.abcnews.com/politicalpunch/2008/07/barack-obamas-1.html. Retrieved on 2009-01-31.\\\\n^ Fornek, Scott (September 9, 2007). \\\\\"HALF-BROTHER GEORGE: \\'I would be there for him\\'\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545447,BSX-News-wotreecc09.stng. Retrieved on 2008-08-04.\\\\n^ a b c d e Pflanz, Mike (August 21, 2008). \\\\\"Barack Obama is my inspiration, says lost brother\\\\\". The Daily Telegraph. http://www.telegraph.co.uk/news/newstopics/uselection2008/barackobama/2595688/Barack-Obama-is-my-inspiration-says-lost-brother.html. Retrieved on 2008-08-23.\\\\n^ a b c Crilly, Rob (August 22, 2008). \\\\\"Life is good in my Nairobi slum, says Barack Obama\\'s younger brother\\\\\". The Times. http://www.timesonline.co.uk/tol/news/world/us_and_americas/us_elections/article4583353.ece. Retrieved on 2008-08-23.\\\\n^ a b McKenzie, David (2008-08-23). \\\\\"Behind the Scenes: Meet George Obama\\\\\". CNN. http://www.cnn.com/2008/POLITICS/08/22/bts.obama.brother/. Retrieved on 2008-10-26.\\\\n^ Pisa, Nick (August 20, 2008). \\\\\"Barack Obama\\'s \\'lost\\' brother found in Kenya\\\\\". Daily Telegraph. http://www.telegraph.co.uk/news/newstopics/uselection2008/barackobama/2590614/Barack-Obamas-lost-brother-found-in-Kenya.html. Retrieved on 2008-08-20.\\\\n^ a b Wadhams, Nick (2008-10-07). \\\\\"Corsi in Kenya: Obama\\'s Nation Boots Obama Nation Author\\\\\". TIME. http://www.time.com/time/world/article/0,8599,1847965,00.html?imw=Y. Retrieved on 2009-01-31.\\\\n^ by Dinesh D\\'Souza. \\\\\"Dinesh D\\'Souza\\xa0: George Obama, Start Packing\\\\\". Townhall.com. http://townhall.com/columnists/DineshDSouza/2008/09/22/george_obama,_start_packing. Retrieved on 2009-01-31.\\\\n^ a b c \\\\\"The Obama Family Tree\\\\\" (PDF). Chicago Sun-Times. September 9, 2007. http://www.suntimes.com/images/cds/MP3/obamatree.pdf. Retrieved on 2008-11-23.\\\\n^ First read, MSNBC\\\\n^ \\\\\"Barack Obama\\'s aunt found living in rundown public housing estate | The Australian\\\\\". Theaustralian.news.com.au. 2008-10-31. http://www.theaustralian.news.com.au/story/0,25197,24578185-5017121,00.html. Retrieved on 2009-01-31.\\\\n^ Boston Housing Authority ýýýflabbergasteredýýý Barack Obamaýýýs aunt living in Southie\\\\n^ Kilner, Derek (2008-11-05). \\\\\"Kenya Celebrates President Obama as Native Son\\\\\". Voice Of America. http://www.voanews.com/english/archive/2008-11/2008-11-05-voa45.cfm. Retrieved on 2008-12-24.\\\\n^ \\\\\"Oregon State University Beavers: Craig Robinson bio\\\\\". http://www.osubeavers.com/ViewArticle.dbml?SPSID=106239&SPID=1954&DB_OEM_ID=4700&ATCLID=1436883&Q_SEASON=2008. Retrieved on 2008-08-21.\\\\n^ \\\\\"RootsWeb\\'s WorldConnect Project: Dowling Family Genealogy\\\\\". Wc.rootsweb.ancestry.com. http://wc.rootsweb.ancestry.com/cgi-bin/igm.cgi?op=GET&db=dowfam3&id=I105855. Retrieved on 2009-01-31.\\\\n^ Weiss, Anthony (September 2, 2008). \\\\\"Michelle Obama Has a Rabbi in Her Family\\\\\". The Forward. http://www.forward.com/articles/14121/. Retrieved on 2008-10-09.\\\\n^ Gary Boyd Roberts. \\\\\"Notes on the Ancestry of Senator Barack Hussein Obama, Jr\\\\\". New England Historic Genealogical Society. http://www.newenglandancestors.org/research/services/articles_ancestry_barack_obama.asp. Retrieved on 2009-01-31.\\\\n^ Scott Fornek (2007-09-09). \\\\\"Obama Family Tree\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545463,BSX-News-wotreer09.stng. Retrieved on 2009-01-31.\\\\n^ \\\\\"Obama-Bush (family tree)\\\\\" (PDF). New England Historic Genealogical Society. http://www.newenglandancestors.org/pdfs/obama_bush.pdf. Retrieved on 2009-01-31.\\\\n^ Wade, Nicholas (2007-10-21). \\\\\"Barack Obama - Dick Cheney - Ancestry and Genealogy - Washington - New York Times\\\\\". Nytimes.com. http://www.nytimes.com/2007/10/21/weekinreview/21basic.html. Retrieved on 2009-01-31.\\\\n^ Eastman, Dick (2008-07-30). \\\\\"Barack Obama is Related to Wild Bill Hickok\\\\\". Blog.eogn.com. http://blog.eogn.com/eastmans_online_genealogy/2008/07/barack-obama-is.html. Retrieved on 2009-01-31.\\\\n^ a b c Scott Fornek (2007-09-09). \\\\\"Obama Family Tree\\\\\". Suntimes.com. http://www.suntimes.com/news/politics/obama/familytree/545441,BSX-News-wotreec09.stng. Retrieved on 2009-01-31.\\\\n^ a b \\\\\"Notes on the Ancestry of Senator Barack Hussein Obama, Jr\\\\\". New England Historic Genealogical Society. 2002-08-14. http://www.newenglandancestors.org/research/services/articles_ancestry_barack_obama.asp. Retrieved on 2009-01-31.\\\\n^ \\\\\"Obama, Clinton and McCain have some famous relations\\\\\". HeraldNet - AP. 2008-03-26. http://www.heraldnet.com/article/20080326/NEWS02/151343661. Retrieved on 2009-01-31.\\\\n^ \\\\\"Barack Obama and Joe Biden: The Change We Need\\\\\". My.barackobama.com. 2008-07-31. http://my.barackobama.com/page/community/post/williambrehm/gG5TVR. Retrieved on 2009-01-31.\\\\nExternal links\\\\nBarack Obama\\'s Family Tree - Photo Essays - TIME\\\\n\\\\\"Though Obama Had to Leave to Find Himself, It Is Hawaii That Made His Rise Possible,\\\\\" by David Maraniss\\\\nBarack Obama\\'s Branch-y Family Tree by Jake Tapper\\\\n\\\\\"Obama Family Tree\\\\\" series, by Scott Fornek\\\\n\\\\\"Six Degrees of Barack Obama\\\\\"\\\\n\\\\\"Notes on the Ancestry of Senator Barack Hussein Obama, Jr.,\\\\\" by Gary Boyd Roberts\\\\n\\\\\"Obama, Clinton and McCain have some famous relations,\\\\\" by The Associated Press\\\\n\\\\\"Obama\\'s Patriotic Family Tree,\\\\\" by Bill Brehm\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nBarack Obama\\\\nPresidency\\\\nTransition\\xa0ýý Inauguration\\xa0ýý Timeline\\xa0ýý Cabinet\\xa0ýý Judiciary\\xa0ýý Foreign policy\\xa0ýý First 100 days\\\\nEarly life and\\\\npolitical career\\\\nPublic image\\xa0ýý Illinois Senate career\\xa0ýý 2004 Democratic National Convention\\xa0ýý U.S. Senate election in Illinois\\xa0ýý U.S. Senate career\\xa0ýý Presidential primary campaign\\xa0ýý ObamaýýýBiden 2008\\xa0ýý Electoral history\\xa0ýý Political positions\\\\nBooks\\\\nDreams from My Father\\xa0ýý The Audacity of Hope\\\\nSpeeches\\\\nThe Audacity of Hope\\xa0ýý A More Perfect Union\\xa0ýý Change Has Come to America\\xa0ýý 2009 speech to joint session of Congress\\\\nFamily\\\\nMichelle Obama\\xa0ýý Barack Obama, Sr.\\xa0ýý Ann Dunham\\xa0ýý Lolo Soetoro (stepfather)\\xa0ýý Maya Soetoro-Ng (half-sister)\\xa0ýý Marian Robinson (mother-in-law)\\xa0ýý Stanley Armour Dunham (grandfather)\\xa0ýý Madelyn Dunham (grandmother)\\xa0ýý Extended family\\xa0ýý Family tree\\\\nRetrieved from \\\\\"http://en.wikipedia.org/wiki/Family_of_Barack_Obama#Sarah_Obama\\\\\"\\\\nCategories: Obama family | African American history | African American families | Luo Kenyans | People of mixed Black African-European ethnicity | African Americans | Asian Americans | Dutch Americans | English Americans | French Americans | German-Americans | Irish-Americans | Indonesian Americans | Kenyan-Americans | Scottish-Americans | Chinese Canadians | People of mixed Asian-European ethnicity | American families | First Families of the United States | Family treesHidden categories: Wikipedia semi-protected pages | Wikipedia indefinitely move-protected pages | All pages needing cleanup | Wikipedia articles needing factual verification since October 2008 | All pages needing factual verification | All articles with unsourced statements | Articles with unsourced statements since November 2008\\\\nViews\\\\nArticle\\\\nDiscussion\\\\nView source\\\\nHistory\\\\nPersonal tools\\\\nLog in / create account\\\\nNavigation\\\\nMain page\\\\nContents\\\\nFeatured content\\\\nCurrent events\\\\nRandom article\\\\nSearch\\\\nInteraction\\\\nAbout Wikipedia\\\\nCommunity portal\\\\nRecent changes\\\\nContact Wikipedia\\\\nDonate to Wikipedia\\\\nHelp\\\\nToolbox\\\\nWhat links here\\\\nRelated changes\\\\nUpload file\\\\nSpecial pages\\\\nPrintable version Permanent linkCite this page\\\\nLanguages\\\\nBahasa Indonesia\\\\nSvenska\\\\nýýýýýý\\\\nýýýýýý\\\\nThis page was last modified on 14 March 2009, at 05:50.\\\\nAll text is available under the terms of the GNU Free Documentation License. (See Copyrights for details.)\\\\nWikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a U.S. registered 501(c)(3) tax-deductible nonprofit charity.\\\\nPrivacy policy\\\\nAbout Wikipedia\\\\nDisclaimers', 'output': \"Where did Barack Obama's parents and grandparents come from?\", 'input_ids': [784, 15367, 11824, 908, 2588, 251, 30, 1661, 20653, 4534, 31, 7, 384, 892, 6, 379, 31960, 6, 1157, 5233, 7, 6, 1747, 11, 5128, 13, 3879, 6, 672, 5, 784, 9857, 5211, 11810, 134, 908, 784, 26472, 209, 908, 3712, 13, 20653, 4534, 3, 18, 16885, 6, 8, 339, 3, 4392, 3903, 24477, 2, 29, 371, 3690, 120, 13, 20653, 4534, 2, 29, 22674, 16885, 6, 8, 339, 3, 4392, 3903, 24477, 2, 29, 599, 1649, 22955, 45, 180, 3198, 9, 4534, 61, 2, 29, 683, 440, 102, 12, 10, 8789, 6, 960, 2, 29, 667, 115, 265, 9, 3712, 2, 29, 345, 15704, 20653, 4534, 6, 1485, 8571, 15275, 6, 11, 16649, 283, 5434, 11, 180, 3198, 9, 6772, 12, 8, 4374, 227, 112, 22145, 1115, 3049, 5, 16047, 2464, 6, 30, 8, 4653, 2245, 13, 8, 412, 5, 134, 5, 18808, 5, 6306, 536, 908, 2, 29, 254, 450, 5320, 1719, 2, 29, 518, 3198, 6029, 6, 5795, 2, 29, 1570, 14678, 2, 29, 345, 11706, 13, 5233, 2, 29, 5110, 23, 1054, 1323, 2, 29, 10358, 179, 724, 2, 29, 14851, 4365, 4534, 6, 15275, 4534, 6, 6206, 6393, 1483, 6, 20653, 4534, 6, 180, 52, 5, 6, 672, 5, 2, 29, 25772, 15, 26, 1791, 2, 29, 24372, 77, 739, 6, 6393, 1483, 6, 264, 15, 17, 127, 32, 6, 445, 122, 2, 29, 3713, 1108, 19, 294, 13, 3, 9, 939, 81, 2, 29, 14851, 4365, 4534, 2, 29, 21106, 9232, 3, 2, 7659, 7819, 3, 2, 412, 5, 134, 5, 7819, 2, 29, 8931, 155, 1950, 4655, 3, 2, 2575, 1023, 3, 2, 3712, 2, 29, 16128, 3778, 2593, 3, 2, 4534, 2, 279, 23, 537, 2066, 2, 29, 18474, 4749, 3, 2, 86, 402, 7840, 257, 3, 2, 3, 21543, 8563, 892, 2, 29, 10572, 1583, 11298, 41, 13368, 747, 6, 1485, 910, 477, 61, 2, 29, 329, 5434, 6, 15275, 11, 180, 3198, 9, 30, 1726, 44, 8, 2628, 10021, 868, 11347, 2, 29, 634, 3712, 13, 20653, 4534, 19, 46, 4760, 18297, 13, 3850, 797, 6, 1566, 6, 9995, 29, 6, 11, 12605, 29, 41, 434, 76, 32, 61, 8681, 801, 190, 8, 913, 7, 11, 1827, 1415, 13, 20653, 4534, 6, 8, 1661, 13, 8, 907, 1323, 13, 1371, 6, 6306, 357, 908, 6306, 519, 908, 6306, 591, 908, 6306, 755, 908, 11, 119, 2279, 5, 978, 5299, 384, 19, 8, 1485, 3712, 13, 8, 907, 1323, 5, 37, 4534, 7, 33, 8, 166, 1485, 3712, 13, 3850, 797, 19991, 16, 8, 907, 1323, 11, 8, 19147, 12, 2058, 8, 1945, 1384, 437, 8, 14532, 7, 5, 555, 6710, 343, 2832, 6, 3, 2, 667, 115, 265, 9, 31, 7, 1021, 6, 11273, 384, 3, 3272, 157, 7, 223, 12, 477, 13, 5184, 15, 3171, 5, 2, 6306, 948, 908, 2, 29, 4302, 4669, 7, 2, 29, 536, 1318, 5700, 342, 384, 2, 29, 357, 27944, 384, 3, 18, 28574, 5836, 2, 29, 519, 27944, 384, 3, 18, 2576, 2947, 138, 5836, 2, 29, 591, 15275, 17461, 4534, 31, 7, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [2840, 410, 20653, 4534, 31, 7, 1362, 11, 22229, 369, 45, 58, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "tokenizer,tokenized_dataset= tokenize_and_split_dataset(\"training_top5_qulac_PREPROCESSED_FOR_MODEL.json\",\"t5-small\")\n",
    "print(tokenized_dataset[0])  # Optional: check one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe93aef1-844a-4a6f-b041-2eaccd8852a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds= generate_folds(tokenized_dataset, n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1d89eb52-b327-4743-8997-6121fb993b0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/users/nfs/Etu0/21402600/wandb/run-20250509_013816-uhtrlok0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/uhtrlok0' target=\"_blank\">T5_SMALL_TOP_5_DOCS</a></strong> to <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/uhtrlok0' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/uhtrlok0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1981337/3761292519.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [450/450 07:18, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>24.110600</td>\n",
       "      <td>7.909437</td>\n",
       "      <td>0.047588</td>\n",
       "      <td>0.374766</td>\n",
       "      <td>0.157747</td>\n",
       "      <td>0.363523</td>\n",
       "      <td>0.275237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>12.190700</td>\n",
       "      <td>1.011101</td>\n",
       "      <td>0.103273</td>\n",
       "      <td>0.386857</td>\n",
       "      <td>0.166034</td>\n",
       "      <td>0.377508</td>\n",
       "      <td>0.301269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.538600</td>\n",
       "      <td>0.872277</td>\n",
       "      <td>0.114142</td>\n",
       "      <td>0.398919</td>\n",
       "      <td>0.178434</td>\n",
       "      <td>0.387180</td>\n",
       "      <td>0.319090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.164700</td>\n",
       "      <td>0.828989</td>\n",
       "      <td>0.117945</td>\n",
       "      <td>0.409622</td>\n",
       "      <td>0.173707</td>\n",
       "      <td>0.393014</td>\n",
       "      <td>0.330334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.164700</td>\n",
       "      <td>0.721032</td>\n",
       "      <td>0.135739</td>\n",
       "      <td>0.427961</td>\n",
       "      <td>0.195236</td>\n",
       "      <td>0.414530</td>\n",
       "      <td>0.358241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.847300</td>\n",
       "      <td>0.673097</td>\n",
       "      <td>0.143557</td>\n",
       "      <td>0.433076</td>\n",
       "      <td>0.206606</td>\n",
       "      <td>0.422220</td>\n",
       "      <td>0.369737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.747500</td>\n",
       "      <td>0.636529</td>\n",
       "      <td>0.148427</td>\n",
       "      <td>0.436883</td>\n",
       "      <td>0.212886</td>\n",
       "      <td>0.428279</td>\n",
       "      <td>0.378551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.657900</td>\n",
       "      <td>0.590409</td>\n",
       "      <td>0.159183</td>\n",
       "      <td>0.450980</td>\n",
       "      <td>0.225189</td>\n",
       "      <td>0.442551</td>\n",
       "      <td>0.391678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.657900</td>\n",
       "      <td>0.556572</td>\n",
       "      <td>0.167652</td>\n",
       "      <td>0.474435</td>\n",
       "      <td>0.229413</td>\n",
       "      <td>0.466709</td>\n",
       "      <td>0.418650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.583200</td>\n",
       "      <td>0.544981</td>\n",
       "      <td>0.171319</td>\n",
       "      <td>0.479601</td>\n",
       "      <td>0.226465</td>\n",
       "      <td>0.472042</td>\n",
       "      <td>0.423932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.538300</td>\n",
       "      <td>0.537315</td>\n",
       "      <td>0.169903</td>\n",
       "      <td>0.484318</td>\n",
       "      <td>0.226112</td>\n",
       "      <td>0.474042</td>\n",
       "      <td>0.427499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.504800</td>\n",
       "      <td>0.533065</td>\n",
       "      <td>0.169556</td>\n",
       "      <td>0.482238</td>\n",
       "      <td>0.220682</td>\n",
       "      <td>0.473084</td>\n",
       "      <td>0.421973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.504800</td>\n",
       "      <td>0.530522</td>\n",
       "      <td>0.177291</td>\n",
       "      <td>0.485835</td>\n",
       "      <td>0.231536</td>\n",
       "      <td>0.477122</td>\n",
       "      <td>0.427798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.486300</td>\n",
       "      <td>0.527241</td>\n",
       "      <td>0.175608</td>\n",
       "      <td>0.483763</td>\n",
       "      <td>0.228803</td>\n",
       "      <td>0.474618</td>\n",
       "      <td>0.425666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.489400</td>\n",
       "      <td>0.523926</td>\n",
       "      <td>0.177900</td>\n",
       "      <td>0.485893</td>\n",
       "      <td>0.230996</td>\n",
       "      <td>0.475810</td>\n",
       "      <td>0.429522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.479100</td>\n",
       "      <td>0.521356</td>\n",
       "      <td>0.180087</td>\n",
       "      <td>0.486335</td>\n",
       "      <td>0.232589</td>\n",
       "      <td>0.476741</td>\n",
       "      <td>0.430383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.479100</td>\n",
       "      <td>0.519820</td>\n",
       "      <td>0.180061</td>\n",
       "      <td>0.486199</td>\n",
       "      <td>0.232381</td>\n",
       "      <td>0.476313</td>\n",
       "      <td>0.430324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.466400</td>\n",
       "      <td>0.518212</td>\n",
       "      <td>0.184195</td>\n",
       "      <td>0.488098</td>\n",
       "      <td>0.234597</td>\n",
       "      <td>0.478191</td>\n",
       "      <td>0.432997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.463500</td>\n",
       "      <td>0.517498</td>\n",
       "      <td>0.182706</td>\n",
       "      <td>0.489047</td>\n",
       "      <td>0.236792</td>\n",
       "      <td>0.479070</td>\n",
       "      <td>0.435227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.460900</td>\n",
       "      <td>0.516440</td>\n",
       "      <td>0.180808</td>\n",
       "      <td>0.487784</td>\n",
       "      <td>0.235710</td>\n",
       "      <td>0.478534</td>\n",
       "      <td>0.432403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.460900</td>\n",
       "      <td>0.515080</td>\n",
       "      <td>0.183027</td>\n",
       "      <td>0.488974</td>\n",
       "      <td>0.236652</td>\n",
       "      <td>0.480782</td>\n",
       "      <td>0.433855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.453600</td>\n",
       "      <td>0.514179</td>\n",
       "      <td>0.186609</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.240421</td>\n",
       "      <td>0.482084</td>\n",
       "      <td>0.438209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.452700</td>\n",
       "      <td>0.513694</td>\n",
       "      <td>0.186710</td>\n",
       "      <td>0.492348</td>\n",
       "      <td>0.240421</td>\n",
       "      <td>0.483541</td>\n",
       "      <td>0.438477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.448200</td>\n",
       "      <td>0.513186</td>\n",
       "      <td>0.186737</td>\n",
       "      <td>0.492244</td>\n",
       "      <td>0.240041</td>\n",
       "      <td>0.483362</td>\n",
       "      <td>0.438496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.448200</td>\n",
       "      <td>0.512549</td>\n",
       "      <td>0.187207</td>\n",
       "      <td>0.491146</td>\n",
       "      <td>0.241812</td>\n",
       "      <td>0.482542</td>\n",
       "      <td>0.439854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.435900</td>\n",
       "      <td>0.512018</td>\n",
       "      <td>0.188984</td>\n",
       "      <td>0.491524</td>\n",
       "      <td>0.242821</td>\n",
       "      <td>0.483351</td>\n",
       "      <td>0.441620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.456100</td>\n",
       "      <td>0.511792</td>\n",
       "      <td>0.188808</td>\n",
       "      <td>0.491524</td>\n",
       "      <td>0.241501</td>\n",
       "      <td>0.482099</td>\n",
       "      <td>0.441620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.442400</td>\n",
       "      <td>0.511576</td>\n",
       "      <td>0.189059</td>\n",
       "      <td>0.493813</td>\n",
       "      <td>0.243877</td>\n",
       "      <td>0.484650</td>\n",
       "      <td>0.443585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.442400</td>\n",
       "      <td>0.511359</td>\n",
       "      <td>0.189310</td>\n",
       "      <td>0.494954</td>\n",
       "      <td>0.244892</td>\n",
       "      <td>0.485735</td>\n",
       "      <td>0.444662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.444400</td>\n",
       "      <td>0.511338</td>\n",
       "      <td>0.189310</td>\n",
       "      <td>0.494954</td>\n",
       "      <td>0.244892</td>\n",
       "      <td>0.485735</td>\n",
       "      <td>0.444662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 results: {'eval_loss': 0.5113382935523987, 'eval_bleu': 0.1893099536860164, 'eval_rouge1': 0.4949540176339151, 'eval_rouge2': 0.24489164019572845, 'eval_rougeL': 0.4857354966359395, 'eval_meteor': 0.44466199798519823, 'eval_runtime': 1.6267, 'eval_samples_per_second': 73.77, 'eval_steps_per_second': 2.459, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 46410.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 1 saved to crossval_T5_SMALL_TOP5DOCS_fold_1.jsonl\n",
      "Processing Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1981337/3761292519.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [450/450 07:18, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>23.505700</td>\n",
       "      <td>7.940170</td>\n",
       "      <td>0.050934</td>\n",
       "      <td>0.370665</td>\n",
       "      <td>0.158977</td>\n",
       "      <td>0.362204</td>\n",
       "      <td>0.280213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>12.294400</td>\n",
       "      <td>1.066327</td>\n",
       "      <td>0.114260</td>\n",
       "      <td>0.382475</td>\n",
       "      <td>0.171940</td>\n",
       "      <td>0.376345</td>\n",
       "      <td>0.302115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.605700</td>\n",
       "      <td>0.861255</td>\n",
       "      <td>0.116127</td>\n",
       "      <td>0.394917</td>\n",
       "      <td>0.182430</td>\n",
       "      <td>0.388872</td>\n",
       "      <td>0.319728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.151400</td>\n",
       "      <td>0.817595</td>\n",
       "      <td>0.123737</td>\n",
       "      <td>0.406307</td>\n",
       "      <td>0.186882</td>\n",
       "      <td>0.399597</td>\n",
       "      <td>0.344140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.151400</td>\n",
       "      <td>0.708440</td>\n",
       "      <td>0.138450</td>\n",
       "      <td>0.420978</td>\n",
       "      <td>0.203982</td>\n",
       "      <td>0.412892</td>\n",
       "      <td>0.369331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.846600</td>\n",
       "      <td>0.644693</td>\n",
       "      <td>0.147756</td>\n",
       "      <td>0.433454</td>\n",
       "      <td>0.213056</td>\n",
       "      <td>0.423685</td>\n",
       "      <td>0.385310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.744400</td>\n",
       "      <td>0.608788</td>\n",
       "      <td>0.149544</td>\n",
       "      <td>0.436275</td>\n",
       "      <td>0.218419</td>\n",
       "      <td>0.426298</td>\n",
       "      <td>0.389329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.569370</td>\n",
       "      <td>0.155394</td>\n",
       "      <td>0.451408</td>\n",
       "      <td>0.230495</td>\n",
       "      <td>0.440874</td>\n",
       "      <td>0.405376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.534808</td>\n",
       "      <td>0.162999</td>\n",
       "      <td>0.466417</td>\n",
       "      <td>0.232215</td>\n",
       "      <td>0.457027</td>\n",
       "      <td>0.424069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.583700</td>\n",
       "      <td>0.523624</td>\n",
       "      <td>0.166071</td>\n",
       "      <td>0.474904</td>\n",
       "      <td>0.235152</td>\n",
       "      <td>0.465789</td>\n",
       "      <td>0.436950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.542800</td>\n",
       "      <td>0.517126</td>\n",
       "      <td>0.177888</td>\n",
       "      <td>0.483728</td>\n",
       "      <td>0.239014</td>\n",
       "      <td>0.472722</td>\n",
       "      <td>0.443699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.509000</td>\n",
       "      <td>0.513670</td>\n",
       "      <td>0.178287</td>\n",
       "      <td>0.486572</td>\n",
       "      <td>0.241704</td>\n",
       "      <td>0.475245</td>\n",
       "      <td>0.448656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.509000</td>\n",
       "      <td>0.509130</td>\n",
       "      <td>0.174522</td>\n",
       "      <td>0.488581</td>\n",
       "      <td>0.239806</td>\n",
       "      <td>0.477077</td>\n",
       "      <td>0.449860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.500500</td>\n",
       "      <td>0.506172</td>\n",
       "      <td>0.177143</td>\n",
       "      <td>0.491805</td>\n",
       "      <td>0.241926</td>\n",
       "      <td>0.479809</td>\n",
       "      <td>0.452243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.485600</td>\n",
       "      <td>0.503882</td>\n",
       "      <td>0.179849</td>\n",
       "      <td>0.491332</td>\n",
       "      <td>0.244030</td>\n",
       "      <td>0.480109</td>\n",
       "      <td>0.453904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.470900</td>\n",
       "      <td>0.502146</td>\n",
       "      <td>0.184786</td>\n",
       "      <td>0.492644</td>\n",
       "      <td>0.247601</td>\n",
       "      <td>0.481743</td>\n",
       "      <td>0.455765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.470900</td>\n",
       "      <td>0.499719</td>\n",
       "      <td>0.187418</td>\n",
       "      <td>0.491806</td>\n",
       "      <td>0.249472</td>\n",
       "      <td>0.481358</td>\n",
       "      <td>0.454218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.478800</td>\n",
       "      <td>0.498231</td>\n",
       "      <td>0.187443</td>\n",
       "      <td>0.491943</td>\n",
       "      <td>0.249667</td>\n",
       "      <td>0.480979</td>\n",
       "      <td>0.454235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.458200</td>\n",
       "      <td>0.497519</td>\n",
       "      <td>0.188977</td>\n",
       "      <td>0.493430</td>\n",
       "      <td>0.251425</td>\n",
       "      <td>0.482318</td>\n",
       "      <td>0.455434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.461700</td>\n",
       "      <td>0.496796</td>\n",
       "      <td>0.189515</td>\n",
       "      <td>0.498480</td>\n",
       "      <td>0.256435</td>\n",
       "      <td>0.487493</td>\n",
       "      <td>0.459725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.461700</td>\n",
       "      <td>0.495910</td>\n",
       "      <td>0.185162</td>\n",
       "      <td>0.490145</td>\n",
       "      <td>0.252433</td>\n",
       "      <td>0.479651</td>\n",
       "      <td>0.453973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.460600</td>\n",
       "      <td>0.495084</td>\n",
       "      <td>0.185357</td>\n",
       "      <td>0.492714</td>\n",
       "      <td>0.252948</td>\n",
       "      <td>0.482635</td>\n",
       "      <td>0.455166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.446900</td>\n",
       "      <td>0.494512</td>\n",
       "      <td>0.186733</td>\n",
       "      <td>0.493394</td>\n",
       "      <td>0.254413</td>\n",
       "      <td>0.483003</td>\n",
       "      <td>0.456316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.450600</td>\n",
       "      <td>0.493823</td>\n",
       "      <td>0.189892</td>\n",
       "      <td>0.495249</td>\n",
       "      <td>0.257201</td>\n",
       "      <td>0.485099</td>\n",
       "      <td>0.458948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.450600</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>0.189866</td>\n",
       "      <td>0.495249</td>\n",
       "      <td>0.257201</td>\n",
       "      <td>0.485099</td>\n",
       "      <td>0.458944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.450600</td>\n",
       "      <td>0.492913</td>\n",
       "      <td>0.188639</td>\n",
       "      <td>0.495863</td>\n",
       "      <td>0.256036</td>\n",
       "      <td>0.485552</td>\n",
       "      <td>0.457832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.445900</td>\n",
       "      <td>0.492713</td>\n",
       "      <td>0.185757</td>\n",
       "      <td>0.494164</td>\n",
       "      <td>0.253238</td>\n",
       "      <td>0.483836</td>\n",
       "      <td>0.455620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.441400</td>\n",
       "      <td>0.492383</td>\n",
       "      <td>0.187019</td>\n",
       "      <td>0.494306</td>\n",
       "      <td>0.253492</td>\n",
       "      <td>0.484150</td>\n",
       "      <td>0.456667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.441400</td>\n",
       "      <td>0.492238</td>\n",
       "      <td>0.187019</td>\n",
       "      <td>0.494306</td>\n",
       "      <td>0.253492</td>\n",
       "      <td>0.484150</td>\n",
       "      <td>0.456667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.448500</td>\n",
       "      <td>0.492179</td>\n",
       "      <td>0.187093</td>\n",
       "      <td>0.495323</td>\n",
       "      <td>0.254196</td>\n",
       "      <td>0.484847</td>\n",
       "      <td>0.457071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 results: {'eval_loss': 0.492179274559021, 'eval_bleu': 0.18709251573004337, 'eval_rouge1': 0.4953230691007793, 'eval_rouge2': 0.25419599837399587, 'eval_rougeL': 0.48484664268120403, 'eval_meteor': 0.45707138727979757, 'eval_runtime': 2.3616, 'eval_samples_per_second': 50.814, 'eval_steps_per_second': 1.694, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 46495.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 2 saved to crossval_T5_SMALL_TOP5DOCS_fold_2.jsonl\n",
      "Processing Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1981337/3761292519.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [450/450 07:18, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>23.328100</td>\n",
       "      <td>8.096430</td>\n",
       "      <td>0.055385</td>\n",
       "      <td>0.357908</td>\n",
       "      <td>0.161374</td>\n",
       "      <td>0.350420</td>\n",
       "      <td>0.269925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>12.432700</td>\n",
       "      <td>1.002345</td>\n",
       "      <td>0.103265</td>\n",
       "      <td>0.360592</td>\n",
       "      <td>0.153494</td>\n",
       "      <td>0.351926</td>\n",
       "      <td>0.284345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.649600</td>\n",
       "      <td>0.861524</td>\n",
       "      <td>0.114531</td>\n",
       "      <td>0.365002</td>\n",
       "      <td>0.158698</td>\n",
       "      <td>0.357462</td>\n",
       "      <td>0.294214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.178800</td>\n",
       "      <td>0.815570</td>\n",
       "      <td>0.116965</td>\n",
       "      <td>0.380148</td>\n",
       "      <td>0.162180</td>\n",
       "      <td>0.373489</td>\n",
       "      <td>0.314761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.178800</td>\n",
       "      <td>0.696713</td>\n",
       "      <td>0.136258</td>\n",
       "      <td>0.398439</td>\n",
       "      <td>0.187520</td>\n",
       "      <td>0.393987</td>\n",
       "      <td>0.347386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.870100</td>\n",
       "      <td>0.631138</td>\n",
       "      <td>0.151025</td>\n",
       "      <td>0.415786</td>\n",
       "      <td>0.204817</td>\n",
       "      <td>0.409936</td>\n",
       "      <td>0.368813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.745600</td>\n",
       "      <td>0.584815</td>\n",
       "      <td>0.151458</td>\n",
       "      <td>0.420237</td>\n",
       "      <td>0.203879</td>\n",
       "      <td>0.413175</td>\n",
       "      <td>0.376094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.668800</td>\n",
       "      <td>0.543954</td>\n",
       "      <td>0.155305</td>\n",
       "      <td>0.432323</td>\n",
       "      <td>0.207701</td>\n",
       "      <td>0.425633</td>\n",
       "      <td>0.385146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.668800</td>\n",
       "      <td>0.508514</td>\n",
       "      <td>0.165670</td>\n",
       "      <td>0.453704</td>\n",
       "      <td>0.214391</td>\n",
       "      <td>0.448437</td>\n",
       "      <td>0.406393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.597600</td>\n",
       "      <td>0.491800</td>\n",
       "      <td>0.171525</td>\n",
       "      <td>0.468051</td>\n",
       "      <td>0.220034</td>\n",
       "      <td>0.463571</td>\n",
       "      <td>0.418190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.536200</td>\n",
       "      <td>0.486961</td>\n",
       "      <td>0.171678</td>\n",
       "      <td>0.470743</td>\n",
       "      <td>0.221525</td>\n",
       "      <td>0.464735</td>\n",
       "      <td>0.424185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.525600</td>\n",
       "      <td>0.482558</td>\n",
       "      <td>0.175600</td>\n",
       "      <td>0.472647</td>\n",
       "      <td>0.226655</td>\n",
       "      <td>0.466910</td>\n",
       "      <td>0.427843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.525600</td>\n",
       "      <td>0.479175</td>\n",
       "      <td>0.177658</td>\n",
       "      <td>0.473605</td>\n",
       "      <td>0.228753</td>\n",
       "      <td>0.468578</td>\n",
       "      <td>0.430152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.503700</td>\n",
       "      <td>0.474252</td>\n",
       "      <td>0.178200</td>\n",
       "      <td>0.476048</td>\n",
       "      <td>0.227129</td>\n",
       "      <td>0.471031</td>\n",
       "      <td>0.435318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.496800</td>\n",
       "      <td>0.470813</td>\n",
       "      <td>0.181982</td>\n",
       "      <td>0.477998</td>\n",
       "      <td>0.229432</td>\n",
       "      <td>0.473335</td>\n",
       "      <td>0.438164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.485600</td>\n",
       "      <td>0.468495</td>\n",
       "      <td>0.183291</td>\n",
       "      <td>0.475471</td>\n",
       "      <td>0.230393</td>\n",
       "      <td>0.471108</td>\n",
       "      <td>0.436664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.485600</td>\n",
       "      <td>0.466696</td>\n",
       "      <td>0.187776</td>\n",
       "      <td>0.476889</td>\n",
       "      <td>0.232445</td>\n",
       "      <td>0.472287</td>\n",
       "      <td>0.438028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.478700</td>\n",
       "      <td>0.465300</td>\n",
       "      <td>0.188352</td>\n",
       "      <td>0.479397</td>\n",
       "      <td>0.233941</td>\n",
       "      <td>0.474502</td>\n",
       "      <td>0.438870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.480100</td>\n",
       "      <td>0.464052</td>\n",
       "      <td>0.190570</td>\n",
       "      <td>0.478730</td>\n",
       "      <td>0.235110</td>\n",
       "      <td>0.473532</td>\n",
       "      <td>0.439404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.468700</td>\n",
       "      <td>0.462676</td>\n",
       "      <td>0.190112</td>\n",
       "      <td>0.479239</td>\n",
       "      <td>0.234368</td>\n",
       "      <td>0.474054</td>\n",
       "      <td>0.439060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.468700</td>\n",
       "      <td>0.461321</td>\n",
       "      <td>0.190019</td>\n",
       "      <td>0.480006</td>\n",
       "      <td>0.235213</td>\n",
       "      <td>0.474533</td>\n",
       "      <td>0.440007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.461600</td>\n",
       "      <td>0.460510</td>\n",
       "      <td>0.189858</td>\n",
       "      <td>0.476114</td>\n",
       "      <td>0.235256</td>\n",
       "      <td>0.471264</td>\n",
       "      <td>0.438729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.470600</td>\n",
       "      <td>0.459875</td>\n",
       "      <td>0.190803</td>\n",
       "      <td>0.479170</td>\n",
       "      <td>0.236593</td>\n",
       "      <td>0.473830</td>\n",
       "      <td>0.441150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.458200</td>\n",
       "      <td>0.458973</td>\n",
       "      <td>0.190993</td>\n",
       "      <td>0.480079</td>\n",
       "      <td>0.237786</td>\n",
       "      <td>0.475146</td>\n",
       "      <td>0.441865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.458200</td>\n",
       "      <td>0.458433</td>\n",
       "      <td>0.190279</td>\n",
       "      <td>0.478203</td>\n",
       "      <td>0.236856</td>\n",
       "      <td>0.473564</td>\n",
       "      <td>0.440277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.450900</td>\n",
       "      <td>0.458117</td>\n",
       "      <td>0.190372</td>\n",
       "      <td>0.478578</td>\n",
       "      <td>0.236856</td>\n",
       "      <td>0.473799</td>\n",
       "      <td>0.440402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.460400</td>\n",
       "      <td>0.457962</td>\n",
       "      <td>0.190894</td>\n",
       "      <td>0.478166</td>\n",
       "      <td>0.237714</td>\n",
       "      <td>0.473780</td>\n",
       "      <td>0.441347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.455100</td>\n",
       "      <td>0.457800</td>\n",
       "      <td>0.190925</td>\n",
       "      <td>0.478450</td>\n",
       "      <td>0.237714</td>\n",
       "      <td>0.474168</td>\n",
       "      <td>0.440856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.455100</td>\n",
       "      <td>0.457713</td>\n",
       "      <td>0.190814</td>\n",
       "      <td>0.477161</td>\n",
       "      <td>0.237714</td>\n",
       "      <td>0.472879</td>\n",
       "      <td>0.440236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.450200</td>\n",
       "      <td>0.457697</td>\n",
       "      <td>0.190845</td>\n",
       "      <td>0.477421</td>\n",
       "      <td>0.237714</td>\n",
       "      <td>0.473177</td>\n",
       "      <td>0.440259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 results: {'eval_loss': 0.4576970934867859, 'eval_bleu': 0.19084476001215522, 'eval_rouge1': 0.4774208557361214, 'eval_rouge2': 0.2377144272562019, 'eval_rougeL': 0.473177172964063, 'eval_meteor': 0.440258519080202, 'eval_runtime': 1.7084, 'eval_samples_per_second': 70.242, 'eval_steps_per_second': 2.341, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 46120.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 3 saved to crossval_T5_SMALL_TOP5DOCS_fold_3.jsonl\n",
      "Processing Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1981337/3761292519.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [450/450 07:21, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>24.423100</td>\n",
       "      <td>7.922876</td>\n",
       "      <td>0.041999</td>\n",
       "      <td>0.346055</td>\n",
       "      <td>0.140593</td>\n",
       "      <td>0.333109</td>\n",
       "      <td>0.252670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>12.217200</td>\n",
       "      <td>1.003843</td>\n",
       "      <td>0.090906</td>\n",
       "      <td>0.357478</td>\n",
       "      <td>0.147780</td>\n",
       "      <td>0.346236</td>\n",
       "      <td>0.271056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.661100</td>\n",
       "      <td>0.850357</td>\n",
       "      <td>0.090823</td>\n",
       "      <td>0.367325</td>\n",
       "      <td>0.152041</td>\n",
       "      <td>0.356209</td>\n",
       "      <td>0.290821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.166300</td>\n",
       "      <td>0.800059</td>\n",
       "      <td>0.102207</td>\n",
       "      <td>0.381367</td>\n",
       "      <td>0.163857</td>\n",
       "      <td>0.371026</td>\n",
       "      <td>0.313867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.166300</td>\n",
       "      <td>0.693316</td>\n",
       "      <td>0.130541</td>\n",
       "      <td>0.401187</td>\n",
       "      <td>0.197060</td>\n",
       "      <td>0.388240</td>\n",
       "      <td>0.343843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.858300</td>\n",
       "      <td>0.641209</td>\n",
       "      <td>0.146986</td>\n",
       "      <td>0.413773</td>\n",
       "      <td>0.209488</td>\n",
       "      <td>0.401735</td>\n",
       "      <td>0.362795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.745400</td>\n",
       "      <td>0.597495</td>\n",
       "      <td>0.149567</td>\n",
       "      <td>0.422369</td>\n",
       "      <td>0.212035</td>\n",
       "      <td>0.411369</td>\n",
       "      <td>0.367227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.683300</td>\n",
       "      <td>0.558066</td>\n",
       "      <td>0.159802</td>\n",
       "      <td>0.431925</td>\n",
       "      <td>0.222289</td>\n",
       "      <td>0.422825</td>\n",
       "      <td>0.375573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.683300</td>\n",
       "      <td>0.520543</td>\n",
       "      <td>0.170670</td>\n",
       "      <td>0.460920</td>\n",
       "      <td>0.234290</td>\n",
       "      <td>0.450344</td>\n",
       "      <td>0.409113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.600500</td>\n",
       "      <td>0.505494</td>\n",
       "      <td>0.173857</td>\n",
       "      <td>0.479401</td>\n",
       "      <td>0.231259</td>\n",
       "      <td>0.470351</td>\n",
       "      <td>0.427682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.546500</td>\n",
       "      <td>0.499390</td>\n",
       "      <td>0.178529</td>\n",
       "      <td>0.480072</td>\n",
       "      <td>0.231448</td>\n",
       "      <td>0.471608</td>\n",
       "      <td>0.430866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.518600</td>\n",
       "      <td>0.494980</td>\n",
       "      <td>0.184645</td>\n",
       "      <td>0.485061</td>\n",
       "      <td>0.237826</td>\n",
       "      <td>0.475602</td>\n",
       "      <td>0.432729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.518600</td>\n",
       "      <td>0.493064</td>\n",
       "      <td>0.188728</td>\n",
       "      <td>0.486628</td>\n",
       "      <td>0.237777</td>\n",
       "      <td>0.477574</td>\n",
       "      <td>0.435741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.500300</td>\n",
       "      <td>0.491406</td>\n",
       "      <td>0.187123</td>\n",
       "      <td>0.485110</td>\n",
       "      <td>0.235997</td>\n",
       "      <td>0.477504</td>\n",
       "      <td>0.433575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.488600</td>\n",
       "      <td>0.488767</td>\n",
       "      <td>0.187318</td>\n",
       "      <td>0.485629</td>\n",
       "      <td>0.237108</td>\n",
       "      <td>0.477516</td>\n",
       "      <td>0.435809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.489600</td>\n",
       "      <td>0.486822</td>\n",
       "      <td>0.184655</td>\n",
       "      <td>0.483884</td>\n",
       "      <td>0.236642</td>\n",
       "      <td>0.476340</td>\n",
       "      <td>0.434463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.489600</td>\n",
       "      <td>0.485068</td>\n",
       "      <td>0.187001</td>\n",
       "      <td>0.483923</td>\n",
       "      <td>0.236826</td>\n",
       "      <td>0.477634</td>\n",
       "      <td>0.435505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.481800</td>\n",
       "      <td>0.483874</td>\n",
       "      <td>0.187180</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>0.235001</td>\n",
       "      <td>0.476480</td>\n",
       "      <td>0.434896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.469800</td>\n",
       "      <td>0.482829</td>\n",
       "      <td>0.183125</td>\n",
       "      <td>0.483011</td>\n",
       "      <td>0.232648</td>\n",
       "      <td>0.475366</td>\n",
       "      <td>0.433911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.461900</td>\n",
       "      <td>0.482136</td>\n",
       "      <td>0.182606</td>\n",
       "      <td>0.480064</td>\n",
       "      <td>0.231963</td>\n",
       "      <td>0.472757</td>\n",
       "      <td>0.431149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.461900</td>\n",
       "      <td>0.481369</td>\n",
       "      <td>0.181217</td>\n",
       "      <td>0.478464</td>\n",
       "      <td>0.230801</td>\n",
       "      <td>0.470618</td>\n",
       "      <td>0.429056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.467900</td>\n",
       "      <td>0.480239</td>\n",
       "      <td>0.184131</td>\n",
       "      <td>0.481912</td>\n",
       "      <td>0.233621</td>\n",
       "      <td>0.474171</td>\n",
       "      <td>0.432449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.455500</td>\n",
       "      <td>0.479722</td>\n",
       "      <td>0.183880</td>\n",
       "      <td>0.481281</td>\n",
       "      <td>0.233586</td>\n",
       "      <td>0.473445</td>\n",
       "      <td>0.431538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.452800</td>\n",
       "      <td>0.479376</td>\n",
       "      <td>0.182859</td>\n",
       "      <td>0.476567</td>\n",
       "      <td>0.230610</td>\n",
       "      <td>0.469202</td>\n",
       "      <td>0.427598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.452800</td>\n",
       "      <td>0.478994</td>\n",
       "      <td>0.182859</td>\n",
       "      <td>0.476567</td>\n",
       "      <td>0.230610</td>\n",
       "      <td>0.468723</td>\n",
       "      <td>0.427566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.451000</td>\n",
       "      <td>0.478794</td>\n",
       "      <td>0.185422</td>\n",
       "      <td>0.477169</td>\n",
       "      <td>0.232401</td>\n",
       "      <td>0.469391</td>\n",
       "      <td>0.429274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.455600</td>\n",
       "      <td>0.478751</td>\n",
       "      <td>0.185422</td>\n",
       "      <td>0.477169</td>\n",
       "      <td>0.232401</td>\n",
       "      <td>0.469898</td>\n",
       "      <td>0.428616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.452200</td>\n",
       "      <td>0.478639</td>\n",
       "      <td>0.182784</td>\n",
       "      <td>0.476062</td>\n",
       "      <td>0.230610</td>\n",
       "      <td>0.468814</td>\n",
       "      <td>0.425997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.452200</td>\n",
       "      <td>0.478499</td>\n",
       "      <td>0.182784</td>\n",
       "      <td>0.476018</td>\n",
       "      <td>0.230610</td>\n",
       "      <td>0.468743</td>\n",
       "      <td>0.425997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.451000</td>\n",
       "      <td>0.478491</td>\n",
       "      <td>0.182784</td>\n",
       "      <td>0.476018</td>\n",
       "      <td>0.230610</td>\n",
       "      <td>0.468743</td>\n",
       "      <td>0.425997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 results: {'eval_loss': 0.4784913957118988, 'eval_bleu': 0.18278447619229612, 'eval_rouge1': 0.4760184309750496, 'eval_rouge2': 0.23060972344865766, 'eval_rougeL': 0.46874332718886214, 'eval_meteor': 0.42599713806917544, 'eval_runtime': 1.7334, 'eval_samples_per_second': 69.228, 'eval_steps_per_second': 2.308, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 47617.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 4 saved to crossval_T5_SMALL_TOP5DOCS_fold_4.jsonl\n",
      "Processing Fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1981337/3761292519.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [450/450 07:16, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>23.898800</td>\n",
       "      <td>7.936031</td>\n",
       "      <td>0.066495</td>\n",
       "      <td>0.386179</td>\n",
       "      <td>0.166075</td>\n",
       "      <td>0.378805</td>\n",
       "      <td>0.283774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>12.000200</td>\n",
       "      <td>1.004953</td>\n",
       "      <td>0.115424</td>\n",
       "      <td>0.389591</td>\n",
       "      <td>0.166647</td>\n",
       "      <td>0.383703</td>\n",
       "      <td>0.296994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.493300</td>\n",
       "      <td>0.854488</td>\n",
       "      <td>0.114650</td>\n",
       "      <td>0.397639</td>\n",
       "      <td>0.170023</td>\n",
       "      <td>0.389756</td>\n",
       "      <td>0.307615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.150700</td>\n",
       "      <td>0.798124</td>\n",
       "      <td>0.136465</td>\n",
       "      <td>0.402984</td>\n",
       "      <td>0.181164</td>\n",
       "      <td>0.395077</td>\n",
       "      <td>0.331188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.150700</td>\n",
       "      <td>0.696073</td>\n",
       "      <td>0.148521</td>\n",
       "      <td>0.419650</td>\n",
       "      <td>0.193164</td>\n",
       "      <td>0.409715</td>\n",
       "      <td>0.358577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.865200</td>\n",
       "      <td>0.623379</td>\n",
       "      <td>0.162405</td>\n",
       "      <td>0.433229</td>\n",
       "      <td>0.206757</td>\n",
       "      <td>0.423072</td>\n",
       "      <td>0.378308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.753400</td>\n",
       "      <td>0.601476</td>\n",
       "      <td>0.163957</td>\n",
       "      <td>0.440597</td>\n",
       "      <td>0.209819</td>\n",
       "      <td>0.429251</td>\n",
       "      <td>0.385870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.668800</td>\n",
       "      <td>0.555373</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.450529</td>\n",
       "      <td>0.216583</td>\n",
       "      <td>0.439000</td>\n",
       "      <td>0.395253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.668800</td>\n",
       "      <td>0.521964</td>\n",
       "      <td>0.178880</td>\n",
       "      <td>0.469579</td>\n",
       "      <td>0.218129</td>\n",
       "      <td>0.457868</td>\n",
       "      <td>0.413532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.601900</td>\n",
       "      <td>0.501299</td>\n",
       "      <td>0.194098</td>\n",
       "      <td>0.493574</td>\n",
       "      <td>0.228036</td>\n",
       "      <td>0.482472</td>\n",
       "      <td>0.439482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.556100</td>\n",
       "      <td>0.494871</td>\n",
       "      <td>0.193544</td>\n",
       "      <td>0.492536</td>\n",
       "      <td>0.224180</td>\n",
       "      <td>0.481533</td>\n",
       "      <td>0.439459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.521500</td>\n",
       "      <td>0.490976</td>\n",
       "      <td>0.193569</td>\n",
       "      <td>0.494187</td>\n",
       "      <td>0.224108</td>\n",
       "      <td>0.483831</td>\n",
       "      <td>0.439251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.521500</td>\n",
       "      <td>0.487548</td>\n",
       "      <td>0.194720</td>\n",
       "      <td>0.493827</td>\n",
       "      <td>0.225753</td>\n",
       "      <td>0.484353</td>\n",
       "      <td>0.439916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.501300</td>\n",
       "      <td>0.485152</td>\n",
       "      <td>0.193190</td>\n",
       "      <td>0.493221</td>\n",
       "      <td>0.223426</td>\n",
       "      <td>0.484174</td>\n",
       "      <td>0.439310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.500800</td>\n",
       "      <td>0.482798</td>\n",
       "      <td>0.195626</td>\n",
       "      <td>0.492662</td>\n",
       "      <td>0.223118</td>\n",
       "      <td>0.483120</td>\n",
       "      <td>0.441149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.485300</td>\n",
       "      <td>0.481393</td>\n",
       "      <td>0.198885</td>\n",
       "      <td>0.498433</td>\n",
       "      <td>0.231465</td>\n",
       "      <td>0.490268</td>\n",
       "      <td>0.448057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.485300</td>\n",
       "      <td>0.479589</td>\n",
       "      <td>0.198409</td>\n",
       "      <td>0.498507</td>\n",
       "      <td>0.230808</td>\n",
       "      <td>0.489090</td>\n",
       "      <td>0.447488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.482800</td>\n",
       "      <td>0.478611</td>\n",
       "      <td>0.198103</td>\n",
       "      <td>0.496737</td>\n",
       "      <td>0.229203</td>\n",
       "      <td>0.486980</td>\n",
       "      <td>0.445704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>0.477555</td>\n",
       "      <td>0.199607</td>\n",
       "      <td>0.499189</td>\n",
       "      <td>0.231592</td>\n",
       "      <td>0.489438</td>\n",
       "      <td>0.448880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.469700</td>\n",
       "      <td>0.476650</td>\n",
       "      <td>0.200720</td>\n",
       "      <td>0.498243</td>\n",
       "      <td>0.231959</td>\n",
       "      <td>0.488733</td>\n",
       "      <td>0.448702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.469700</td>\n",
       "      <td>0.475661</td>\n",
       "      <td>0.199624</td>\n",
       "      <td>0.499279</td>\n",
       "      <td>0.232948</td>\n",
       "      <td>0.489996</td>\n",
       "      <td>0.451779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.462700</td>\n",
       "      <td>0.474794</td>\n",
       "      <td>0.198420</td>\n",
       "      <td>0.499327</td>\n",
       "      <td>0.232750</td>\n",
       "      <td>0.489671</td>\n",
       "      <td>0.451110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.460600</td>\n",
       "      <td>0.474110</td>\n",
       "      <td>0.200847</td>\n",
       "      <td>0.500236</td>\n",
       "      <td>0.233577</td>\n",
       "      <td>0.490441</td>\n",
       "      <td>0.452001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.459200</td>\n",
       "      <td>0.473614</td>\n",
       "      <td>0.200847</td>\n",
       "      <td>0.500361</td>\n",
       "      <td>0.233806</td>\n",
       "      <td>0.490706</td>\n",
       "      <td>0.452001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.459200</td>\n",
       "      <td>0.473165</td>\n",
       "      <td>0.200073</td>\n",
       "      <td>0.499145</td>\n",
       "      <td>0.230996</td>\n",
       "      <td>0.489643</td>\n",
       "      <td>0.450486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.450100</td>\n",
       "      <td>0.472845</td>\n",
       "      <td>0.200277</td>\n",
       "      <td>0.499350</td>\n",
       "      <td>0.232247</td>\n",
       "      <td>0.489661</td>\n",
       "      <td>0.450504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.464400</td>\n",
       "      <td>0.472548</td>\n",
       "      <td>0.202948</td>\n",
       "      <td>0.498756</td>\n",
       "      <td>0.233691</td>\n",
       "      <td>0.489082</td>\n",
       "      <td>0.450799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.446800</td>\n",
       "      <td>0.472454</td>\n",
       "      <td>0.202948</td>\n",
       "      <td>0.498756</td>\n",
       "      <td>0.233691</td>\n",
       "      <td>0.489082</td>\n",
       "      <td>0.450799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.446800</td>\n",
       "      <td>0.472351</td>\n",
       "      <td>0.202868</td>\n",
       "      <td>0.497877</td>\n",
       "      <td>0.233691</td>\n",
       "      <td>0.487846</td>\n",
       "      <td>0.450149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.450300</td>\n",
       "      <td>0.472312</td>\n",
       "      <td>0.205260</td>\n",
       "      <td>0.498378</td>\n",
       "      <td>0.235239</td>\n",
       "      <td>0.488540</td>\n",
       "      <td>0.450927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 results: {'eval_loss': 0.47231248021125793, 'eval_bleu': 0.205260004896688, 'eval_rouge1': 0.49837764631903186, 'eval_rouge2': 0.23523881251568848, 'eval_rougeL': 0.4885402364015746, 'eval_meteor': 0.45092694757338664, 'eval_runtime': 1.7126, 'eval_samples_per_second': 69.486, 'eval_steps_per_second': 2.336, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:00<00:00, 44648.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 5 saved to crossval_T5_SMALL_TOP5DOCS_fold_5.jsonl\n",
      "Average metrics over all folds: {'eval_loss': 0.48240370750427247, 'eval_bleu': 0.19105834210343983, 'eval_rouge1': 0.48841880395297943, 'eval_rouge2': 0.2405301203580545, 'eval_rougeL': 0.48020857517432863, 'eval_meteor': 0.44378319799755195, 'eval_runtime': 1.8285399999999998, 'eval_samples_per_second': 66.708, 'eval_steps_per_second': 2.2276, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Averages/epoch</td><td>▁</td></tr><tr><td>Averages/eval_bleu</td><td>▁</td></tr><tr><td>Averages/eval_loss</td><td>▁</td></tr><tr><td>Averages/eval_meteor</td><td>▁</td></tr><tr><td>Averages/eval_rouge1</td><td>▁</td></tr><tr><td>Averages/eval_rouge2</td><td>▁</td></tr><tr><td>Averages/eval_rougeL</td><td>▁</td></tr><tr><td>Averages/eval_runtime</td><td>▁</td></tr><tr><td>Averages/eval_samples_per_second</td><td>▁</td></tr><tr><td>Averages/eval_steps_per_second</td><td>▁</td></tr><tr><td>Fold_1/eval/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>Fold_1/eval/eval_bleu</td><td>▁▄▄▄▅▆▆▇▇▇▇▇▇▇▇████████████████</td></tr><tr><td>Fold_1/eval/eval_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_1/eval/eval_meteor</td><td>▁▂▃▃▄▅▅▆▇▇▇▇▇▇▇▇▇██▇███████████</td></tr><tr><td>Fold_1/eval/eval_rouge1</td><td>▁▂▂▃▄▄▅▅▇▇▇▇▇▇▇▇▇██████████████</td></tr><tr><td>Fold_1/eval/eval_rouge2</td><td>▁▂▃▂▄▅▅▆▇▇▆▆▇▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>Fold_1/eval/eval_rougeL</td><td>▁▂▂▃▄▄▅▆▇▇▇▇█▇▇▇▇██████████████</td></tr><tr><td>Fold_1/eval/eval_runtime</td><td>▂▂▁▄▁▃▁▂▂▂▂▁▁▁▁▂▁█▁▁▁▁▁▂▁▁▁▁▁▁▁</td></tr><tr><td>Fold_1/eval/eval_samples_per_second</td><td>▇▆█▅▇▅█▇▇▇▇████▇█▁▇█▇█▇▇▇▇▇██▇█</td></tr><tr><td>Fold_1/eval/eval_steps_per_second</td><td>▇▆█▅▇▅█▇▇▇▇████▇█▁▇█▇█▇▇▇▇▇██▇█</td></tr><tr><td>Fold_2/eval/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>Fold_2/eval/eval_bleu</td><td>▁▄▄▅▅▆▆▆▇▇▇▇▇▇▇████████████████</td></tr><tr><td>Fold_2/eval/eval_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_2/eval/eval_meteor</td><td>▁▂▃▃▄▅▅▆▇▇▇████████████████████</td></tr><tr><td>Fold_2/eval/eval_rouge1</td><td>▁▂▂▃▄▄▅▅▆▇▇▇▇██████████████████</td></tr><tr><td>Fold_2/eval/eval_rouge2</td><td>▁▂▃▃▄▅▅▆▆▆▇▇▇▇▇▇▇▇█████████████</td></tr><tr><td>Fold_2/eval/eval_rougeL</td><td>▁▂▂▃▄▄▅▅▆▇▇▇▇██████████████████</td></tr><tr><td>Fold_2/eval/eval_runtime</td><td>▂▁▂▁▁▁▁▁▂▁▁▁▇▁▁▁▂▁▁▂▂▂▁▁▂█▂▂▂▁█</td></tr><tr><td>Fold_2/eval/eval_samples_per_second</td><td>▇█▇█▇███▇███▂█▇█▇▇█▇▇▆██▇▁▇▇▇█▁</td></tr><tr><td>Fold_2/eval/eval_steps_per_second</td><td>▇█▇█▇███▇███▁█▇█▇▇█▇▇▆██▇▁▇▇▇█▁</td></tr><tr><td>Fold_3/eval/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>Fold_3/eval/eval_bleu</td><td>▁▃▄▄▅▆▆▆▇▇▇▇▇▇█████████████████</td></tr><tr><td>Fold_3/eval/eval_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_3/eval/eval_meteor</td><td>▁▂▂▃▄▅▅▆▇▇▇▇███████████████████</td></tr><tr><td>Fold_3/eval/eval_rouge1</td><td>▁▁▁▂▃▄▅▅▆▇▇████████████████████</td></tr><tr><td>Fold_3/eval/eval_rouge2</td><td>▂▁▁▂▄▅▅▆▆▇▇▇▇▇▇▇███████████████</td></tr><tr><td>Fold_3/eval/eval_rougeL</td><td>▁▁▁▂▃▄▅▅▇▇▇████████████████████</td></tr><tr><td>Fold_3/eval/eval_runtime</td><td>▁▁▁▂▇▁▂▂▂█▁▂▁▂▁▁▁▂▂▂▂▂▁▂▁▂▂▂▁▁▂</td></tr><tr><td>Fold_3/eval/eval_samples_per_second</td><td>▇██▇▁█▇▇▇▁█▇▇▇█▇█▆▇▆▇▇█▇█▇▇▇█▇▇</td></tr><tr><td>Fold_3/eval/eval_steps_per_second</td><td>▇██▇▁█▇▇▇▁█▇▇▇█▇█▆▇▆▇▇█▇█▇▇▇█▇▇</td></tr><tr><td>Fold_4/eval/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>Fold_4/eval/eval_bleu</td><td>▁▃▃▄▅▆▆▇▇▇█████████████████████</td></tr><tr><td>Fold_4/eval/eval_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_4/eval/eval_meteor</td><td>▁▂▂▃▄▅▅▆▇██████████████████████</td></tr><tr><td>Fold_4/eval/eval_rouge1</td><td>▁▂▂▃▄▄▅▅▇██████████████▇▇██▇▇▇▇</td></tr><tr><td>Fold_4/eval/eval_rouge2</td><td>▁▂▂▃▅▆▆▇████████████▇██▇▇██▇▇▇▇</td></tr><tr><td>Fold_4/eval/eval_rougeL</td><td>▁▂▂▃▄▄▅▅▇██████████████████████</td></tr><tr><td>Fold_4/eval/eval_runtime</td><td>▂▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▂</td></tr><tr><td>Fold_4/eval/eval_samples_per_second</td><td>▇███▇▇█▇███▇██▇▇████▇█▁███████▆</td></tr><tr><td>Fold_4/eval/eval_steps_per_second</td><td>▇███▇▇█▇███▇██▇▇████▇█▁███████▆</td></tr><tr><td>Fold_5/eval/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>Fold_5/eval/eval_bleu</td><td>▁▃▃▅▅▆▆▆▇▇▇▇▇▇█████████████████</td></tr><tr><td>Fold_5/eval/eval_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_5/eval/eval_meteor</td><td>▁▂▂▃▄▅▅▆▆▇▇▇▇▇█████████████████</td></tr><tr><td>Fold_5/eval/eval_rouge1</td><td>▁▁▂▂▃▄▄▅▆██████████████████████</td></tr><tr><td>Fold_5/eval/eval_rouge2</td><td>▁▁▁▃▄▅▅▆▆▇▇▇▇▇▇██▇█████████████</td></tr><tr><td>Fold_5/eval/eval_rougeL</td><td>▁▁▂▂▃▄▄▅▆▇▇████████████████████</td></tr><tr><td>Fold_5/eval/eval_runtime</td><td>▆▆▅▅▆▅▅▆█▆▆▅▆▇▅▅▁▆▅▅▅█▅▄▄▄▂▃▅▅█</td></tr><tr><td>Fold_5/eval/eval_samples_per_second</td><td>▃▃▄▃▃▄▄▃▁▂▃▃▃▂▄▄█▃▄▄▄▁▄▅▅▅▆▅▄▄▁</td></tr><tr><td>Fold_5/eval/eval_steps_per_second</td><td>▃▃▄▃▃▄▄▃▁▂▃▃▃▂▄▄█▃▄▄▄▁▄▅▅▅▆▅▄▄▁</td></tr><tr><td>eval/bleu</td><td>▄▄▄▆▇▇██▁▆▆▇██▇▄▄▆▆▆████▁▆▇▇█▇▇▇▇▇▄▇████</td></tr><tr><td>eval/loss</td><td>▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/meteor</td><td>▂▅▇▇▇▇▂███████▁▄▅▇▇▇▇▇▇▂▅▇▇▇▇▇▇▇▁▃▅█████</td></tr><tr><td>eval/rouge1</td><td>▁▄██████▁▄███▇█▄▆▆▇▇▇▇▇▇▇▄▆▇▇▇▇▇▂▂▃▆████</td></tr><tr><td>eval/rouge2</td><td>▂▅▅▆▆▇▇▇▇▇▃▆█████▆▇▇▇▇▇▁▂▇▇▇▇▇▇▇▃▃▄▇▇▇▇▇</td></tr><tr><td>eval/rougeL</td><td>▁▃▇▇▇▇█▁▃▄▆▇▇▇▇█████▃▆▇▇▇▇▇▇▄▇▇▇▇▇▇█████</td></tr><tr><td>eval/runtime</td><td>▄▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂█▂▃▂▂▂▂▂▁▂▂▁▂▂▁▁▂▁▁▁</td></tr><tr><td>eval/samples_per_second</td><td>▅▇▆▇▇▇▆▇▇▇▇▂▇▆▆▆▇█▇▇▆▇▇▇▇▇▇▇▇▁▇▇▇▆▇▆▇██▇</td></tr><tr><td>eval/steps_per_second</td><td>▇▇▆▇▇▇▇▇█▇▇▇▇▇▇▇▆▁▆▇█▆▁▇▇▇▇▇▇▇▇▆▇▇▇▇█▆█▇</td></tr><tr><td>test/bleu</td><td>▃▂▄▁█</td></tr><tr><td>test/loss</td><td>█▅▁▄▃</td></tr><tr><td>test/meteor</td><td>▅█▄▁▇</td></tr><tr><td>test/rouge1</td><td>▇▇▁▁█</td></tr><tr><td>test/rouge2</td><td>▅█▃▁▂</td></tr><tr><td>test/rougeL</td><td>▇▇▃▁█</td></tr><tr><td>test/runtime</td><td>▁██▄▄</td></tr><tr><td>test/samples_per_second</td><td>█▁▁▅▄</td></tr><tr><td>test/steps_per_second</td><td>█▁▁▅▅</td></tr><tr><td>train/epoch</td><td>▂▂▃▃▄▅▆▆██▃▄▄▄▅██▁▃▃▃▄▅▇▇▁▂▂▃▄▇███▁▄▄▄▆▆</td></tr><tr><td>train/global_step</td><td>▂▂▂▃▃▃▃▅▅▅▇▁▂▂▄▅▅▇▇█▅▅▆▆▇█▂▂▃▄▅▅▆██▃▄▅▇█</td></tr><tr><td>train/grad_norm</td><td>▃▃▂▁▁▁▁▂▂▂▂▁▁▂▁▃▂▂▁▁▁▄▁█▃▂▂▂▁▁▂▁▁▁▄▃▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▇▇▇▆▆▃▂▁▁█▆▅▅▅▄▃▂██▇▅▄▃▂█▇▆▅▅▅▄▄▁█▇▅▄▄▃▁</td></tr><tr><td>train/loss</td><td>█▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Averages/epoch</td><td>30</td></tr><tr><td>Averages/eval_bleu</td><td>0.19106</td></tr><tr><td>Averages/eval_loss</td><td>0.4824</td></tr><tr><td>Averages/eval_meteor</td><td>0.44378</td></tr><tr><td>Averages/eval_rouge1</td><td>0.48842</td></tr><tr><td>Averages/eval_rouge2</td><td>0.24053</td></tr><tr><td>Averages/eval_rougeL</td><td>0.48021</td></tr><tr><td>Averages/eval_runtime</td><td>1.82854</td></tr><tr><td>Averages/eval_samples_per_second</td><td>66.708</td></tr><tr><td>Averages/eval_steps_per_second</td><td>2.2276</td></tr><tr><td>Fold_1/eval/epoch</td><td>30</td></tr><tr><td>Fold_1/eval/eval_bleu</td><td>0.18931</td></tr><tr><td>Fold_1/eval/eval_loss</td><td>0.51134</td></tr><tr><td>Fold_1/eval/eval_meteor</td><td>0.44466</td></tr><tr><td>Fold_1/eval/eval_rouge1</td><td>0.49495</td></tr><tr><td>Fold_1/eval/eval_rouge2</td><td>0.24489</td></tr><tr><td>Fold_1/eval/eval_rougeL</td><td>0.48574</td></tr><tr><td>Fold_1/eval/eval_runtime</td><td>1.6267</td></tr><tr><td>Fold_1/eval/eval_samples_per_second</td><td>73.77</td></tr><tr><td>Fold_1/eval/eval_steps_per_second</td><td>2.459</td></tr><tr><td>Fold_2/eval/epoch</td><td>30</td></tr><tr><td>Fold_2/eval/eval_bleu</td><td>0.18709</td></tr><tr><td>Fold_2/eval/eval_loss</td><td>0.49218</td></tr><tr><td>Fold_2/eval/eval_meteor</td><td>0.45707</td></tr><tr><td>Fold_2/eval/eval_rouge1</td><td>0.49532</td></tr><tr><td>Fold_2/eval/eval_rouge2</td><td>0.2542</td></tr><tr><td>Fold_2/eval/eval_rougeL</td><td>0.48485</td></tr><tr><td>Fold_2/eval/eval_runtime</td><td>2.3616</td></tr><tr><td>Fold_2/eval/eval_samples_per_second</td><td>50.814</td></tr><tr><td>Fold_2/eval/eval_steps_per_second</td><td>1.694</td></tr><tr><td>Fold_3/eval/epoch</td><td>30</td></tr><tr><td>Fold_3/eval/eval_bleu</td><td>0.19084</td></tr><tr><td>Fold_3/eval/eval_loss</td><td>0.4577</td></tr><tr><td>Fold_3/eval/eval_meteor</td><td>0.44026</td></tr><tr><td>Fold_3/eval/eval_rouge1</td><td>0.47742</td></tr><tr><td>Fold_3/eval/eval_rouge2</td><td>0.23771</td></tr><tr><td>Fold_3/eval/eval_rougeL</td><td>0.47318</td></tr><tr><td>Fold_3/eval/eval_runtime</td><td>1.7084</td></tr><tr><td>Fold_3/eval/eval_samples_per_second</td><td>70.242</td></tr><tr><td>Fold_3/eval/eval_steps_per_second</td><td>2.341</td></tr><tr><td>Fold_4/eval/epoch</td><td>30</td></tr><tr><td>Fold_4/eval/eval_bleu</td><td>0.18278</td></tr><tr><td>Fold_4/eval/eval_loss</td><td>0.47849</td></tr><tr><td>Fold_4/eval/eval_meteor</td><td>0.426</td></tr><tr><td>Fold_4/eval/eval_rouge1</td><td>0.47602</td></tr><tr><td>Fold_4/eval/eval_rouge2</td><td>0.23061</td></tr><tr><td>Fold_4/eval/eval_rougeL</td><td>0.46874</td></tr><tr><td>Fold_4/eval/eval_runtime</td><td>1.7334</td></tr><tr><td>Fold_4/eval/eval_samples_per_second</td><td>69.228</td></tr><tr><td>Fold_4/eval/eval_steps_per_second</td><td>2.308</td></tr><tr><td>Fold_5/eval/epoch</td><td>30</td></tr><tr><td>Fold_5/eval/eval_bleu</td><td>0.20526</td></tr><tr><td>Fold_5/eval/eval_loss</td><td>0.47231</td></tr><tr><td>Fold_5/eval/eval_meteor</td><td>0.45093</td></tr><tr><td>Fold_5/eval/eval_rouge1</td><td>0.49838</td></tr><tr><td>Fold_5/eval/eval_rouge2</td><td>0.23524</td></tr><tr><td>Fold_5/eval/eval_rougeL</td><td>0.48854</td></tr><tr><td>Fold_5/eval/eval_runtime</td><td>1.7126</td></tr><tr><td>Fold_5/eval/eval_samples_per_second</td><td>69.486</td></tr><tr><td>Fold_5/eval/eval_steps_per_second</td><td>2.336</td></tr><tr><td>eval/bleu</td><td>0.20526</td></tr><tr><td>eval/loss</td><td>0.47231</td></tr><tr><td>eval/meteor</td><td>0.45093</td></tr><tr><td>eval/rouge1</td><td>0.49838</td></tr><tr><td>eval/rouge2</td><td>0.23524</td></tr><tr><td>eval/rougeL</td><td>0.48854</td></tr><tr><td>eval/runtime</td><td>1.7126</td></tr><tr><td>eval/samples_per_second</td><td>69.486</td></tr><tr><td>eval/steps_per_second</td><td>2.336</td></tr><tr><td>test/bleu</td><td>0.20526</td></tr><tr><td>test/loss</td><td>0.47231</td></tr><tr><td>test/meteor</td><td>0.45093</td></tr><tr><td>test/rouge1</td><td>0.49838</td></tr><tr><td>test/rouge2</td><td>0.23524</td></tr><tr><td>test/rougeL</td><td>0.48854</td></tr><tr><td>test/runtime</td><td>1.6318</td></tr><tr><td>test/samples_per_second</td><td>72.927</td></tr><tr><td>test/steps_per_second</td><td>2.451</td></tr><tr><td>total_flos</td><td>1948921941196800.0</td></tr><tr><td>train/epoch</td><td>30</td></tr><tr><td>train/global_step</td><td>450</td></tr><tr><td>train/grad_norm</td><td>0.64161</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4503</td></tr><tr><td>train_loss</td><td>1.17942</td></tr><tr><td>train_runtime</td><td>437.3954</td></tr><tr><td>train_samples_per_second</td><td>32.922</td></tr><tr><td>train_steps_per_second</td><td>1.029</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">T5_SMALL_TOP_5_DOCS</strong> at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/uhtrlok0' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/uhtrlok0</a><br> View project at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250509_013816-uhtrlok0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final predicitons for all folds foudn in crossval_T5_SMALL_TOP5DOCS.jsonl\n"
     ]
    }
   ],
   "source": [
    "cross_val_train(modelname=\"t5-small\", run_name=\"T5_SMALL_TOP_5_DOCS\",filename=\"crossval_T5_SMALL_TOP5DOCS.jsonl\", \n",
    "                tokenizer=tokenizer, tokenized_dataset=tokenized_dataset, \n",
    "                folds=folds, nb_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de42d12c-1bf5-4928-9394-081f9ea5fd7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d665bc2-895a-4922-878b-9ecc1b0a08e4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# T5 SMALL----------------- TOP5 QUERY ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c97720a2-1ed6-4275-bd08-00920a1ec630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b981dd6506e445899020b78b76e2fc9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': \"Find information on President Barack Obama's family history, including genealogy, national origins, places and dates of birth, etc.\", 'output': \"Where did Barack Obama's parents and grandparents come from?\", 'input_ids': [2588, 251, 30, 1661, 20653, 4534, 31, 7, 384, 892, 6, 379, 31960, 6, 1157, 5233, 7, 6, 1747, 11, 5128, 13, 3879, 6, 672, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [2840, 410, 20653, 4534, 31, 7, 1362, 11, 22229, 369, 45, 58, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/Etu0/21402600/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer,tokenized_dataset= tokenize_and_split_dataset(\"training_queryonly_qulac_PREPROCESSED_FOR_MODEL.json\",\"t5-small\")\n",
    "print(tokenized_dataset[0])  # Optional: check one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04cf68a0-1e6b-4e4a-9986-04c620250f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds= generate_folds(tokenized_dataset, n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52f6996f-2cac-4005-bf08-e41832ffe26d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/users/nfs/Etu0/21402600/wandb/run-20250510_181509-jvw9nm91</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/jvw9nm91' target=\"_blank\">T5_SMALL_QUERY_ONLY</a></strong> to <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/jvw9nm91' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/jvw9nm91</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1746893712.776653    2863 gpu_device.cc:2341] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_2863/3761292519.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1560' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1560/1800 09:57 < 01:32, 2.61 it/s, Epoch 26/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.855500</td>\n",
       "      <td>0.994875</td>\n",
       "      <td>0.098887</td>\n",
       "      <td>0.342733</td>\n",
       "      <td>0.148488</td>\n",
       "      <td>0.331003</td>\n",
       "      <td>0.283723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.923700</td>\n",
       "      <td>0.791439</td>\n",
       "      <td>0.126398</td>\n",
       "      <td>0.393167</td>\n",
       "      <td>0.187927</td>\n",
       "      <td>0.382688</td>\n",
       "      <td>0.332641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.734600</td>\n",
       "      <td>0.619414</td>\n",
       "      <td>0.150396</td>\n",
       "      <td>0.431079</td>\n",
       "      <td>0.207093</td>\n",
       "      <td>0.418767</td>\n",
       "      <td>0.377791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.591800</td>\n",
       "      <td>0.566767</td>\n",
       "      <td>0.160119</td>\n",
       "      <td>0.457525</td>\n",
       "      <td>0.212743</td>\n",
       "      <td>0.444881</td>\n",
       "      <td>0.407786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.556700</td>\n",
       "      <td>0.550978</td>\n",
       "      <td>0.167941</td>\n",
       "      <td>0.462883</td>\n",
       "      <td>0.221320</td>\n",
       "      <td>0.449165</td>\n",
       "      <td>0.412682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.518600</td>\n",
       "      <td>0.541416</td>\n",
       "      <td>0.176522</td>\n",
       "      <td>0.472386</td>\n",
       "      <td>0.229268</td>\n",
       "      <td>0.457819</td>\n",
       "      <td>0.424240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.493400</td>\n",
       "      <td>0.533020</td>\n",
       "      <td>0.188335</td>\n",
       "      <td>0.480884</td>\n",
       "      <td>0.239085</td>\n",
       "      <td>0.466859</td>\n",
       "      <td>0.437689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.493900</td>\n",
       "      <td>0.527780</td>\n",
       "      <td>0.192626</td>\n",
       "      <td>0.478057</td>\n",
       "      <td>0.239658</td>\n",
       "      <td>0.465535</td>\n",
       "      <td>0.435083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.492900</td>\n",
       "      <td>0.523187</td>\n",
       "      <td>0.190525</td>\n",
       "      <td>0.486239</td>\n",
       "      <td>0.242472</td>\n",
       "      <td>0.468519</td>\n",
       "      <td>0.439281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.468100</td>\n",
       "      <td>0.519190</td>\n",
       "      <td>0.187495</td>\n",
       "      <td>0.482558</td>\n",
       "      <td>0.240865</td>\n",
       "      <td>0.465692</td>\n",
       "      <td>0.437932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.442700</td>\n",
       "      <td>0.516189</td>\n",
       "      <td>0.192460</td>\n",
       "      <td>0.488473</td>\n",
       "      <td>0.244677</td>\n",
       "      <td>0.471748</td>\n",
       "      <td>0.444880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.450400</td>\n",
       "      <td>0.515641</td>\n",
       "      <td>0.196697</td>\n",
       "      <td>0.491343</td>\n",
       "      <td>0.246929</td>\n",
       "      <td>0.475893</td>\n",
       "      <td>0.449110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.448400</td>\n",
       "      <td>0.513875</td>\n",
       "      <td>0.199179</td>\n",
       "      <td>0.491121</td>\n",
       "      <td>0.249624</td>\n",
       "      <td>0.474625</td>\n",
       "      <td>0.449812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.445600</td>\n",
       "      <td>0.512838</td>\n",
       "      <td>0.195035</td>\n",
       "      <td>0.485471</td>\n",
       "      <td>0.243753</td>\n",
       "      <td>0.469287</td>\n",
       "      <td>0.440915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.451100</td>\n",
       "      <td>0.512058</td>\n",
       "      <td>0.196900</td>\n",
       "      <td>0.491729</td>\n",
       "      <td>0.251497</td>\n",
       "      <td>0.475803</td>\n",
       "      <td>0.446663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.415300</td>\n",
       "      <td>0.510354</td>\n",
       "      <td>0.194452</td>\n",
       "      <td>0.489158</td>\n",
       "      <td>0.244593</td>\n",
       "      <td>0.472908</td>\n",
       "      <td>0.444897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.409100</td>\n",
       "      <td>0.510094</td>\n",
       "      <td>0.191926</td>\n",
       "      <td>0.488943</td>\n",
       "      <td>0.243148</td>\n",
       "      <td>0.472149</td>\n",
       "      <td>0.444001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.419800</td>\n",
       "      <td>0.509917</td>\n",
       "      <td>0.190948</td>\n",
       "      <td>0.484990</td>\n",
       "      <td>0.238740</td>\n",
       "      <td>0.469157</td>\n",
       "      <td>0.439294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>0.509339</td>\n",
       "      <td>0.193157</td>\n",
       "      <td>0.486446</td>\n",
       "      <td>0.240566</td>\n",
       "      <td>0.470262</td>\n",
       "      <td>0.437761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.404200</td>\n",
       "      <td>0.508842</td>\n",
       "      <td>0.188308</td>\n",
       "      <td>0.483258</td>\n",
       "      <td>0.238598</td>\n",
       "      <td>0.466923</td>\n",
       "      <td>0.439917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.392500</td>\n",
       "      <td>0.508873</td>\n",
       "      <td>0.192733</td>\n",
       "      <td>0.484559</td>\n",
       "      <td>0.240611</td>\n",
       "      <td>0.467884</td>\n",
       "      <td>0.437080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.397900</td>\n",
       "      <td>0.508274</td>\n",
       "      <td>0.194303</td>\n",
       "      <td>0.485141</td>\n",
       "      <td>0.240675</td>\n",
       "      <td>0.467746</td>\n",
       "      <td>0.436737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.389700</td>\n",
       "      <td>0.508118</td>\n",
       "      <td>0.196181</td>\n",
       "      <td>0.489460</td>\n",
       "      <td>0.242383</td>\n",
       "      <td>0.471825</td>\n",
       "      <td>0.439626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.398900</td>\n",
       "      <td>0.508443</td>\n",
       "      <td>0.194892</td>\n",
       "      <td>0.484607</td>\n",
       "      <td>0.240560</td>\n",
       "      <td>0.467187</td>\n",
       "      <td>0.441424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.388000</td>\n",
       "      <td>0.508344</td>\n",
       "      <td>0.194532</td>\n",
       "      <td>0.482715</td>\n",
       "      <td>0.239523</td>\n",
       "      <td>0.465073</td>\n",
       "      <td>0.439657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.396700</td>\n",
       "      <td>0.508232</td>\n",
       "      <td>0.192775</td>\n",
       "      <td>0.483152</td>\n",
       "      <td>0.238915</td>\n",
       "      <td>0.465262</td>\n",
       "      <td>0.439118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 results: {'eval_loss': 0.5081183910369873, 'eval_bleu': 0.19618116080673445, 'eval_rouge1': 0.48946004794296377, 'eval_rouge2': 0.24238264179214067, 'eval_rougeL': 0.4718249742908727, 'eval_meteor': 0.4396262780902425, 'eval_runtime': 2.2058, 'eval_samples_per_second': 54.402, 'eval_steps_per_second': 6.8, 'epoch': 26.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 113283.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 1 saved to crossval_T5_SMALL_QUERY_ONLY_fold_1.jsonl\n",
      "Processing Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_2863/3761292519.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1380' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1380/1800 08:35 < 02:37, 2.67 it/s, Epoch 23/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.717800</td>\n",
       "      <td>0.918376</td>\n",
       "      <td>0.119962</td>\n",
       "      <td>0.368699</td>\n",
       "      <td>0.173433</td>\n",
       "      <td>0.364089</td>\n",
       "      <td>0.304374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.933800</td>\n",
       "      <td>0.756122</td>\n",
       "      <td>0.149005</td>\n",
       "      <td>0.411796</td>\n",
       "      <td>0.210029</td>\n",
       "      <td>0.405520</td>\n",
       "      <td>0.371784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.680400</td>\n",
       "      <td>0.601544</td>\n",
       "      <td>0.157589</td>\n",
       "      <td>0.438393</td>\n",
       "      <td>0.218302</td>\n",
       "      <td>0.430408</td>\n",
       "      <td>0.393714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.560300</td>\n",
       "      <td>0.551737</td>\n",
       "      <td>0.171849</td>\n",
       "      <td>0.467298</td>\n",
       "      <td>0.226980</td>\n",
       "      <td>0.460670</td>\n",
       "      <td>0.426804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.530600</td>\n",
       "      <td>0.534421</td>\n",
       "      <td>0.170777</td>\n",
       "      <td>0.468126</td>\n",
       "      <td>0.224634</td>\n",
       "      <td>0.461093</td>\n",
       "      <td>0.426588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.533400</td>\n",
       "      <td>0.523041</td>\n",
       "      <td>0.179697</td>\n",
       "      <td>0.475674</td>\n",
       "      <td>0.239256</td>\n",
       "      <td>0.469466</td>\n",
       "      <td>0.438252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.515200</td>\n",
       "      <td>0.517500</td>\n",
       "      <td>0.184003</td>\n",
       "      <td>0.482215</td>\n",
       "      <td>0.245874</td>\n",
       "      <td>0.475956</td>\n",
       "      <td>0.447754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.485800</td>\n",
       "      <td>0.509965</td>\n",
       "      <td>0.192219</td>\n",
       "      <td>0.491042</td>\n",
       "      <td>0.255798</td>\n",
       "      <td>0.483838</td>\n",
       "      <td>0.456348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.471200</td>\n",
       "      <td>0.506050</td>\n",
       "      <td>0.189342</td>\n",
       "      <td>0.493833</td>\n",
       "      <td>0.246486</td>\n",
       "      <td>0.482809</td>\n",
       "      <td>0.453183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.479800</td>\n",
       "      <td>0.502876</td>\n",
       "      <td>0.189115</td>\n",
       "      <td>0.495410</td>\n",
       "      <td>0.254650</td>\n",
       "      <td>0.487652</td>\n",
       "      <td>0.458616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.447200</td>\n",
       "      <td>0.500194</td>\n",
       "      <td>0.191089</td>\n",
       "      <td>0.500897</td>\n",
       "      <td>0.257354</td>\n",
       "      <td>0.490659</td>\n",
       "      <td>0.462337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.458000</td>\n",
       "      <td>0.498831</td>\n",
       "      <td>0.191317</td>\n",
       "      <td>0.499738</td>\n",
       "      <td>0.259771</td>\n",
       "      <td>0.489824</td>\n",
       "      <td>0.464108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.455100</td>\n",
       "      <td>0.496422</td>\n",
       "      <td>0.190147</td>\n",
       "      <td>0.499261</td>\n",
       "      <td>0.258761</td>\n",
       "      <td>0.492090</td>\n",
       "      <td>0.463249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.415200</td>\n",
       "      <td>0.495133</td>\n",
       "      <td>0.188871</td>\n",
       "      <td>0.499983</td>\n",
       "      <td>0.256502</td>\n",
       "      <td>0.491575</td>\n",
       "      <td>0.462077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.416600</td>\n",
       "      <td>0.493958</td>\n",
       "      <td>0.190117</td>\n",
       "      <td>0.500204</td>\n",
       "      <td>0.256338</td>\n",
       "      <td>0.490857</td>\n",
       "      <td>0.461983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.434000</td>\n",
       "      <td>0.493018</td>\n",
       "      <td>0.191880</td>\n",
       "      <td>0.501130</td>\n",
       "      <td>0.258889</td>\n",
       "      <td>0.492053</td>\n",
       "      <td>0.466726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.435800</td>\n",
       "      <td>0.491697</td>\n",
       "      <td>0.192723</td>\n",
       "      <td>0.500263</td>\n",
       "      <td>0.259945</td>\n",
       "      <td>0.490929</td>\n",
       "      <td>0.465006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.398900</td>\n",
       "      <td>0.491491</td>\n",
       "      <td>0.192750</td>\n",
       "      <td>0.500194</td>\n",
       "      <td>0.260605</td>\n",
       "      <td>0.491220</td>\n",
       "      <td>0.466305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.420500</td>\n",
       "      <td>0.491339</td>\n",
       "      <td>0.193566</td>\n",
       "      <td>0.501500</td>\n",
       "      <td>0.261931</td>\n",
       "      <td>0.492429</td>\n",
       "      <td>0.468303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.397300</td>\n",
       "      <td>0.489956</td>\n",
       "      <td>0.192535</td>\n",
       "      <td>0.499828</td>\n",
       "      <td>0.259952</td>\n",
       "      <td>0.491895</td>\n",
       "      <td>0.465574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.404500</td>\n",
       "      <td>0.490938</td>\n",
       "      <td>0.193989</td>\n",
       "      <td>0.502425</td>\n",
       "      <td>0.266617</td>\n",
       "      <td>0.495620</td>\n",
       "      <td>0.468039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.384200</td>\n",
       "      <td>0.490682</td>\n",
       "      <td>0.194402</td>\n",
       "      <td>0.506247</td>\n",
       "      <td>0.271667</td>\n",
       "      <td>0.500240</td>\n",
       "      <td>0.472338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.410800</td>\n",
       "      <td>0.490952</td>\n",
       "      <td>0.198653</td>\n",
       "      <td>0.507960</td>\n",
       "      <td>0.272655</td>\n",
       "      <td>0.501540</td>\n",
       "      <td>0.471379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 results: {'eval_loss': 0.48995569348335266, 'eval_bleu': 0.1925353181450674, 'eval_rouge1': 0.49982805150387644, 'eval_rouge2': 0.25995248728069476, 'eval_rougeL': 0.49189537648855153, 'eval_meteor': 0.4655735088020691, 'eval_runtime': 2.1153, 'eval_samples_per_second': 56.729, 'eval_steps_per_second': 7.091, 'epoch': 23.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 133011.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 2 saved to crossval_T5_SMALL_QUERY_ONLY_fold_2.jsonl\n",
      "Processing Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_2863/3761292519.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1800' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1800/1800 11:04, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.786800</td>\n",
       "      <td>0.890753</td>\n",
       "      <td>0.104437</td>\n",
       "      <td>0.348852</td>\n",
       "      <td>0.147722</td>\n",
       "      <td>0.339417</td>\n",
       "      <td>0.284185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.947500</td>\n",
       "      <td>0.711857</td>\n",
       "      <td>0.141463</td>\n",
       "      <td>0.386180</td>\n",
       "      <td>0.184735</td>\n",
       "      <td>0.375542</td>\n",
       "      <td>0.343268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.729200</td>\n",
       "      <td>0.558069</td>\n",
       "      <td>0.161688</td>\n",
       "      <td>0.438118</td>\n",
       "      <td>0.208398</td>\n",
       "      <td>0.429499</td>\n",
       "      <td>0.387831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.603800</td>\n",
       "      <td>0.508693</td>\n",
       "      <td>0.171945</td>\n",
       "      <td>0.458578</td>\n",
       "      <td>0.214448</td>\n",
       "      <td>0.449434</td>\n",
       "      <td>0.411594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.576000</td>\n",
       "      <td>0.489261</td>\n",
       "      <td>0.182085</td>\n",
       "      <td>0.456236</td>\n",
       "      <td>0.221861</td>\n",
       "      <td>0.451832</td>\n",
       "      <td>0.413646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.498100</td>\n",
       "      <td>0.476073</td>\n",
       "      <td>0.186615</td>\n",
       "      <td>0.470617</td>\n",
       "      <td>0.227675</td>\n",
       "      <td>0.463067</td>\n",
       "      <td>0.424330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.529100</td>\n",
       "      <td>0.467639</td>\n",
       "      <td>0.180996</td>\n",
       "      <td>0.468580</td>\n",
       "      <td>0.222782</td>\n",
       "      <td>0.459601</td>\n",
       "      <td>0.422465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.510400</td>\n",
       "      <td>0.461688</td>\n",
       "      <td>0.181094</td>\n",
       "      <td>0.471018</td>\n",
       "      <td>0.220485</td>\n",
       "      <td>0.461097</td>\n",
       "      <td>0.424991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.472900</td>\n",
       "      <td>0.458955</td>\n",
       "      <td>0.182672</td>\n",
       "      <td>0.477023</td>\n",
       "      <td>0.227521</td>\n",
       "      <td>0.467218</td>\n",
       "      <td>0.434569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.465900</td>\n",
       "      <td>0.454638</td>\n",
       "      <td>0.187378</td>\n",
       "      <td>0.486616</td>\n",
       "      <td>0.236638</td>\n",
       "      <td>0.476489</td>\n",
       "      <td>0.442245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.502000</td>\n",
       "      <td>0.451926</td>\n",
       "      <td>0.185555</td>\n",
       "      <td>0.482270</td>\n",
       "      <td>0.234518</td>\n",
       "      <td>0.474692</td>\n",
       "      <td>0.442053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.438200</td>\n",
       "      <td>0.449307</td>\n",
       "      <td>0.182119</td>\n",
       "      <td>0.485131</td>\n",
       "      <td>0.233567</td>\n",
       "      <td>0.475402</td>\n",
       "      <td>0.441799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.470400</td>\n",
       "      <td>0.447274</td>\n",
       "      <td>0.189948</td>\n",
       "      <td>0.492496</td>\n",
       "      <td>0.244280</td>\n",
       "      <td>0.482607</td>\n",
       "      <td>0.451631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.441200</td>\n",
       "      <td>0.445431</td>\n",
       "      <td>0.188983</td>\n",
       "      <td>0.489720</td>\n",
       "      <td>0.242520</td>\n",
       "      <td>0.480095</td>\n",
       "      <td>0.447700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.435600</td>\n",
       "      <td>0.445226</td>\n",
       "      <td>0.189440</td>\n",
       "      <td>0.491404</td>\n",
       "      <td>0.240185</td>\n",
       "      <td>0.481377</td>\n",
       "      <td>0.446844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.445700</td>\n",
       "      <td>0.442766</td>\n",
       "      <td>0.186676</td>\n",
       "      <td>0.489750</td>\n",
       "      <td>0.237970</td>\n",
       "      <td>0.479897</td>\n",
       "      <td>0.444684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.429000</td>\n",
       "      <td>0.441562</td>\n",
       "      <td>0.188088</td>\n",
       "      <td>0.490651</td>\n",
       "      <td>0.242628</td>\n",
       "      <td>0.482809</td>\n",
       "      <td>0.448285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.412300</td>\n",
       "      <td>0.441914</td>\n",
       "      <td>0.188397</td>\n",
       "      <td>0.485713</td>\n",
       "      <td>0.241333</td>\n",
       "      <td>0.477113</td>\n",
       "      <td>0.444718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.420200</td>\n",
       "      <td>0.441130</td>\n",
       "      <td>0.185622</td>\n",
       "      <td>0.487146</td>\n",
       "      <td>0.239482</td>\n",
       "      <td>0.478694</td>\n",
       "      <td>0.442299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.407100</td>\n",
       "      <td>0.440256</td>\n",
       "      <td>0.184479</td>\n",
       "      <td>0.485217</td>\n",
       "      <td>0.239321</td>\n",
       "      <td>0.477058</td>\n",
       "      <td>0.442239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.392900</td>\n",
       "      <td>0.439687</td>\n",
       "      <td>0.186243</td>\n",
       "      <td>0.485406</td>\n",
       "      <td>0.238953</td>\n",
       "      <td>0.476167</td>\n",
       "      <td>0.442900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.415100</td>\n",
       "      <td>0.439387</td>\n",
       "      <td>0.186498</td>\n",
       "      <td>0.487151</td>\n",
       "      <td>0.242518</td>\n",
       "      <td>0.478491</td>\n",
       "      <td>0.445728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.393100</td>\n",
       "      <td>0.439560</td>\n",
       "      <td>0.186580</td>\n",
       "      <td>0.487365</td>\n",
       "      <td>0.242839</td>\n",
       "      <td>0.478993</td>\n",
       "      <td>0.446157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.436400</td>\n",
       "      <td>0.439232</td>\n",
       "      <td>0.186695</td>\n",
       "      <td>0.486930</td>\n",
       "      <td>0.242927</td>\n",
       "      <td>0.478483</td>\n",
       "      <td>0.445801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.399100</td>\n",
       "      <td>0.438974</td>\n",
       "      <td>0.183781</td>\n",
       "      <td>0.484255</td>\n",
       "      <td>0.241331</td>\n",
       "      <td>0.476085</td>\n",
       "      <td>0.442694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.416300</td>\n",
       "      <td>0.438608</td>\n",
       "      <td>0.194948</td>\n",
       "      <td>0.494388</td>\n",
       "      <td>0.250240</td>\n",
       "      <td>0.485512</td>\n",
       "      <td>0.452250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.413800</td>\n",
       "      <td>0.438390</td>\n",
       "      <td>0.193677</td>\n",
       "      <td>0.493008</td>\n",
       "      <td>0.249488</td>\n",
       "      <td>0.484369</td>\n",
       "      <td>0.450877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.404100</td>\n",
       "      <td>0.438268</td>\n",
       "      <td>0.191017</td>\n",
       "      <td>0.492776</td>\n",
       "      <td>0.247735</td>\n",
       "      <td>0.483931</td>\n",
       "      <td>0.450191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.406000</td>\n",
       "      <td>0.438175</td>\n",
       "      <td>0.191017</td>\n",
       "      <td>0.492776</td>\n",
       "      <td>0.247735</td>\n",
       "      <td>0.483931</td>\n",
       "      <td>0.450191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.394500</td>\n",
       "      <td>0.438178</td>\n",
       "      <td>0.191096</td>\n",
       "      <td>0.493385</td>\n",
       "      <td>0.247496</td>\n",
       "      <td>0.484743</td>\n",
       "      <td>0.450917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 results: {'eval_loss': 0.43817469477653503, 'eval_bleu': 0.1910172588767731, 'eval_rouge1': 0.4927757191802943, 'eval_rouge2': 0.24773495264811413, 'eval_rougeL': 0.48393139409084684, 'eval_meteor': 0.45019052541579124, 'eval_runtime': 2.0942, 'eval_samples_per_second': 57.301, 'eval_steps_per_second': 7.163, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 115254.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 3 saved to crossval_T5_SMALL_QUERY_ONLY_fold_3.jsonl\n",
      "Processing Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_2863/3761292519.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1440' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1440/1800 09:07 < 02:17, 2.62 it/s, Epoch 24/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.744500</td>\n",
       "      <td>0.895914</td>\n",
       "      <td>0.110022</td>\n",
       "      <td>0.370034</td>\n",
       "      <td>0.173886</td>\n",
       "      <td>0.363677</td>\n",
       "      <td>0.299301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.867700</td>\n",
       "      <td>0.727595</td>\n",
       "      <td>0.139334</td>\n",
       "      <td>0.392575</td>\n",
       "      <td>0.195798</td>\n",
       "      <td>0.386306</td>\n",
       "      <td>0.342794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.729200</td>\n",
       "      <td>0.569565</td>\n",
       "      <td>0.155508</td>\n",
       "      <td>0.413682</td>\n",
       "      <td>0.210497</td>\n",
       "      <td>0.408656</td>\n",
       "      <td>0.367389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.599200</td>\n",
       "      <td>0.522300</td>\n",
       "      <td>0.168776</td>\n",
       "      <td>0.458210</td>\n",
       "      <td>0.214314</td>\n",
       "      <td>0.454002</td>\n",
       "      <td>0.407916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.562300</td>\n",
       "      <td>0.507000</td>\n",
       "      <td>0.169551</td>\n",
       "      <td>0.458463</td>\n",
       "      <td>0.207770</td>\n",
       "      <td>0.449684</td>\n",
       "      <td>0.411256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.522300</td>\n",
       "      <td>0.496170</td>\n",
       "      <td>0.175770</td>\n",
       "      <td>0.464726</td>\n",
       "      <td>0.219917</td>\n",
       "      <td>0.455432</td>\n",
       "      <td>0.419052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.540700</td>\n",
       "      <td>0.491010</td>\n",
       "      <td>0.181808</td>\n",
       "      <td>0.473229</td>\n",
       "      <td>0.226314</td>\n",
       "      <td>0.464076</td>\n",
       "      <td>0.422723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.515200</td>\n",
       "      <td>0.486087</td>\n",
       "      <td>0.181176</td>\n",
       "      <td>0.473283</td>\n",
       "      <td>0.228023</td>\n",
       "      <td>0.464350</td>\n",
       "      <td>0.425871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.494000</td>\n",
       "      <td>0.482846</td>\n",
       "      <td>0.177536</td>\n",
       "      <td>0.471686</td>\n",
       "      <td>0.223599</td>\n",
       "      <td>0.462734</td>\n",
       "      <td>0.422839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.466200</td>\n",
       "      <td>0.481039</td>\n",
       "      <td>0.176583</td>\n",
       "      <td>0.469394</td>\n",
       "      <td>0.224597</td>\n",
       "      <td>0.460881</td>\n",
       "      <td>0.421665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.449500</td>\n",
       "      <td>0.479368</td>\n",
       "      <td>0.179370</td>\n",
       "      <td>0.470951</td>\n",
       "      <td>0.225765</td>\n",
       "      <td>0.462928</td>\n",
       "      <td>0.421018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.431600</td>\n",
       "      <td>0.477944</td>\n",
       "      <td>0.180209</td>\n",
       "      <td>0.469185</td>\n",
       "      <td>0.226281</td>\n",
       "      <td>0.460742</td>\n",
       "      <td>0.422545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.450900</td>\n",
       "      <td>0.476970</td>\n",
       "      <td>0.180235</td>\n",
       "      <td>0.469347</td>\n",
       "      <td>0.227047</td>\n",
       "      <td>0.460124</td>\n",
       "      <td>0.421693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.433300</td>\n",
       "      <td>0.476104</td>\n",
       "      <td>0.179947</td>\n",
       "      <td>0.467608</td>\n",
       "      <td>0.219750</td>\n",
       "      <td>0.455301</td>\n",
       "      <td>0.416444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.457800</td>\n",
       "      <td>0.476103</td>\n",
       "      <td>0.180886</td>\n",
       "      <td>0.469731</td>\n",
       "      <td>0.220707</td>\n",
       "      <td>0.457335</td>\n",
       "      <td>0.418993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.456400</td>\n",
       "      <td>0.474724</td>\n",
       "      <td>0.183137</td>\n",
       "      <td>0.465531</td>\n",
       "      <td>0.217271</td>\n",
       "      <td>0.452178</td>\n",
       "      <td>0.414944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.421400</td>\n",
       "      <td>0.474193</td>\n",
       "      <td>0.183970</td>\n",
       "      <td>0.468347</td>\n",
       "      <td>0.220608</td>\n",
       "      <td>0.457016</td>\n",
       "      <td>0.416766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.419100</td>\n",
       "      <td>0.474445</td>\n",
       "      <td>0.183918</td>\n",
       "      <td>0.469036</td>\n",
       "      <td>0.221567</td>\n",
       "      <td>0.458555</td>\n",
       "      <td>0.419778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.406200</td>\n",
       "      <td>0.473958</td>\n",
       "      <td>0.184763</td>\n",
       "      <td>0.466504</td>\n",
       "      <td>0.222586</td>\n",
       "      <td>0.455919</td>\n",
       "      <td>0.418951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.409500</td>\n",
       "      <td>0.473729</td>\n",
       "      <td>0.184973</td>\n",
       "      <td>0.468020</td>\n",
       "      <td>0.222741</td>\n",
       "      <td>0.457835</td>\n",
       "      <td>0.422471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.398100</td>\n",
       "      <td>0.473400</td>\n",
       "      <td>0.183709</td>\n",
       "      <td>0.467391</td>\n",
       "      <td>0.222023</td>\n",
       "      <td>0.457091</td>\n",
       "      <td>0.423639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.387200</td>\n",
       "      <td>0.473550</td>\n",
       "      <td>0.185248</td>\n",
       "      <td>0.467590</td>\n",
       "      <td>0.223592</td>\n",
       "      <td>0.457620</td>\n",
       "      <td>0.421595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.391800</td>\n",
       "      <td>0.473793</td>\n",
       "      <td>0.183475</td>\n",
       "      <td>0.465689</td>\n",
       "      <td>0.219704</td>\n",
       "      <td>0.454763</td>\n",
       "      <td>0.420322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.398600</td>\n",
       "      <td>0.474259</td>\n",
       "      <td>0.186015</td>\n",
       "      <td>0.466689</td>\n",
       "      <td>0.220807</td>\n",
       "      <td>0.455240</td>\n",
       "      <td>0.421609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 results: {'eval_loss': 0.47339963912963867, 'eval_bleu': 0.18370917574488144, 'eval_rouge1': 0.4673907103496292, 'eval_rouge2': 0.2220233146804768, 'eval_rougeL': 0.457091261476803, 'eval_meteor': 0.42363899092266094, 'eval_runtime': 2.6236, 'eval_samples_per_second': 45.739, 'eval_steps_per_second': 5.717, 'epoch': 24.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 105010.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 4 saved to crossval_T5_SMALL_QUERY_ONLY_fold_4.jsonl\n",
      "Processing Fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_2863/3761292519.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1800' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1800/1800 11:26, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.676700</td>\n",
       "      <td>0.917436</td>\n",
       "      <td>0.137211</td>\n",
       "      <td>0.369948</td>\n",
       "      <td>0.164801</td>\n",
       "      <td>0.363264</td>\n",
       "      <td>0.301165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.937200</td>\n",
       "      <td>0.746211</td>\n",
       "      <td>0.164838</td>\n",
       "      <td>0.407303</td>\n",
       "      <td>0.197454</td>\n",
       "      <td>0.399225</td>\n",
       "      <td>0.357090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.749600</td>\n",
       "      <td>0.581362</td>\n",
       "      <td>0.173625</td>\n",
       "      <td>0.436462</td>\n",
       "      <td>0.208491</td>\n",
       "      <td>0.429708</td>\n",
       "      <td>0.381156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.603500</td>\n",
       "      <td>0.524458</td>\n",
       "      <td>0.187619</td>\n",
       "      <td>0.482107</td>\n",
       "      <td>0.222646</td>\n",
       "      <td>0.474825</td>\n",
       "      <td>0.429563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.545700</td>\n",
       "      <td>0.510868</td>\n",
       "      <td>0.179293</td>\n",
       "      <td>0.482690</td>\n",
       "      <td>0.216257</td>\n",
       "      <td>0.473486</td>\n",
       "      <td>0.427340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.516300</td>\n",
       "      <td>0.499446</td>\n",
       "      <td>0.188437</td>\n",
       "      <td>0.480986</td>\n",
       "      <td>0.224741</td>\n",
       "      <td>0.472386</td>\n",
       "      <td>0.429339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.500800</td>\n",
       "      <td>0.492702</td>\n",
       "      <td>0.186757</td>\n",
       "      <td>0.479738</td>\n",
       "      <td>0.220645</td>\n",
       "      <td>0.469906</td>\n",
       "      <td>0.429228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.465600</td>\n",
       "      <td>0.487743</td>\n",
       "      <td>0.197092</td>\n",
       "      <td>0.488425</td>\n",
       "      <td>0.228777</td>\n",
       "      <td>0.476966</td>\n",
       "      <td>0.439543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.467100</td>\n",
       "      <td>0.483415</td>\n",
       "      <td>0.196648</td>\n",
       "      <td>0.485729</td>\n",
       "      <td>0.231033</td>\n",
       "      <td>0.479139</td>\n",
       "      <td>0.440615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.475400</td>\n",
       "      <td>0.480684</td>\n",
       "      <td>0.201127</td>\n",
       "      <td>0.491399</td>\n",
       "      <td>0.233739</td>\n",
       "      <td>0.482228</td>\n",
       "      <td>0.443100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.463900</td>\n",
       "      <td>0.477924</td>\n",
       "      <td>0.205087</td>\n",
       "      <td>0.497340</td>\n",
       "      <td>0.243506</td>\n",
       "      <td>0.489021</td>\n",
       "      <td>0.453545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.454700</td>\n",
       "      <td>0.475895</td>\n",
       "      <td>0.205868</td>\n",
       "      <td>0.498330</td>\n",
       "      <td>0.241509</td>\n",
       "      <td>0.490370</td>\n",
       "      <td>0.453524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.447600</td>\n",
       "      <td>0.473901</td>\n",
       "      <td>0.209780</td>\n",
       "      <td>0.498587</td>\n",
       "      <td>0.243615</td>\n",
       "      <td>0.490050</td>\n",
       "      <td>0.455160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.442200</td>\n",
       "      <td>0.471928</td>\n",
       "      <td>0.212581</td>\n",
       "      <td>0.497324</td>\n",
       "      <td>0.246183</td>\n",
       "      <td>0.490188</td>\n",
       "      <td>0.453170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.426300</td>\n",
       "      <td>0.471465</td>\n",
       "      <td>0.212578</td>\n",
       "      <td>0.499733</td>\n",
       "      <td>0.247262</td>\n",
       "      <td>0.491286</td>\n",
       "      <td>0.455370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.421700</td>\n",
       "      <td>0.470447</td>\n",
       "      <td>0.205257</td>\n",
       "      <td>0.494650</td>\n",
       "      <td>0.236942</td>\n",
       "      <td>0.486683</td>\n",
       "      <td>0.448704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.422300</td>\n",
       "      <td>0.469137</td>\n",
       "      <td>0.208284</td>\n",
       "      <td>0.494180</td>\n",
       "      <td>0.238725</td>\n",
       "      <td>0.485929</td>\n",
       "      <td>0.447937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.403000</td>\n",
       "      <td>0.469453</td>\n",
       "      <td>0.204867</td>\n",
       "      <td>0.492905</td>\n",
       "      <td>0.236945</td>\n",
       "      <td>0.484570</td>\n",
       "      <td>0.446349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.421800</td>\n",
       "      <td>0.468233</td>\n",
       "      <td>0.209343</td>\n",
       "      <td>0.496982</td>\n",
       "      <td>0.242186</td>\n",
       "      <td>0.488822</td>\n",
       "      <td>0.452487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.405100</td>\n",
       "      <td>0.468044</td>\n",
       "      <td>0.207208</td>\n",
       "      <td>0.499432</td>\n",
       "      <td>0.243118</td>\n",
       "      <td>0.492066</td>\n",
       "      <td>0.453798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.395200</td>\n",
       "      <td>0.467846</td>\n",
       "      <td>0.204752</td>\n",
       "      <td>0.495168</td>\n",
       "      <td>0.238693</td>\n",
       "      <td>0.487196</td>\n",
       "      <td>0.449048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.385100</td>\n",
       "      <td>0.467359</td>\n",
       "      <td>0.205835</td>\n",
       "      <td>0.493235</td>\n",
       "      <td>0.238964</td>\n",
       "      <td>0.485593</td>\n",
       "      <td>0.448679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.411100</td>\n",
       "      <td>0.467258</td>\n",
       "      <td>0.208697</td>\n",
       "      <td>0.494403</td>\n",
       "      <td>0.239464</td>\n",
       "      <td>0.486067</td>\n",
       "      <td>0.452817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.415500</td>\n",
       "      <td>0.467327</td>\n",
       "      <td>0.208439</td>\n",
       "      <td>0.494300</td>\n",
       "      <td>0.239464</td>\n",
       "      <td>0.485979</td>\n",
       "      <td>0.452158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.401800</td>\n",
       "      <td>0.467163</td>\n",
       "      <td>0.208350</td>\n",
       "      <td>0.496936</td>\n",
       "      <td>0.238822</td>\n",
       "      <td>0.487468</td>\n",
       "      <td>0.453623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.406000</td>\n",
       "      <td>0.467405</td>\n",
       "      <td>0.206968</td>\n",
       "      <td>0.497484</td>\n",
       "      <td>0.238046</td>\n",
       "      <td>0.487930</td>\n",
       "      <td>0.452643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.377100</td>\n",
       "      <td>0.466934</td>\n",
       "      <td>0.207301</td>\n",
       "      <td>0.495215</td>\n",
       "      <td>0.237500</td>\n",
       "      <td>0.485525</td>\n",
       "      <td>0.451163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.400900</td>\n",
       "      <td>0.466853</td>\n",
       "      <td>0.207273</td>\n",
       "      <td>0.495215</td>\n",
       "      <td>0.237500</td>\n",
       "      <td>0.485525</td>\n",
       "      <td>0.451134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.389900</td>\n",
       "      <td>0.466719</td>\n",
       "      <td>0.207301</td>\n",
       "      <td>0.495215</td>\n",
       "      <td>0.237500</td>\n",
       "      <td>0.485525</td>\n",
       "      <td>0.451163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.397900</td>\n",
       "      <td>0.466670</td>\n",
       "      <td>0.207124</td>\n",
       "      <td>0.497035</td>\n",
       "      <td>0.237500</td>\n",
       "      <td>0.487187</td>\n",
       "      <td>0.450843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 results: {'eval_loss': 0.4666701555252075, 'eval_bleu': 0.2071235891410905, 'eval_rouge1': 0.49703457415389246, 'eval_rouge2': 0.23749999177333164, 'eval_rougeL': 0.48718710276889843, 'eval_meteor': 0.4508434373130092, 'eval_runtime': 2.4394, 'eval_samples_per_second': 48.782, 'eval_steps_per_second': 6.149, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:00<00:00, 112465.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 5 saved to crossval_T5_SMALL_QUERY_ONLY_fold_5.jsonl\n",
      "Average metrics over all folds: {'eval_loss': 0.47526371479034424, 'eval_bleu': 0.19411330054290937, 'eval_rouge1': 0.4892978206261313, 'eval_rouge2': 0.24191867763495162, 'eval_rougeL': 0.4783860218231945, 'eval_meteor': 0.4459745481087546, 'eval_runtime': 2.2956600000000003, 'eval_samples_per_second': 52.59060000000001, 'eval_steps_per_second': 6.5840000000000005, 'epoch': 26.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Averages/epoch</td><td>▁</td></tr><tr><td>Averages/eval_bleu</td><td>▁</td></tr><tr><td>Averages/eval_loss</td><td>▁</td></tr><tr><td>Averages/eval_meteor</td><td>▁</td></tr><tr><td>Averages/eval_rouge1</td><td>▁</td></tr><tr><td>Averages/eval_rouge2</td><td>▁</td></tr><tr><td>Averages/eval_rougeL</td><td>▁</td></tr><tr><td>Averages/eval_runtime</td><td>▁</td></tr><tr><td>Averages/eval_samples_per_second</td><td>▁</td></tr><tr><td>Averages/eval_steps_per_second</td><td>▁</td></tr><tr><td>Fold_1/eval/epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇███</td></tr><tr><td>Fold_1/eval/eval_bleu</td><td>▁▃▅▅▆▆▇█▇▇██████▇▇█▇███████</td></tr><tr><td>Fold_1/eval/eval_loss</td><td>█▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_1/eval/eval_meteor</td><td>▁▃▅▆▆▇▇▇█▇████████▇█▇▇█████</td></tr><tr><td>Fold_1/eval/eval_rouge1</td><td>▁▃▅▆▇▇▇▇███████████████████</td></tr><tr><td>Fold_1/eval/eval_rouge2</td><td>▁▄▅▅▆▆▇▇▇▇███▇██▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>Fold_1/eval/eval_rougeL</td><td>▁▃▅▇▇▇█▇████████████████▇▇█</td></tr><tr><td>Fold_1/eval/eval_runtime</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_1/eval/eval_samples_per_second</td><td>▁██▇███████████████████▇▇▇▇</td></tr><tr><td>Fold_1/eval/eval_steps_per_second</td><td>▁██▇███████████████████▇▇▇▇</td></tr><tr><td>Fold_2/eval/epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇███</td></tr><tr><td>Fold_2/eval/eval_bleu</td><td>▁▄▄▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█▇███▇</td></tr><tr><td>Fold_2/eval/eval_loss</td><td>█▅▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_2/eval/eval_meteor</td><td>▁▄▅▆▆▇▇▇▇▇██████████████</td></tr><tr><td>Fold_2/eval/eval_rouge1</td><td>▁▃▅▆▆▆▇▇▇▇██████████████</td></tr><tr><td>Fold_2/eval/eval_rouge2</td><td>▁▄▄▅▅▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇███▇</td></tr><tr><td>Fold_2/eval/eval_rougeL</td><td>▁▃▄▆▆▆▇▇▇▇▇▇█▇▇█▇▇██████</td></tr><tr><td>Fold_2/eval/eval_runtime</td><td>▃▂▁▂▁▁▁█▁███████▃▁▁▁▁▁▁▂</td></tr><tr><td>Fold_2/eval/eval_samples_per_second</td><td>▆▇█▇███▁█▁▁▁▁▁▁▁▆██████▇</td></tr><tr><td>Fold_2/eval/eval_steps_per_second</td><td>▆▇█▇███▁█▁▁▁▁▁▁▁▆██████▇</td></tr><tr><td>Fold_3/eval/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>Fold_3/eval/eval_bleu</td><td>▁▄▅▆▇▇▇▇▇▇▇▇███▇▇▇▇▇▇▇▇▇▇██████</td></tr><tr><td>Fold_3/eval/eval_loss</td><td>█▅▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_3/eval/eval_meteor</td><td>▁▃▅▆▆▇▇▇▇██████████████████████</td></tr><tr><td>Fold_3/eval/eval_rouge1</td><td>▁▃▅▆▆▇▇▇▇█▇████████████████████</td></tr><tr><td>Fold_3/eval/eval_rouge2</td><td>▁▄▅▆▆▆▆▆▆▇▇▇█▇▇▇▇▇▇▇▇▇▇█▇██████</td></tr><tr><td>Fold_3/eval/eval_rougeL</td><td>▁▃▅▆▆▇▇▇▇█▇████████████████████</td></tr><tr><td>Fold_3/eval/eval_runtime</td><td>▆▃▅▃█▄▄▂▂▁▃▁▂▁▂▃▃▄▃▂▂▄▂▁▁▂▂▄▃▃▃</td></tr><tr><td>Fold_3/eval/eval_samples_per_second</td><td>▃▆▄▆▁▅▅▇▇█▆█▇▇▇▆▆▅▆▇▇▅▇██▇▇▄▆▆▅</td></tr><tr><td>Fold_3/eval/eval_steps_per_second</td><td>▃▆▄▆▁▅▅▇▇█▆█▇▇▇▆▆▅▆▇▇▅▇██▇▇▄▆▅▅</td></tr><tr><td>Fold_4/eval/epoch</td><td>▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇███</td></tr><tr><td>Fold_4/eval/eval_bleu</td><td>▁▄▅▆▆▇██▇▇▇▇▇▇███████████</td></tr><tr><td>Fold_4/eval/eval_loss</td><td>█▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_4/eval/eval_meteor</td><td>▁▃▅▇▇████████▇█▇▇████████</td></tr><tr><td>Fold_4/eval/eval_rouge1</td><td>▁▃▄▇▇▇█████████▇██████▇██</td></tr><tr><td>Fold_4/eval/eval_rouge2</td><td>▁▄▆▆▅▇██▇████▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>Fold_4/eval/eval_rougeL</td><td>▁▃▄▇▇▇███████▇█▇▇█▇█▇█▇▇▇</td></tr><tr><td>Fold_4/eval/eval_runtime</td><td>▁▁▁▁▁▁▃▆▆▆▆▆▆█▇▆▆▆▆▆▆▆▆▆█</td></tr><tr><td>Fold_4/eval/eval_samples_per_second</td><td>██████▆▃▃▃▃▃▃▁▂▃▃▂▃▃▃▃▂▃▁</td></tr><tr><td>Fold_4/eval/eval_steps_per_second</td><td>██████▆▃▃▃▃▃▃▁▂▃▃▂▃▃▃▃▂▃▁</td></tr><tr><td>Fold_5/eval/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>Fold_5/eval/eval_bleu</td><td>▁▄▄▆▅▆▆▇▇▇▇▇███▇█▇██▇▇███▇███▇▇</td></tr><tr><td>Fold_5/eval/eval_loss</td><td>█▅▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_5/eval/eval_meteor</td><td>▁▄▅▇▇▇▇▇▇▇█████████████████████</td></tr><tr><td>Fold_5/eval/eval_rouge1</td><td>▁▃▅▇▇▇▇▇▇██████████████████████</td></tr><tr><td>Fold_5/eval/eval_rouge2</td><td>▁▄▅▆▅▆▆▆▇▇█████▇▇▇██▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>Fold_5/eval/eval_rougeL</td><td>▁▃▅▇▇▇▇▇▇▇█████████████████████</td></tr><tr><td>Fold_5/eval/eval_runtime</td><td>▁▇█▅█▆▆████▅▆▃▆▆▆█▆▆▆▆█▆▆▆▆▆▅▇▆</td></tr><tr><td>Fold_5/eval/eval_samples_per_second</td><td>█▂▁▃▁▃▃▁▁▁▁▄▃▅▃▃▃▁▃▃▃▃▁▃▃▃▃▃▃▂▂</td></tr><tr><td>Fold_5/eval/eval_steps_per_second</td><td>█▂▁▃▁▃▃▁▁▁▁▄▃▅▃▃▃▁▃▃▃▃▁▃▃▃▃▃▃▂▂</td></tr><tr><td>eval/bleu</td><td>▁▅▅▇▇▇▇▇▇▇▇▅▇▇▇▇▇▇▇▇▇▆▇▇▂▆▆▆▆▆▃▅▆▇▆█████</td></tr><tr><td>eval/loss</td><td>▃▂▂▂▂▂▂▂█▂▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▆▃▂▂▂▂▁▁▁▁</td></tr><tr><td>eval/meteor</td><td>▃▆▆▇▇▇▇▇▂▄▇███▁▆▆▆▇▇▇▇▇▇▇▂▆▆▆▆▆▇▇▇▇▇▇▇▇▇</td></tr><tr><td>eval/rouge1</td><td>▂▄▇▇▇▇▇▃█████▅▆▇▇▇▇▇▇▇██▇█▁▂▆▆▆▆▅▇▇█████</td></tr><tr><td>eval/rouge2</td><td>▄▅▆▇▇▆▆▆▁▅▇████▂▄▅▆▆▆▆▆▆▇▁▄▄▅▅▅▅▄▅▆▇▇▆▆▆</td></tr><tr><td>eval/rougeL</td><td>▁▃▆▆▇▇▇▇▆▇▇▇████████▆▆▇▇▇▇▇▂▃▄▆▅▇▇▇██▇▇▇</td></tr><tr><td>eval/runtime</td><td>▂▂▂▂▂▂▂▃▂▂▃▃▃▃▃▂▂▂▁▁▁▁▁▂▁▃▇█▇▇█▇██▆▇▅▅▆▆</td></tr><tr><td>eval/samples_per_second</td><td>▇▇▇▇▇▇▆▆▇▇▇▆▇▇▇▇█████▇█▆▆▁▁▁▁▁▁▁▂▁▃▂▁▂▂▂</td></tr><tr><td>eval/steps_per_second</td><td>▁█▇████▇▇▇█▇▇▇▇███████████▇▇▇▇▆▆▆▇▆▆▇▆▆▆</td></tr><tr><td>test/bleu</td><td>▅▄▃▁█</td></tr><tr><td>test/loss</td><td>█▆▁▅▄</td></tr><tr><td>test/meteor</td><td>▄█▅▁▆</td></tr><tr><td>test/rouge1</td><td>▆█▆▁▇</td></tr><tr><td>test/rouge2</td><td>▅█▆▁▄</td></tr><tr><td>test/rougeL</td><td>▄█▆▁▇</td></tr><tr><td>test/runtime</td><td>▃▁▁█▅</td></tr><tr><td>test/samples_per_second</td><td>▆▇█▁▃</td></tr><tr><td>test/steps_per_second</td><td>▆▇█▁▃</td></tr><tr><td>train/epoch</td><td>▁▂▃▃▄▅▇▁▂▃▄▄▆▆▆▂▂▃▄▄▅▆▆▇█▂▃▄▄▄▆▇▁▂▃▆▆▆▇█</td></tr><tr><td>train/global_step</td><td>▃▃▃▄▄▆▆▇▁▁▄▄▅▂▂▃▄▄▆▆▂▃▃▃▄▅▅▅▆▁▂▂▃▃▄▅▆▆██</td></tr><tr><td>train/grad_norm</td><td>▁▁▁▂▁▃▂▃▂▁▁▁▁▁▁▂█▃▇▂▁▁▁▁▁▂▃▁▁▁▁▁▂▁▂▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▆▆▅▄▄▃▂█▇▆▅▅▅▄▄█▇▇▆▆▁█▇▆▆▄▄▄█▆▆▅▅▅▄▃▂▂▂▁</td></tr><tr><td>train/loss</td><td>▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Averages/epoch</td><td>26.6</td></tr><tr><td>Averages/eval_bleu</td><td>0.19411</td></tr><tr><td>Averages/eval_loss</td><td>0.47526</td></tr><tr><td>Averages/eval_meteor</td><td>0.44597</td></tr><tr><td>Averages/eval_rouge1</td><td>0.4893</td></tr><tr><td>Averages/eval_rouge2</td><td>0.24192</td></tr><tr><td>Averages/eval_rougeL</td><td>0.47839</td></tr><tr><td>Averages/eval_runtime</td><td>2.29566</td></tr><tr><td>Averages/eval_samples_per_second</td><td>52.5906</td></tr><tr><td>Averages/eval_steps_per_second</td><td>6.584</td></tr><tr><td>Fold_1/eval/epoch</td><td>26</td></tr><tr><td>Fold_1/eval/eval_bleu</td><td>0.19618</td></tr><tr><td>Fold_1/eval/eval_loss</td><td>0.50812</td></tr><tr><td>Fold_1/eval/eval_meteor</td><td>0.43963</td></tr><tr><td>Fold_1/eval/eval_rouge1</td><td>0.48946</td></tr><tr><td>Fold_1/eval/eval_rouge2</td><td>0.24238</td></tr><tr><td>Fold_1/eval/eval_rougeL</td><td>0.47182</td></tr><tr><td>Fold_1/eval/eval_runtime</td><td>2.2058</td></tr><tr><td>Fold_1/eval/eval_samples_per_second</td><td>54.402</td></tr><tr><td>Fold_1/eval/eval_steps_per_second</td><td>6.8</td></tr><tr><td>Fold_2/eval/epoch</td><td>23</td></tr><tr><td>Fold_2/eval/eval_bleu</td><td>0.19254</td></tr><tr><td>Fold_2/eval/eval_loss</td><td>0.48996</td></tr><tr><td>Fold_2/eval/eval_meteor</td><td>0.46557</td></tr><tr><td>Fold_2/eval/eval_rouge1</td><td>0.49983</td></tr><tr><td>Fold_2/eval/eval_rouge2</td><td>0.25995</td></tr><tr><td>Fold_2/eval/eval_rougeL</td><td>0.4919</td></tr><tr><td>Fold_2/eval/eval_runtime</td><td>2.1153</td></tr><tr><td>Fold_2/eval/eval_samples_per_second</td><td>56.729</td></tr><tr><td>Fold_2/eval/eval_steps_per_second</td><td>7.091</td></tr><tr><td>Fold_3/eval/epoch</td><td>30</td></tr><tr><td>Fold_3/eval/eval_bleu</td><td>0.19102</td></tr><tr><td>Fold_3/eval/eval_loss</td><td>0.43817</td></tr><tr><td>Fold_3/eval/eval_meteor</td><td>0.45019</td></tr><tr><td>Fold_3/eval/eval_rouge1</td><td>0.49278</td></tr><tr><td>Fold_3/eval/eval_rouge2</td><td>0.24773</td></tr><tr><td>Fold_3/eval/eval_rougeL</td><td>0.48393</td></tr><tr><td>Fold_3/eval/eval_runtime</td><td>2.0942</td></tr><tr><td>Fold_3/eval/eval_samples_per_second</td><td>57.301</td></tr><tr><td>Fold_3/eval/eval_steps_per_second</td><td>7.163</td></tr><tr><td>Fold_4/eval/epoch</td><td>24</td></tr><tr><td>Fold_4/eval/eval_bleu</td><td>0.18371</td></tr><tr><td>Fold_4/eval/eval_loss</td><td>0.4734</td></tr><tr><td>Fold_4/eval/eval_meteor</td><td>0.42364</td></tr><tr><td>Fold_4/eval/eval_rouge1</td><td>0.46739</td></tr><tr><td>Fold_4/eval/eval_rouge2</td><td>0.22202</td></tr><tr><td>Fold_4/eval/eval_rougeL</td><td>0.45709</td></tr><tr><td>Fold_4/eval/eval_runtime</td><td>2.6236</td></tr><tr><td>Fold_4/eval/eval_samples_per_second</td><td>45.739</td></tr><tr><td>Fold_4/eval/eval_steps_per_second</td><td>5.717</td></tr><tr><td>Fold_5/eval/epoch</td><td>30</td></tr><tr><td>Fold_5/eval/eval_bleu</td><td>0.20712</td></tr><tr><td>Fold_5/eval/eval_loss</td><td>0.46667</td></tr><tr><td>Fold_5/eval/eval_meteor</td><td>0.45084</td></tr><tr><td>Fold_5/eval/eval_rouge1</td><td>0.49703</td></tr><tr><td>Fold_5/eval/eval_rouge2</td><td>0.2375</td></tr><tr><td>Fold_5/eval/eval_rougeL</td><td>0.48719</td></tr><tr><td>Fold_5/eval/eval_runtime</td><td>2.4394</td></tr><tr><td>Fold_5/eval/eval_samples_per_second</td><td>48.782</td></tr><tr><td>Fold_5/eval/eval_steps_per_second</td><td>6.149</td></tr><tr><td>eval/bleu</td><td>0.20712</td></tr><tr><td>eval/loss</td><td>0.46667</td></tr><tr><td>eval/meteor</td><td>0.45084</td></tr><tr><td>eval/rouge1</td><td>0.49703</td></tr><tr><td>eval/rouge2</td><td>0.2375</td></tr><tr><td>eval/rougeL</td><td>0.48719</td></tr><tr><td>eval/runtime</td><td>2.4394</td></tr><tr><td>eval/samples_per_second</td><td>48.782</td></tr><tr><td>eval/steps_per_second</td><td>6.149</td></tr><tr><td>test/bleu</td><td>0.20712</td></tr><tr><td>test/loss</td><td>0.46667</td></tr><tr><td>test/meteor</td><td>0.45084</td></tr><tr><td>test/rouge1</td><td>0.49703</td></tr><tr><td>test/rouge2</td><td>0.2375</td></tr><tr><td>test/rougeL</td><td>0.48719</td></tr><tr><td>test/runtime</td><td>2.4024</td></tr><tr><td>test/samples_per_second</td><td>49.533</td></tr><tr><td>test/steps_per_second</td><td>6.244</td></tr><tr><td>total_flos</td><td>1948921941196800.0</td></tr><tr><td>train/epoch</td><td>30</td></tr><tr><td>train/global_step</td><td>1800</td></tr><tr><td>train/grad_norm</td><td>0.65231</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>0.3979</td></tr><tr><td>train_loss</td><td>0.62459</td></tr><tr><td>train_runtime</td><td>686.5421</td></tr><tr><td>train_samples_per_second</td><td>20.975</td></tr><tr><td>train_steps_per_second</td><td>2.622</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">T5_SMALL_QUERY_ONLY</strong> at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/jvw9nm91' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/jvw9nm91</a><br> View project at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250510_181509-jvw9nm91/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final predicitons for all folds foudn in crossval_T5_SMALL_QUERY_ONLY.jsonl\n"
     ]
    }
   ],
   "source": [
    "cross_val_train(modelname=\"t5-small\", run_name=\"T5_SMALL_QUERY_ONLY\",filename=\"crossval_T5_SMALL_QUERY_ONLY.jsonl\", \n",
    "                tokenizer=tokenizer, tokenized_dataset=tokenized_dataset, \n",
    "                folds=folds, nb_epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7668e0a-cc70-4e2a-9109-2a8ddaf21e6b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# ---------- TOP 5 DOUCMENTS SUMMARIZED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6f325fa-3e1f-4093-98f3-9b9d33869d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a1560c7117e40878d3c653e89f73254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': \"[QUERY] Find information on President Barack Obama's family history, including genealogy, national origins, places and dates of birth, etc.\\n[DOCUMENTS]\\n[DOC 1] This page was last modified on 14 March 2009, at 05:50 . It contains articles with unsourced statements and articles needing factual verification since October 2008 .\\n[DOC 2] This page was last modified on 14 March 2009, at 05:50 . It contains articles needing factual verification since October 2008 . It is available under the terms of the GNU Free Documentation License .\\n[DOC 3] This page was last modified on 16 March 2009, at 03:22 . It contains articles with unsourced statements and articles needing factual verification since October 2008 . Hidden categories: Wikipedia semi-protected pages | All pages needing cleanup .\\n[DOC 4] This page was last modified on 14 March 2009, at 05:50 . Use this page to help users with reading comprehension and vocabulary . Use the weekly Newsquiz to test your knowledge of stories you need to share .\\n[DOC 5] This page was last modified on 14 March 2009, at 05:50 . It contains articles with unsourced statements . The Audacity of Hope was written in 2009 .\", 'output': \"Where did Barack Obama's parents and grandparents come from?\", 'input_ids': [784, 15367, 11824, 908, 2588, 251, 30, 1661, 20653, 4534, 31, 7, 384, 892, 6, 379, 31960, 6, 1157, 5233, 7, 6, 1747, 11, 5128, 13, 3879, 6, 672, 5, 784, 9857, 5211, 11810, 134, 908, 784, 26472, 209, 908, 100, 543, 47, 336, 8473, 30, 968, 1332, 2464, 6, 44, 3, 3076, 10, 1752, 3, 5, 94, 2579, 2984, 28, 73, 15551, 6643, 11, 2984, 174, 53, 685, 3471, 17549, 437, 1797, 2628, 3, 5, 784, 26472, 204, 908, 100, 543, 47, 336, 8473, 30, 968, 1332, 2464, 6, 44, 3, 3076, 10, 1752, 3, 5, 94, 2579, 2984, 174, 53, 685, 3471, 17549, 437, 1797, 2628, 3, 5, 94, 19, 347, 365, 8, 1353, 13, 8, 350, 17052, 1443, 11167, 257, 16452, 3, 5, 784, 26472, 220, 908, 100, 543, 47, 336, 8473, 30, 898, 1332, 2464, 6, 44, 12811, 10, 2884, 3, 5, 94, 2579, 2984, 28, 73, 15551, 6643, 11, 2984, 174, 53, 685, 3471, 17549, 437, 1797, 2628, 3, 5, 27194, 5897, 10, 16885, 4772, 18, 19812, 15, 26, 1688, 1820, 432, 1688, 174, 53, 23175, 3, 5, 784, 26472, 314, 908, 100, 543, 47, 336, 8473, 30, 968, 1332, 2464, 6, 44, 3, 3076, 10, 1752, 3, 5, 2048, 48, 543, 12, 199, 1105, 28, 1183, 27160, 11, 19067, 3, 5, 2048, 8, 5547, 3529, 1169, 172, 12, 794, 39, 1103, 13, 1937, 25, 174, 12, 698, 3, 5, 784, 26472, 305, 908, 100, 543, 47, 336, 8473, 30, 968, 1332, 2464, 6, 44, 3, 3076, 10, 1752, 3, 5, 94, 2579, 2984, 28, 73, 15551, 6643, 3, 5, 37, 1957, 26, 9, 6726, 13, 6000, 47, 1545, 16, 2464, 3, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [2840, 410, 20653, 4534, 31, 7, 1362, 11, 22229, 369, 45, 58, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "tokenizer,tokenized_dataset= tokenize_and_split_dataset(\"training_top5_qulac_SUMMARIZED_FOR_MODEL.json\",\"t5-small\")\n",
    "print(tokenized_dataset[0])  # Optional: check one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c3f69a1-2bcc-4294-b048-2e129e8f24b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds= generate_folds(tokenized_dataset, n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3efe5ac-26f7-4f19-b8b1-cc3e817e37ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/users/nfs/Etu0/21402600/wandb/run-20250510_224706-yqrgb5pz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/yqrgb5pz' target=\"_blank\">T5_SMALL_TOP5DOCS_SUMMARIZED</a></strong> to <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/yqrgb5pz' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/yqrgb5pz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_14586/3145572364.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='900' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 900/1800 05:37 < 05:37, 2.66 it/s, Epoch 15/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.393400</td>\n",
       "      <td>0.844550</td>\n",
       "      <td>0.098140</td>\n",
       "      <td>0.393720</td>\n",
       "      <td>0.172560</td>\n",
       "      <td>0.383675</td>\n",
       "      <td>0.300530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.730600</td>\n",
       "      <td>0.601212</td>\n",
       "      <td>0.150760</td>\n",
       "      <td>0.431580</td>\n",
       "      <td>0.206855</td>\n",
       "      <td>0.424689</td>\n",
       "      <td>0.377058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.586500</td>\n",
       "      <td>0.545290</td>\n",
       "      <td>0.170607</td>\n",
       "      <td>0.469422</td>\n",
       "      <td>0.220446</td>\n",
       "      <td>0.459851</td>\n",
       "      <td>0.417890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.519100</td>\n",
       "      <td>0.530222</td>\n",
       "      <td>0.178776</td>\n",
       "      <td>0.470718</td>\n",
       "      <td>0.228101</td>\n",
       "      <td>0.461492</td>\n",
       "      <td>0.423201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.491400</td>\n",
       "      <td>0.515488</td>\n",
       "      <td>0.182384</td>\n",
       "      <td>0.477691</td>\n",
       "      <td>0.228463</td>\n",
       "      <td>0.468648</td>\n",
       "      <td>0.429876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.460800</td>\n",
       "      <td>0.508271</td>\n",
       "      <td>0.182371</td>\n",
       "      <td>0.478430</td>\n",
       "      <td>0.229796</td>\n",
       "      <td>0.469196</td>\n",
       "      <td>0.429252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.438900</td>\n",
       "      <td>0.503539</td>\n",
       "      <td>0.185633</td>\n",
       "      <td>0.474441</td>\n",
       "      <td>0.231480</td>\n",
       "      <td>0.463858</td>\n",
       "      <td>0.425513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.429800</td>\n",
       "      <td>0.499806</td>\n",
       "      <td>0.180491</td>\n",
       "      <td>0.475617</td>\n",
       "      <td>0.225040</td>\n",
       "      <td>0.467359</td>\n",
       "      <td>0.426904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.432800</td>\n",
       "      <td>0.499436</td>\n",
       "      <td>0.186712</td>\n",
       "      <td>0.484756</td>\n",
       "      <td>0.230496</td>\n",
       "      <td>0.472845</td>\n",
       "      <td>0.436418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.413600</td>\n",
       "      <td>0.499159</td>\n",
       "      <td>0.193403</td>\n",
       "      <td>0.480349</td>\n",
       "      <td>0.237614</td>\n",
       "      <td>0.470217</td>\n",
       "      <td>0.433564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.384800</td>\n",
       "      <td>0.498770</td>\n",
       "      <td>0.189728</td>\n",
       "      <td>0.488274</td>\n",
       "      <td>0.236147</td>\n",
       "      <td>0.476311</td>\n",
       "      <td>0.440612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.392300</td>\n",
       "      <td>0.498631</td>\n",
       "      <td>0.190746</td>\n",
       "      <td>0.489043</td>\n",
       "      <td>0.231242</td>\n",
       "      <td>0.474459</td>\n",
       "      <td>0.438389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.390700</td>\n",
       "      <td>0.502049</td>\n",
       "      <td>0.187945</td>\n",
       "      <td>0.487172</td>\n",
       "      <td>0.228681</td>\n",
       "      <td>0.471949</td>\n",
       "      <td>0.436189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.396000</td>\n",
       "      <td>0.499222</td>\n",
       "      <td>0.191505</td>\n",
       "      <td>0.486842</td>\n",
       "      <td>0.229736</td>\n",
       "      <td>0.472041</td>\n",
       "      <td>0.438110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.387600</td>\n",
       "      <td>0.500895</td>\n",
       "      <td>0.195191</td>\n",
       "      <td>0.488448</td>\n",
       "      <td>0.231025</td>\n",
       "      <td>0.471668</td>\n",
       "      <td>0.439554</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 results: {'eval_loss': 0.4986312985420227, 'eval_bleu': 0.19074583115428184, 'eval_rouge1': 0.48904300069121504, 'eval_rouge2': 0.23124195063454211, 'eval_rougeL': 0.4744587812246184, 'eval_meteor': 0.43838852433833125, 'eval_runtime': 2.1723, 'eval_samples_per_second': 55.242, 'eval_steps_per_second': 6.905, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 40152.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 1 saved to crossval_T5_TOP5DOCS_SUMMARIZED_fold_1.jsonl\n",
      "Processing Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_14586/3145572364.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='960' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 960/1800 05:57 < 05:13, 2.68 it/s, Epoch 16/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.367300</td>\n",
       "      <td>0.840007</td>\n",
       "      <td>0.113379</td>\n",
       "      <td>0.387473</td>\n",
       "      <td>0.178009</td>\n",
       "      <td>0.379653</td>\n",
       "      <td>0.301322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.738900</td>\n",
       "      <td>0.610343</td>\n",
       "      <td>0.156122</td>\n",
       "      <td>0.445008</td>\n",
       "      <td>0.223891</td>\n",
       "      <td>0.438057</td>\n",
       "      <td>0.399772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.540500</td>\n",
       "      <td>0.529365</td>\n",
       "      <td>0.168870</td>\n",
       "      <td>0.471875</td>\n",
       "      <td>0.242810</td>\n",
       "      <td>0.465599</td>\n",
       "      <td>0.435242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.510330</td>\n",
       "      <td>0.179504</td>\n",
       "      <td>0.481421</td>\n",
       "      <td>0.244129</td>\n",
       "      <td>0.471905</td>\n",
       "      <td>0.445479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.465300</td>\n",
       "      <td>0.500303</td>\n",
       "      <td>0.182987</td>\n",
       "      <td>0.485587</td>\n",
       "      <td>0.245132</td>\n",
       "      <td>0.475302</td>\n",
       "      <td>0.448616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.473500</td>\n",
       "      <td>0.493620</td>\n",
       "      <td>0.189127</td>\n",
       "      <td>0.494526</td>\n",
       "      <td>0.252337</td>\n",
       "      <td>0.484085</td>\n",
       "      <td>0.454191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.453500</td>\n",
       "      <td>0.488064</td>\n",
       "      <td>0.191051</td>\n",
       "      <td>0.498080</td>\n",
       "      <td>0.262739</td>\n",
       "      <td>0.490697</td>\n",
       "      <td>0.462276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.425500</td>\n",
       "      <td>0.486565</td>\n",
       "      <td>0.201974</td>\n",
       "      <td>0.508990</td>\n",
       "      <td>0.276899</td>\n",
       "      <td>0.502499</td>\n",
       "      <td>0.475723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.411100</td>\n",
       "      <td>0.484984</td>\n",
       "      <td>0.199641</td>\n",
       "      <td>0.501817</td>\n",
       "      <td>0.270618</td>\n",
       "      <td>0.495777</td>\n",
       "      <td>0.471454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.423700</td>\n",
       "      <td>0.483069</td>\n",
       "      <td>0.204154</td>\n",
       "      <td>0.509511</td>\n",
       "      <td>0.274646</td>\n",
       "      <td>0.501453</td>\n",
       "      <td>0.477426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.395100</td>\n",
       "      <td>0.482153</td>\n",
       "      <td>0.206338</td>\n",
       "      <td>0.510360</td>\n",
       "      <td>0.277479</td>\n",
       "      <td>0.502644</td>\n",
       "      <td>0.476093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.401700</td>\n",
       "      <td>0.480806</td>\n",
       "      <td>0.212268</td>\n",
       "      <td>0.514025</td>\n",
       "      <td>0.286867</td>\n",
       "      <td>0.508378</td>\n",
       "      <td>0.481110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.393100</td>\n",
       "      <td>0.480254</td>\n",
       "      <td>0.213526</td>\n",
       "      <td>0.520957</td>\n",
       "      <td>0.293721</td>\n",
       "      <td>0.511678</td>\n",
       "      <td>0.488205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.365400</td>\n",
       "      <td>0.480462</td>\n",
       "      <td>0.221452</td>\n",
       "      <td>0.523811</td>\n",
       "      <td>0.298465</td>\n",
       "      <td>0.516387</td>\n",
       "      <td>0.492296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.364700</td>\n",
       "      <td>0.480471</td>\n",
       "      <td>0.217460</td>\n",
       "      <td>0.518835</td>\n",
       "      <td>0.291437</td>\n",
       "      <td>0.509693</td>\n",
       "      <td>0.488028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.379200</td>\n",
       "      <td>0.480256</td>\n",
       "      <td>0.220664</td>\n",
       "      <td>0.518195</td>\n",
       "      <td>0.295599</td>\n",
       "      <td>0.509999</td>\n",
       "      <td>0.486927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 results: {'eval_loss': 0.4802541732788086, 'eval_bleu': 0.2135260089579745, 'eval_rouge1': 0.5209566480659632, 'eval_rouge2': 0.29372120823634895, 'eval_rougeL': 0.5116782657622396, 'eval_meteor': 0.48820488273763574, 'eval_runtime': 2.1116, 'eval_samples_per_second': 56.828, 'eval_steps_per_second': 7.103, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 40508.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 2 saved to crossval_T5_TOP5DOCS_SUMMARIZED_fold_2.jsonl\n",
      "Processing Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_14586/3145572364.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='840' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 840/1800 05:11 < 05:57, 2.69 it/s, Epoch 14/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.417400</td>\n",
       "      <td>0.801048</td>\n",
       "      <td>0.095761</td>\n",
       "      <td>0.352721</td>\n",
       "      <td>0.137421</td>\n",
       "      <td>0.348417</td>\n",
       "      <td>0.272684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.764300</td>\n",
       "      <td>0.555041</td>\n",
       "      <td>0.145036</td>\n",
       "      <td>0.417869</td>\n",
       "      <td>0.187483</td>\n",
       "      <td>0.406528</td>\n",
       "      <td>0.368363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.485662</td>\n",
       "      <td>0.166677</td>\n",
       "      <td>0.464391</td>\n",
       "      <td>0.207743</td>\n",
       "      <td>0.452142</td>\n",
       "      <td>0.414841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.498800</td>\n",
       "      <td>0.465997</td>\n",
       "      <td>0.164523</td>\n",
       "      <td>0.467255</td>\n",
       "      <td>0.215336</td>\n",
       "      <td>0.458143</td>\n",
       "      <td>0.424279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.493300</td>\n",
       "      <td>0.454443</td>\n",
       "      <td>0.174174</td>\n",
       "      <td>0.469166</td>\n",
       "      <td>0.220938</td>\n",
       "      <td>0.460548</td>\n",
       "      <td>0.427358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.435700</td>\n",
       "      <td>0.445464</td>\n",
       "      <td>0.178225</td>\n",
       "      <td>0.485080</td>\n",
       "      <td>0.236502</td>\n",
       "      <td>0.474365</td>\n",
       "      <td>0.443416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.464100</td>\n",
       "      <td>0.439881</td>\n",
       "      <td>0.175345</td>\n",
       "      <td>0.484308</td>\n",
       "      <td>0.234134</td>\n",
       "      <td>0.474197</td>\n",
       "      <td>0.438026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.446800</td>\n",
       "      <td>0.438407</td>\n",
       "      <td>0.187259</td>\n",
       "      <td>0.488912</td>\n",
       "      <td>0.242603</td>\n",
       "      <td>0.476630</td>\n",
       "      <td>0.442679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.428900</td>\n",
       "      <td>0.436724</td>\n",
       "      <td>0.186676</td>\n",
       "      <td>0.488738</td>\n",
       "      <td>0.243386</td>\n",
       "      <td>0.479285</td>\n",
       "      <td>0.446805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.414600</td>\n",
       "      <td>0.434739</td>\n",
       "      <td>0.188200</td>\n",
       "      <td>0.485970</td>\n",
       "      <td>0.248061</td>\n",
       "      <td>0.475252</td>\n",
       "      <td>0.448227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.439600</td>\n",
       "      <td>0.432647</td>\n",
       "      <td>0.191484</td>\n",
       "      <td>0.489137</td>\n",
       "      <td>0.249856</td>\n",
       "      <td>0.478485</td>\n",
       "      <td>0.450919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.382400</td>\n",
       "      <td>0.433788</td>\n",
       "      <td>0.179684</td>\n",
       "      <td>0.482556</td>\n",
       "      <td>0.235384</td>\n",
       "      <td>0.472421</td>\n",
       "      <td>0.441930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.414200</td>\n",
       "      <td>0.432732</td>\n",
       "      <td>0.185265</td>\n",
       "      <td>0.491529</td>\n",
       "      <td>0.242147</td>\n",
       "      <td>0.478092</td>\n",
       "      <td>0.446372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.388600</td>\n",
       "      <td>0.433045</td>\n",
       "      <td>0.191464</td>\n",
       "      <td>0.487693</td>\n",
       "      <td>0.249661</td>\n",
       "      <td>0.477232</td>\n",
       "      <td>0.449795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 results: {'eval_loss': 0.4326474964618683, 'eval_bleu': 0.1914836933547287, 'eval_rouge1': 0.48913709117429016, 'eval_rouge2': 0.2498560464342759, 'eval_rougeL': 0.478485452172861, 'eval_meteor': 0.45091889778587657, 'eval_runtime': 2.169, 'eval_samples_per_second': 55.324, 'eval_steps_per_second': 6.916, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 39740.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 3 saved to crossval_T5_TOP5DOCS_SUMMARIZED_fold_3.jsonl\n",
      "Processing Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_14586/3145572364.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1020' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1020/1800 06:17 < 04:49, 2.70 it/s, Epoch 17/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.377000</td>\n",
       "      <td>0.791551</td>\n",
       "      <td>0.090261</td>\n",
       "      <td>0.366213</td>\n",
       "      <td>0.157842</td>\n",
       "      <td>0.356367</td>\n",
       "      <td>0.278005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.704200</td>\n",
       "      <td>0.597976</td>\n",
       "      <td>0.157428</td>\n",
       "      <td>0.418159</td>\n",
       "      <td>0.219276</td>\n",
       "      <td>0.412624</td>\n",
       "      <td>0.369767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.566300</td>\n",
       "      <td>0.506203</td>\n",
       "      <td>0.169720</td>\n",
       "      <td>0.463146</td>\n",
       "      <td>0.223751</td>\n",
       "      <td>0.457998</td>\n",
       "      <td>0.415361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.520500</td>\n",
       "      <td>0.488971</td>\n",
       "      <td>0.176850</td>\n",
       "      <td>0.470499</td>\n",
       "      <td>0.226383</td>\n",
       "      <td>0.463468</td>\n",
       "      <td>0.424061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.500200</td>\n",
       "      <td>0.481674</td>\n",
       "      <td>0.184402</td>\n",
       "      <td>0.477144</td>\n",
       "      <td>0.234679</td>\n",
       "      <td>0.471809</td>\n",
       "      <td>0.427656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.465700</td>\n",
       "      <td>0.474956</td>\n",
       "      <td>0.186629</td>\n",
       "      <td>0.482668</td>\n",
       "      <td>0.238557</td>\n",
       "      <td>0.475631</td>\n",
       "      <td>0.434827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.481300</td>\n",
       "      <td>0.471873</td>\n",
       "      <td>0.190036</td>\n",
       "      <td>0.488868</td>\n",
       "      <td>0.241084</td>\n",
       "      <td>0.482209</td>\n",
       "      <td>0.440292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.456500</td>\n",
       "      <td>0.470310</td>\n",
       "      <td>0.187724</td>\n",
       "      <td>0.484474</td>\n",
       "      <td>0.237553</td>\n",
       "      <td>0.479127</td>\n",
       "      <td>0.436081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.440500</td>\n",
       "      <td>0.469708</td>\n",
       "      <td>0.192194</td>\n",
       "      <td>0.484080</td>\n",
       "      <td>0.240327</td>\n",
       "      <td>0.477736</td>\n",
       "      <td>0.439035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.414500</td>\n",
       "      <td>0.467988</td>\n",
       "      <td>0.191969</td>\n",
       "      <td>0.484945</td>\n",
       "      <td>0.238418</td>\n",
       "      <td>0.477834</td>\n",
       "      <td>0.439417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.395100</td>\n",
       "      <td>0.467141</td>\n",
       "      <td>0.192317</td>\n",
       "      <td>0.485998</td>\n",
       "      <td>0.229999</td>\n",
       "      <td>0.474854</td>\n",
       "      <td>0.436621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.386300</td>\n",
       "      <td>0.467068</td>\n",
       "      <td>0.194684</td>\n",
       "      <td>0.481532</td>\n",
       "      <td>0.237875</td>\n",
       "      <td>0.474804</td>\n",
       "      <td>0.438131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.404600</td>\n",
       "      <td>0.466466</td>\n",
       "      <td>0.188263</td>\n",
       "      <td>0.487101</td>\n",
       "      <td>0.233310</td>\n",
       "      <td>0.476960</td>\n",
       "      <td>0.438797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.385600</td>\n",
       "      <td>0.466139</td>\n",
       "      <td>0.194575</td>\n",
       "      <td>0.482742</td>\n",
       "      <td>0.232682</td>\n",
       "      <td>0.476693</td>\n",
       "      <td>0.437826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.398900</td>\n",
       "      <td>0.466219</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.487013</td>\n",
       "      <td>0.236613</td>\n",
       "      <td>0.478876</td>\n",
       "      <td>0.439632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.400700</td>\n",
       "      <td>0.467383</td>\n",
       "      <td>0.201620</td>\n",
       "      <td>0.487977</td>\n",
       "      <td>0.239830</td>\n",
       "      <td>0.479239</td>\n",
       "      <td>0.438173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.368700</td>\n",
       "      <td>0.467425</td>\n",
       "      <td>0.201982</td>\n",
       "      <td>0.486406</td>\n",
       "      <td>0.237136</td>\n",
       "      <td>0.478852</td>\n",
       "      <td>0.438646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 results: {'eval_loss': 0.4661385715007782, 'eval_bleu': 0.19457517805226662, 'eval_rouge1': 0.482741747158943, 'eval_rouge2': 0.2326822519641224, 'eval_rougeL': 0.4766927359735248, 'eval_meteor': 0.43782645239734896, 'eval_runtime': 2.1092, 'eval_samples_per_second': 56.895, 'eval_steps_per_second': 7.112, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 40143.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 4 saved to crossval_T5_TOP5DOCS_SUMMARIZED_fold_4.jsonl\n",
      "Processing Fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_14586/3145572364.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1020' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1020/1800 06:17 < 04:48, 2.70 it/s, Epoch 17/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.397000</td>\n",
       "      <td>0.794929</td>\n",
       "      <td>0.108309</td>\n",
       "      <td>0.382341</td>\n",
       "      <td>0.161152</td>\n",
       "      <td>0.374477</td>\n",
       "      <td>0.279044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.750700</td>\n",
       "      <td>0.594559</td>\n",
       "      <td>0.149185</td>\n",
       "      <td>0.427434</td>\n",
       "      <td>0.205773</td>\n",
       "      <td>0.421939</td>\n",
       "      <td>0.373557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.619200</td>\n",
       "      <td>0.511499</td>\n",
       "      <td>0.175556</td>\n",
       "      <td>0.485338</td>\n",
       "      <td>0.216709</td>\n",
       "      <td>0.478702</td>\n",
       "      <td>0.424604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.528200</td>\n",
       "      <td>0.491683</td>\n",
       "      <td>0.181541</td>\n",
       "      <td>0.494446</td>\n",
       "      <td>0.229014</td>\n",
       "      <td>0.486793</td>\n",
       "      <td>0.438727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.483400</td>\n",
       "      <td>0.482940</td>\n",
       "      <td>0.194483</td>\n",
       "      <td>0.503939</td>\n",
       "      <td>0.237962</td>\n",
       "      <td>0.495802</td>\n",
       "      <td>0.451597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.453000</td>\n",
       "      <td>0.474159</td>\n",
       "      <td>0.183317</td>\n",
       "      <td>0.500930</td>\n",
       "      <td>0.232308</td>\n",
       "      <td>0.494014</td>\n",
       "      <td>0.447944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.436700</td>\n",
       "      <td>0.468930</td>\n",
       "      <td>0.197238</td>\n",
       "      <td>0.510891</td>\n",
       "      <td>0.241939</td>\n",
       "      <td>0.502802</td>\n",
       "      <td>0.460617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.414400</td>\n",
       "      <td>0.466137</td>\n",
       "      <td>0.202176</td>\n",
       "      <td>0.512554</td>\n",
       "      <td>0.248068</td>\n",
       "      <td>0.502779</td>\n",
       "      <td>0.464847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.411000</td>\n",
       "      <td>0.464585</td>\n",
       "      <td>0.199741</td>\n",
       "      <td>0.516188</td>\n",
       "      <td>0.244048</td>\n",
       "      <td>0.506714</td>\n",
       "      <td>0.466188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.420300</td>\n",
       "      <td>0.463048</td>\n",
       "      <td>0.206423</td>\n",
       "      <td>0.517674</td>\n",
       "      <td>0.246133</td>\n",
       "      <td>0.505992</td>\n",
       "      <td>0.468281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.410700</td>\n",
       "      <td>0.462020</td>\n",
       "      <td>0.204180</td>\n",
       "      <td>0.521864</td>\n",
       "      <td>0.248746</td>\n",
       "      <td>0.510810</td>\n",
       "      <td>0.467678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.402500</td>\n",
       "      <td>0.460841</td>\n",
       "      <td>0.205031</td>\n",
       "      <td>0.527238</td>\n",
       "      <td>0.251864</td>\n",
       "      <td>0.514758</td>\n",
       "      <td>0.470509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.395800</td>\n",
       "      <td>0.460246</td>\n",
       "      <td>0.202611</td>\n",
       "      <td>0.521127</td>\n",
       "      <td>0.252698</td>\n",
       "      <td>0.511057</td>\n",
       "      <td>0.471009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.388500</td>\n",
       "      <td>0.458438</td>\n",
       "      <td>0.205575</td>\n",
       "      <td>0.516602</td>\n",
       "      <td>0.250902</td>\n",
       "      <td>0.508839</td>\n",
       "      <td>0.469133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.367000</td>\n",
       "      <td>0.460046</td>\n",
       "      <td>0.204248</td>\n",
       "      <td>0.523148</td>\n",
       "      <td>0.245730</td>\n",
       "      <td>0.509342</td>\n",
       "      <td>0.469296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.369600</td>\n",
       "      <td>0.460176</td>\n",
       "      <td>0.202020</td>\n",
       "      <td>0.523817</td>\n",
       "      <td>0.248358</td>\n",
       "      <td>0.511206</td>\n",
       "      <td>0.472629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.372400</td>\n",
       "      <td>0.458559</td>\n",
       "      <td>0.200832</td>\n",
       "      <td>0.518609</td>\n",
       "      <td>0.241211</td>\n",
       "      <td>0.505559</td>\n",
       "      <td>0.463901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 results: {'eval_loss': 0.45843765139579773, 'eval_bleu': 0.20557456979612207, 'eval_rouge1': 0.5166019243725455, 'eval_rouge2': 0.25090219860397556, 'eval_rougeL': 0.5088385907134892, 'eval_meteor': 0.4691332159650356, 'eval_runtime': 2.0821, 'eval_samples_per_second': 57.155, 'eval_steps_per_second': 7.204, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:00<00:00, 41414.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 5 saved to crossval_T5_TOP5DOCS_SUMMARIZED_fold_5.jsonl\n",
      "Average metrics over all folds: {'eval_loss': 0.4672218382358551, 'eval_bleu': 0.19918105626307475, 'eval_rouge1': 0.4996960822925914, 'eval_rouge2': 0.251680731174653, 'eval_rougeL': 0.4900307651693466, 'eval_meteor': 0.45689439464484566, 'eval_runtime': 2.12884, 'eval_samples_per_second': 56.2888, 'eval_steps_per_second': 7.048, 'epoch': 15.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Averages/epoch</td><td>▁</td></tr><tr><td>Averages/eval_bleu</td><td>▁</td></tr><tr><td>Averages/eval_loss</td><td>▁</td></tr><tr><td>Averages/eval_meteor</td><td>▁</td></tr><tr><td>Averages/eval_rouge1</td><td>▁</td></tr><tr><td>Averages/eval_rouge2</td><td>▁</td></tr><tr><td>Averages/eval_rougeL</td><td>▁</td></tr><tr><td>Averages/eval_runtime</td><td>▁</td></tr><tr><td>Averages/eval_samples_per_second</td><td>▁</td></tr><tr><td>Averages/eval_steps_per_second</td><td>▁</td></tr><tr><td>Fold_1/eval/epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇██</td></tr><tr><td>Fold_1/eval/eval_bleu</td><td>▁▅▆▇▇▇▇▇▇███▇███</td></tr><tr><td>Fold_1/eval/eval_loss</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_1/eval/eval_meteor</td><td>▁▅▇▇▇▇▇▇████████</td></tr><tr><td>Fold_1/eval/eval_rouge1</td><td>▁▄▇▇▇▇▇▇█▇██████</td></tr><tr><td>Fold_1/eval/eval_rouge2</td><td>▁▅▆▇▇▇▇▇▇██▇▇▇▇▇</td></tr><tr><td>Fold_1/eval/eval_rougeL</td><td>▁▄▇▇▇▇▇▇████████</td></tr><tr><td>Fold_1/eval/eval_runtime</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_1/eval/eval_samples_per_second</td><td>▁█████▇▇█▇██████</td></tr><tr><td>Fold_1/eval/eval_steps_per_second</td><td>▁█████▇▇█▇██████</td></tr><tr><td>Fold_2/eval/epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇███</td></tr><tr><td>Fold_2/eval/eval_bleu</td><td>▁▄▅▅▆▆▆▇▇▇▇▇▇███▇</td></tr><tr><td>Fold_2/eval/eval_loss</td><td>█▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_2/eval/eval_meteor</td><td>▁▅▆▆▆▇▇▇▇▇▇██████</td></tr><tr><td>Fold_2/eval/eval_rouge1</td><td>▁▄▅▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>Fold_2/eval/eval_rouge2</td><td>▁▄▅▅▅▅▆▇▆▇▇▇█████</td></tr><tr><td>Fold_2/eval/eval_rougeL</td><td>▁▄▅▆▆▆▇▇▇▇▇██████</td></tr><tr><td>Fold_2/eval/eval_runtime</td><td>█▇▇▇▁▂▁▁▇▁▂▁▁▁▇▃▂</td></tr><tr><td>Fold_2/eval/eval_samples_per_second</td><td>▁▂▂▂█▇██▂█▇███▂▆▇</td></tr><tr><td>Fold_2/eval/eval_steps_per_second</td><td>▁▂▂▂█▇██▂█▇███▂▆▇</td></tr><tr><td>Fold_3/eval/epoch</td><td>▁▂▂▃▃▄▄▅▅▆▆▇▇██</td></tr><tr><td>Fold_3/eval/eval_bleu</td><td>▁▅▆▆▇▇▇████▇███</td></tr><tr><td>Fold_3/eval/eval_loss</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_3/eval/eval_meteor</td><td>▁▅▇▇▇█▇████████</td></tr><tr><td>Fold_3/eval/eval_rouge1</td><td>▁▄▇▇▇██████████</td></tr><tr><td>Fold_3/eval/eval_rouge2</td><td>▁▄▅▆▆▇▇████▇███</td></tr><tr><td>Fold_3/eval/eval_rougeL</td><td>▁▄▇▇▇██████████</td></tr><tr><td>Fold_3/eval/eval_runtime</td><td>▃▁▁▁█▆▇▅▁▂▁▇▂▂▆</td></tr><tr><td>Fold_3/eval/eval_samples_per_second</td><td>▆███▁▃▂▃█▇█▂▇▇▃</td></tr><tr><td>Fold_3/eval/eval_steps_per_second</td><td>▆███▁▃▂▃█▇█▂▇▇▃</td></tr><tr><td>Fold_4/eval/epoch</td><td>▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇███</td></tr><tr><td>Fold_4/eval/eval_bleu</td><td>▁▅▆▆▇▇▇▇▇▇▇▇▇▇███▇</td></tr><tr><td>Fold_4/eval/eval_loss</td><td>█▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_4/eval/eval_meteor</td><td>▁▅▇▇▇█████████████</td></tr><tr><td>Fold_4/eval/eval_rouge1</td><td>▁▄▇▇▇█████████████</td></tr><tr><td>Fold_4/eval/eval_rouge2</td><td>▁▆▇▇▇█████▇█▇▇███▇</td></tr><tr><td>Fold_4/eval/eval_rougeL</td><td>▁▄▇▇▇█████████████</td></tr><tr><td>Fold_4/eval/eval_runtime</td><td>█▅▄▆▃▄▂▄▁▅▄▇▂▄▁▄▁▇</td></tr><tr><td>Fold_4/eval/eval_samples_per_second</td><td>▁▄▅▃▆▅▇▅█▄▅▂▇▅█▅█▂</td></tr><tr><td>Fold_4/eval/eval_steps_per_second</td><td>▁▄▅▃▆▅▇▅█▄▅▂▇▅█▅█▂</td></tr><tr><td>Fold_5/eval/epoch</td><td>▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇███</td></tr><tr><td>Fold_5/eval/eval_bleu</td><td>▁▄▆▆▇▆▇███████████</td></tr><tr><td>Fold_5/eval/eval_loss</td><td>█▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_5/eval/eval_meteor</td><td>▁▄▆▇▇▇████████████</td></tr><tr><td>Fold_5/eval/eval_rouge1</td><td>▁▃▆▆▇▇▇▇▇████▇███▇</td></tr><tr><td>Fold_5/eval/eval_rouge2</td><td>▁▄▅▆▇▆▇█▇▇████▇█▇█</td></tr><tr><td>Fold_5/eval/eval_rougeL</td><td>▁▃▆▇▇▇▇▇██████████</td></tr><tr><td>Fold_5/eval/eval_runtime</td><td>█▅▅▅▃▇▁▆▃▅▂▅▃▄▃▄▃▄</td></tr><tr><td>Fold_5/eval/eval_samples_per_second</td><td>▁▄▄▄▆▂█▃▆▄▇▄▆▅▆▅▆▅</td></tr><tr><td>Fold_5/eval/eval_steps_per_second</td><td>▁▄▄▄▆▂█▃▆▄▇▄▆▅▆▅▆▅</td></tr><tr><td>eval/bleu</td><td>▁▆▆▆▆▇▇▇▆▇▇▇▇███▁▅▆▇▇▇▅▆▆▆▇▆▇▇▂▄▆▆▇▇▇▇▇█</td></tr><tr><td>eval/loss</td><td>█▃▂▂▂█▄▃▂▂▂▂▂▇▂▁▁▁▁▁▇▂▂▂▂▂▂▂▂▂▄▂▂▂▂▂▂▁▁▁</td></tr><tr><td>eval/meteor</td><td>▂▄▆▆▆▆▆▆▆▂▆▇██████▆▆▆▇▇▇▆▄▆▆▆▆▆▆▆▁▄▆▇▇▇▇</td></tr><tr><td>eval/rouge1</td><td>▂▄▆▆▆▆▆▆▆▄▆▆▇▇▇███▃▆▆▆▆▁▃▆▆▆▆▆▆▆▆▄▇▇████</td></tr><tr><td>eval/rouge2</td><td>▄▅▅▅▅▅▅▅▅▅▆▆▆▇▇█▁▅▅▆▆▆▂▅▅▆▆▆▆▅▂▅▅▆▅▆▆▆▆▆</td></tr><tr><td>eval/rougeL</td><td>▁▃▅▅▅▆▆▆▁▄▇▇▇███▅▆▆▆▆▆▅▆▆▆▆▆▆▆▃▆▇▇██████</td></tr><tr><td>eval/runtime</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/samples_per_second</td><td>▁███▇█▇▇▇█████████▇▇████████████████████</td></tr><tr><td>eval/steps_per_second</td><td>▄▅▅▅▂▃▂▅▆▅▁▁▆▁▆▁▅▆██▇▃▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇</td></tr><tr><td>test/bleu</td><td>▁█▁▂▆</td></tr><tr><td>test/loss</td><td>█▆▁▅▄</td></tr><tr><td>test/meteor</td><td>▁█▃▁▅</td></tr><tr><td>test/rouge1</td><td>▂█▂▁▇</td></tr><tr><td>test/rouge2</td><td>▁█▃▁▃</td></tr><tr><td>test/rougeL</td><td>▁█▂▁▇</td></tr><tr><td>test/runtime</td><td>█▂█▂▁</td></tr><tr><td>test/samples_per_second</td><td>▁█▁██</td></tr><tr><td>test/steps_per_second</td><td>▁▇▁▇█</td></tr><tr><td>train/epoch</td><td>▂▃▄▄▄▇▂▂▂▃▄▇███▁▁▂▂▂▄▄▄▄▅▇▂▅▆▆▆▆▇▇█▂▂▃▃█</td></tr><tr><td>train/global_step</td><td>▃▃▃▃▄▅▅▅▅▆▁▂▂▂▂▃▄▆█▂▃▃▄▅▅▆▁▂▂▂▃▄▅▅▇██▁▄▇</td></tr><tr><td>train/grad_norm</td><td>▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▇▇▆▄▃▃▂▂▇▇▆▅▄▃▃█▇▆▆▆▅▂███▆▆▅▄▃▃▂▂▁▁▆▄▃▃▂</td></tr><tr><td>train/loss</td><td>█▁▁▁▁▁▁█▂▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Averages/epoch</td><td>15.8</td></tr><tr><td>Averages/eval_bleu</td><td>0.19918</td></tr><tr><td>Averages/eval_loss</td><td>0.46722</td></tr><tr><td>Averages/eval_meteor</td><td>0.45689</td></tr><tr><td>Averages/eval_rouge1</td><td>0.4997</td></tr><tr><td>Averages/eval_rouge2</td><td>0.25168</td></tr><tr><td>Averages/eval_rougeL</td><td>0.49003</td></tr><tr><td>Averages/eval_runtime</td><td>2.12884</td></tr><tr><td>Averages/eval_samples_per_second</td><td>56.2888</td></tr><tr><td>Averages/eval_steps_per_second</td><td>7.048</td></tr><tr><td>Fold_1/eval/epoch</td><td>15</td></tr><tr><td>Fold_1/eval/eval_bleu</td><td>0.19075</td></tr><tr><td>Fold_1/eval/eval_loss</td><td>0.49863</td></tr><tr><td>Fold_1/eval/eval_meteor</td><td>0.43839</td></tr><tr><td>Fold_1/eval/eval_rouge1</td><td>0.48904</td></tr><tr><td>Fold_1/eval/eval_rouge2</td><td>0.23124</td></tr><tr><td>Fold_1/eval/eval_rougeL</td><td>0.47446</td></tr><tr><td>Fold_1/eval/eval_runtime</td><td>2.1723</td></tr><tr><td>Fold_1/eval/eval_samples_per_second</td><td>55.242</td></tr><tr><td>Fold_1/eval/eval_steps_per_second</td><td>6.905</td></tr><tr><td>Fold_2/eval/epoch</td><td>16</td></tr><tr><td>Fold_2/eval/eval_bleu</td><td>0.21353</td></tr><tr><td>Fold_2/eval/eval_loss</td><td>0.48025</td></tr><tr><td>Fold_2/eval/eval_meteor</td><td>0.4882</td></tr><tr><td>Fold_2/eval/eval_rouge1</td><td>0.52096</td></tr><tr><td>Fold_2/eval/eval_rouge2</td><td>0.29372</td></tr><tr><td>Fold_2/eval/eval_rougeL</td><td>0.51168</td></tr><tr><td>Fold_2/eval/eval_runtime</td><td>2.1116</td></tr><tr><td>Fold_2/eval/eval_samples_per_second</td><td>56.828</td></tr><tr><td>Fold_2/eval/eval_steps_per_second</td><td>7.103</td></tr><tr><td>Fold_3/eval/epoch</td><td>14</td></tr><tr><td>Fold_3/eval/eval_bleu</td><td>0.19148</td></tr><tr><td>Fold_3/eval/eval_loss</td><td>0.43265</td></tr><tr><td>Fold_3/eval/eval_meteor</td><td>0.45092</td></tr><tr><td>Fold_3/eval/eval_rouge1</td><td>0.48914</td></tr><tr><td>Fold_3/eval/eval_rouge2</td><td>0.24986</td></tr><tr><td>Fold_3/eval/eval_rougeL</td><td>0.47849</td></tr><tr><td>Fold_3/eval/eval_runtime</td><td>2.169</td></tr><tr><td>Fold_3/eval/eval_samples_per_second</td><td>55.324</td></tr><tr><td>Fold_3/eval/eval_steps_per_second</td><td>6.916</td></tr><tr><td>Fold_4/eval/epoch</td><td>17</td></tr><tr><td>Fold_4/eval/eval_bleu</td><td>0.19458</td></tr><tr><td>Fold_4/eval/eval_loss</td><td>0.46614</td></tr><tr><td>Fold_4/eval/eval_meteor</td><td>0.43783</td></tr><tr><td>Fold_4/eval/eval_rouge1</td><td>0.48274</td></tr><tr><td>Fold_4/eval/eval_rouge2</td><td>0.23268</td></tr><tr><td>Fold_4/eval/eval_rougeL</td><td>0.47669</td></tr><tr><td>Fold_4/eval/eval_runtime</td><td>2.1092</td></tr><tr><td>Fold_4/eval/eval_samples_per_second</td><td>56.895</td></tr><tr><td>Fold_4/eval/eval_steps_per_second</td><td>7.112</td></tr><tr><td>Fold_5/eval/epoch</td><td>17</td></tr><tr><td>Fold_5/eval/eval_bleu</td><td>0.20557</td></tr><tr><td>Fold_5/eval/eval_loss</td><td>0.45844</td></tr><tr><td>Fold_5/eval/eval_meteor</td><td>0.46913</td></tr><tr><td>Fold_5/eval/eval_rouge1</td><td>0.5166</td></tr><tr><td>Fold_5/eval/eval_rouge2</td><td>0.2509</td></tr><tr><td>Fold_5/eval/eval_rougeL</td><td>0.50884</td></tr><tr><td>Fold_5/eval/eval_runtime</td><td>2.0821</td></tr><tr><td>Fold_5/eval/eval_samples_per_second</td><td>57.155</td></tr><tr><td>Fold_5/eval/eval_steps_per_second</td><td>7.204</td></tr><tr><td>eval/bleu</td><td>0.20557</td></tr><tr><td>eval/loss</td><td>0.45844</td></tr><tr><td>eval/meteor</td><td>0.46913</td></tr><tr><td>eval/rouge1</td><td>0.5166</td></tr><tr><td>eval/rouge2</td><td>0.2509</td></tr><tr><td>eval/rougeL</td><td>0.50884</td></tr><tr><td>eval/runtime</td><td>2.0821</td></tr><tr><td>eval/samples_per_second</td><td>57.155</td></tr><tr><td>eval/steps_per_second</td><td>7.204</td></tr><tr><td>test/bleu</td><td>0.20557</td></tr><tr><td>test/loss</td><td>0.45844</td></tr><tr><td>test/meteor</td><td>0.46913</td></tr><tr><td>test/rouge1</td><td>0.5166</td></tr><tr><td>test/rouge2</td><td>0.2509</td></tr><tr><td>test/rougeL</td><td>0.50884</td></tr><tr><td>test/runtime</td><td>2.0791</td></tr><tr><td>test/samples_per_second</td><td>57.237</td></tr><tr><td>test/steps_per_second</td><td>7.215</td></tr><tr><td>total_flos</td><td>1104389100011520.0</td></tr><tr><td>train/epoch</td><td>17</td></tr><tr><td>train/global_step</td><td>1020</td></tr><tr><td>train/grad_norm</td><td>0.77074</td></tr><tr><td>train/learning_rate</td><td>2e-05</td></tr><tr><td>train/loss</td><td>0.3724</td></tr><tr><td>train_loss</td><td>0.77424</td></tr><tr><td>train_runtime</td><td>377.2822</td></tr><tr><td>train_samples_per_second</td><td>38.168</td></tr><tr><td>train_steps_per_second</td><td>4.771</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">T5_SMALL_TOP5DOCS_SUMMARIZED</strong> at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/yqrgb5pz' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/yqrgb5pz</a><br> View project at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250510_224706-yqrgb5pz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final predicitons for all folds foudn in crossval_T5_TOP5DOCS_SUMMARIZED.jsonl\n"
     ]
    }
   ],
   "source": [
    "cross_val_train(modelname=\"t5-small\", run_name=\"T5_SMALL_TOP5DOCS_SUMMARIZED\",filename=\"crossval_T5_TOP5DOCS_SUMMARIZED.jsonl\", \n",
    "                tokenizer=tokenizer, tokenized_dataset=tokenized_dataset, \n",
    "                folds=folds, nb_epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4374c301-3125-45ad-b576-8499cd8ff6b0",
   "metadata": {},
   "source": [
    "# HanDLE TFIDF WITH CROSS VAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1448a7d7-2820-4102-a141-85c0128e1a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "#fucntion to tokenize the dataset based on the model\n",
    "def tokenize_and_split_dataset(filename, modelname):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_data = json.load(f)\n",
    "        \n",
    "    # Choose your model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(modelname)\n",
    "\n",
    "    # Tokenization function\n",
    "    def preprocess(batch):\n",
    "        model_inputs = tokenizer(\n",
    "            batch[\"input\"],\n",
    "            max_length=512,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(\n",
    "                batch[\"output\"],\n",
    "                max_length=64,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True\n",
    "            )\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "    \n",
    "    # Convert raw data to Dataset object\n",
    "    dataset = Dataset.from_list(raw_data)\n",
    "    tokenized_dataset = dataset.map(preprocess, batched=True)\n",
    "\n",
    "    return tokenizer, tokenized_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "977de5fe-62bc-47cb-821e-9976ac9293f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "def compute_metrics(eval_pred, tokenizer):\n",
    "    \"\"\"\n",
    "    This function calculates BLEU, ROUGE, and METEOR for the model predictions.\n",
    "    eval_pred: tuple (predictions, references), where predictions are model outputs\n",
    "    and references are the true target labels.\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions= predictions[0]\n",
    "    pred_ids = np.argmax(predictions, axis=-1) \n",
    "    # Decode the model's predicted tokens and true labels\n",
    "    predicted_texts = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    true_texts = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Calculate BLEU score\n",
    "    bleu_score = bleu.compute(predictions=predicted_texts, references=true_texts)\n",
    "    rouge_score = rouge.compute(predictions=predicted_texts, references=true_texts)\n",
    "    meteor_score = meteor.compute(predictions=predicted_texts, references=true_texts)\n",
    "   \n",
    "    # Combine the metrics into a dictionary\n",
    "    return {\n",
    "        \"bleu\": bleu_score[\"bleu\"],\n",
    "        \"rouge1\": rouge_score[\"rouge1\"],\n",
    "        \"rouge2\": rouge_score[\"rouge2\"],\n",
    "        \"rougeL\": rouge_score[\"rougeL\"],\n",
    "        \"meteor\": meteor_score[\"meteor\"]\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d2befff-370a-484c-8f34-93feefd25171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#fucntion to split dataset into folds\n",
    "def generate_folds(tokenized_dataset, n_splits=5, seed=42):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    folds = list(kf.split(tokenized_dataset))\n",
    "    return folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f46c519a-b793-4b88-a1ac-92b008bfce13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3542719d5a674a29a1500698d5b9ffe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/Etu0/21402600/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# recuperer les folds pour les indices de chaque fichier tfidf\n",
    "tokenizer_global,tokenized_dataset_global= tokenize_and_split_dataset(\"training_queryonly_qulac_PREPROCESSED_FOR_MODEL.json\",\"t5-small\")\n",
    "folds= generate_folds(tokenized_dataset_global, n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1965ad1b-658c-4871-8727-2bed1fa1c8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([  0,   1,   3,   4,   5,   7,   8,   9,  12,  13,  14,  15,  16,\n",
       "          17,  18,  19,  20,  21,  22,  23,  25,  26,  27,  28,  31,  32,\n",
       "          33,  34,  35,  36,  37,  38,  40,  41,  42,  43,  44,  45,  46,\n",
       "          47,  48,  49,  50,  51,  52,  53,  54,  56,  57,  58,  59,  61,\n",
       "          62,  64,  65,  66,  67,  68,  69,  71,  74,  75,  80,  84,  85,\n",
       "          87,  88,  89,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100,\n",
       "         102, 103, 105, 106, 107, 108, 111, 112, 113, 114, 115, 116, 117,\n",
       "         119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131,\n",
       "         133, 134, 136, 138, 139, 141, 142, 143, 144, 145, 146, 147, 149,\n",
       "         150, 151, 152, 153, 154, 156, 157, 158, 159, 160, 161, 162, 164,\n",
       "         166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 177, 178, 179,\n",
       "         180, 183, 185, 186, 187, 188, 189, 190, 191, 193, 194, 196, 197,\n",
       "         198, 200, 201, 202, 203, 204, 205, 206, 207, 211, 213, 214, 215,\n",
       "         216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 229, 230,\n",
       "         231, 232, 233, 236, 237, 239, 240, 241, 242, 243, 244, 246, 247,\n",
       "         249, 250, 251, 252, 253, 254, 255, 256, 258, 260, 261, 262, 263,\n",
       "         264, 265, 266, 267, 269, 270, 271, 272, 273, 274, 276, 277, 278,\n",
       "         279, 282, 283, 285, 287, 288, 289, 290, 291, 292, 293, 294, 295,\n",
       "         296, 297, 298, 299, 301, 302, 303, 305, 306, 307, 308, 309, 310,\n",
       "         311, 313, 314, 315, 317, 318, 319, 320, 321, 322, 323, 324, 325,\n",
       "         326, 327, 328, 330, 331, 332, 333, 334, 336, 337, 338, 339, 341,\n",
       "         342, 343, 344, 345, 346, 347, 348, 350, 351, 352, 353, 354, 355,\n",
       "         357, 358, 359, 360, 362, 363, 365, 366, 367, 372, 373, 375, 376,\n",
       "         377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n",
       "         390, 391, 392, 393, 394, 395, 396, 397, 398, 400, 401, 402, 403,\n",
       "         404, 405, 406, 407, 408, 409, 410, 411, 413, 415, 416, 417, 418,\n",
       "         420, 421, 422, 424, 425, 427, 428, 430, 431, 433, 435, 436, 438,\n",
       "         440, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 454,\n",
       "         455, 456, 457, 458, 459, 460, 461, 462, 463, 465, 466, 467, 468,\n",
       "         470, 471, 472, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483,\n",
       "         484, 485, 486, 487, 488, 489, 491, 492, 493, 495, 496, 497, 498,\n",
       "         499, 500, 501, 502, 503, 504, 505, 507, 508, 509, 510, 511, 512,\n",
       "         513, 514, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526,\n",
       "         527, 528, 529, 530, 531, 532, 533, 539, 540, 541, 542, 543, 544,\n",
       "         546, 548, 549, 550, 551, 552, 553, 556, 557, 559, 560, 561, 562,\n",
       "         563, 565, 566, 567, 568, 570, 572, 573, 575, 576, 577, 578, 579,\n",
       "         580, 581, 582, 584, 585, 589, 591, 592, 593, 596, 598]),\n",
       "  array([  2,   6,  10,  11,  24,  29,  30,  39,  55,  60,  63,  70,  72,\n",
       "          73,  76,  77,  78,  79,  81,  82,  83,  86,  90, 101, 104, 109,\n",
       "         110, 118, 132, 135, 137, 140, 148, 155, 163, 165, 176, 181, 182,\n",
       "         184, 192, 195, 199, 208, 209, 210, 212, 218, 228, 234, 235, 238,\n",
       "         245, 248, 257, 259, 268, 275, 280, 281, 284, 286, 300, 304, 312,\n",
       "         316, 329, 335, 340, 349, 356, 361, 364, 368, 369, 370, 371, 374,\n",
       "         399, 412, 414, 419, 423, 426, 429, 432, 434, 437, 439, 441, 453,\n",
       "         464, 469, 473, 490, 494, 506, 515, 534, 535, 536, 537, 538, 545,\n",
       "         547, 554, 555, 558, 564, 569, 571, 574, 583, 586, 587, 588, 590,\n",
       "         594, 595, 597])),\n",
       " (array([  1,   2,   3,   4,   5,   6,   8,  10,  11,  12,  13,  14,  16,\n",
       "          17,  20,  21,  23,  24,  26,  27,  28,  29,  30,  31,  32,  34,\n",
       "          35,  36,  37,  38,  39,  40,  41,  43,  44,  45,  47,  48,  50,\n",
       "          51,  52,  53,  55,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
       "          66,  67,  70,  71,  72,  73,  74,  76,  77,  78,  79,  80,  81,\n",
       "          82,  83,  85,  86,  87,  90,  91,  94,  95,  96,  97,  98,  99,\n",
       "         100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 113,\n",
       "         115, 118, 119, 120, 121, 122, 123, 125, 126, 127, 128, 129, 130,\n",
       "         132, 133, 134, 135, 136, 137, 138, 139, 140, 142, 143, 146, 147,\n",
       "         148, 150, 151, 152, 154, 155, 156, 157, 159, 160, 161, 162, 163,\n",
       "         164, 165, 166, 168, 169, 170, 171, 173, 174, 175, 176, 178, 179,\n",
       "         180, 181, 182, 183, 184, 186, 187, 189, 190, 191, 192, 193, 194,\n",
       "         195, 197, 198, 199, 200, 201, 202, 205, 206, 207, 208, 209, 210,\n",
       "         212, 213, 214, 215, 216, 217, 218, 219, 221, 222, 223, 224, 225,\n",
       "         226, 228, 229, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240,\n",
       "         241, 242, 243, 245, 246, 248, 251, 252, 253, 254, 255, 256, 257,\n",
       "         258, 259, 260, 262, 263, 266, 267, 268, 269, 270, 271, 273, 275,\n",
       "         276, 279, 280, 281, 282, 283, 284, 285, 286, 288, 292, 293, 294,\n",
       "         295, 297, 299, 300, 302, 303, 304, 305, 306, 307, 308, 309, 311,\n",
       "         312, 313, 314, 315, 316, 317, 318, 319, 321, 323, 324, 325, 326,\n",
       "         327, 328, 329, 330, 331, 333, 334, 335, 337, 339, 340, 341, 342,\n",
       "         343, 345, 346, 347, 348, 349, 351, 352, 354, 355, 356, 357, 358,\n",
       "         359, 361, 362, 363, 364, 366, 367, 368, 369, 370, 371, 372, 373,\n",
       "         374, 376, 377, 378, 379, 381, 384, 385, 386, 387, 388, 389, 390,\n",
       "         391, 392, 393, 395, 396, 397, 398, 399, 401, 402, 404, 406, 409,\n",
       "         410, 411, 412, 413, 414, 415, 417, 418, 419, 420, 421, 422, 423,\n",
       "         424, 425, 426, 427, 429, 430, 431, 432, 433, 434, 435, 437, 438,\n",
       "         439, 441, 444, 445, 447, 448, 449, 453, 454, 455, 456, 457, 458,\n",
       "         459, 460, 461, 463, 464, 466, 467, 468, 469, 470, 471, 473, 474,\n",
       "         475, 476, 477, 478, 481, 482, 484, 485, 486, 487, 488, 489, 490,\n",
       "         491, 492, 494, 495, 496, 498, 499, 500, 501, 502, 503, 504, 505,\n",
       "         506, 507, 508, 510, 511, 512, 513, 514, 515, 516, 518, 519, 521,\n",
       "         523, 524, 525, 526, 528, 529, 531, 532, 534, 535, 536, 537, 538,\n",
       "         541, 542, 543, 544, 545, 546, 547, 548, 552, 553, 554, 555, 556,\n",
       "         557, 558, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570,\n",
       "         571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583,\n",
       "         584, 586, 587, 588, 589, 590, 593, 594, 595, 596, 597]),\n",
       "  array([  0,   7,   9,  15,  18,  19,  22,  25,  33,  42,  46,  49,  54,\n",
       "          56,  68,  69,  75,  84,  88,  89,  92,  93, 108, 114, 116, 117,\n",
       "         124, 131, 141, 144, 145, 149, 153, 158, 167, 172, 177, 185, 188,\n",
       "         196, 203, 204, 211, 220, 227, 231, 244, 247, 249, 250, 261, 264,\n",
       "         265, 272, 274, 277, 278, 287, 289, 290, 291, 296, 298, 301, 310,\n",
       "         320, 322, 332, 336, 338, 344, 350, 353, 360, 365, 375, 380, 382,\n",
       "         383, 394, 400, 403, 405, 407, 408, 416, 428, 436, 440, 442, 443,\n",
       "         446, 450, 451, 452, 462, 465, 472, 479, 480, 483, 493, 497, 509,\n",
       "         517, 520, 522, 527, 530, 533, 539, 540, 549, 550, 551, 559, 585,\n",
       "         591, 592, 598])),\n",
       " (array([  0,   1,   2,   4,   6,   7,   9,  10,  11,  12,  13,  14,  15,\n",
       "          18,  19,  20,  21,  22,  24,  25,  27,  28,  29,  30,  32,  33,\n",
       "          34,  35,  39,  40,  41,  42,  43,  44,  46,  47,  49,  51,  52,\n",
       "          53,  54,  55,  56,  58,  60,  61,  62,  63,  64,  65,  68,  69,\n",
       "          70,  71,  72,  73,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
       "          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  95,  96,  97,\n",
       "          98,  99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 110, 112,\n",
       "         114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 127, 128,\n",
       "         129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 140, 141, 142,\n",
       "         143, 144, 145, 148, 149, 151, 153, 155, 156, 158, 159, 160, 161,\n",
       "         163, 164, 165, 166, 167, 169, 170, 171, 172, 176, 177, 178, 179,\n",
       "         181, 182, 183, 184, 185, 186, 187, 188, 189, 191, 192, 195, 196,\n",
       "         197, 199, 200, 201, 202, 203, 204, 205, 206, 208, 209, 210, 211,\n",
       "         212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 223, 224, 226,\n",
       "         227, 228, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241,\n",
       "         242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 254, 256,\n",
       "         257, 258, 259, 260, 261, 264, 265, 267, 268, 269, 270, 272, 273,\n",
       "         274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286,\n",
       "         287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 298, 300, 301,\n",
       "         304, 306, 308, 310, 312, 313, 314, 315, 316, 317, 320, 322, 323,\n",
       "         324, 326, 327, 329, 330, 332, 335, 336, 337, 338, 339, 340, 342,\n",
       "         343, 344, 345, 347, 349, 350, 351, 353, 356, 358, 359, 360, 361,\n",
       "         364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 378,\n",
       "         379, 380, 382, 383, 384, 385, 386, 387, 388, 389, 391, 392, 393,\n",
       "         394, 395, 397, 399, 400, 401, 402, 403, 405, 406, 407, 408, 410,\n",
       "         412, 413, 414, 415, 416, 417, 418, 419, 421, 423, 425, 426, 427,\n",
       "         428, 429, 430, 432, 434, 435, 436, 437, 439, 440, 441, 442, 443,\n",
       "         444, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457,\n",
       "         458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 469, 471, 472,\n",
       "         473, 474, 475, 476, 477, 478, 479, 480, 482, 483, 484, 488, 489,\n",
       "         490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 502, 504,\n",
       "         505, 506, 507, 508, 509, 510, 512, 515, 516, 517, 520, 521, 522,\n",
       "         523, 525, 527, 528, 530, 532, 533, 534, 535, 536, 537, 538, 539,\n",
       "         540, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553,\n",
       "         554, 555, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567,\n",
       "         569, 570, 571, 574, 579, 580, 581, 582, 583, 584, 585, 586, 587,\n",
       "         588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598]),\n",
       "  array([  3,   5,   8,  16,  17,  23,  26,  31,  36,  37,  38,  45,  48,\n",
       "          50,  57,  59,  66,  67,  74,  94, 103, 111, 113, 119, 126, 139,\n",
       "         146, 147, 150, 152, 154, 157, 162, 168, 173, 174, 175, 180, 190,\n",
       "         193, 194, 198, 207, 222, 225, 229, 237, 253, 255, 262, 263, 266,\n",
       "         271, 297, 299, 302, 303, 305, 307, 309, 311, 318, 319, 321, 325,\n",
       "         328, 331, 333, 334, 341, 346, 348, 352, 354, 355, 357, 362, 363,\n",
       "         367, 377, 381, 390, 396, 398, 404, 409, 411, 420, 422, 424, 431,\n",
       "         433, 438, 445, 468, 470, 481, 485, 486, 487, 501, 503, 511, 513,\n",
       "         514, 518, 519, 524, 526, 529, 531, 541, 556, 568, 572, 573, 575,\n",
       "         576, 577, 578])),\n",
       " (array([  0,   1,   2,   3,   5,   6,   7,   8,   9,  10,  11,  13,  14,\n",
       "          15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
       "          29,  30,  31,  32,  33,  34,  36,  37,  38,  39,  40,  42,  45,\n",
       "          46,  47,  48,  49,  50,  52,  54,  55,  56,  57,  58,  59,  60,\n",
       "          61,  62,  63,  64,  66,  67,  68,  69,  70,  71,  72,  73,  74,\n",
       "          75,  76,  77,  78,  79,  80,  81,  82,  83,  84,  86,  87,  88,\n",
       "          89,  90,  91,  92,  93,  94,  98,  99, 100, 101, 102, 103, 104,\n",
       "         105, 106, 108, 109, 110, 111, 113, 114, 116, 117, 118, 119, 121,\n",
       "         124, 126, 128, 130, 131, 132, 134, 135, 137, 138, 139, 140, 141,\n",
       "         144, 145, 146, 147, 148, 149, 150, 152, 153, 154, 155, 156, 157,\n",
       "         158, 160, 161, 162, 163, 165, 166, 167, 168, 171, 172, 173, 174,\n",
       "         175, 176, 177, 180, 181, 182, 184, 185, 187, 188, 189, 190, 191,\n",
       "         192, 193, 194, 195, 196, 198, 199, 200, 201, 203, 204, 205, 207,\n",
       "         208, 209, 210, 211, 212, 213, 214, 215, 216, 218, 220, 222, 225,\n",
       "         226, 227, 228, 229, 230, 231, 234, 235, 237, 238, 241, 243, 244,\n",
       "         245, 247, 248, 249, 250, 251, 252, 253, 255, 256, 257, 259, 260,\n",
       "         261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273,\n",
       "         274, 275, 276, 277, 278, 280, 281, 284, 286, 287, 288, 289, 290,\n",
       "         291, 292, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305,\n",
       "         307, 308, 309, 310, 311, 312, 313, 315, 316, 318, 319, 320, 321,\n",
       "         322, 325, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
       "         338, 339, 340, 341, 343, 344, 345, 346, 348, 349, 350, 352, 353,\n",
       "         354, 355, 356, 357, 359, 360, 361, 362, 363, 364, 365, 366, 367,\n",
       "         368, 369, 370, 371, 372, 374, 375, 377, 378, 379, 380, 381, 382,\n",
       "         383, 385, 387, 389, 390, 391, 394, 396, 398, 399, 400, 401, 403,\n",
       "         404, 405, 406, 407, 408, 409, 411, 412, 413, 414, 416, 417, 418,\n",
       "         419, 420, 422, 423, 424, 426, 427, 428, 429, 430, 431, 432, 433,\n",
       "         434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 445, 446, 448,\n",
       "         450, 451, 452, 453, 454, 455, 458, 459, 461, 462, 464, 465, 466,\n",
       "         468, 469, 470, 471, 472, 473, 474, 475, 476, 479, 480, 481, 483,\n",
       "         484, 485, 486, 487, 489, 490, 491, 492, 493, 494, 497, 498, 501,\n",
       "         503, 504, 505, 506, 508, 509, 510, 511, 513, 514, 515, 517, 518,\n",
       "         519, 520, 522, 523, 524, 526, 527, 529, 530, 531, 532, 533, 534,\n",
       "         535, 536, 537, 538, 539, 540, 541, 545, 546, 547, 549, 550, 551,\n",
       "         554, 555, 556, 557, 558, 559, 560, 562, 564, 565, 568, 569, 570,\n",
       "         571, 572, 573, 574, 575, 576, 577, 578, 582, 583, 584, 585, 586,\n",
       "         587, 588, 590, 591, 592, 593, 594, 595, 596, 597, 598]),\n",
       "  array([  4,  12,  28,  35,  41,  43,  44,  51,  53,  65,  85,  95,  96,\n",
       "          97, 107, 112, 115, 120, 122, 123, 125, 127, 129, 133, 136, 142,\n",
       "         143, 151, 159, 164, 169, 170, 178, 179, 183, 186, 197, 202, 206,\n",
       "         217, 219, 221, 223, 224, 232, 233, 236, 239, 240, 242, 246, 254,\n",
       "         258, 279, 282, 283, 285, 293, 294, 306, 314, 317, 323, 324, 326,\n",
       "         342, 347, 351, 358, 373, 376, 384, 386, 388, 392, 393, 395, 397,\n",
       "         402, 410, 415, 421, 425, 444, 447, 449, 456, 457, 460, 463, 467,\n",
       "         477, 478, 482, 488, 495, 496, 499, 500, 502, 507, 512, 516, 521,\n",
       "         525, 528, 542, 543, 544, 548, 552, 553, 561, 563, 566, 567, 579,\n",
       "         580, 581, 589])),\n",
       " (array([  0,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  15,\n",
       "          16,  17,  18,  19,  22,  23,  24,  25,  26,  28,  29,  30,  31,\n",
       "          33,  35,  36,  37,  38,  39,  41,  42,  43,  44,  45,  46,  48,\n",
       "          49,  50,  51,  53,  54,  55,  56,  57,  59,  60,  63,  65,  66,\n",
       "          67,  68,  69,  70,  72,  73,  74,  75,  76,  77,  78,  79,  81,\n",
       "          82,  83,  84,  85,  86,  88,  89,  90,  92,  93,  94,  95,  96,\n",
       "          97, 101, 103, 104, 107, 108, 109, 110, 111, 112, 113, 114, 115,\n",
       "         116, 117, 118, 119, 120, 122, 123, 124, 125, 126, 127, 129, 131,\n",
       "         132, 133, 135, 136, 137, 139, 140, 141, 142, 143, 144, 145, 146,\n",
       "         147, 148, 149, 150, 151, 152, 153, 154, 155, 157, 158, 159, 162,\n",
       "         163, 164, 165, 167, 168, 169, 170, 172, 173, 174, 175, 176, 177,\n",
       "         178, 179, 180, 181, 182, 183, 184, 185, 186, 188, 190, 192, 193,\n",
       "         194, 195, 196, 197, 198, 199, 202, 203, 204, 206, 207, 208, 209,\n",
       "         210, 211, 212, 217, 218, 219, 220, 221, 222, 223, 224, 225, 227,\n",
       "         228, 229, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 242,\n",
       "         244, 245, 246, 247, 248, 249, 250, 253, 254, 255, 257, 258, 259,\n",
       "         261, 262, 263, 264, 265, 266, 268, 271, 272, 274, 275, 277, 278,\n",
       "         279, 280, 281, 282, 283, 284, 285, 286, 287, 289, 290, 291, 293,\n",
       "         294, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
       "         309, 310, 311, 312, 314, 316, 317, 318, 319, 320, 321, 322, 323,\n",
       "         324, 325, 326, 328, 329, 331, 332, 333, 334, 335, 336, 338, 340,\n",
       "         341, 342, 344, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355,\n",
       "         356, 357, 358, 360, 361, 362, 363, 364, 365, 367, 368, 369, 370,\n",
       "         371, 373, 374, 375, 376, 377, 380, 381, 382, 383, 384, 386, 388,\n",
       "         390, 392, 393, 394, 395, 396, 397, 398, 399, 400, 402, 403, 404,\n",
       "         405, 407, 408, 409, 410, 411, 412, 414, 415, 416, 419, 420, 421,\n",
       "         422, 423, 424, 425, 426, 428, 429, 431, 432, 433, 434, 436, 437,\n",
       "         438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 449, 450, 451,\n",
       "         452, 453, 456, 457, 460, 462, 463, 464, 465, 467, 468, 469, 470,\n",
       "         472, 473, 477, 478, 479, 480, 481, 482, 483, 485, 486, 487, 488,\n",
       "         490, 493, 494, 495, 496, 497, 499, 500, 501, 502, 503, 506, 507,\n",
       "         509, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522,\n",
       "         524, 525, 526, 527, 528, 529, 530, 531, 533, 534, 535, 536, 537,\n",
       "         538, 539, 540, 541, 542, 543, 544, 545, 547, 548, 549, 550, 551,\n",
       "         552, 553, 554, 555, 556, 558, 559, 561, 563, 564, 566, 567, 568,\n",
       "         569, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 583,\n",
       "         585, 586, 587, 588, 589, 590, 591, 592, 594, 595, 597, 598]),\n",
       "  array([  1,  13,  14,  20,  21,  27,  32,  34,  40,  47,  52,  58,  61,\n",
       "          62,  64,  71,  80,  87,  91,  98,  99, 100, 102, 105, 106, 121,\n",
       "         128, 130, 134, 138, 156, 160, 161, 166, 171, 187, 189, 191, 200,\n",
       "         201, 205, 213, 214, 215, 216, 226, 230, 241, 243, 251, 252, 256,\n",
       "         260, 267, 269, 270, 273, 276, 288, 292, 295, 308, 313, 315, 327,\n",
       "         330, 337, 339, 343, 345, 359, 366, 372, 378, 379, 385, 387, 389,\n",
       "         391, 401, 406, 413, 417, 418, 427, 430, 435, 448, 454, 455, 458,\n",
       "         459, 461, 466, 471, 474, 475, 476, 484, 489, 491, 492, 498, 504,\n",
       "         505, 508, 510, 523, 532, 546, 557, 560, 562, 565, 570, 582, 584,\n",
       "         593, 596]))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92589b26-d9ad-42ec-9a15-258873181d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from transformers import AutoModelForSeq2SeqLM, Trainer, TrainingArguments, EarlyStoppingCallback, TrainerCallback\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Custom callback to log metrics after each evaluation step\n",
    "class WandbLoggingCallback(TrainerCallback):\n",
    "    def __init__(self, fold_num):\n",
    "        self.fold_num = fold_num\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        if state.is_world_process_zero:\n",
    "            log_data = {f\"Fold_{self.fold_num}/eval/{key}\": value for key, value in metrics.items()}\n",
    "            wandb.log(log_data)\n",
    "\n",
    "def cross_val_train(modelname, fold_filename, filename, run_name, tokenizer_global, tokenized_dataset_global, folds, nb_epochs=30):\n",
    "    # Start a single WandB run for all folds\n",
    "    wandb.init(project=\"cross_val_T5\", name=run_name, config={\"epochs\": nb_epochs})\n",
    "    filenamebis= filename.split(\".jsonl\")[0]\n",
    "    fold_metrics = []\n",
    "\n",
    "    for fold_num, (train_idx, test_idx) in enumerate(folds):\n",
    "        print(f\"Processing Fold {fold_num + 1}\")\n",
    "        \n",
    "        #Ici on modifie pour recuperer le bon doc et appliquer le split dessus 3la 7ssab the fold\n",
    "        file_tfdidf=fold_filename\n",
    "        file_tfidf_fold=f\"FOLD{fold_num}_{file_tfdidf}\"\n",
    "        tokenizer,tokenized_dataset= tokenize_and_split_dataset(file_tfidf_fold,modelname)\n",
    "        print(f\"Recuperation du Fichier {file_tfidf_fold} pour le training du Fold {fold_num}\")\n",
    "        \n",
    "        # Split the dataset\n",
    "        train_dataset = tokenized_dataset.select(train_idx)\n",
    "        test_dataset = tokenized_dataset.select(test_idx)\n",
    "\n",
    "        # Initialize the model\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(modelname, from_tf=True)\n",
    "\n",
    "        # Training arguments with early stopping\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"./results/{run_name}/fold_{fold_num+1}\",\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=5e-5,\n",
    "            per_device_train_batch_size=8,\n",
    "            per_device_eval_batch_size=8,\n",
    "            num_train_epochs=nb_epochs,\n",
    "            load_best_model_at_end=True,\n",
    "            logging_steps=20,\n",
    "            logging_first_step=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            seed=42,\n",
    "        )\n",
    "\n",
    "        # Initialize Trainer with the logging callback and early stopping\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=test_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=lambda eval_pred: compute_metrics(eval_pred, tokenizer),\n",
    "            callbacks=[\n",
    "                WandbLoggingCallback(fold_num + 1),\n",
    "                EarlyStoppingCallback(early_stopping_patience=3)\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "\n",
    "        # Final evaluation for the fold\n",
    "        eval_results = trainer.evaluate()\n",
    "        print(f\"Fold {fold_num + 1} results: {eval_results}\")\n",
    "\n",
    "        # Store the final evaluation results for this fold\n",
    "        fold_metrics.append(eval_results)\n",
    "\n",
    "        # Predict on the test dataset\n",
    "        predictions = trainer.predict(test_dataset)\n",
    "\n",
    "    \n",
    "        # Decode predictions and save to file\n",
    "        pred_ids = np.argmax(predictions.predictions[0], axis=-1)\n",
    "        predicted_texts = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "        true_texts = tokenizer.batch_decode(predictions.label_ids, skip_special_tokens=True)\n",
    "        input_ids = test_dataset[\"input_ids\"]\n",
    "        input_texts = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Save predictions to a file\n",
    "\n",
    "        output_predictions_file = f\"{filenamebis}_fold_{fold_num+1}.jsonl\"\n",
    "        with open(output_predictions_file, \"w\", encoding=\"utf-8\") as writer:\n",
    "            for input_text, true_text, predicted_text in tqdm(zip(input_texts, true_texts, predicted_texts), total=len(true_texts)):\n",
    "                output_dict = {\n",
    "                    \"input\": input_text,\n",
    "                    \"true\": true_text,\n",
    "                    \"predicted\": predicted_text\n",
    "                }\n",
    "                writer.write(json.dumps(output_dict, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "        print(f\"Predictions for fold {fold_num + 1} saved to {output_predictions_file}\")\n",
    "\n",
    "    # Calculate and log average metrics\n",
    "    avg_metrics = {key: np.mean([fold[key] for fold in fold_metrics]) for key in fold_metrics[0]}\n",
    "    wandb.log({f\"Averages/{key}\": value for key, value in avg_metrics.items()})\n",
    "    print(\"Average metrics over all folds:\", avg_metrics)\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    #merge all to a single file for predcitions\n",
    "\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as merged_file:\n",
    "        for fold_num in range(len(folds)):\n",
    "            fold_file = f\"{filenamebis}_fold_{fold_num+1}.jsonl\"\n",
    "            if os.path.exists(fold_file):\n",
    "                with open(fold_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                    for line in f:\n",
    "                        merged_file.write(line)\n",
    "    print(f\"Final predicitons for all folds foudn in {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a4e485b-e8a1-4213-bf27-b54e97c880fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/users/nfs/Etu0/21402600/wandb/run-20250510_195855-91kj8061</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/91kj8061' target=\"_blank\">T5_SMALL_TOP5DOCS_TFIDF</a></strong> to <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/91kj8061' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/91kj8061</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Fold 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2297f3123b3c468ca41626e69d6065d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recuperation du Fichier FOLD0_training_top5_qulac_en_tfidf_CLEANED_rep_PREPROCESSED_FOR_MODEL.json pour le training du Fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_8374/4125602158.py:59: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='960' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 960/1800 05:59 < 05:15, 2.66 it/s, Epoch 16/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.443900</td>\n",
       "      <td>0.798685</td>\n",
       "      <td>0.128292</td>\n",
       "      <td>0.398007</td>\n",
       "      <td>0.179725</td>\n",
       "      <td>0.386804</td>\n",
       "      <td>0.326892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.747400</td>\n",
       "      <td>0.635106</td>\n",
       "      <td>0.148264</td>\n",
       "      <td>0.415373</td>\n",
       "      <td>0.203379</td>\n",
       "      <td>0.406510</td>\n",
       "      <td>0.362502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.612900</td>\n",
       "      <td>0.554118</td>\n",
       "      <td>0.173760</td>\n",
       "      <td>0.464859</td>\n",
       "      <td>0.221757</td>\n",
       "      <td>0.456705</td>\n",
       "      <td>0.409950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.527100</td>\n",
       "      <td>0.532810</td>\n",
       "      <td>0.187899</td>\n",
       "      <td>0.480551</td>\n",
       "      <td>0.235226</td>\n",
       "      <td>0.469740</td>\n",
       "      <td>0.425820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.505400</td>\n",
       "      <td>0.522377</td>\n",
       "      <td>0.191176</td>\n",
       "      <td>0.482766</td>\n",
       "      <td>0.237267</td>\n",
       "      <td>0.474386</td>\n",
       "      <td>0.435280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.476400</td>\n",
       "      <td>0.513959</td>\n",
       "      <td>0.181937</td>\n",
       "      <td>0.478732</td>\n",
       "      <td>0.229788</td>\n",
       "      <td>0.469182</td>\n",
       "      <td>0.430597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.448300</td>\n",
       "      <td>0.511224</td>\n",
       "      <td>0.194721</td>\n",
       "      <td>0.484278</td>\n",
       "      <td>0.243391</td>\n",
       "      <td>0.476282</td>\n",
       "      <td>0.446110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.453400</td>\n",
       "      <td>0.508367</td>\n",
       "      <td>0.191117</td>\n",
       "      <td>0.479950</td>\n",
       "      <td>0.239953</td>\n",
       "      <td>0.470527</td>\n",
       "      <td>0.437823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.453000</td>\n",
       "      <td>0.504915</td>\n",
       "      <td>0.199213</td>\n",
       "      <td>0.488394</td>\n",
       "      <td>0.247031</td>\n",
       "      <td>0.480011</td>\n",
       "      <td>0.446302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.429500</td>\n",
       "      <td>0.505179</td>\n",
       "      <td>0.201045</td>\n",
       "      <td>0.485842</td>\n",
       "      <td>0.248044</td>\n",
       "      <td>0.476194</td>\n",
       "      <td>0.444705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.403300</td>\n",
       "      <td>0.502128</td>\n",
       "      <td>0.199258</td>\n",
       "      <td>0.489330</td>\n",
       "      <td>0.245819</td>\n",
       "      <td>0.477167</td>\n",
       "      <td>0.447516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.413500</td>\n",
       "      <td>0.502235</td>\n",
       "      <td>0.196399</td>\n",
       "      <td>0.478609</td>\n",
       "      <td>0.239259</td>\n",
       "      <td>0.469705</td>\n",
       "      <td>0.440541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.413600</td>\n",
       "      <td>0.501776</td>\n",
       "      <td>0.201497</td>\n",
       "      <td>0.491660</td>\n",
       "      <td>0.247658</td>\n",
       "      <td>0.480752</td>\n",
       "      <td>0.447473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.414700</td>\n",
       "      <td>0.501797</td>\n",
       "      <td>0.197345</td>\n",
       "      <td>0.489243</td>\n",
       "      <td>0.246966</td>\n",
       "      <td>0.478682</td>\n",
       "      <td>0.447119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.412700</td>\n",
       "      <td>0.503075</td>\n",
       "      <td>0.200527</td>\n",
       "      <td>0.491092</td>\n",
       "      <td>0.247129</td>\n",
       "      <td>0.479572</td>\n",
       "      <td>0.452245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.379600</td>\n",
       "      <td>0.502229</td>\n",
       "      <td>0.198356</td>\n",
       "      <td>0.485232</td>\n",
       "      <td>0.243548</td>\n",
       "      <td>0.475476</td>\n",
       "      <td>0.447715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 results: {'eval_loss': 0.5017755627632141, 'eval_bleu': 0.20149678283350198, 'eval_rouge1': 0.4916598947302916, 'eval_rouge2': 0.2476577926603578, 'eval_rougeL': 0.4807522435731729, 'eval_meteor': 0.44747323403098993, 'eval_runtime': 2.0931, 'eval_samples_per_second': 57.332, 'eval_steps_per_second': 7.167, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 65137.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 1 saved to crossval_T5_SMALL_TOP5DOCS_TFIDF_fold_1.jsonl\n",
      "Processing Fold 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88b3f1e5d1c74c3ab2c2080ebb7bd93f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/Etu0/21402600/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recuperation du Fichier FOLD1_training_top5_qulac_en_tfidf_CLEANED_rep_PREPROCESSED_FOR_MODEL.json pour le training du Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_8374/4125602158.py:59: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='960' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 960/1800 05:55 < 05:11, 2.69 it/s, Epoch 16/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.463200</td>\n",
       "      <td>0.780546</td>\n",
       "      <td>0.133177</td>\n",
       "      <td>0.403097</td>\n",
       "      <td>0.199721</td>\n",
       "      <td>0.394575</td>\n",
       "      <td>0.339780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.780900</td>\n",
       "      <td>0.641367</td>\n",
       "      <td>0.149509</td>\n",
       "      <td>0.422840</td>\n",
       "      <td>0.219522</td>\n",
       "      <td>0.415121</td>\n",
       "      <td>0.380203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.566800</td>\n",
       "      <td>0.537610</td>\n",
       "      <td>0.181438</td>\n",
       "      <td>0.472415</td>\n",
       "      <td>0.245627</td>\n",
       "      <td>0.463905</td>\n",
       "      <td>0.430027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.497500</td>\n",
       "      <td>0.519481</td>\n",
       "      <td>0.180898</td>\n",
       "      <td>0.476876</td>\n",
       "      <td>0.241232</td>\n",
       "      <td>0.466816</td>\n",
       "      <td>0.437803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.482400</td>\n",
       "      <td>0.507605</td>\n",
       "      <td>0.182871</td>\n",
       "      <td>0.487421</td>\n",
       "      <td>0.251688</td>\n",
       "      <td>0.478789</td>\n",
       "      <td>0.451785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.487100</td>\n",
       "      <td>0.500482</td>\n",
       "      <td>0.184376</td>\n",
       "      <td>0.491205</td>\n",
       "      <td>0.251437</td>\n",
       "      <td>0.480222</td>\n",
       "      <td>0.456540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.465800</td>\n",
       "      <td>0.494178</td>\n",
       "      <td>0.191029</td>\n",
       "      <td>0.490951</td>\n",
       "      <td>0.258567</td>\n",
       "      <td>0.480916</td>\n",
       "      <td>0.458148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.447100</td>\n",
       "      <td>0.491048</td>\n",
       "      <td>0.191504</td>\n",
       "      <td>0.495011</td>\n",
       "      <td>0.259288</td>\n",
       "      <td>0.485238</td>\n",
       "      <td>0.462323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.428200</td>\n",
       "      <td>0.488886</td>\n",
       "      <td>0.197614</td>\n",
       "      <td>0.498044</td>\n",
       "      <td>0.261744</td>\n",
       "      <td>0.488367</td>\n",
       "      <td>0.464292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.432500</td>\n",
       "      <td>0.486212</td>\n",
       "      <td>0.195922</td>\n",
       "      <td>0.502611</td>\n",
       "      <td>0.263854</td>\n",
       "      <td>0.492736</td>\n",
       "      <td>0.468351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.409600</td>\n",
       "      <td>0.486155</td>\n",
       "      <td>0.195574</td>\n",
       "      <td>0.500838</td>\n",
       "      <td>0.267321</td>\n",
       "      <td>0.491608</td>\n",
       "      <td>0.467630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.413500</td>\n",
       "      <td>0.485594</td>\n",
       "      <td>0.198531</td>\n",
       "      <td>0.502637</td>\n",
       "      <td>0.269809</td>\n",
       "      <td>0.492902</td>\n",
       "      <td>0.472540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.410200</td>\n",
       "      <td>0.483004</td>\n",
       "      <td>0.206625</td>\n",
       "      <td>0.510015</td>\n",
       "      <td>0.275696</td>\n",
       "      <td>0.499129</td>\n",
       "      <td>0.482478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.378000</td>\n",
       "      <td>0.484128</td>\n",
       "      <td>0.212592</td>\n",
       "      <td>0.508467</td>\n",
       "      <td>0.276107</td>\n",
       "      <td>0.497767</td>\n",
       "      <td>0.478966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.382700</td>\n",
       "      <td>0.483297</td>\n",
       "      <td>0.211555</td>\n",
       "      <td>0.507569</td>\n",
       "      <td>0.272752</td>\n",
       "      <td>0.496431</td>\n",
       "      <td>0.478924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.396700</td>\n",
       "      <td>0.483193</td>\n",
       "      <td>0.209743</td>\n",
       "      <td>0.508475</td>\n",
       "      <td>0.273910</td>\n",
       "      <td>0.499055</td>\n",
       "      <td>0.478968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 results: {'eval_loss': 0.48300397396087646, 'eval_bleu': 0.20662512319216494, 'eval_rouge1': 0.5100149457076142, 'eval_rouge2': 0.2756961597233148, 'eval_rougeL': 0.4991286884094547, 'eval_meteor': 0.482478145547519, 'eval_runtime': 2.1459, 'eval_samples_per_second': 55.922, 'eval_steps_per_second': 6.99, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 64560.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 2 saved to crossval_T5_SMALL_TOP5DOCS_TFIDF_fold_2.jsonl\n",
      "Processing Fold 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef65fa55713e42c4b7a727c9266de2b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/Etu0/21402600/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recuperation du Fichier FOLD2_training_top5_qulac_en_tfidf_CLEANED_rep_PREPROCESSED_FOR_MODEL.json pour le training du Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_8374/4125602158.py:59: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1140' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1140/1800 07:03 < 04:05, 2.69 it/s, Epoch 19/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.491000</td>\n",
       "      <td>0.736121</td>\n",
       "      <td>0.125389</td>\n",
       "      <td>0.379993</td>\n",
       "      <td>0.165819</td>\n",
       "      <td>0.371398</td>\n",
       "      <td>0.317493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.792400</td>\n",
       "      <td>0.589754</td>\n",
       "      <td>0.150333</td>\n",
       "      <td>0.409069</td>\n",
       "      <td>0.192260</td>\n",
       "      <td>0.402010</td>\n",
       "      <td>0.361465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.600500</td>\n",
       "      <td>0.492412</td>\n",
       "      <td>0.171402</td>\n",
       "      <td>0.466052</td>\n",
       "      <td>0.214746</td>\n",
       "      <td>0.457325</td>\n",
       "      <td>0.414034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.523400</td>\n",
       "      <td>0.473070</td>\n",
       "      <td>0.179673</td>\n",
       "      <td>0.470650</td>\n",
       "      <td>0.221219</td>\n",
       "      <td>0.461977</td>\n",
       "      <td>0.427702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.516300</td>\n",
       "      <td>0.462021</td>\n",
       "      <td>0.183168</td>\n",
       "      <td>0.481406</td>\n",
       "      <td>0.234171</td>\n",
       "      <td>0.472190</td>\n",
       "      <td>0.441690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.451900</td>\n",
       "      <td>0.455156</td>\n",
       "      <td>0.184194</td>\n",
       "      <td>0.488423</td>\n",
       "      <td>0.239597</td>\n",
       "      <td>0.481160</td>\n",
       "      <td>0.447105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.482300</td>\n",
       "      <td>0.451249</td>\n",
       "      <td>0.186042</td>\n",
       "      <td>0.485913</td>\n",
       "      <td>0.243994</td>\n",
       "      <td>0.479505</td>\n",
       "      <td>0.451575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.470900</td>\n",
       "      <td>0.446767</td>\n",
       "      <td>0.180675</td>\n",
       "      <td>0.484690</td>\n",
       "      <td>0.231917</td>\n",
       "      <td>0.474072</td>\n",
       "      <td>0.440569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.438400</td>\n",
       "      <td>0.444258</td>\n",
       "      <td>0.187110</td>\n",
       "      <td>0.485672</td>\n",
       "      <td>0.233200</td>\n",
       "      <td>0.473540</td>\n",
       "      <td>0.443990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.433700</td>\n",
       "      <td>0.439712</td>\n",
       "      <td>0.192889</td>\n",
       "      <td>0.484780</td>\n",
       "      <td>0.237946</td>\n",
       "      <td>0.473332</td>\n",
       "      <td>0.445336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.453800</td>\n",
       "      <td>0.439144</td>\n",
       "      <td>0.190662</td>\n",
       "      <td>0.485255</td>\n",
       "      <td>0.240808</td>\n",
       "      <td>0.474683</td>\n",
       "      <td>0.443847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.404600</td>\n",
       "      <td>0.439382</td>\n",
       "      <td>0.193436</td>\n",
       "      <td>0.491298</td>\n",
       "      <td>0.248040</td>\n",
       "      <td>0.480755</td>\n",
       "      <td>0.454167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.433500</td>\n",
       "      <td>0.435573</td>\n",
       "      <td>0.188330</td>\n",
       "      <td>0.488609</td>\n",
       "      <td>0.235158</td>\n",
       "      <td>0.476972</td>\n",
       "      <td>0.446866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.409300</td>\n",
       "      <td>0.435582</td>\n",
       "      <td>0.194023</td>\n",
       "      <td>0.489929</td>\n",
       "      <td>0.239471</td>\n",
       "      <td>0.478957</td>\n",
       "      <td>0.447230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.403700</td>\n",
       "      <td>0.434119</td>\n",
       "      <td>0.191663</td>\n",
       "      <td>0.489197</td>\n",
       "      <td>0.240793</td>\n",
       "      <td>0.478672</td>\n",
       "      <td>0.447644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.413900</td>\n",
       "      <td>0.431820</td>\n",
       "      <td>0.191553</td>\n",
       "      <td>0.489375</td>\n",
       "      <td>0.243393</td>\n",
       "      <td>0.478452</td>\n",
       "      <td>0.447076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.386800</td>\n",
       "      <td>0.432365</td>\n",
       "      <td>0.191843</td>\n",
       "      <td>0.485822</td>\n",
       "      <td>0.241414</td>\n",
       "      <td>0.476612</td>\n",
       "      <td>0.445062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.380600</td>\n",
       "      <td>0.434106</td>\n",
       "      <td>0.191634</td>\n",
       "      <td>0.486214</td>\n",
       "      <td>0.240169</td>\n",
       "      <td>0.475790</td>\n",
       "      <td>0.441900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.375300</td>\n",
       "      <td>0.433037</td>\n",
       "      <td>0.192057</td>\n",
       "      <td>0.488999</td>\n",
       "      <td>0.242292</td>\n",
       "      <td>0.478917</td>\n",
       "      <td>0.445129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 results: {'eval_loss': 0.4318200945854187, 'eval_bleu': 0.19155270966779173, 'eval_rouge1': 0.48937534107657854, 'eval_rouge2': 0.24339343018520898, 'eval_rougeL': 0.478452076060515, 'eval_meteor': 0.44707603620281516, 'eval_runtime': 2.1312, 'eval_samples_per_second': 56.305, 'eval_steps_per_second': 7.038, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 64445.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 3 saved to crossval_T5_SMALL_TOP5DOCS_TFIDF_fold_3.jsonl\n",
      "Processing Fold 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "694eb4a930ca45c48f461fe188b9466e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/Etu0/21402600/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recuperation du Fichier FOLD3_training_top5_qulac_en_tfidf_CLEANED_rep_PREPROCESSED_FOR_MODEL.json pour le training du Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_8374/4125602158.py:59: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1140' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1140/1800 07:01 < 04:04, 2.70 it/s, Epoch 19/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.439500</td>\n",
       "      <td>0.752554</td>\n",
       "      <td>0.122543</td>\n",
       "      <td>0.382766</td>\n",
       "      <td>0.178701</td>\n",
       "      <td>0.373278</td>\n",
       "      <td>0.314966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.728700</td>\n",
       "      <td>0.603530</td>\n",
       "      <td>0.139005</td>\n",
       "      <td>0.408740</td>\n",
       "      <td>0.201236</td>\n",
       "      <td>0.398472</td>\n",
       "      <td>0.358261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.601300</td>\n",
       "      <td>0.516924</td>\n",
       "      <td>0.167995</td>\n",
       "      <td>0.461858</td>\n",
       "      <td>0.218276</td>\n",
       "      <td>0.452948</td>\n",
       "      <td>0.414003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.536100</td>\n",
       "      <td>0.498115</td>\n",
       "      <td>0.175541</td>\n",
       "      <td>0.466396</td>\n",
       "      <td>0.224288</td>\n",
       "      <td>0.457741</td>\n",
       "      <td>0.418804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.521100</td>\n",
       "      <td>0.489681</td>\n",
       "      <td>0.177115</td>\n",
       "      <td>0.470015</td>\n",
       "      <td>0.224035</td>\n",
       "      <td>0.461688</td>\n",
       "      <td>0.421775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.483100</td>\n",
       "      <td>0.484422</td>\n",
       "      <td>0.176772</td>\n",
       "      <td>0.474254</td>\n",
       "      <td>0.223404</td>\n",
       "      <td>0.465724</td>\n",
       "      <td>0.421824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.499000</td>\n",
       "      <td>0.481164</td>\n",
       "      <td>0.183674</td>\n",
       "      <td>0.473392</td>\n",
       "      <td>0.232130</td>\n",
       "      <td>0.465431</td>\n",
       "      <td>0.426137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.483300</td>\n",
       "      <td>0.478384</td>\n",
       "      <td>0.182293</td>\n",
       "      <td>0.474465</td>\n",
       "      <td>0.225803</td>\n",
       "      <td>0.464804</td>\n",
       "      <td>0.424708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.460300</td>\n",
       "      <td>0.477527</td>\n",
       "      <td>0.184506</td>\n",
       "      <td>0.477462</td>\n",
       "      <td>0.221847</td>\n",
       "      <td>0.464499</td>\n",
       "      <td>0.424125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.431700</td>\n",
       "      <td>0.477305</td>\n",
       "      <td>0.183828</td>\n",
       "      <td>0.475618</td>\n",
       "      <td>0.223313</td>\n",
       "      <td>0.463448</td>\n",
       "      <td>0.422631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.418200</td>\n",
       "      <td>0.476227</td>\n",
       "      <td>0.181961</td>\n",
       "      <td>0.475204</td>\n",
       "      <td>0.221009</td>\n",
       "      <td>0.461779</td>\n",
       "      <td>0.422556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.407300</td>\n",
       "      <td>0.476893</td>\n",
       "      <td>0.183847</td>\n",
       "      <td>0.476867</td>\n",
       "      <td>0.222274</td>\n",
       "      <td>0.463059</td>\n",
       "      <td>0.425074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.421200</td>\n",
       "      <td>0.476631</td>\n",
       "      <td>0.183439</td>\n",
       "      <td>0.477311</td>\n",
       "      <td>0.222901</td>\n",
       "      <td>0.463993</td>\n",
       "      <td>0.428053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.406400</td>\n",
       "      <td>0.475084</td>\n",
       "      <td>0.183475</td>\n",
       "      <td>0.476724</td>\n",
       "      <td>0.222114</td>\n",
       "      <td>0.462870</td>\n",
       "      <td>0.426683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.427800</td>\n",
       "      <td>0.475022</td>\n",
       "      <td>0.184138</td>\n",
       "      <td>0.478913</td>\n",
       "      <td>0.221438</td>\n",
       "      <td>0.464848</td>\n",
       "      <td>0.427424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.428000</td>\n",
       "      <td>0.474371</td>\n",
       "      <td>0.184656</td>\n",
       "      <td>0.478044</td>\n",
       "      <td>0.222754</td>\n",
       "      <td>0.463678</td>\n",
       "      <td>0.428883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.386400</td>\n",
       "      <td>0.474798</td>\n",
       "      <td>0.180684</td>\n",
       "      <td>0.473821</td>\n",
       "      <td>0.217743</td>\n",
       "      <td>0.459197</td>\n",
       "      <td>0.425046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.387500</td>\n",
       "      <td>0.476647</td>\n",
       "      <td>0.180445</td>\n",
       "      <td>0.475161</td>\n",
       "      <td>0.218343</td>\n",
       "      <td>0.461379</td>\n",
       "      <td>0.425189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.377300</td>\n",
       "      <td>0.476506</td>\n",
       "      <td>0.180520</td>\n",
       "      <td>0.474515</td>\n",
       "      <td>0.220292</td>\n",
       "      <td>0.460906</td>\n",
       "      <td>0.427065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 results: {'eval_loss': 0.4743708372116089, 'eval_bleu': 0.1846556061787415, 'eval_rouge1': 0.4780438489289358, 'eval_rouge2': 0.22275408705187483, 'eval_rougeL': 0.4636783340511479, 'eval_meteor': 0.4288834558729936, 'eval_runtime': 2.1381, 'eval_samples_per_second': 56.125, 'eval_steps_per_second': 7.016, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 62773.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 4 saved to crossval_T5_SMALL_TOP5DOCS_TFIDF_fold_4.jsonl\n",
      "Processing Fold 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d7a46ac0a86482f8d4754fa087ea22b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/Etu0/21402600/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recuperation du Fichier FOLD4_training_top5_qulac_en_tfidf_CLEANED_rep_PREPROCESSED_FOR_MODEL.json pour le training du Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_8374/4125602158.py:59: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1200' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1200/1800 07:31 < 03:45, 2.66 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.481600</td>\n",
       "      <td>0.751598</td>\n",
       "      <td>0.137031</td>\n",
       "      <td>0.396084</td>\n",
       "      <td>0.175229</td>\n",
       "      <td>0.387070</td>\n",
       "      <td>0.322823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.777100</td>\n",
       "      <td>0.628106</td>\n",
       "      <td>0.159912</td>\n",
       "      <td>0.424517</td>\n",
       "      <td>0.194637</td>\n",
       "      <td>0.412941</td>\n",
       "      <td>0.367930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.642800</td>\n",
       "      <td>0.519728</td>\n",
       "      <td>0.168447</td>\n",
       "      <td>0.477359</td>\n",
       "      <td>0.207825</td>\n",
       "      <td>0.466122</td>\n",
       "      <td>0.414637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.543900</td>\n",
       "      <td>0.497801</td>\n",
       "      <td>0.180328</td>\n",
       "      <td>0.488545</td>\n",
       "      <td>0.217692</td>\n",
       "      <td>0.477814</td>\n",
       "      <td>0.427869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.493100</td>\n",
       "      <td>0.488083</td>\n",
       "      <td>0.181264</td>\n",
       "      <td>0.493408</td>\n",
       "      <td>0.224387</td>\n",
       "      <td>0.480877</td>\n",
       "      <td>0.434939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.470400</td>\n",
       "      <td>0.482647</td>\n",
       "      <td>0.182138</td>\n",
       "      <td>0.500700</td>\n",
       "      <td>0.227251</td>\n",
       "      <td>0.486963</td>\n",
       "      <td>0.444382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.450700</td>\n",
       "      <td>0.476620</td>\n",
       "      <td>0.189343</td>\n",
       "      <td>0.502772</td>\n",
       "      <td>0.228840</td>\n",
       "      <td>0.490504</td>\n",
       "      <td>0.446629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.424800</td>\n",
       "      <td>0.474067</td>\n",
       "      <td>0.180999</td>\n",
       "      <td>0.502900</td>\n",
       "      <td>0.227281</td>\n",
       "      <td>0.490864</td>\n",
       "      <td>0.446453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.427500</td>\n",
       "      <td>0.469650</td>\n",
       "      <td>0.191439</td>\n",
       "      <td>0.504297</td>\n",
       "      <td>0.225615</td>\n",
       "      <td>0.492370</td>\n",
       "      <td>0.447164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.438200</td>\n",
       "      <td>0.468650</td>\n",
       "      <td>0.194269</td>\n",
       "      <td>0.504207</td>\n",
       "      <td>0.230782</td>\n",
       "      <td>0.493513</td>\n",
       "      <td>0.449824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.428300</td>\n",
       "      <td>0.467033</td>\n",
       "      <td>0.196345</td>\n",
       "      <td>0.508824</td>\n",
       "      <td>0.230114</td>\n",
       "      <td>0.498336</td>\n",
       "      <td>0.452664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.415100</td>\n",
       "      <td>0.465468</td>\n",
       "      <td>0.196762</td>\n",
       "      <td>0.509779</td>\n",
       "      <td>0.229852</td>\n",
       "      <td>0.497083</td>\n",
       "      <td>0.451674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.413300</td>\n",
       "      <td>0.466134</td>\n",
       "      <td>0.195768</td>\n",
       "      <td>0.505687</td>\n",
       "      <td>0.227682</td>\n",
       "      <td>0.492538</td>\n",
       "      <td>0.451980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.398600</td>\n",
       "      <td>0.463761</td>\n",
       "      <td>0.197402</td>\n",
       "      <td>0.508231</td>\n",
       "      <td>0.233240</td>\n",
       "      <td>0.494968</td>\n",
       "      <td>0.453413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.387400</td>\n",
       "      <td>0.465282</td>\n",
       "      <td>0.200385</td>\n",
       "      <td>0.506851</td>\n",
       "      <td>0.228180</td>\n",
       "      <td>0.494172</td>\n",
       "      <td>0.451321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.391000</td>\n",
       "      <td>0.464561</td>\n",
       "      <td>0.194426</td>\n",
       "      <td>0.506784</td>\n",
       "      <td>0.226411</td>\n",
       "      <td>0.491911</td>\n",
       "      <td>0.449652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.387600</td>\n",
       "      <td>0.462651</td>\n",
       "      <td>0.190048</td>\n",
       "      <td>0.505296</td>\n",
       "      <td>0.223464</td>\n",
       "      <td>0.491984</td>\n",
       "      <td>0.447509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.369600</td>\n",
       "      <td>0.462876</td>\n",
       "      <td>0.198537</td>\n",
       "      <td>0.508621</td>\n",
       "      <td>0.229307</td>\n",
       "      <td>0.493815</td>\n",
       "      <td>0.453616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.384500</td>\n",
       "      <td>0.462740</td>\n",
       "      <td>0.200452</td>\n",
       "      <td>0.509159</td>\n",
       "      <td>0.231894</td>\n",
       "      <td>0.493383</td>\n",
       "      <td>0.455890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.370500</td>\n",
       "      <td>0.462794</td>\n",
       "      <td>0.216836</td>\n",
       "      <td>0.514479</td>\n",
       "      <td>0.243630</td>\n",
       "      <td>0.500361</td>\n",
       "      <td>0.468586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 results: {'eval_loss': 0.4626505672931671, 'eval_bleu': 0.19004817514984165, 'eval_rouge1': 0.5052960018772827, 'eval_rouge2': 0.22346352779872528, 'eval_rougeL': 0.49198371599245694, 'eval_meteor': 0.44750932540482385, 'eval_runtime': 2.5222, 'eval_samples_per_second': 47.182, 'eval_steps_per_second': 5.947, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:00<00:00, 65219.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 5 saved to crossval_T5_SMALL_TOP5DOCS_TFIDF_fold_5.jsonl\n",
      "Average metrics over all folds: {'eval_loss': 0.47072420716285707, 'eval_bleu': 0.19487567940440836, 'eval_rouge1': 0.4948780064641406, 'eval_rouge2': 0.24259299948389632, 'eval_rougeL': 0.48279901161734956, 'eval_meteor': 0.4506840394118282, 'eval_runtime': 2.2061, 'eval_samples_per_second': 54.5732, 'eval_steps_per_second': 6.8316, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Averages/epoch</td><td>▁</td></tr><tr><td>Averages/eval_bleu</td><td>▁</td></tr><tr><td>Averages/eval_loss</td><td>▁</td></tr><tr><td>Averages/eval_meteor</td><td>▁</td></tr><tr><td>Averages/eval_rouge1</td><td>▁</td></tr><tr><td>Averages/eval_rouge2</td><td>▁</td></tr><tr><td>Averages/eval_rougeL</td><td>▁</td></tr><tr><td>Averages/eval_runtime</td><td>▁</td></tr><tr><td>Averages/eval_samples_per_second</td><td>▁</td></tr><tr><td>Averages/eval_steps_per_second</td><td>▁</td></tr><tr><td>Fold_1/eval/epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇███</td></tr><tr><td>Fold_1/eval/eval_bleu</td><td>▁▃▅▇▇▆▇▇█████████</td></tr><tr><td>Fold_1/eval/eval_loss</td><td>█▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_1/eval/eval_meteor</td><td>▁▃▆▇▇▇█▇███▇█████</td></tr><tr><td>Fold_1/eval/eval_rouge1</td><td>▁▂▆▇▇▇▇▇███▇█████</td></tr><tr><td>Fold_1/eval/eval_rouge2</td><td>▁▃▅▇▇▆█▇███▇█████</td></tr><tr><td>Fold_1/eval/eval_rougeL</td><td>▁▂▆▇█▇█▇███▇█████</td></tr><tr><td>Fold_1/eval/eval_runtime</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_1/eval/eval_samples_per_second</td><td>▁████████████████</td></tr><tr><td>Fold_1/eval/eval_steps_per_second</td><td>▁████████████████</td></tr><tr><td>Fold_2/eval/epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇███</td></tr><tr><td>Fold_2/eval/eval_bleu</td><td>▁▂▅▅▅▆▆▆▇▇▇▇▇███▇</td></tr><tr><td>Fold_2/eval/eval_loss</td><td>█▅▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_2/eval/eval_meteor</td><td>▁▃▅▆▆▇▇▇▇▇▇██████</td></tr><tr><td>Fold_2/eval/eval_rouge1</td><td>▁▂▆▆▇▇▇▇▇█▇██████</td></tr><tr><td>Fold_2/eval/eval_rouge2</td><td>▁▃▅▅▆▆▆▆▇▇▇▇█████</td></tr><tr><td>Fold_2/eval/eval_rougeL</td><td>▁▂▆▆▇▇▇▇▇█▇██████</td></tr><tr><td>Fold_2/eval/eval_runtime</td><td>█▁▂▂▂▂▁▂▁▂▁▂▁▁▂▁▂</td></tr><tr><td>Fold_2/eval/eval_samples_per_second</td><td>▁█▇▇▇▇█▇█▇█▇██▇█▇</td></tr><tr><td>Fold_2/eval/eval_steps_per_second</td><td>▁█▇▇▇▇█▇█▇█▇██▇█▇</td></tr><tr><td>Fold_3/eval/epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇███</td></tr><tr><td>Fold_3/eval/eval_bleu</td><td>▁▄▆▇▇▇▇▇▇███▇███████</td></tr><tr><td>Fold_3/eval/eval_loss</td><td>█▅▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_3/eval/eval_meteor</td><td>▁▃▆▇▇██▇▇█▇██████▇██</td></tr><tr><td>Fold_3/eval/eval_rouge1</td><td>▁▃▆▇▇███████████████</td></tr><tr><td>Fold_3/eval/eval_rouge2</td><td>▁▃▅▆▇▇█▇▇▇▇█▇▇▇█▇▇██</td></tr><tr><td>Fold_3/eval/eval_rougeL</td><td>▁▃▆▇▇███████████████</td></tr><tr><td>Fold_3/eval/eval_runtime</td><td>▆▃▂▁▂▂▂▁▂█▂▁▁▂▁▁▁▃▁▂</td></tr><tr><td>Fold_3/eval/eval_samples_per_second</td><td>▃▆▇█▇▇▇█▇▁▇██▇███▆█▇</td></tr><tr><td>Fold_3/eval/eval_steps_per_second</td><td>▃▆▇█▇▇██▇▁▇██▇███▆█▇</td></tr><tr><td>Fold_4/eval/epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇███</td></tr><tr><td>Fold_4/eval/eval_bleu</td><td>▁▃▆▇▇▇██████████████</td></tr><tr><td>Fold_4/eval/eval_loss</td><td>█▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_4/eval/eval_meteor</td><td>▁▄▇▇████████████████</td></tr><tr><td>Fold_4/eval/eval_rouge1</td><td>▁▃▇▇▇███████████████</td></tr><tr><td>Fold_4/eval/eval_rouge2</td><td>▁▄▆▇▇▇█▇▇▇▇▇▇▇▇▇▆▆▆▇</td></tr><tr><td>Fold_4/eval/eval_rougeL</td><td>▁▃▇▇████████████████</td></tr><tr><td>Fold_4/eval/eval_runtime</td><td>▆▂▁▆▃▂▃▂▂▁▃▂▂▂▂▇▇▇▆█</td></tr><tr><td>Fold_4/eval/eval_samples_per_second</td><td>▃▇█▃▆▇▆▇▇█▆▇▇▇▇▂▂▂▃▁</td></tr><tr><td>Fold_4/eval/eval_steps_per_second</td><td>▃▇█▃▆▇▆▇▇█▆▇▇▇▇▂▂▂▃▁</td></tr><tr><td>Fold_5/eval/epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇███</td></tr><tr><td>Fold_5/eval/eval_bleu</td><td>▁▃▄▅▅▅▆▅▆▆▆▆▆▆▇▆▆▆▇█▆</td></tr><tr><td>Fold_5/eval/eval_loss</td><td>█▅▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_5/eval/eval_meteor</td><td>▁▃▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇</td></tr><tr><td>Fold_5/eval/eval_rouge1</td><td>▁▃▆▆▇▇▇▇▇▇██▇███▇███▇</td></tr><tr><td>Fold_5/eval/eval_rouge2</td><td>▁▃▄▅▆▆▆▆▆▇▇▇▆▇▆▆▆▇▇█▆</td></tr><tr><td>Fold_5/eval/eval_rougeL</td><td>▁▃▆▇▇▇▇▇███████▇▇███▇</td></tr><tr><td>Fold_5/eval/eval_runtime</td><td>▁▁▁▁▁▁▁▁▁▂▂▇█████████</td></tr><tr><td>Fold_5/eval/eval_samples_per_second</td><td>█████████▇▆▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_5/eval/eval_steps_per_second</td><td>█████████▇▆▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/bleu</td><td>▃▆▆▇▇▇▇▇▂▆▆████▁▃▅▆▆▆▆▆▇▆▆▆▆▆▆▆▅▅▆▂▆▇▇▇▇</td></tr><tr><td>eval/loss</td><td>▅▃▃▃▃▃▆▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁█▂▂▂▂▂▅▃▂▂▂▂▂▂▂▂</td></tr><tr><td>eval/meteor</td><td>▁▃▆▆▆▇▇▇▄▆▇▇▇▇████▅▆▇▆▇▆▁▅▅▆▆▅▆▆▆▆▁▆▆▇▇▇</td></tr><tr><td>eval/rouge1</td><td>▁▂▅▆▆▆▇▇▆▇▃▆▇▇▇█▇▂▆▇▆▇▇▇▇▅▅▆▆▆▆▆▆▆▃█████</td></tr><tr><td>eval/rouge2</td><td>▂▄▆▆▇▇▆▇▅▆▇▇▇██▁▃▅▆▅▆▆▆▂▅▅▅▅▅▅▅▅▅▃▄▅▅▅▆▅</td></tr><tr><td>eval/rougeL</td><td>▂▃▇▆▇▇▇▇▃▇▇▇███▁▃▆▇▇▇▇▇▇▂▆▆▆▆▆▆▆▆▇██████</td></tr><tr><td>eval/runtime</td><td>▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▂▁▁▁▃████</td></tr><tr><td>eval/samples_per_second</td><td>▁███████████████████████████████████▇▆▆▆</td></tr><tr><td>eval/steps_per_second</td><td>█▇███▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████▇▇▇▇▇██▇▇▂▁▁▁▁▁</td></tr><tr><td>test/bleu</td><td>▆█▃▁▃</td></tr><tr><td>test/loss</td><td>█▆▁▅▄</td></tr><tr><td>test/meteor</td><td>▃█▃▁▃</td></tr><tr><td>test/rouge1</td><td>▄█▃▁▇</td></tr><tr><td>test/rouge2</td><td>▄█▄▁▁</td></tr><tr><td>test/rougeL</td><td>▄█▄▁▇</td></tr><tr><td>test/runtime</td><td>▁▂▂▂█</td></tr><tr><td>test/samples_per_second</td><td>█▇▇▆▁</td></tr><tr><td>test/steps_per_second</td><td>█▇▇▆▁</td></tr><tr><td>train/epoch</td><td>▂▂▂▃▅▆▆▆▆▇▃▃▃▃▅▇▂▂▃▄▅▆▆▇██▁▁▂▂▄▄▆▇▇▂▃▇▇▇</td></tr><tr><td>train/global_step</td><td>▁▂▂▃▃▅▅▆▆▆▂▂▂▃▃▅▅▇▂▃▃▄▅▅▅▆▇█▁▂▇▂▃▃▅▆▆▇▇█</td></tr><tr><td>train/grad_norm</td><td>▆▁▁▁▁▁▁▁▁▂▁▁▁▁▁▆▁▁▁▁▁▁▁█▁▁▁▁▁▁▅▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>█▆▆▅▅▅▂▇▇▆▄▄▃▃▃▇▆▆▅▃▂███▇▅▅▄▃▃▂██▇▆▅▄▄▃▁</td></tr><tr><td>train/loss</td><td>▁▁▁▁▁▁▁█▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Averages/epoch</td><td>18</td></tr><tr><td>Averages/eval_bleu</td><td>0.19488</td></tr><tr><td>Averages/eval_loss</td><td>0.47072</td></tr><tr><td>Averages/eval_meteor</td><td>0.45068</td></tr><tr><td>Averages/eval_rouge1</td><td>0.49488</td></tr><tr><td>Averages/eval_rouge2</td><td>0.24259</td></tr><tr><td>Averages/eval_rougeL</td><td>0.4828</td></tr><tr><td>Averages/eval_runtime</td><td>2.2061</td></tr><tr><td>Averages/eval_samples_per_second</td><td>54.5732</td></tr><tr><td>Averages/eval_steps_per_second</td><td>6.8316</td></tr><tr><td>Fold_1/eval/epoch</td><td>16</td></tr><tr><td>Fold_1/eval/eval_bleu</td><td>0.2015</td></tr><tr><td>Fold_1/eval/eval_loss</td><td>0.50178</td></tr><tr><td>Fold_1/eval/eval_meteor</td><td>0.44747</td></tr><tr><td>Fold_1/eval/eval_rouge1</td><td>0.49166</td></tr><tr><td>Fold_1/eval/eval_rouge2</td><td>0.24766</td></tr><tr><td>Fold_1/eval/eval_rougeL</td><td>0.48075</td></tr><tr><td>Fold_1/eval/eval_runtime</td><td>2.0931</td></tr><tr><td>Fold_1/eval/eval_samples_per_second</td><td>57.332</td></tr><tr><td>Fold_1/eval/eval_steps_per_second</td><td>7.167</td></tr><tr><td>Fold_2/eval/epoch</td><td>16</td></tr><tr><td>Fold_2/eval/eval_bleu</td><td>0.20663</td></tr><tr><td>Fold_2/eval/eval_loss</td><td>0.483</td></tr><tr><td>Fold_2/eval/eval_meteor</td><td>0.48248</td></tr><tr><td>Fold_2/eval/eval_rouge1</td><td>0.51001</td></tr><tr><td>Fold_2/eval/eval_rouge2</td><td>0.2757</td></tr><tr><td>Fold_2/eval/eval_rougeL</td><td>0.49913</td></tr><tr><td>Fold_2/eval/eval_runtime</td><td>2.1459</td></tr><tr><td>Fold_2/eval/eval_samples_per_second</td><td>55.922</td></tr><tr><td>Fold_2/eval/eval_steps_per_second</td><td>6.99</td></tr><tr><td>Fold_3/eval/epoch</td><td>19</td></tr><tr><td>Fold_3/eval/eval_bleu</td><td>0.19155</td></tr><tr><td>Fold_3/eval/eval_loss</td><td>0.43182</td></tr><tr><td>Fold_3/eval/eval_meteor</td><td>0.44708</td></tr><tr><td>Fold_3/eval/eval_rouge1</td><td>0.48938</td></tr><tr><td>Fold_3/eval/eval_rouge2</td><td>0.24339</td></tr><tr><td>Fold_3/eval/eval_rougeL</td><td>0.47845</td></tr><tr><td>Fold_3/eval/eval_runtime</td><td>2.1312</td></tr><tr><td>Fold_3/eval/eval_samples_per_second</td><td>56.305</td></tr><tr><td>Fold_3/eval/eval_steps_per_second</td><td>7.038</td></tr><tr><td>Fold_4/eval/epoch</td><td>19</td></tr><tr><td>Fold_4/eval/eval_bleu</td><td>0.18466</td></tr><tr><td>Fold_4/eval/eval_loss</td><td>0.47437</td></tr><tr><td>Fold_4/eval/eval_meteor</td><td>0.42888</td></tr><tr><td>Fold_4/eval/eval_rouge1</td><td>0.47804</td></tr><tr><td>Fold_4/eval/eval_rouge2</td><td>0.22275</td></tr><tr><td>Fold_4/eval/eval_rougeL</td><td>0.46368</td></tr><tr><td>Fold_4/eval/eval_runtime</td><td>2.1381</td></tr><tr><td>Fold_4/eval/eval_samples_per_second</td><td>56.125</td></tr><tr><td>Fold_4/eval/eval_steps_per_second</td><td>7.016</td></tr><tr><td>Fold_5/eval/epoch</td><td>20</td></tr><tr><td>Fold_5/eval/eval_bleu</td><td>0.19005</td></tr><tr><td>Fold_5/eval/eval_loss</td><td>0.46265</td></tr><tr><td>Fold_5/eval/eval_meteor</td><td>0.44751</td></tr><tr><td>Fold_5/eval/eval_rouge1</td><td>0.5053</td></tr><tr><td>Fold_5/eval/eval_rouge2</td><td>0.22346</td></tr><tr><td>Fold_5/eval/eval_rougeL</td><td>0.49198</td></tr><tr><td>Fold_5/eval/eval_runtime</td><td>2.5222</td></tr><tr><td>Fold_5/eval/eval_samples_per_second</td><td>47.182</td></tr><tr><td>Fold_5/eval/eval_steps_per_second</td><td>5.947</td></tr><tr><td>eval/bleu</td><td>0.19005</td></tr><tr><td>eval/loss</td><td>0.46265</td></tr><tr><td>eval/meteor</td><td>0.44751</td></tr><tr><td>eval/rouge1</td><td>0.5053</td></tr><tr><td>eval/rouge2</td><td>0.22346</td></tr><tr><td>eval/rougeL</td><td>0.49198</td></tr><tr><td>eval/runtime</td><td>2.5222</td></tr><tr><td>eval/samples_per_second</td><td>47.182</td></tr><tr><td>eval/steps_per_second</td><td>5.947</td></tr><tr><td>test/bleu</td><td>0.19005</td></tr><tr><td>test/loss</td><td>0.46265</td></tr><tr><td>test/meteor</td><td>0.44751</td></tr><tr><td>test/rouge1</td><td>0.5053</td></tr><tr><td>test/rouge2</td><td>0.22346</td></tr><tr><td>test/rougeL</td><td>0.49198</td></tr><tr><td>test/runtime</td><td>2.5053</td></tr><tr><td>test/samples_per_second</td><td>47.499</td></tr><tr><td>test/steps_per_second</td><td>5.987</td></tr><tr><td>total_flos</td><td>1299281294131200.0</td></tr><tr><td>train/epoch</td><td>20</td></tr><tr><td>train/global_step</td><td>1200</td></tr><tr><td>train/grad_norm</td><td>0.88539</td></tr><tr><td>train/learning_rate</td><td>2e-05</td></tr><tr><td>train/loss</td><td>0.3705</td></tr><tr><td>train_loss</td><td>0.7119</td></tr><tr><td>train_runtime</td><td>451.2521</td></tr><tr><td>train_samples_per_second</td><td>31.911</td></tr><tr><td>train_steps_per_second</td><td>3.989</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">T5_SMALL_TOP5DOCS_TFIDF</strong> at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/91kj8061' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/91kj8061</a><br> View project at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250510_195855-91kj8061/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final predicitons for all folds foudn in crossval_T5_SMALL_TOP5DOCS_TFIDF.jsonl\n"
     ]
    }
   ],
   "source": [
    "cross_val_train(modelname=\"t5-small\", run_name=\"T5_SMALL_TOP5DOCS_TFIDF\",fold_filename=\"training_top10_qulac_en_tfidf_CLEANED_rep_PREPROCESSED_FOR_MODEL.json\",filename=\"crossval_T5_SMALL_TOP5DOCS_TFIDF.jsonl\", \n",
    "                tokenizer_global=tokenizer_global, tokenized_dataset_global=tokenized_dataset_global, \n",
    "                folds=folds, nb_epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0460ea58-2035-4f95-b089-44d3b6b62220",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# TEST of top10 TFIDF INSTEAD OF TOP 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc6814d5-34f9-411a-b9c3-52a0ee9d3c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/users/nfs/Etu0/21402600/wandb/run-20250510_235611-0tncng9c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/0tncng9c' target=\"_blank\">T5_SMALL_TOP10DOCS_TFIDF</a></strong> to <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/0tncng9c' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/0tncng9c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Fold 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1443f2c59d8a4ebb8aaab2f5335db109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recuperation du Fichier FOLD0_training_top10_qulac_en_tfidf_CLEANED_rep_PREPROCESSED_FOR_MODEL.json pour le training du Fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1746914173.151919   18289 gpu_device.cc:2341] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_18289/1707016551.py:59: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1260' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1260/1800 07:51 < 03:22, 2.67 it/s, Epoch 21/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.270000</td>\n",
       "      <td>0.777021</td>\n",
       "      <td>0.133553</td>\n",
       "      <td>0.389895</td>\n",
       "      <td>0.188643</td>\n",
       "      <td>0.382010</td>\n",
       "      <td>0.328259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.721600</td>\n",
       "      <td>0.617617</td>\n",
       "      <td>0.151810</td>\n",
       "      <td>0.421219</td>\n",
       "      <td>0.205314</td>\n",
       "      <td>0.410922</td>\n",
       "      <td>0.372651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.602700</td>\n",
       "      <td>0.552534</td>\n",
       "      <td>0.173743</td>\n",
       "      <td>0.458097</td>\n",
       "      <td>0.218951</td>\n",
       "      <td>0.451483</td>\n",
       "      <td>0.406900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.527500</td>\n",
       "      <td>0.529759</td>\n",
       "      <td>0.182090</td>\n",
       "      <td>0.476399</td>\n",
       "      <td>0.233983</td>\n",
       "      <td>0.465897</td>\n",
       "      <td>0.425703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.501300</td>\n",
       "      <td>0.521005</td>\n",
       "      <td>0.185558</td>\n",
       "      <td>0.482259</td>\n",
       "      <td>0.233757</td>\n",
       "      <td>0.471428</td>\n",
       "      <td>0.431041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.471700</td>\n",
       "      <td>0.513119</td>\n",
       "      <td>0.184655</td>\n",
       "      <td>0.482775</td>\n",
       "      <td>0.235356</td>\n",
       "      <td>0.472689</td>\n",
       "      <td>0.437127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.448800</td>\n",
       "      <td>0.511730</td>\n",
       "      <td>0.188841</td>\n",
       "      <td>0.491928</td>\n",
       "      <td>0.240126</td>\n",
       "      <td>0.480332</td>\n",
       "      <td>0.443843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.446300</td>\n",
       "      <td>0.505437</td>\n",
       "      <td>0.196351</td>\n",
       "      <td>0.487957</td>\n",
       "      <td>0.249770</td>\n",
       "      <td>0.477195</td>\n",
       "      <td>0.445810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.449100</td>\n",
       "      <td>0.503571</td>\n",
       "      <td>0.197003</td>\n",
       "      <td>0.490868</td>\n",
       "      <td>0.245304</td>\n",
       "      <td>0.477374</td>\n",
       "      <td>0.443795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.425900</td>\n",
       "      <td>0.501816</td>\n",
       "      <td>0.195673</td>\n",
       "      <td>0.493893</td>\n",
       "      <td>0.248153</td>\n",
       "      <td>0.481236</td>\n",
       "      <td>0.447750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.395200</td>\n",
       "      <td>0.500370</td>\n",
       "      <td>0.193928</td>\n",
       "      <td>0.492155</td>\n",
       "      <td>0.244415</td>\n",
       "      <td>0.476610</td>\n",
       "      <td>0.443864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.408800</td>\n",
       "      <td>0.501075</td>\n",
       "      <td>0.190666</td>\n",
       "      <td>0.485102</td>\n",
       "      <td>0.237554</td>\n",
       "      <td>0.470315</td>\n",
       "      <td>0.438409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.409400</td>\n",
       "      <td>0.500452</td>\n",
       "      <td>0.195613</td>\n",
       "      <td>0.489319</td>\n",
       "      <td>0.246287</td>\n",
       "      <td>0.476423</td>\n",
       "      <td>0.444807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.409300</td>\n",
       "      <td>0.499290</td>\n",
       "      <td>0.197834</td>\n",
       "      <td>0.489766</td>\n",
       "      <td>0.248692</td>\n",
       "      <td>0.475365</td>\n",
       "      <td>0.448711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.405500</td>\n",
       "      <td>0.499777</td>\n",
       "      <td>0.200062</td>\n",
       "      <td>0.492314</td>\n",
       "      <td>0.253249</td>\n",
       "      <td>0.478844</td>\n",
       "      <td>0.450079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.372200</td>\n",
       "      <td>0.499088</td>\n",
       "      <td>0.196717</td>\n",
       "      <td>0.488092</td>\n",
       "      <td>0.248972</td>\n",
       "      <td>0.473474</td>\n",
       "      <td>0.448003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.368900</td>\n",
       "      <td>0.499546</td>\n",
       "      <td>0.193621</td>\n",
       "      <td>0.485847</td>\n",
       "      <td>0.244768</td>\n",
       "      <td>0.471569</td>\n",
       "      <td>0.444881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.371200</td>\n",
       "      <td>0.499049</td>\n",
       "      <td>0.191677</td>\n",
       "      <td>0.488452</td>\n",
       "      <td>0.244188</td>\n",
       "      <td>0.473715</td>\n",
       "      <td>0.447991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.368300</td>\n",
       "      <td>0.500900</td>\n",
       "      <td>0.192420</td>\n",
       "      <td>0.487197</td>\n",
       "      <td>0.244572</td>\n",
       "      <td>0.472691</td>\n",
       "      <td>0.445740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.360900</td>\n",
       "      <td>0.500798</td>\n",
       "      <td>0.194192</td>\n",
       "      <td>0.486183</td>\n",
       "      <td>0.247435</td>\n",
       "      <td>0.473197</td>\n",
       "      <td>0.447229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.347400</td>\n",
       "      <td>0.499334</td>\n",
       "      <td>0.192356</td>\n",
       "      <td>0.487795</td>\n",
       "      <td>0.249437</td>\n",
       "      <td>0.475233</td>\n",
       "      <td>0.447203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 results: {'eval_loss': 0.49904876947402954, 'eval_bleu': 0.1916772032435903, 'eval_rouge1': 0.48845220414658286, 'eval_rouge2': 0.244188286097146, 'eval_rougeL': 0.4737154377863579, 'eval_meteor': 0.44799058699090005, 'eval_runtime': 2.1367, 'eval_samples_per_second': 56.163, 'eval_steps_per_second': 7.02, 'epoch': 21.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 45864.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 1 saved to crossval_T5_SMALL_TOP10DOCS_TFIDF_fold_1.jsonl\n",
      "Processing Fold 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0562668dc6ed42fea486af307f879e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/Etu0/21402600/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recuperation du Fichier FOLD1_training_top10_qulac_en_tfidf_CLEANED_rep_PREPROCESSED_FOR_MODEL.json pour le training du Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_18289/1707016551.py:59: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='960' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 960/1800 05:54 < 05:10, 2.70 it/s, Epoch 16/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.319100</td>\n",
       "      <td>0.770451</td>\n",
       "      <td>0.136797</td>\n",
       "      <td>0.406941</td>\n",
       "      <td>0.199617</td>\n",
       "      <td>0.397100</td>\n",
       "      <td>0.342598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.765500</td>\n",
       "      <td>0.633288</td>\n",
       "      <td>0.149712</td>\n",
       "      <td>0.420344</td>\n",
       "      <td>0.218442</td>\n",
       "      <td>0.408808</td>\n",
       "      <td>0.371790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.562400</td>\n",
       "      <td>0.541259</td>\n",
       "      <td>0.177020</td>\n",
       "      <td>0.468629</td>\n",
       "      <td>0.239409</td>\n",
       "      <td>0.458657</td>\n",
       "      <td>0.431418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.496600</td>\n",
       "      <td>0.518170</td>\n",
       "      <td>0.180112</td>\n",
       "      <td>0.478998</td>\n",
       "      <td>0.241797</td>\n",
       "      <td>0.466850</td>\n",
       "      <td>0.441449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.481400</td>\n",
       "      <td>0.509396</td>\n",
       "      <td>0.189899</td>\n",
       "      <td>0.496100</td>\n",
       "      <td>0.256236</td>\n",
       "      <td>0.483042</td>\n",
       "      <td>0.457765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.484200</td>\n",
       "      <td>0.502828</td>\n",
       "      <td>0.186545</td>\n",
       "      <td>0.496788</td>\n",
       "      <td>0.257685</td>\n",
       "      <td>0.484813</td>\n",
       "      <td>0.458781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.463000</td>\n",
       "      <td>0.496222</td>\n",
       "      <td>0.200551</td>\n",
       "      <td>0.504160</td>\n",
       "      <td>0.270813</td>\n",
       "      <td>0.492299</td>\n",
       "      <td>0.470222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.441400</td>\n",
       "      <td>0.494713</td>\n",
       "      <td>0.198556</td>\n",
       "      <td>0.506372</td>\n",
       "      <td>0.275109</td>\n",
       "      <td>0.492479</td>\n",
       "      <td>0.473018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.425800</td>\n",
       "      <td>0.491606</td>\n",
       "      <td>0.204939</td>\n",
       "      <td>0.517842</td>\n",
       "      <td>0.278816</td>\n",
       "      <td>0.502917</td>\n",
       "      <td>0.483710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.433000</td>\n",
       "      <td>0.487471</td>\n",
       "      <td>0.205678</td>\n",
       "      <td>0.510752</td>\n",
       "      <td>0.275600</td>\n",
       "      <td>0.496411</td>\n",
       "      <td>0.476418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.407200</td>\n",
       "      <td>0.487581</td>\n",
       "      <td>0.206072</td>\n",
       "      <td>0.514336</td>\n",
       "      <td>0.275193</td>\n",
       "      <td>0.499199</td>\n",
       "      <td>0.480237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.412500</td>\n",
       "      <td>0.486855</td>\n",
       "      <td>0.210899</td>\n",
       "      <td>0.516806</td>\n",
       "      <td>0.283428</td>\n",
       "      <td>0.503223</td>\n",
       "      <td>0.483814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.409100</td>\n",
       "      <td>0.484372</td>\n",
       "      <td>0.217452</td>\n",
       "      <td>0.521338</td>\n",
       "      <td>0.291555</td>\n",
       "      <td>0.508451</td>\n",
       "      <td>0.487415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.379100</td>\n",
       "      <td>0.484764</td>\n",
       "      <td>0.215789</td>\n",
       "      <td>0.520665</td>\n",
       "      <td>0.289443</td>\n",
       "      <td>0.507236</td>\n",
       "      <td>0.484552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.381400</td>\n",
       "      <td>0.486318</td>\n",
       "      <td>0.214454</td>\n",
       "      <td>0.519975</td>\n",
       "      <td>0.288575</td>\n",
       "      <td>0.507226</td>\n",
       "      <td>0.485132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.394300</td>\n",
       "      <td>0.485158</td>\n",
       "      <td>0.211948</td>\n",
       "      <td>0.518639</td>\n",
       "      <td>0.283998</td>\n",
       "      <td>0.507040</td>\n",
       "      <td>0.486322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 results: {'eval_loss': 0.4843716025352478, 'eval_bleu': 0.21745225188828352, 'eval_rouge1': 0.5213380619956383, 'eval_rouge2': 0.29155525569415713, 'eval_rougeL': 0.508450778431235, 'eval_meteor': 0.4874151267824249, 'eval_runtime': 2.1139, 'eval_samples_per_second': 56.766, 'eval_steps_per_second': 7.096, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 47384.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 2 saved to crossval_T5_SMALL_TOP10DOCS_TFIDF_fold_2.jsonl\n",
      "Processing Fold 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8288f0c7e88f4d5bb94f014cbd015cc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/Etu0/21402600/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recuperation du Fichier FOLD2_training_top10_qulac_en_tfidf_CLEANED_rep_PREPROCESSED_FOR_MODEL.json pour le training du Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_18289/1707016551.py:59: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1140' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1140/1800 07:02 < 04:04, 2.69 it/s, Epoch 19/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.367000</td>\n",
       "      <td>0.741107</td>\n",
       "      <td>0.131588</td>\n",
       "      <td>0.382852</td>\n",
       "      <td>0.173046</td>\n",
       "      <td>0.373491</td>\n",
       "      <td>0.318185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.775500</td>\n",
       "      <td>0.578833</td>\n",
       "      <td>0.143370</td>\n",
       "      <td>0.414697</td>\n",
       "      <td>0.184868</td>\n",
       "      <td>0.403156</td>\n",
       "      <td>0.359217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.598600</td>\n",
       "      <td>0.492715</td>\n",
       "      <td>0.169373</td>\n",
       "      <td>0.466280</td>\n",
       "      <td>0.208748</td>\n",
       "      <td>0.456975</td>\n",
       "      <td>0.418188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.520700</td>\n",
       "      <td>0.472796</td>\n",
       "      <td>0.180157</td>\n",
       "      <td>0.468919</td>\n",
       "      <td>0.224355</td>\n",
       "      <td>0.462033</td>\n",
       "      <td>0.428019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.515200</td>\n",
       "      <td>0.459283</td>\n",
       "      <td>0.181016</td>\n",
       "      <td>0.479065</td>\n",
       "      <td>0.226514</td>\n",
       "      <td>0.470483</td>\n",
       "      <td>0.435489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.450300</td>\n",
       "      <td>0.451198</td>\n",
       "      <td>0.182516</td>\n",
       "      <td>0.480109</td>\n",
       "      <td>0.230331</td>\n",
       "      <td>0.471451</td>\n",
       "      <td>0.437066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.479400</td>\n",
       "      <td>0.446409</td>\n",
       "      <td>0.182690</td>\n",
       "      <td>0.481642</td>\n",
       "      <td>0.230017</td>\n",
       "      <td>0.471942</td>\n",
       "      <td>0.442437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.467600</td>\n",
       "      <td>0.442625</td>\n",
       "      <td>0.180147</td>\n",
       "      <td>0.480115</td>\n",
       "      <td>0.225040</td>\n",
       "      <td>0.469636</td>\n",
       "      <td>0.435405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.436400</td>\n",
       "      <td>0.440135</td>\n",
       "      <td>0.181887</td>\n",
       "      <td>0.483225</td>\n",
       "      <td>0.228498</td>\n",
       "      <td>0.474513</td>\n",
       "      <td>0.437063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.431100</td>\n",
       "      <td>0.436817</td>\n",
       "      <td>0.189557</td>\n",
       "      <td>0.489352</td>\n",
       "      <td>0.237889</td>\n",
       "      <td>0.477820</td>\n",
       "      <td>0.443279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.451200</td>\n",
       "      <td>0.436270</td>\n",
       "      <td>0.189880</td>\n",
       "      <td>0.491639</td>\n",
       "      <td>0.239572</td>\n",
       "      <td>0.482550</td>\n",
       "      <td>0.445829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.401900</td>\n",
       "      <td>0.436827</td>\n",
       "      <td>0.189403</td>\n",
       "      <td>0.498593</td>\n",
       "      <td>0.241997</td>\n",
       "      <td>0.488766</td>\n",
       "      <td>0.453348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.432300</td>\n",
       "      <td>0.433499</td>\n",
       "      <td>0.194818</td>\n",
       "      <td>0.498221</td>\n",
       "      <td>0.243432</td>\n",
       "      <td>0.489836</td>\n",
       "      <td>0.454616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.406500</td>\n",
       "      <td>0.433348</td>\n",
       "      <td>0.190966</td>\n",
       "      <td>0.495571</td>\n",
       "      <td>0.238581</td>\n",
       "      <td>0.487580</td>\n",
       "      <td>0.449274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.400500</td>\n",
       "      <td>0.432015</td>\n",
       "      <td>0.192301</td>\n",
       "      <td>0.498394</td>\n",
       "      <td>0.241170</td>\n",
       "      <td>0.489975</td>\n",
       "      <td>0.451458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.407900</td>\n",
       "      <td>0.429830</td>\n",
       "      <td>0.189965</td>\n",
       "      <td>0.492234</td>\n",
       "      <td>0.237222</td>\n",
       "      <td>0.483882</td>\n",
       "      <td>0.441328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.385800</td>\n",
       "      <td>0.430498</td>\n",
       "      <td>0.192150</td>\n",
       "      <td>0.493296</td>\n",
       "      <td>0.239041</td>\n",
       "      <td>0.484980</td>\n",
       "      <td>0.447838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.374800</td>\n",
       "      <td>0.431363</td>\n",
       "      <td>0.192228</td>\n",
       "      <td>0.493970</td>\n",
       "      <td>0.239327</td>\n",
       "      <td>0.486376</td>\n",
       "      <td>0.449822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.373500</td>\n",
       "      <td>0.430027</td>\n",
       "      <td>0.190315</td>\n",
       "      <td>0.493433</td>\n",
       "      <td>0.237091</td>\n",
       "      <td>0.485045</td>\n",
       "      <td>0.444673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 results: {'eval_loss': 0.42982980608940125, 'eval_bleu': 0.18996510264699404, 'eval_rouge1': 0.49223360125420446, 'eval_rouge2': 0.23722169765415402, 'eval_rougeL': 0.48388211893798094, 'eval_meteor': 0.4413277090105548, 'eval_runtime': 2.136, 'eval_samples_per_second': 56.181, 'eval_steps_per_second': 7.023, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 46011.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 3 saved to crossval_T5_SMALL_TOP10DOCS_TFIDF_fold_3.jsonl\n",
      "Processing Fold 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aa19490c0ab42aa98b3af13bd05ae5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/Etu0/21402600/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recuperation du Fichier FOLD3_training_top10_qulac_en_tfidf_CLEANED_rep_PREPROCESSED_FOR_MODEL.json pour le training du Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_18289/1707016551.py:59: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1140' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1140/1800 07:01 < 04:04, 2.70 it/s, Epoch 19/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.297800</td>\n",
       "      <td>0.731936</td>\n",
       "      <td>0.126301</td>\n",
       "      <td>0.381267</td>\n",
       "      <td>0.179858</td>\n",
       "      <td>0.373153</td>\n",
       "      <td>0.321870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.703300</td>\n",
       "      <td>0.574073</td>\n",
       "      <td>0.146881</td>\n",
       "      <td>0.415194</td>\n",
       "      <td>0.206685</td>\n",
       "      <td>0.407467</td>\n",
       "      <td>0.371068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.581100</td>\n",
       "      <td>0.510318</td>\n",
       "      <td>0.159392</td>\n",
       "      <td>0.465132</td>\n",
       "      <td>0.211824</td>\n",
       "      <td>0.456694</td>\n",
       "      <td>0.414672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.527800</td>\n",
       "      <td>0.491726</td>\n",
       "      <td>0.180640</td>\n",
       "      <td>0.478367</td>\n",
       "      <td>0.225821</td>\n",
       "      <td>0.467191</td>\n",
       "      <td>0.424774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.509300</td>\n",
       "      <td>0.484773</td>\n",
       "      <td>0.185741</td>\n",
       "      <td>0.483315</td>\n",
       "      <td>0.229251</td>\n",
       "      <td>0.472181</td>\n",
       "      <td>0.429126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.474900</td>\n",
       "      <td>0.479646</td>\n",
       "      <td>0.184365</td>\n",
       "      <td>0.483267</td>\n",
       "      <td>0.234725</td>\n",
       "      <td>0.475769</td>\n",
       "      <td>0.435271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.492200</td>\n",
       "      <td>0.475574</td>\n",
       "      <td>0.182123</td>\n",
       "      <td>0.480387</td>\n",
       "      <td>0.229284</td>\n",
       "      <td>0.470951</td>\n",
       "      <td>0.427607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.473500</td>\n",
       "      <td>0.472562</td>\n",
       "      <td>0.187078</td>\n",
       "      <td>0.484187</td>\n",
       "      <td>0.230904</td>\n",
       "      <td>0.474136</td>\n",
       "      <td>0.432485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.455500</td>\n",
       "      <td>0.472388</td>\n",
       "      <td>0.175363</td>\n",
       "      <td>0.477833</td>\n",
       "      <td>0.221130</td>\n",
       "      <td>0.465504</td>\n",
       "      <td>0.425764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.425700</td>\n",
       "      <td>0.471327</td>\n",
       "      <td>0.177726</td>\n",
       "      <td>0.480915</td>\n",
       "      <td>0.223207</td>\n",
       "      <td>0.467263</td>\n",
       "      <td>0.427737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.406500</td>\n",
       "      <td>0.470543</td>\n",
       "      <td>0.179313</td>\n",
       "      <td>0.481292</td>\n",
       "      <td>0.224291</td>\n",
       "      <td>0.469437</td>\n",
       "      <td>0.431774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.399500</td>\n",
       "      <td>0.471410</td>\n",
       "      <td>0.178573</td>\n",
       "      <td>0.478568</td>\n",
       "      <td>0.224512</td>\n",
       "      <td>0.465334</td>\n",
       "      <td>0.430168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.415200</td>\n",
       "      <td>0.468276</td>\n",
       "      <td>0.179853</td>\n",
       "      <td>0.481008</td>\n",
       "      <td>0.225500</td>\n",
       "      <td>0.467810</td>\n",
       "      <td>0.430449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.395800</td>\n",
       "      <td>0.469613</td>\n",
       "      <td>0.183613</td>\n",
       "      <td>0.480916</td>\n",
       "      <td>0.226344</td>\n",
       "      <td>0.467712</td>\n",
       "      <td>0.431365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.420900</td>\n",
       "      <td>0.468770</td>\n",
       "      <td>0.181501</td>\n",
       "      <td>0.482562</td>\n",
       "      <td>0.228450</td>\n",
       "      <td>0.469527</td>\n",
       "      <td>0.433828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.420600</td>\n",
       "      <td>0.467518</td>\n",
       "      <td>0.181658</td>\n",
       "      <td>0.483867</td>\n",
       "      <td>0.228772</td>\n",
       "      <td>0.472039</td>\n",
       "      <td>0.437715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.377100</td>\n",
       "      <td>0.467945</td>\n",
       "      <td>0.180173</td>\n",
       "      <td>0.478744</td>\n",
       "      <td>0.227262</td>\n",
       "      <td>0.466925</td>\n",
       "      <td>0.432502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.381000</td>\n",
       "      <td>0.470478</td>\n",
       "      <td>0.180489</td>\n",
       "      <td>0.478194</td>\n",
       "      <td>0.224972</td>\n",
       "      <td>0.466684</td>\n",
       "      <td>0.429152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.367100</td>\n",
       "      <td>0.469514</td>\n",
       "      <td>0.179347</td>\n",
       "      <td>0.481927</td>\n",
       "      <td>0.226703</td>\n",
       "      <td>0.470089</td>\n",
       "      <td>0.433443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 results: {'eval_loss': 0.4675177037715912, 'eval_bleu': 0.18165823548746218, 'eval_rouge1': 0.4838669405153859, 'eval_rouge2': 0.22877161342976401, 'eval_rougeL': 0.4720390868147756, 'eval_meteor': 0.4377153422351074, 'eval_runtime': 2.168, 'eval_samples_per_second': 55.352, 'eval_steps_per_second': 6.919, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 47193.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 4 saved to crossval_T5_SMALL_TOP10DOCS_TFIDF_fold_4.jsonl\n",
      "Processing Fold 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "846c5df61026406282b518d3b3e7b12c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/Etu0/21402600/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recuperation du Fichier FOLD4_training_top10_qulac_en_tfidf_CLEANED_rep_PREPROCESSED_FOR_MODEL.json pour le training du Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_18289/1707016551.py:59: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1200' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1200/1800 07:31 < 03:46, 2.65 it/s, Epoch 20/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.278100</td>\n",
       "      <td>0.754320</td>\n",
       "      <td>0.148964</td>\n",
       "      <td>0.401739</td>\n",
       "      <td>0.183104</td>\n",
       "      <td>0.396303</td>\n",
       "      <td>0.335676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.749800</td>\n",
       "      <td>0.591912</td>\n",
       "      <td>0.179303</td>\n",
       "      <td>0.441702</td>\n",
       "      <td>0.215773</td>\n",
       "      <td>0.431440</td>\n",
       "      <td>0.381434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.618100</td>\n",
       "      <td>0.514378</td>\n",
       "      <td>0.178830</td>\n",
       "      <td>0.486837</td>\n",
       "      <td>0.219644</td>\n",
       "      <td>0.476465</td>\n",
       "      <td>0.423992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.538700</td>\n",
       "      <td>0.497186</td>\n",
       "      <td>0.187266</td>\n",
       "      <td>0.497588</td>\n",
       "      <td>0.227731</td>\n",
       "      <td>0.485805</td>\n",
       "      <td>0.436870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.491400</td>\n",
       "      <td>0.488342</td>\n",
       "      <td>0.182545</td>\n",
       "      <td>0.498188</td>\n",
       "      <td>0.219236</td>\n",
       "      <td>0.484333</td>\n",
       "      <td>0.431472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.468000</td>\n",
       "      <td>0.481652</td>\n",
       "      <td>0.187216</td>\n",
       "      <td>0.503151</td>\n",
       "      <td>0.228950</td>\n",
       "      <td>0.490709</td>\n",
       "      <td>0.439965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.440900</td>\n",
       "      <td>0.476448</td>\n",
       "      <td>0.192700</td>\n",
       "      <td>0.502934</td>\n",
       "      <td>0.234764</td>\n",
       "      <td>0.494518</td>\n",
       "      <td>0.447551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.424000</td>\n",
       "      <td>0.474212</td>\n",
       "      <td>0.194897</td>\n",
       "      <td>0.509419</td>\n",
       "      <td>0.232908</td>\n",
       "      <td>0.496102</td>\n",
       "      <td>0.447082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.424300</td>\n",
       "      <td>0.470075</td>\n",
       "      <td>0.196470</td>\n",
       "      <td>0.514910</td>\n",
       "      <td>0.238187</td>\n",
       "      <td>0.502295</td>\n",
       "      <td>0.456113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.433700</td>\n",
       "      <td>0.468452</td>\n",
       "      <td>0.200777</td>\n",
       "      <td>0.509978</td>\n",
       "      <td>0.233664</td>\n",
       "      <td>0.499191</td>\n",
       "      <td>0.451000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.421200</td>\n",
       "      <td>0.467713</td>\n",
       "      <td>0.190185</td>\n",
       "      <td>0.505739</td>\n",
       "      <td>0.226728</td>\n",
       "      <td>0.491564</td>\n",
       "      <td>0.446063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.411400</td>\n",
       "      <td>0.467351</td>\n",
       "      <td>0.198371</td>\n",
       "      <td>0.510554</td>\n",
       "      <td>0.234635</td>\n",
       "      <td>0.498232</td>\n",
       "      <td>0.451721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.409600</td>\n",
       "      <td>0.467956</td>\n",
       "      <td>0.198098</td>\n",
       "      <td>0.506739</td>\n",
       "      <td>0.234256</td>\n",
       "      <td>0.495722</td>\n",
       "      <td>0.448602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.395400</td>\n",
       "      <td>0.466364</td>\n",
       "      <td>0.204532</td>\n",
       "      <td>0.510721</td>\n",
       "      <td>0.236474</td>\n",
       "      <td>0.499133</td>\n",
       "      <td>0.452464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.379500</td>\n",
       "      <td>0.467330</td>\n",
       "      <td>0.196531</td>\n",
       "      <td>0.512130</td>\n",
       "      <td>0.236527</td>\n",
       "      <td>0.500405</td>\n",
       "      <td>0.453872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.382500</td>\n",
       "      <td>0.466344</td>\n",
       "      <td>0.203513</td>\n",
       "      <td>0.517495</td>\n",
       "      <td>0.239771</td>\n",
       "      <td>0.503227</td>\n",
       "      <td>0.457627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.382000</td>\n",
       "      <td>0.465216</td>\n",
       "      <td>0.198112</td>\n",
       "      <td>0.511833</td>\n",
       "      <td>0.234026</td>\n",
       "      <td>0.500049</td>\n",
       "      <td>0.455155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.363700</td>\n",
       "      <td>0.465837</td>\n",
       "      <td>0.200416</td>\n",
       "      <td>0.515321</td>\n",
       "      <td>0.237697</td>\n",
       "      <td>0.503921</td>\n",
       "      <td>0.457964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.385500</td>\n",
       "      <td>0.465578</td>\n",
       "      <td>0.200397</td>\n",
       "      <td>0.513028</td>\n",
       "      <td>0.235460</td>\n",
       "      <td>0.501284</td>\n",
       "      <td>0.456641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.365100</td>\n",
       "      <td>0.465820</td>\n",
       "      <td>0.203704</td>\n",
       "      <td>0.517014</td>\n",
       "      <td>0.240551</td>\n",
       "      <td>0.506020</td>\n",
       "      <td>0.460178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 results: {'eval_loss': 0.46521592140197754, 'eval_bleu': 0.1981122765840142, 'eval_rouge1': 0.5118333330408786, 'eval_rouge2': 0.23402551947507522, 'eval_rougeL': 0.5000489220240332, 'eval_meteor': 0.4551551575491318, 'eval_runtime': 2.5417, 'eval_samples_per_second': 46.819, 'eval_steps_per_second': 5.902, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:00<00:00, 44485.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 5 saved to crossval_T5_SMALL_TOP10DOCS_TFIDF_fold_5.jsonl\n",
      "Average metrics over all folds: {'eval_loss': 0.4691967606544495, 'eval_bleu': 0.19577301397006885, 'eval_rouge1': 0.499544828190538, 'eval_rouge2': 0.24715247447005922, 'eval_rougeL': 0.48762726879887647, 'eval_meteor': 0.4539207845136238, 'eval_runtime': 2.2192600000000002, 'eval_samples_per_second': 54.2562, 'eval_steps_per_second': 6.792, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Averages/epoch</td><td>▁</td></tr><tr><td>Averages/eval_bleu</td><td>▁</td></tr><tr><td>Averages/eval_loss</td><td>▁</td></tr><tr><td>Averages/eval_meteor</td><td>▁</td></tr><tr><td>Averages/eval_rouge1</td><td>▁</td></tr><tr><td>Averages/eval_rouge2</td><td>▁</td></tr><tr><td>Averages/eval_rougeL</td><td>▁</td></tr><tr><td>Averages/eval_runtime</td><td>▁</td></tr><tr><td>Averages/eval_samples_per_second</td><td>▁</td></tr><tr><td>Averages/eval_steps_per_second</td><td>▁</td></tr><tr><td>Fold_1/eval/epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇███</td></tr><tr><td>Fold_1/eval/eval_bleu</td><td>▁▃▅▆▆▆▇███▇▇████▇▇▇▇▇▇</td></tr><tr><td>Fold_1/eval/eval_loss</td><td>█▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_1/eval/eval_meteor</td><td>▁▄▆▇▇▇█████▇██████████</td></tr><tr><td>Fold_1/eval/eval_rouge1</td><td>▁▃▆▇▇▇█████▇████▇██▇██</td></tr><tr><td>Fold_1/eval/eval_rouge2</td><td>▁▃▄▆▆▆▇█▇▇▇▆▇███▇▇▇▇█▇</td></tr><tr><td>Fold_1/eval/eval_rougeL</td><td>▁▃▆▇▇▇█████▇███▇▇▇▇▇█▇</td></tr><tr><td>Fold_1/eval/eval_runtime</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_1/eval/eval_samples_per_second</td><td>▁█████████████████████</td></tr><tr><td>Fold_1/eval/eval_steps_per_second</td><td>▁█████████████████████</td></tr><tr><td>Fold_2/eval/epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇███</td></tr><tr><td>Fold_2/eval/eval_bleu</td><td>▁▂▄▅▆▅▇▆▇▇▇▇█████</td></tr><tr><td>Fold_2/eval/eval_loss</td><td>█▅▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_2/eval/eval_meteor</td><td>▁▂▅▆▇▇▇▇█▇███████</td></tr><tr><td>Fold_2/eval/eval_rouge1</td><td>▁▂▅▅▆▆▇▇█▇███████</td></tr><tr><td>Fold_2/eval/eval_rouge2</td><td>▁▂▄▄▅▅▆▇▇▇▇▇███▇█</td></tr><tr><td>Fold_2/eval/eval_rougeL</td><td>▁▂▅▅▆▇▇▇█▇▇██████</td></tr><tr><td>Fold_2/eval/eval_runtime</td><td>█▂▂▂▁▂▁▃▁▃▁▂▁▃▁▁▂</td></tr><tr><td>Fold_2/eval/eval_samples_per_second</td><td>▁▇▇▇█▇█▆█▆█▇█▆██▇</td></tr><tr><td>Fold_2/eval/eval_steps_per_second</td><td>▁▇▇▇█▇█▆█▆█▇█▆██▇</td></tr><tr><td>Fold_3/eval/epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇███</td></tr><tr><td>Fold_3/eval/eval_bleu</td><td>▁▂▅▆▆▇▇▆▇▇▇▇███▇███▇</td></tr><tr><td>Fold_3/eval/eval_loss</td><td>█▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_3/eval/eval_meteor</td><td>▁▃▆▇▇▇▇▇▇▇█████▇██▇▇</td></tr><tr><td>Fold_3/eval/eval_rouge1</td><td>▁▃▆▆▇▇▇▇▇▇██████████</td></tr><tr><td>Fold_3/eval/eval_rouge2</td><td>▁▂▅▆▆▇▇▆▇▇█████▇██▇▇</td></tr><tr><td>Fold_3/eval/eval_rougeL</td><td>▁▃▆▆▇▇▇▇▇▇██████████</td></tr><tr><td>Fold_3/eval/eval_runtime</td><td>█▂▂▂▃▂█▂▂▇▁▃▁▇▂▁▇▁▃▆</td></tr><tr><td>Fold_3/eval/eval_samples_per_second</td><td>▁▇▇▇▆▇▁▇▇▂█▆█▂▇█▂█▆▃</td></tr><tr><td>Fold_3/eval/eval_steps_per_second</td><td>▁▇▇▇▆▇▁▇▇▂█▆█▂▇█▂█▆▃</td></tr><tr><td>Fold_4/eval/epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇███</td></tr><tr><td>Fold_4/eval/eval_bleu</td><td>▁▃▅▇██▇█▇▇▇▇▇█▇▇▇▇▇▇</td></tr><tr><td>Fold_4/eval/eval_loss</td><td>█▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_4/eval/eval_meteor</td><td>▁▄▇▇▇█▇█▇▇███████▇██</td></tr><tr><td>Fold_4/eval/eval_rouge1</td><td>▁▃▇█████████████████</td></tr><tr><td>Fold_4/eval/eval_rouge2</td><td>▁▄▅▇▇█▇█▆▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>Fold_4/eval/eval_rougeL</td><td>▁▃▇▇████▇▇█▇▇▇██▇▇██</td></tr><tr><td>Fold_4/eval/eval_runtime</td><td>█▄▂▃▂▂▂▂▁▂▂▃▁▂▂▂▂▂▂▇</td></tr><tr><td>Fold_4/eval/eval_samples_per_second</td><td>▁▅▇▆▇▇▇▇█▇▇▆█▇▇▇▇▇▇▂</td></tr><tr><td>Fold_4/eval/eval_steps_per_second</td><td>▁▅▇▆▇▇▇▇█▇▇▆█▇▇▇▇▇▇▂</td></tr><tr><td>Fold_5/eval/epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇███</td></tr><tr><td>Fold_5/eval/eval_bleu</td><td>▁▅▅▆▅▆▇▇▇█▆▇▇█▇█▇▇▇█▇</td></tr><tr><td>Fold_5/eval/eval_loss</td><td>█▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_5/eval/eval_meteor</td><td>▁▄▆▇▆▇▇▇█▇▇█▇████████</td></tr><tr><td>Fold_5/eval/eval_rouge1</td><td>▁▃▆▇▇▇▇███▇█▇████████</td></tr><tr><td>Fold_5/eval/eval_rouge2</td><td>▁▅▅▆▅▇▇▇█▇▆▇▇███▇█▇█▇</td></tr><tr><td>Fold_5/eval/eval_rougeL</td><td>▁▃▆▇▇▇▇▇██▇█▇████████</td></tr><tr><td>Fold_5/eval/eval_runtime</td><td>▂▁▁▂▁▁▁▅█████████████</td></tr><tr><td>Fold_5/eval/eval_samples_per_second</td><td>▇██▇▇▇▇▄▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_5/eval/eval_steps_per_second</td><td>▇██▇▇▇▇▄▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/bleu</td><td>▂▄▅▅▆▆▆▆▆▆▆▆▁▄▅▇███▅▅▆▆▆▆▅▅▅▅▄▅▅▅▅▅▅▅▅▆▆</td></tr><tr><td>eval/loss</td><td>█▃▃▃▃▂▂▂▂▂▂█▅▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▃▂▂▂▂▂█▂▂▂▂▂</td></tr><tr><td>eval/meteor</td><td>▁▃▅▆▆▆▆▆▆▆▆▇▇████▁▅▆▆▇▇▆▆▆▁▃▅▆▆▆▆▆▇▆▆▇▇▇</td></tr><tr><td>eval/rouge1</td><td>▅▆▇▆▇▆▆▇██████▅▆▆▆▆▇▇▇▇▇▁▅▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>eval/rouge2</td><td>▂▃▄▅▅▆▅▅▅▅▆▇██▃▅▅▅▅▅▅▅▅▁▃▄▄▄▄▄▁▃▄▅▄▄▅▄▄▄</td></tr><tr><td>eval/rougeL</td><td>▅▆▆▆▇▆▆▆▆▆▆▃▅▇███▁▅▆▇▇▇▇▇▅▆▆▆▆▆▆▆▇▇█▇███</td></tr><tr><td>eval/runtime</td><td>▂▂▂▂▂▂▂▂▃▁▁▁▁▁▁▁▂▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂███████</td></tr><tr><td>eval/samples_per_second</td><td>▁████████████████████████████████▇█▆▆▆▆▆</td></tr><tr><td>eval/steps_per_second</td><td>▇▇▇▇▇▇▇▇▇▇█████▇▇█▇██▇██▇█▇█▇▇▇▇▇▇▇▁▁▁▁▁</td></tr><tr><td>test/bleu</td><td>▃█▃▁▄</td></tr><tr><td>test/loss</td><td>█▇▁▅▅</td></tr><tr><td>test/meteor</td><td>▂█▂▁▃</td></tr><tr><td>test/rouge1</td><td>▂█▃▁▆</td></tr><tr><td>test/rouge2</td><td>▃█▂▁▂</td></tr><tr><td>test/rougeL</td><td>▁█▃▁▆</td></tr><tr><td>test/runtime</td><td>▂▁▂▂█</td></tr><tr><td>test/samples_per_second</td><td>▇█▇▇▁</td></tr><tr><td>test/steps_per_second</td><td>▇█▇▇▁</td></tr><tr><td>train/epoch</td><td>▂▂▃▄▅▆▇▇██▁▂▂▃▄▄▄▄▄▅▆▇▁▂▃▄▄▆▇▂▃▃▆█▁▃▄▄▅█</td></tr><tr><td>train/global_step</td><td>▁▂▃▄▄▅▅▅▅▆▇▇▁▂▃▄▅▅▅▅▃▄▄▄▅▆▆▆▇▇▇▁▂▅▇▁▂▄▄█</td></tr><tr><td>train/grad_norm</td><td>▅▂▁▂▁▁▁▁▁▁▂▂▁▁▂▁▄▂▁▁▂▁▁▂▂▅▁▂▂▂▅▄█▃▃▁▂▁▁▁</td></tr><tr><td>train/learning_rate</td><td>██▆▆▆▅▄▄▃▃▁▁▁█▇▇▅▅▄▄▄▃█▆▅▅▄▂▂▇▆▅▅▃█▇▆▆▄▁</td></tr><tr><td>train/loss</td><td>▃▂▂▂▂▁▂▂▂▁▁█▃▂▂▂▁▁▁▁▁▂▂▂▂▁▂▁▁▁▁▁▇▃▂▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Averages/epoch</td><td>19</td></tr><tr><td>Averages/eval_bleu</td><td>0.19577</td></tr><tr><td>Averages/eval_loss</td><td>0.4692</td></tr><tr><td>Averages/eval_meteor</td><td>0.45392</td></tr><tr><td>Averages/eval_rouge1</td><td>0.49954</td></tr><tr><td>Averages/eval_rouge2</td><td>0.24715</td></tr><tr><td>Averages/eval_rougeL</td><td>0.48763</td></tr><tr><td>Averages/eval_runtime</td><td>2.21926</td></tr><tr><td>Averages/eval_samples_per_second</td><td>54.2562</td></tr><tr><td>Averages/eval_steps_per_second</td><td>6.792</td></tr><tr><td>Fold_1/eval/epoch</td><td>21</td></tr><tr><td>Fold_1/eval/eval_bleu</td><td>0.19168</td></tr><tr><td>Fold_1/eval/eval_loss</td><td>0.49905</td></tr><tr><td>Fold_1/eval/eval_meteor</td><td>0.44799</td></tr><tr><td>Fold_1/eval/eval_rouge1</td><td>0.48845</td></tr><tr><td>Fold_1/eval/eval_rouge2</td><td>0.24419</td></tr><tr><td>Fold_1/eval/eval_rougeL</td><td>0.47372</td></tr><tr><td>Fold_1/eval/eval_runtime</td><td>2.1367</td></tr><tr><td>Fold_1/eval/eval_samples_per_second</td><td>56.163</td></tr><tr><td>Fold_1/eval/eval_steps_per_second</td><td>7.02</td></tr><tr><td>Fold_2/eval/epoch</td><td>16</td></tr><tr><td>Fold_2/eval/eval_bleu</td><td>0.21745</td></tr><tr><td>Fold_2/eval/eval_loss</td><td>0.48437</td></tr><tr><td>Fold_2/eval/eval_meteor</td><td>0.48742</td></tr><tr><td>Fold_2/eval/eval_rouge1</td><td>0.52134</td></tr><tr><td>Fold_2/eval/eval_rouge2</td><td>0.29156</td></tr><tr><td>Fold_2/eval/eval_rougeL</td><td>0.50845</td></tr><tr><td>Fold_2/eval/eval_runtime</td><td>2.1139</td></tr><tr><td>Fold_2/eval/eval_samples_per_second</td><td>56.766</td></tr><tr><td>Fold_2/eval/eval_steps_per_second</td><td>7.096</td></tr><tr><td>Fold_3/eval/epoch</td><td>19</td></tr><tr><td>Fold_3/eval/eval_bleu</td><td>0.18997</td></tr><tr><td>Fold_3/eval/eval_loss</td><td>0.42983</td></tr><tr><td>Fold_3/eval/eval_meteor</td><td>0.44133</td></tr><tr><td>Fold_3/eval/eval_rouge1</td><td>0.49223</td></tr><tr><td>Fold_3/eval/eval_rouge2</td><td>0.23722</td></tr><tr><td>Fold_3/eval/eval_rougeL</td><td>0.48388</td></tr><tr><td>Fold_3/eval/eval_runtime</td><td>2.136</td></tr><tr><td>Fold_3/eval/eval_samples_per_second</td><td>56.181</td></tr><tr><td>Fold_3/eval/eval_steps_per_second</td><td>7.023</td></tr><tr><td>Fold_4/eval/epoch</td><td>19</td></tr><tr><td>Fold_4/eval/eval_bleu</td><td>0.18166</td></tr><tr><td>Fold_4/eval/eval_loss</td><td>0.46752</td></tr><tr><td>Fold_4/eval/eval_meteor</td><td>0.43772</td></tr><tr><td>Fold_4/eval/eval_rouge1</td><td>0.48387</td></tr><tr><td>Fold_4/eval/eval_rouge2</td><td>0.22877</td></tr><tr><td>Fold_4/eval/eval_rougeL</td><td>0.47204</td></tr><tr><td>Fold_4/eval/eval_runtime</td><td>2.168</td></tr><tr><td>Fold_4/eval/eval_samples_per_second</td><td>55.352</td></tr><tr><td>Fold_4/eval/eval_steps_per_second</td><td>6.919</td></tr><tr><td>Fold_5/eval/epoch</td><td>20</td></tr><tr><td>Fold_5/eval/eval_bleu</td><td>0.19811</td></tr><tr><td>Fold_5/eval/eval_loss</td><td>0.46522</td></tr><tr><td>Fold_5/eval/eval_meteor</td><td>0.45516</td></tr><tr><td>Fold_5/eval/eval_rouge1</td><td>0.51183</td></tr><tr><td>Fold_5/eval/eval_rouge2</td><td>0.23403</td></tr><tr><td>Fold_5/eval/eval_rougeL</td><td>0.50005</td></tr><tr><td>Fold_5/eval/eval_runtime</td><td>2.5417</td></tr><tr><td>Fold_5/eval/eval_samples_per_second</td><td>46.819</td></tr><tr><td>Fold_5/eval/eval_steps_per_second</td><td>5.902</td></tr><tr><td>eval/bleu</td><td>0.19811</td></tr><tr><td>eval/loss</td><td>0.46522</td></tr><tr><td>eval/meteor</td><td>0.45516</td></tr><tr><td>eval/rouge1</td><td>0.51183</td></tr><tr><td>eval/rouge2</td><td>0.23403</td></tr><tr><td>eval/rougeL</td><td>0.50005</td></tr><tr><td>eval/runtime</td><td>2.5417</td></tr><tr><td>eval/samples_per_second</td><td>46.819</td></tr><tr><td>eval/steps_per_second</td><td>5.902</td></tr><tr><td>test/bleu</td><td>0.19811</td></tr><tr><td>test/loss</td><td>0.46522</td></tr><tr><td>test/meteor</td><td>0.45516</td></tr><tr><td>test/rouge1</td><td>0.51183</td></tr><tr><td>test/rouge2</td><td>0.23403</td></tr><tr><td>test/rougeL</td><td>0.50005</td></tr><tr><td>test/runtime</td><td>2.535</td></tr><tr><td>test/samples_per_second</td><td>46.944</td></tr><tr><td>test/steps_per_second</td><td>5.917</td></tr><tr><td>total_flos</td><td>1299281294131200.0</td></tr><tr><td>train/epoch</td><td>20</td></tr><tr><td>train/global_step</td><td>1200</td></tr><tr><td>train/grad_norm</td><td>2.23891</td></tr><tr><td>train/learning_rate</td><td>2e-05</td></tr><tr><td>train/loss</td><td>0.3651</td></tr><tr><td>train_loss</td><td>0.69422</td></tr><tr><td>train_runtime</td><td>451.7174</td></tr><tr><td>train_samples_per_second</td><td>31.878</td></tr><tr><td>train_steps_per_second</td><td>3.985</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">T5_SMALL_TOP10DOCS_TFIDF</strong> at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/0tncng9c' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/0tncng9c</a><br> View project at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250510_235611-0tncng9c/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final predicitons for all folds foudn in crossval_T5_SMALL_TOP10DOCS_TFIDF.jsonl\n"
     ]
    }
   ],
   "source": [
    "cross_val_train(modelname=\"t5-small\", run_name=\"T5_SMALL_TOP10DOCS_TFIDF\",filename=\"crossval_T5_SMALL_TOP10DOCS_TFIDF.jsonl\", \n",
    "                tokenizer_global=tokenizer_global, tokenized_dataset_global=tokenized_dataset_global, \n",
    "                folds=folds, nb_epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c88ae2c-7531-4d0b-ae15-841a8523eb93",
   "metadata": {},
   "source": [
    "# TEST TOP 5 DOC WITH LDA representation(top 3 topics, 6 words per topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62feb128-fc6f-4dec-95a7-dff1fe4527b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/users/nfs/Etu0/21402600/wandb/run-20250511_232557-vru2jrkd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/vru2jrkd' target=\"_blank\">T5_SMALL_TOP5DOCS_LDA</a></strong> to <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/vru2jrkd' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/vru2jrkd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Fold 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31b693d06e0746dc883e42c7856e494c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recuperation du Fichier FOLD0_training_top5_qulac_en_LDA_rep_PREPROCESSED_FOR_MODEL.json pour le training du Fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1746998759.694531    3576 gpu_device.cc:2341] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_3576/3001149367.py:59: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [450/450 06:46, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>21.841900</td>\n",
       "      <td>7.619828</td>\n",
       "      <td>0.017142</td>\n",
       "      <td>0.379858</td>\n",
       "      <td>0.174562</td>\n",
       "      <td>0.363956</td>\n",
       "      <td>0.211098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>11.342300</td>\n",
       "      <td>0.907027</td>\n",
       "      <td>0.116155</td>\n",
       "      <td>0.376647</td>\n",
       "      <td>0.165702</td>\n",
       "      <td>0.361249</td>\n",
       "      <td>0.298095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.456500</td>\n",
       "      <td>0.821207</td>\n",
       "      <td>0.128128</td>\n",
       "      <td>0.397227</td>\n",
       "      <td>0.181379</td>\n",
       "      <td>0.382465</td>\n",
       "      <td>0.320607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.241000</td>\n",
       "      <td>0.786604</td>\n",
       "      <td>0.129720</td>\n",
       "      <td>0.408453</td>\n",
       "      <td>0.184315</td>\n",
       "      <td>0.396296</td>\n",
       "      <td>0.337304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.241000</td>\n",
       "      <td>0.726773</td>\n",
       "      <td>0.137507</td>\n",
       "      <td>0.422170</td>\n",
       "      <td>0.193836</td>\n",
       "      <td>0.409772</td>\n",
       "      <td>0.357320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.868100</td>\n",
       "      <td>0.679802</td>\n",
       "      <td>0.141635</td>\n",
       "      <td>0.425540</td>\n",
       "      <td>0.202337</td>\n",
       "      <td>0.414602</td>\n",
       "      <td>0.358842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.744400</td>\n",
       "      <td>0.613774</td>\n",
       "      <td>0.148365</td>\n",
       "      <td>0.431258</td>\n",
       "      <td>0.202982</td>\n",
       "      <td>0.420614</td>\n",
       "      <td>0.364893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.644300</td>\n",
       "      <td>0.582682</td>\n",
       "      <td>0.168106</td>\n",
       "      <td>0.458259</td>\n",
       "      <td>0.221986</td>\n",
       "      <td>0.447450</td>\n",
       "      <td>0.394347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.644300</td>\n",
       "      <td>0.560229</td>\n",
       "      <td>0.171907</td>\n",
       "      <td>0.471660</td>\n",
       "      <td>0.223225</td>\n",
       "      <td>0.459810</td>\n",
       "      <td>0.412100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.584400</td>\n",
       "      <td>0.551621</td>\n",
       "      <td>0.178832</td>\n",
       "      <td>0.473128</td>\n",
       "      <td>0.225187</td>\n",
       "      <td>0.463300</td>\n",
       "      <td>0.416329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.560300</td>\n",
       "      <td>0.544855</td>\n",
       "      <td>0.182909</td>\n",
       "      <td>0.479563</td>\n",
       "      <td>0.230498</td>\n",
       "      <td>0.469343</td>\n",
       "      <td>0.422695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.528500</td>\n",
       "      <td>0.541110</td>\n",
       "      <td>0.182484</td>\n",
       "      <td>0.478046</td>\n",
       "      <td>0.229621</td>\n",
       "      <td>0.466957</td>\n",
       "      <td>0.421916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.528500</td>\n",
       "      <td>0.537300</td>\n",
       "      <td>0.182559</td>\n",
       "      <td>0.475593</td>\n",
       "      <td>0.228987</td>\n",
       "      <td>0.464163</td>\n",
       "      <td>0.420466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.513100</td>\n",
       "      <td>0.534436</td>\n",
       "      <td>0.182334</td>\n",
       "      <td>0.474667</td>\n",
       "      <td>0.228041</td>\n",
       "      <td>0.463299</td>\n",
       "      <td>0.418778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.515300</td>\n",
       "      <td>0.532060</td>\n",
       "      <td>0.182233</td>\n",
       "      <td>0.475830</td>\n",
       "      <td>0.227720</td>\n",
       "      <td>0.463338</td>\n",
       "      <td>0.419365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.507700</td>\n",
       "      <td>0.529119</td>\n",
       "      <td>0.182273</td>\n",
       "      <td>0.473481</td>\n",
       "      <td>0.223867</td>\n",
       "      <td>0.460609</td>\n",
       "      <td>0.416234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.507700</td>\n",
       "      <td>0.527215</td>\n",
       "      <td>0.184808</td>\n",
       "      <td>0.474676</td>\n",
       "      <td>0.225467</td>\n",
       "      <td>0.462154</td>\n",
       "      <td>0.417419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.495300</td>\n",
       "      <td>0.525413</td>\n",
       "      <td>0.185652</td>\n",
       "      <td>0.476322</td>\n",
       "      <td>0.228637</td>\n",
       "      <td>0.462990</td>\n",
       "      <td>0.420315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.491500</td>\n",
       "      <td>0.524446</td>\n",
       "      <td>0.183360</td>\n",
       "      <td>0.476020</td>\n",
       "      <td>0.226859</td>\n",
       "      <td>0.462828</td>\n",
       "      <td>0.419853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.486900</td>\n",
       "      <td>0.522618</td>\n",
       "      <td>0.184084</td>\n",
       "      <td>0.475662</td>\n",
       "      <td>0.228955</td>\n",
       "      <td>0.461996</td>\n",
       "      <td>0.421420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.486900</td>\n",
       "      <td>0.521093</td>\n",
       "      <td>0.184335</td>\n",
       "      <td>0.477394</td>\n",
       "      <td>0.226962</td>\n",
       "      <td>0.463392</td>\n",
       "      <td>0.424345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.485000</td>\n",
       "      <td>0.520132</td>\n",
       "      <td>0.184331</td>\n",
       "      <td>0.477031</td>\n",
       "      <td>0.227795</td>\n",
       "      <td>0.462417</td>\n",
       "      <td>0.421542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.480300</td>\n",
       "      <td>0.519860</td>\n",
       "      <td>0.183660</td>\n",
       "      <td>0.475858</td>\n",
       "      <td>0.227127</td>\n",
       "      <td>0.461359</td>\n",
       "      <td>0.420847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.474000</td>\n",
       "      <td>0.519314</td>\n",
       "      <td>0.184806</td>\n",
       "      <td>0.480312</td>\n",
       "      <td>0.229509</td>\n",
       "      <td>0.465989</td>\n",
       "      <td>0.425318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.474000</td>\n",
       "      <td>0.518496</td>\n",
       "      <td>0.184779</td>\n",
       "      <td>0.480422</td>\n",
       "      <td>0.229586</td>\n",
       "      <td>0.466122</td>\n",
       "      <td>0.425242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.463600</td>\n",
       "      <td>0.518036</td>\n",
       "      <td>0.185966</td>\n",
       "      <td>0.481451</td>\n",
       "      <td>0.231507</td>\n",
       "      <td>0.467760</td>\n",
       "      <td>0.426287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.481500</td>\n",
       "      <td>0.517707</td>\n",
       "      <td>0.186020</td>\n",
       "      <td>0.481586</td>\n",
       "      <td>0.231507</td>\n",
       "      <td>0.467761</td>\n",
       "      <td>0.426332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>0.517462</td>\n",
       "      <td>0.186020</td>\n",
       "      <td>0.481586</td>\n",
       "      <td>0.231507</td>\n",
       "      <td>0.467761</td>\n",
       "      <td>0.426332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>0.517263</td>\n",
       "      <td>0.185945</td>\n",
       "      <td>0.480861</td>\n",
       "      <td>0.231507</td>\n",
       "      <td>0.467037</td>\n",
       "      <td>0.425825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.471900</td>\n",
       "      <td>0.517215</td>\n",
       "      <td>0.185945</td>\n",
       "      <td>0.480861</td>\n",
       "      <td>0.231507</td>\n",
       "      <td>0.467037</td>\n",
       "      <td>0.425825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 results: {'eval_loss': 0.5172153115272522, 'eval_bleu': 0.18594487195574103, 'eval_rouge1': 0.4808612306793142, 'eval_rouge2': 0.2315065025447109, 'eval_rougeL': 0.4670373113433284, 'eval_meteor': 0.4258253334829576, 'eval_runtime': 1.5547, 'eval_samples_per_second': 77.185, 'eval_steps_per_second': 2.573, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 87351.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 1 saved to crossval_T5_SMALL_TOP5DOCS_LDA_fold_1.jsonl\n",
      "Processing Fold 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cb1ad2a234048b8b535b0159a6b9fa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/Etu0/21402600/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recuperation du Fichier FOLD1_training_top5_qulac_en_LDA_rep_PREPROCESSED_FOR_MODEL.json pour le training du Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_3576/3001149367.py:59: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [450/450 06:41, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>21.446500</td>\n",
       "      <td>8.242983</td>\n",
       "      <td>0.020150</td>\n",
       "      <td>0.394151</td>\n",
       "      <td>0.187676</td>\n",
       "      <td>0.378662</td>\n",
       "      <td>0.222835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>11.512600</td>\n",
       "      <td>0.952353</td>\n",
       "      <td>0.121146</td>\n",
       "      <td>0.363009</td>\n",
       "      <td>0.184732</td>\n",
       "      <td>0.354778</td>\n",
       "      <td>0.304760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.536700</td>\n",
       "      <td>0.800659</td>\n",
       "      <td>0.128239</td>\n",
       "      <td>0.386520</td>\n",
       "      <td>0.185174</td>\n",
       "      <td>0.379448</td>\n",
       "      <td>0.324793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.245100</td>\n",
       "      <td>0.777868</td>\n",
       "      <td>0.131190</td>\n",
       "      <td>0.395071</td>\n",
       "      <td>0.194737</td>\n",
       "      <td>0.389239</td>\n",
       "      <td>0.341934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.245100</td>\n",
       "      <td>0.708336</td>\n",
       "      <td>0.137817</td>\n",
       "      <td>0.408486</td>\n",
       "      <td>0.202052</td>\n",
       "      <td>0.400584</td>\n",
       "      <td>0.355210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.872700</td>\n",
       "      <td>0.673443</td>\n",
       "      <td>0.142035</td>\n",
       "      <td>0.421170</td>\n",
       "      <td>0.211538</td>\n",
       "      <td>0.411589</td>\n",
       "      <td>0.373849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.751400</td>\n",
       "      <td>0.617608</td>\n",
       "      <td>0.152412</td>\n",
       "      <td>0.422549</td>\n",
       "      <td>0.219723</td>\n",
       "      <td>0.415572</td>\n",
       "      <td>0.382129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.658800</td>\n",
       "      <td>0.571437</td>\n",
       "      <td>0.174728</td>\n",
       "      <td>0.457396</td>\n",
       "      <td>0.233214</td>\n",
       "      <td>0.450375</td>\n",
       "      <td>0.417822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.658800</td>\n",
       "      <td>0.550408</td>\n",
       "      <td>0.178386</td>\n",
       "      <td>0.467257</td>\n",
       "      <td>0.236080</td>\n",
       "      <td>0.458644</td>\n",
       "      <td>0.429141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>0.541360</td>\n",
       "      <td>0.179167</td>\n",
       "      <td>0.472382</td>\n",
       "      <td>0.237367</td>\n",
       "      <td>0.464284</td>\n",
       "      <td>0.433438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.565600</td>\n",
       "      <td>0.534492</td>\n",
       "      <td>0.180566</td>\n",
       "      <td>0.476728</td>\n",
       "      <td>0.238247</td>\n",
       "      <td>0.467750</td>\n",
       "      <td>0.438106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.531200</td>\n",
       "      <td>0.530833</td>\n",
       "      <td>0.179531</td>\n",
       "      <td>0.477577</td>\n",
       "      <td>0.237580</td>\n",
       "      <td>0.470271</td>\n",
       "      <td>0.438328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.531200</td>\n",
       "      <td>0.526566</td>\n",
       "      <td>0.182901</td>\n",
       "      <td>0.482589</td>\n",
       "      <td>0.243583</td>\n",
       "      <td>0.474775</td>\n",
       "      <td>0.442700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.529400</td>\n",
       "      <td>0.523578</td>\n",
       "      <td>0.186392</td>\n",
       "      <td>0.482705</td>\n",
       "      <td>0.245351</td>\n",
       "      <td>0.475013</td>\n",
       "      <td>0.443599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.513300</td>\n",
       "      <td>0.520580</td>\n",
       "      <td>0.185131</td>\n",
       "      <td>0.483268</td>\n",
       "      <td>0.243955</td>\n",
       "      <td>0.475608</td>\n",
       "      <td>0.443459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.499700</td>\n",
       "      <td>0.518351</td>\n",
       "      <td>0.184477</td>\n",
       "      <td>0.489000</td>\n",
       "      <td>0.247207</td>\n",
       "      <td>0.480076</td>\n",
       "      <td>0.446886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.499700</td>\n",
       "      <td>0.516744</td>\n",
       "      <td>0.183937</td>\n",
       "      <td>0.487573</td>\n",
       "      <td>0.245677</td>\n",
       "      <td>0.477402</td>\n",
       "      <td>0.446248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.507400</td>\n",
       "      <td>0.514991</td>\n",
       "      <td>0.186283</td>\n",
       "      <td>0.488621</td>\n",
       "      <td>0.247095</td>\n",
       "      <td>0.479275</td>\n",
       "      <td>0.447100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.486300</td>\n",
       "      <td>0.513724</td>\n",
       "      <td>0.188039</td>\n",
       "      <td>0.490563</td>\n",
       "      <td>0.249295</td>\n",
       "      <td>0.481411</td>\n",
       "      <td>0.452775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.491500</td>\n",
       "      <td>0.512712</td>\n",
       "      <td>0.187765</td>\n",
       "      <td>0.493951</td>\n",
       "      <td>0.255205</td>\n",
       "      <td>0.484554</td>\n",
       "      <td>0.457984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.491500</td>\n",
       "      <td>0.511601</td>\n",
       "      <td>0.187073</td>\n",
       "      <td>0.494579</td>\n",
       "      <td>0.255358</td>\n",
       "      <td>0.485433</td>\n",
       "      <td>0.457469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.492000</td>\n",
       "      <td>0.511029</td>\n",
       "      <td>0.189330</td>\n",
       "      <td>0.497011</td>\n",
       "      <td>0.258803</td>\n",
       "      <td>0.487510</td>\n",
       "      <td>0.460124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.510266</td>\n",
       "      <td>0.191770</td>\n",
       "      <td>0.497105</td>\n",
       "      <td>0.260285</td>\n",
       "      <td>0.487903</td>\n",
       "      <td>0.460614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.479700</td>\n",
       "      <td>0.509911</td>\n",
       "      <td>0.191983</td>\n",
       "      <td>0.496954</td>\n",
       "      <td>0.260047</td>\n",
       "      <td>0.487610</td>\n",
       "      <td>0.461163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.479700</td>\n",
       "      <td>0.509424</td>\n",
       "      <td>0.191845</td>\n",
       "      <td>0.497937</td>\n",
       "      <td>0.260216</td>\n",
       "      <td>0.487931</td>\n",
       "      <td>0.460833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.477300</td>\n",
       "      <td>0.509055</td>\n",
       "      <td>0.191327</td>\n",
       "      <td>0.496717</td>\n",
       "      <td>0.260893</td>\n",
       "      <td>0.487406</td>\n",
       "      <td>0.459592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.478200</td>\n",
       "      <td>0.508665</td>\n",
       "      <td>0.193095</td>\n",
       "      <td>0.497387</td>\n",
       "      <td>0.262589</td>\n",
       "      <td>0.488390</td>\n",
       "      <td>0.460761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.472200</td>\n",
       "      <td>0.508493</td>\n",
       "      <td>0.193069</td>\n",
       "      <td>0.496829</td>\n",
       "      <td>0.261686</td>\n",
       "      <td>0.487857</td>\n",
       "      <td>0.459979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.472200</td>\n",
       "      <td>0.508360</td>\n",
       "      <td>0.193623</td>\n",
       "      <td>0.497032</td>\n",
       "      <td>0.261876</td>\n",
       "      <td>0.488032</td>\n",
       "      <td>0.460687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.476300</td>\n",
       "      <td>0.508323</td>\n",
       "      <td>0.193623</td>\n",
       "      <td>0.497032</td>\n",
       "      <td>0.261876</td>\n",
       "      <td>0.488032</td>\n",
       "      <td>0.460687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 results: {'eval_loss': 0.508323073387146, 'eval_bleu': 0.19362256605258873, 'eval_rouge1': 0.49703235062266754, 'eval_rouge2': 0.26187573269934605, 'eval_rougeL': 0.48803188291314126, 'eval_meteor': 0.4606869309595393, 'eval_runtime': 1.2772, 'eval_samples_per_second': 93.957, 'eval_steps_per_second': 3.132, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 89272.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 2 saved to crossval_T5_SMALL_TOP5DOCS_LDA_fold_2.jsonl\n",
      "Processing Fold 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be51dca226e4488da4f9af52951debd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/Etu0/21402600/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recuperation du Fichier FOLD2_training_top5_qulac_en_LDA_rep_PREPROCESSED_FOR_MODEL.json pour le training du Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_3576/3001149367.py:59: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [450/450 07:33, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>20.237500</td>\n",
       "      <td>5.970094</td>\n",
       "      <td>0.016865</td>\n",
       "      <td>0.346695</td>\n",
       "      <td>0.152966</td>\n",
       "      <td>0.329960</td>\n",
       "      <td>0.202874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>10.150200</td>\n",
       "      <td>0.869684</td>\n",
       "      <td>0.112672</td>\n",
       "      <td>0.359318</td>\n",
       "      <td>0.161666</td>\n",
       "      <td>0.347334</td>\n",
       "      <td>0.288345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.189800</td>\n",
       "      <td>0.742886</td>\n",
       "      <td>0.118481</td>\n",
       "      <td>0.366009</td>\n",
       "      <td>0.155630</td>\n",
       "      <td>0.355385</td>\n",
       "      <td>0.303116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.214500</td>\n",
       "      <td>0.727712</td>\n",
       "      <td>0.140337</td>\n",
       "      <td>0.395882</td>\n",
       "      <td>0.187965</td>\n",
       "      <td>0.386130</td>\n",
       "      <td>0.341384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.214500</td>\n",
       "      <td>0.672525</td>\n",
       "      <td>0.143688</td>\n",
       "      <td>0.408349</td>\n",
       "      <td>0.192223</td>\n",
       "      <td>0.403565</td>\n",
       "      <td>0.357327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.891200</td>\n",
       "      <td>0.640938</td>\n",
       "      <td>0.144356</td>\n",
       "      <td>0.406472</td>\n",
       "      <td>0.186771</td>\n",
       "      <td>0.400491</td>\n",
       "      <td>0.353440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.761700</td>\n",
       "      <td>0.567290</td>\n",
       "      <td>0.154711</td>\n",
       "      <td>0.415250</td>\n",
       "      <td>0.199507</td>\n",
       "      <td>0.409380</td>\n",
       "      <td>0.367869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.688400</td>\n",
       "      <td>0.531394</td>\n",
       "      <td>0.162858</td>\n",
       "      <td>0.438652</td>\n",
       "      <td>0.203702</td>\n",
       "      <td>0.431181</td>\n",
       "      <td>0.384733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.688400</td>\n",
       "      <td>0.507322</td>\n",
       "      <td>0.166022</td>\n",
       "      <td>0.448172</td>\n",
       "      <td>0.203315</td>\n",
       "      <td>0.439948</td>\n",
       "      <td>0.393320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.619600</td>\n",
       "      <td>0.495140</td>\n",
       "      <td>0.176589</td>\n",
       "      <td>0.461586</td>\n",
       "      <td>0.210521</td>\n",
       "      <td>0.453509</td>\n",
       "      <td>0.411362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.567400</td>\n",
       "      <td>0.489654</td>\n",
       "      <td>0.176504</td>\n",
       "      <td>0.459333</td>\n",
       "      <td>0.209867</td>\n",
       "      <td>0.451602</td>\n",
       "      <td>0.411204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.557400</td>\n",
       "      <td>0.486094</td>\n",
       "      <td>0.177594</td>\n",
       "      <td>0.462313</td>\n",
       "      <td>0.215207</td>\n",
       "      <td>0.455305</td>\n",
       "      <td>0.414130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.557400</td>\n",
       "      <td>0.481098</td>\n",
       "      <td>0.175957</td>\n",
       "      <td>0.459987</td>\n",
       "      <td>0.210383</td>\n",
       "      <td>0.453330</td>\n",
       "      <td>0.414469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.539300</td>\n",
       "      <td>0.477016</td>\n",
       "      <td>0.182018</td>\n",
       "      <td>0.463493</td>\n",
       "      <td>0.216412</td>\n",
       "      <td>0.456608</td>\n",
       "      <td>0.419155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.528200</td>\n",
       "      <td>0.473803</td>\n",
       "      <td>0.181496</td>\n",
       "      <td>0.469379</td>\n",
       "      <td>0.219035</td>\n",
       "      <td>0.462778</td>\n",
       "      <td>0.425588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.516500</td>\n",
       "      <td>0.470210</td>\n",
       "      <td>0.178186</td>\n",
       "      <td>0.468335</td>\n",
       "      <td>0.214840</td>\n",
       "      <td>0.461266</td>\n",
       "      <td>0.422751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.516500</td>\n",
       "      <td>0.467575</td>\n",
       "      <td>0.179451</td>\n",
       "      <td>0.470694</td>\n",
       "      <td>0.218888</td>\n",
       "      <td>0.463346</td>\n",
       "      <td>0.428482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.514600</td>\n",
       "      <td>0.466047</td>\n",
       "      <td>0.180699</td>\n",
       "      <td>0.473776</td>\n",
       "      <td>0.223810</td>\n",
       "      <td>0.468040</td>\n",
       "      <td>0.434436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.511700</td>\n",
       "      <td>0.464786</td>\n",
       "      <td>0.186597</td>\n",
       "      <td>0.477740</td>\n",
       "      <td>0.229293</td>\n",
       "      <td>0.471875</td>\n",
       "      <td>0.436793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.501200</td>\n",
       "      <td>0.463664</td>\n",
       "      <td>0.183901</td>\n",
       "      <td>0.478563</td>\n",
       "      <td>0.228308</td>\n",
       "      <td>0.470880</td>\n",
       "      <td>0.437128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.501200</td>\n",
       "      <td>0.462411</td>\n",
       "      <td>0.179299</td>\n",
       "      <td>0.473530</td>\n",
       "      <td>0.224404</td>\n",
       "      <td>0.467704</td>\n",
       "      <td>0.434688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.493900</td>\n",
       "      <td>0.461635</td>\n",
       "      <td>0.180940</td>\n",
       "      <td>0.476094</td>\n",
       "      <td>0.226518</td>\n",
       "      <td>0.469556</td>\n",
       "      <td>0.435275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.502600</td>\n",
       "      <td>0.460503</td>\n",
       "      <td>0.180479</td>\n",
       "      <td>0.475115</td>\n",
       "      <td>0.225023</td>\n",
       "      <td>0.467699</td>\n",
       "      <td>0.433418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.490100</td>\n",
       "      <td>0.459750</td>\n",
       "      <td>0.180924</td>\n",
       "      <td>0.475824</td>\n",
       "      <td>0.224073</td>\n",
       "      <td>0.468012</td>\n",
       "      <td>0.433411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.490100</td>\n",
       "      <td>0.459120</td>\n",
       "      <td>0.184179</td>\n",
       "      <td>0.478928</td>\n",
       "      <td>0.227724</td>\n",
       "      <td>0.470649</td>\n",
       "      <td>0.437149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.481200</td>\n",
       "      <td>0.458700</td>\n",
       "      <td>0.184406</td>\n",
       "      <td>0.477507</td>\n",
       "      <td>0.228731</td>\n",
       "      <td>0.470930</td>\n",
       "      <td>0.439551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.458321</td>\n",
       "      <td>0.184584</td>\n",
       "      <td>0.476782</td>\n",
       "      <td>0.229605</td>\n",
       "      <td>0.470691</td>\n",
       "      <td>0.440180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.490500</td>\n",
       "      <td>0.458011</td>\n",
       "      <td>0.184072</td>\n",
       "      <td>0.475694</td>\n",
       "      <td>0.228573</td>\n",
       "      <td>0.469176</td>\n",
       "      <td>0.437712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.490500</td>\n",
       "      <td>0.457880</td>\n",
       "      <td>0.184555</td>\n",
       "      <td>0.477138</td>\n",
       "      <td>0.230404</td>\n",
       "      <td>0.471071</td>\n",
       "      <td>0.439156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.484200</td>\n",
       "      <td>0.457844</td>\n",
       "      <td>0.184555</td>\n",
       "      <td>0.477138</td>\n",
       "      <td>0.230404</td>\n",
       "      <td>0.471071</td>\n",
       "      <td>0.439156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 results: {'eval_loss': 0.4578440487384796, 'eval_bleu': 0.18455458792022789, 'eval_rouge1': 0.47713755349643555, 'eval_rouge2': 0.230404433651611, 'eval_rougeL': 0.47107075828922357, 'eval_meteor': 0.4391563586856478, 'eval_runtime': 2.2996, 'eval_samples_per_second': 52.182, 'eval_steps_per_second': 1.739, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 87003.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 3 saved to crossval_T5_SMALL_TOP5DOCS_LDA_fold_3.jsonl\n",
      "Processing Fold 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b326e3e9b314eddad0cecdb9172941b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/Etu0/21402600/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recuperation du Fichier FOLD3_training_top5_qulac_en_LDA_rep_PREPROCESSED_FOR_MODEL.json pour le training du Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_3576/3001149367.py:59: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [450/450 06:59, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>21.472200</td>\n",
       "      <td>7.023256</td>\n",
       "      <td>0.017318</td>\n",
       "      <td>0.371339</td>\n",
       "      <td>0.168137</td>\n",
       "      <td>0.364998</td>\n",
       "      <td>0.204400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>10.834500</td>\n",
       "      <td>0.861281</td>\n",
       "      <td>0.113591</td>\n",
       "      <td>0.382541</td>\n",
       "      <td>0.179289</td>\n",
       "      <td>0.370901</td>\n",
       "      <td>0.304575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.214800</td>\n",
       "      <td>0.751446</td>\n",
       "      <td>0.113753</td>\n",
       "      <td>0.390501</td>\n",
       "      <td>0.173084</td>\n",
       "      <td>0.382305</td>\n",
       "      <td>0.320311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.188500</td>\n",
       "      <td>0.744219</td>\n",
       "      <td>0.120998</td>\n",
       "      <td>0.391220</td>\n",
       "      <td>0.172200</td>\n",
       "      <td>0.383903</td>\n",
       "      <td>0.328401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.188500</td>\n",
       "      <td>0.666104</td>\n",
       "      <td>0.122261</td>\n",
       "      <td>0.394144</td>\n",
       "      <td>0.174705</td>\n",
       "      <td>0.387172</td>\n",
       "      <td>0.341122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.875100</td>\n",
       "      <td>0.618021</td>\n",
       "      <td>0.137836</td>\n",
       "      <td>0.410389</td>\n",
       "      <td>0.196059</td>\n",
       "      <td>0.401939</td>\n",
       "      <td>0.358767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.741400</td>\n",
       "      <td>0.567508</td>\n",
       "      <td>0.147499</td>\n",
       "      <td>0.428533</td>\n",
       "      <td>0.206842</td>\n",
       "      <td>0.421020</td>\n",
       "      <td>0.375213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.676800</td>\n",
       "      <td>0.535746</td>\n",
       "      <td>0.153571</td>\n",
       "      <td>0.452470</td>\n",
       "      <td>0.210004</td>\n",
       "      <td>0.446132</td>\n",
       "      <td>0.398960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.676800</td>\n",
       "      <td>0.519634</td>\n",
       "      <td>0.164220</td>\n",
       "      <td>0.468984</td>\n",
       "      <td>0.211300</td>\n",
       "      <td>0.460991</td>\n",
       "      <td>0.413548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.605300</td>\n",
       "      <td>0.511587</td>\n",
       "      <td>0.174599</td>\n",
       "      <td>0.474199</td>\n",
       "      <td>0.212661</td>\n",
       "      <td>0.465398</td>\n",
       "      <td>0.417328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.571100</td>\n",
       "      <td>0.506621</td>\n",
       "      <td>0.162266</td>\n",
       "      <td>0.470475</td>\n",
       "      <td>0.206399</td>\n",
       "      <td>0.462726</td>\n",
       "      <td>0.411803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.545200</td>\n",
       "      <td>0.503147</td>\n",
       "      <td>0.167820</td>\n",
       "      <td>0.472699</td>\n",
       "      <td>0.212254</td>\n",
       "      <td>0.466041</td>\n",
       "      <td>0.416124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.545200</td>\n",
       "      <td>0.500585</td>\n",
       "      <td>0.164808</td>\n",
       "      <td>0.474231</td>\n",
       "      <td>0.212974</td>\n",
       "      <td>0.466623</td>\n",
       "      <td>0.416872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.530900</td>\n",
       "      <td>0.498259</td>\n",
       "      <td>0.163091</td>\n",
       "      <td>0.473186</td>\n",
       "      <td>0.213059</td>\n",
       "      <td>0.467569</td>\n",
       "      <td>0.415657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.515700</td>\n",
       "      <td>0.495936</td>\n",
       "      <td>0.160458</td>\n",
       "      <td>0.472825</td>\n",
       "      <td>0.211611</td>\n",
       "      <td>0.466219</td>\n",
       "      <td>0.415647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.524400</td>\n",
       "      <td>0.494074</td>\n",
       "      <td>0.161689</td>\n",
       "      <td>0.474048</td>\n",
       "      <td>0.212146</td>\n",
       "      <td>0.467986</td>\n",
       "      <td>0.415933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.524400</td>\n",
       "      <td>0.492607</td>\n",
       "      <td>0.161412</td>\n",
       "      <td>0.472043</td>\n",
       "      <td>0.210921</td>\n",
       "      <td>0.466821</td>\n",
       "      <td>0.415910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.512900</td>\n",
       "      <td>0.491122</td>\n",
       "      <td>0.163416</td>\n",
       "      <td>0.472830</td>\n",
       "      <td>0.212421</td>\n",
       "      <td>0.467131</td>\n",
       "      <td>0.414615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.497800</td>\n",
       "      <td>0.489999</td>\n",
       "      <td>0.164195</td>\n",
       "      <td>0.474255</td>\n",
       "      <td>0.213740</td>\n",
       "      <td>0.468657</td>\n",
       "      <td>0.418271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.494600</td>\n",
       "      <td>0.489394</td>\n",
       "      <td>0.160640</td>\n",
       "      <td>0.473606</td>\n",
       "      <td>0.212691</td>\n",
       "      <td>0.467440</td>\n",
       "      <td>0.416266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.494600</td>\n",
       "      <td>0.488382</td>\n",
       "      <td>0.162132</td>\n",
       "      <td>0.475143</td>\n",
       "      <td>0.209824</td>\n",
       "      <td>0.467803</td>\n",
       "      <td>0.415875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.503700</td>\n",
       "      <td>0.487536</td>\n",
       "      <td>0.162381</td>\n",
       "      <td>0.476533</td>\n",
       "      <td>0.209297</td>\n",
       "      <td>0.468496</td>\n",
       "      <td>0.417195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.485400</td>\n",
       "      <td>0.487057</td>\n",
       "      <td>0.162079</td>\n",
       "      <td>0.474965</td>\n",
       "      <td>0.208866</td>\n",
       "      <td>0.467220</td>\n",
       "      <td>0.415490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.483700</td>\n",
       "      <td>0.486778</td>\n",
       "      <td>0.160174</td>\n",
       "      <td>0.475390</td>\n",
       "      <td>0.207614</td>\n",
       "      <td>0.466614</td>\n",
       "      <td>0.415169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.483700</td>\n",
       "      <td>0.486361</td>\n",
       "      <td>0.159453</td>\n",
       "      <td>0.474663</td>\n",
       "      <td>0.206504</td>\n",
       "      <td>0.465027</td>\n",
       "      <td>0.413986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.481000</td>\n",
       "      <td>0.486110</td>\n",
       "      <td>0.159260</td>\n",
       "      <td>0.474287</td>\n",
       "      <td>0.207132</td>\n",
       "      <td>0.464580</td>\n",
       "      <td>0.413467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.488300</td>\n",
       "      <td>0.485868</td>\n",
       "      <td>0.157190</td>\n",
       "      <td>0.472831</td>\n",
       "      <td>0.205096</td>\n",
       "      <td>0.463774</td>\n",
       "      <td>0.411500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.482800</td>\n",
       "      <td>0.485691</td>\n",
       "      <td>0.157190</td>\n",
       "      <td>0.472831</td>\n",
       "      <td>0.205096</td>\n",
       "      <td>0.463774</td>\n",
       "      <td>0.411500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.482800</td>\n",
       "      <td>0.485571</td>\n",
       "      <td>0.157190</td>\n",
       "      <td>0.472831</td>\n",
       "      <td>0.205096</td>\n",
       "      <td>0.463774</td>\n",
       "      <td>0.411500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.483100</td>\n",
       "      <td>0.485527</td>\n",
       "      <td>0.157190</td>\n",
       "      <td>0.472831</td>\n",
       "      <td>0.205096</td>\n",
       "      <td>0.463774</td>\n",
       "      <td>0.411500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 results: {'eval_loss': 0.48552700877189636, 'eval_bleu': 0.1571904975902457, 'eval_rouge1': 0.4728309168353976, 'eval_rouge2': 0.20509646289512456, 'eval_rougeL': 0.46377411138885394, 'eval_meteor': 0.41150041821941036, 'eval_runtime': 1.5923, 'eval_samples_per_second': 75.363, 'eval_steps_per_second': 2.512, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 94786.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 4 saved to crossval_T5_SMALL_TOP5DOCS_LDA_fold_4.jsonl\n",
      "Processing Fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8152a94a10c24e6a82fba40c3cb61095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/Etu0/21402600/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recuperation du Fichier FOLD4_training_top5_qulac_en_LDA_rep_PREPROCESSED_FOR_MODEL.json pour le training du Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_3576/3001149367.py:59: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [450/450 08:46, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>20.312000</td>\n",
       "      <td>7.759392</td>\n",
       "      <td>0.020784</td>\n",
       "      <td>0.390907</td>\n",
       "      <td>0.177154</td>\n",
       "      <td>0.380286</td>\n",
       "      <td>0.213989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>11.166700</td>\n",
       "      <td>0.887190</td>\n",
       "      <td>0.133620</td>\n",
       "      <td>0.384584</td>\n",
       "      <td>0.168662</td>\n",
       "      <td>0.370681</td>\n",
       "      <td>0.301913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.469500</td>\n",
       "      <td>0.790362</td>\n",
       "      <td>0.143686</td>\n",
       "      <td>0.401302</td>\n",
       "      <td>0.181673</td>\n",
       "      <td>0.390463</td>\n",
       "      <td>0.322873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.244100</td>\n",
       "      <td>0.764614</td>\n",
       "      <td>0.152261</td>\n",
       "      <td>0.400311</td>\n",
       "      <td>0.187710</td>\n",
       "      <td>0.389662</td>\n",
       "      <td>0.341708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.244100</td>\n",
       "      <td>0.714197</td>\n",
       "      <td>0.155647</td>\n",
       "      <td>0.416686</td>\n",
       "      <td>0.189391</td>\n",
       "      <td>0.406215</td>\n",
       "      <td>0.358127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.894100</td>\n",
       "      <td>0.654604</td>\n",
       "      <td>0.160860</td>\n",
       "      <td>0.425202</td>\n",
       "      <td>0.194895</td>\n",
       "      <td>0.414105</td>\n",
       "      <td>0.367045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.778100</td>\n",
       "      <td>0.614332</td>\n",
       "      <td>0.173964</td>\n",
       "      <td>0.431894</td>\n",
       "      <td>0.211039</td>\n",
       "      <td>0.424560</td>\n",
       "      <td>0.380113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.676600</td>\n",
       "      <td>0.556903</td>\n",
       "      <td>0.184221</td>\n",
       "      <td>0.459502</td>\n",
       "      <td>0.219576</td>\n",
       "      <td>0.451763</td>\n",
       "      <td>0.406424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.676600</td>\n",
       "      <td>0.533175</td>\n",
       "      <td>0.190632</td>\n",
       "      <td>0.476361</td>\n",
       "      <td>0.223161</td>\n",
       "      <td>0.468004</td>\n",
       "      <td>0.421654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.611300</td>\n",
       "      <td>0.521100</td>\n",
       "      <td>0.184000</td>\n",
       "      <td>0.483062</td>\n",
       "      <td>0.218490</td>\n",
       "      <td>0.475560</td>\n",
       "      <td>0.426873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.574800</td>\n",
       "      <td>0.514763</td>\n",
       "      <td>0.184123</td>\n",
       "      <td>0.486751</td>\n",
       "      <td>0.219632</td>\n",
       "      <td>0.476496</td>\n",
       "      <td>0.429353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.548300</td>\n",
       "      <td>0.510458</td>\n",
       "      <td>0.182187</td>\n",
       "      <td>0.488381</td>\n",
       "      <td>0.219967</td>\n",
       "      <td>0.477697</td>\n",
       "      <td>0.431840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.548300</td>\n",
       "      <td>0.506125</td>\n",
       "      <td>0.184663</td>\n",
       "      <td>0.488711</td>\n",
       "      <td>0.222695</td>\n",
       "      <td>0.478978</td>\n",
       "      <td>0.437631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.530600</td>\n",
       "      <td>0.502912</td>\n",
       "      <td>0.188110</td>\n",
       "      <td>0.492525</td>\n",
       "      <td>0.225706</td>\n",
       "      <td>0.482685</td>\n",
       "      <td>0.440950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.529200</td>\n",
       "      <td>0.500400</td>\n",
       "      <td>0.188410</td>\n",
       "      <td>0.493551</td>\n",
       "      <td>0.227507</td>\n",
       "      <td>0.484689</td>\n",
       "      <td>0.440264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.512000</td>\n",
       "      <td>0.498722</td>\n",
       "      <td>0.183349</td>\n",
       "      <td>0.492343</td>\n",
       "      <td>0.224164</td>\n",
       "      <td>0.482732</td>\n",
       "      <td>0.438448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.512000</td>\n",
       "      <td>0.496368</td>\n",
       "      <td>0.187047</td>\n",
       "      <td>0.491712</td>\n",
       "      <td>0.224974</td>\n",
       "      <td>0.482872</td>\n",
       "      <td>0.439216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.514800</td>\n",
       "      <td>0.494466</td>\n",
       "      <td>0.186778</td>\n",
       "      <td>0.493874</td>\n",
       "      <td>0.224210</td>\n",
       "      <td>0.483057</td>\n",
       "      <td>0.439022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.498300</td>\n",
       "      <td>0.493111</td>\n",
       "      <td>0.186753</td>\n",
       "      <td>0.493060</td>\n",
       "      <td>0.223832</td>\n",
       "      <td>0.482258</td>\n",
       "      <td>0.438889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.499800</td>\n",
       "      <td>0.492122</td>\n",
       "      <td>0.189814</td>\n",
       "      <td>0.493558</td>\n",
       "      <td>0.224769</td>\n",
       "      <td>0.482345</td>\n",
       "      <td>0.440681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.499800</td>\n",
       "      <td>0.491003</td>\n",
       "      <td>0.190063</td>\n",
       "      <td>0.495003</td>\n",
       "      <td>0.225333</td>\n",
       "      <td>0.484366</td>\n",
       "      <td>0.443205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.492000</td>\n",
       "      <td>0.490232</td>\n",
       "      <td>0.188388</td>\n",
       "      <td>0.495005</td>\n",
       "      <td>0.224599</td>\n",
       "      <td>0.484427</td>\n",
       "      <td>0.442671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.490500</td>\n",
       "      <td>0.489624</td>\n",
       "      <td>0.188807</td>\n",
       "      <td>0.495917</td>\n",
       "      <td>0.226412</td>\n",
       "      <td>0.485913</td>\n",
       "      <td>0.443927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.486900</td>\n",
       "      <td>0.488856</td>\n",
       "      <td>0.190806</td>\n",
       "      <td>0.497523</td>\n",
       "      <td>0.227433</td>\n",
       "      <td>0.487436</td>\n",
       "      <td>0.446290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.486900</td>\n",
       "      <td>0.488109</td>\n",
       "      <td>0.189547</td>\n",
       "      <td>0.497333</td>\n",
       "      <td>0.226854</td>\n",
       "      <td>0.486800</td>\n",
       "      <td>0.445352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.479900</td>\n",
       "      <td>0.487440</td>\n",
       "      <td>0.191252</td>\n",
       "      <td>0.498850</td>\n",
       "      <td>0.229659</td>\n",
       "      <td>0.488758</td>\n",
       "      <td>0.447280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.492000</td>\n",
       "      <td>0.487045</td>\n",
       "      <td>0.191893</td>\n",
       "      <td>0.499930</td>\n",
       "      <td>0.231034</td>\n",
       "      <td>0.489470</td>\n",
       "      <td>0.448148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.477700</td>\n",
       "      <td>0.486849</td>\n",
       "      <td>0.191893</td>\n",
       "      <td>0.499692</td>\n",
       "      <td>0.231009</td>\n",
       "      <td>0.489139</td>\n",
       "      <td>0.448564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.477700</td>\n",
       "      <td>0.486667</td>\n",
       "      <td>0.191893</td>\n",
       "      <td>0.499692</td>\n",
       "      <td>0.231009</td>\n",
       "      <td>0.489139</td>\n",
       "      <td>0.448564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.478100</td>\n",
       "      <td>0.486634</td>\n",
       "      <td>0.191893</td>\n",
       "      <td>0.499692</td>\n",
       "      <td>0.231009</td>\n",
       "      <td>0.489139</td>\n",
       "      <td>0.448564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 results: {'eval_loss': 0.4866338074207306, 'eval_bleu': 0.19189267017712466, 'eval_rouge1': 0.49969230881278925, 'eval_rouge2': 0.2310091500467428, 'eval_rougeL': 0.48913889814836475, 'eval_meteor': 0.448563693788046, 'eval_runtime': 1.3225, 'eval_samples_per_second': 89.983, 'eval_steps_per_second': 3.025, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:00<00:00, 83159.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 5 saved to crossval_T5_SMALL_TOP5DOCS_LDA_fold_5.jsonl\n",
      "Average metrics over all folds: {'eval_loss': 0.491108649969101, 'eval_bleu': 0.1826410387391856, 'eval_rouge1': 0.48551087208932076, 'eval_rouge2': 0.23197845636750708, 'eval_rougeL': 0.47581059241658236, 'eval_meteor': 0.4371465470271202, 'eval_runtime': 1.6092600000000001, 'eval_samples_per_second': 77.73400000000001, 'eval_steps_per_second': 2.5962, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Averages/epoch</td><td>▁</td></tr><tr><td>Averages/eval_bleu</td><td>▁</td></tr><tr><td>Averages/eval_loss</td><td>▁</td></tr><tr><td>Averages/eval_meteor</td><td>▁</td></tr><tr><td>Averages/eval_rouge1</td><td>▁</td></tr><tr><td>Averages/eval_rouge2</td><td>▁</td></tr><tr><td>Averages/eval_rougeL</td><td>▁</td></tr><tr><td>Averages/eval_runtime</td><td>▁</td></tr><tr><td>Averages/eval_samples_per_second</td><td>▁</td></tr><tr><td>Averages/eval_steps_per_second</td><td>▁</td></tr><tr><td>Fold_1/eval/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>Fold_1/eval/eval_bleu</td><td>▁▅▆▆▆▆▆▇▇██████████████████████</td></tr><tr><td>Fold_1/eval/eval_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_1/eval/eval_meteor</td><td>▁▄▅▅▆▆▆▇███████████████████████</td></tr><tr><td>Fold_1/eval/eval_rouge1</td><td>▁▁▂▃▄▄▅▆▇▇█████▇███████████████</td></tr><tr><td>Fold_1/eval/eval_rouge2</td><td>▂▁▃▃▄▅▅▇▇▇█████▇▇██████████████</td></tr><tr><td>Fold_1/eval/eval_rougeL</td><td>▁▁▂▃▄▄▅▇▇██████▇██████▇████████</td></tr><tr><td>Fold_1/eval/eval_runtime</td><td>█▂▂▁▁▁▃▂▂▁▁▁▂▂▁▂▂▁▁▁▁▂▂▁▁▂▂▃▁▁▂</td></tr><tr><td>Fold_1/eval/eval_samples_per_second</td><td>▁▆▆█▇▇▄▆▆▇▇█▆▆▇▆▆▇▇▇▇▆▆▇▇▆▆▄█▇▆</td></tr><tr><td>Fold_1/eval/eval_steps_per_second</td><td>▁▆▆█▇▇▄▆▆▇▇█▆▆▇▆▆▇▇▇▇▆▆▇▇▆▆▄█▇▆</td></tr><tr><td>Fold_2/eval/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>Fold_2/eval/eval_bleu</td><td>▁▅▅▅▆▆▆▇▇▇▇▇███████████████████</td></tr><tr><td>Fold_2/eval/eval_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_2/eval/eval_meteor</td><td>▁▃▄▄▅▅▆▇▇▇▇▇▇▇▇████████████████</td></tr><tr><td>Fold_2/eval/eval_rouge1</td><td>▃▁▂▃▃▄▄▆▆▇▇▇▇▇▇█▇██████████████</td></tr><tr><td>Fold_2/eval/eval_rouge2</td><td>▁▁▁▂▃▃▄▅▆▆▆▆▆▆▆▇▆▇▇▇▇██████████</td></tr><tr><td>Fold_2/eval/eval_rougeL</td><td>▂▁▂▃▃▄▄▆▆▇▇▇▇▇▇█▇██████████████</td></tr><tr><td>Fold_2/eval/eval_runtime</td><td>▄▄▃▂▂▃█▃▁▁▁▄▄▄▂▂▂▄▄▄▁▁▂▃▂▄▂▁▄▃▁</td></tr><tr><td>Fold_2/eval/eval_samples_per_second</td><td>▄▄▅▇▇▅▁▅███▅▄▄▇▇▇▄▄▅██▇▅▇▅▇▇▄▅█</td></tr><tr><td>Fold_2/eval/eval_steps_per_second</td><td>▄▄▅▇▇▅▁▅███▅▄▄▇▇▇▄▄▅██▇▅▇▅▇▇▄▅█</td></tr><tr><td>Fold_3/eval/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>Fold_3/eval/eval_bleu</td><td>▁▅▅▆▆▆▇▇▇██████████████████████</td></tr><tr><td>Fold_3/eval/eval_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_3/eval/eval_meteor</td><td>▁▄▄▅▆▅▆▆▇▇▇▇▇▇█▇███████████████</td></tr><tr><td>Fold_3/eval/eval_rouge1</td><td>▁▂▂▄▄▄▅▆▆▇▇▇▇▇▇▇███████████████</td></tr><tr><td>Fold_3/eval/eval_rouge2</td><td>▁▂▁▄▅▄▅▆▆▆▆▇▆▇▇▇▇▇██▇██▇███████</td></tr><tr><td>Fold_3/eval/eval_rougeL</td><td>▁▂▂▄▅▄▅▆▆▇▇▇▇▇█▇███████████████</td></tr><tr><td>Fold_3/eval/eval_runtime</td><td>▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▃▁▁▁█▁▁▁▁▂</td></tr><tr><td>Fold_3/eval/eval_samples_per_second</td><td>▆██▅▆▇███▇▆▆██▅▇▆▆██▂▂█▆▆▁▇▆██▅</td></tr><tr><td>Fold_3/eval/eval_steps_per_second</td><td>▆██▅▆▇███▇▆▆██▅▇▆▆██▂▂█▆▆▁▇▆██▅</td></tr><tr><td>Fold_4/eval/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>Fold_4/eval/eval_bleu</td><td>▁▅▅▆▆▆▇▇██▇██▇▇▇▇██▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>Fold_4/eval/eval_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_4/eval/eval_meteor</td><td>▁▄▅▅▅▆▇▇███████████████████████</td></tr><tr><td>Fold_4/eval/eval_rouge1</td><td>▁▂▂▂▃▄▅▆▇██████████████████████</td></tr><tr><td>Fold_4/eval/eval_rouge2</td><td>▁▃▂▂▂▅▇▇██▇█████████▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>Fold_4/eval/eval_rougeL</td><td>▁▁▂▂▂▃▅▆▇██████████████████████</td></tr><tr><td>Fold_4/eval/eval_runtime</td><td>█▅▂▅▆▆▅▂▆▆▆▅▅▂▆▆▃▅▅▅▆▆▄▂▅▇▆▅▁▅▅</td></tr><tr><td>Fold_4/eval/eval_samples_per_second</td><td>▁▄▇▄▂▃▄▇▃▃▃▄▃▇▃▃▅▄▃▄▃▂▅▇▄▂▃▃█▄▄</td></tr><tr><td>Fold_4/eval/eval_steps_per_second</td><td>▁▄▇▄▂▃▄▇▃▃▃▄▃▇▃▃▅▄▃▄▃▂▅▇▄▂▃▃█▄▄</td></tr><tr><td>Fold_5/eval/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>Fold_5/eval/eval_bleu</td><td>▁▆▆▆▇▇▇████████████████████████</td></tr><tr><td>Fold_5/eval/eval_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_5/eval/eval_meteor</td><td>▁▄▄▅▅▆▆▇▇▇▇████████████████████</td></tr><tr><td>Fold_5/eval/eval_rouge1</td><td>▁▁▂▂▃▃▄▆▇▇▇▇▇██████████████████</td></tr><tr><td>Fold_5/eval/eval_rouge2</td><td>▂▁▂▃▃▄▆▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇████████</td></tr><tr><td>Fold_5/eval/eval_rougeL</td><td>▂▁▂▂▃▄▄▆▇▇▇▇▇██████████████████</td></tr><tr><td>Fold_5/eval/eval_runtime</td><td>▆▅▂▄▄▃▅▅▆▂▂▁▂█▁▁▂▁▁▁▁▂▄▅▅▄▄▃▃▅▂</td></tr><tr><td>Fold_5/eval/eval_samples_per_second</td><td>▂▃▇▄▄▆▃▄▃▇▇█▇▁▇▇▇▇▇▇▇▇▄▃▄▄▄▅▅▃▇</td></tr><tr><td>Fold_5/eval/eval_steps_per_second</td><td>▂▃▇▄▄▆▃▄▃▇▇█▇▁▇▇▇▇▇▇▇▇▄▃▄▄▄▅▅▃▇</td></tr><tr><td>eval/bleu</td><td>▁▂▄▆▇▇▂▂▄▄▇▇▇██▁▆▇▇▇▇▇▄▅▅▆▆▅▅▅▅▄▄▅▆▇████</td></tr><tr><td>eval/loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/meteor</td><td>▂▄▅▆▆▇▆▇▄▅▇▇████▁▂▅▆▇▇▇▇▅▆▆▆▆▆▆▆▆▆▆▇▇▇▇█</td></tr><tr><td>eval/rouge1</td><td>▄▄▇▇▇▇▇▃▂▃▄▇▇▇▇███▁▅▆▇▇▇▃▇▇▇▇▇▇▇▇▂▄▇▇███</td></tr><tr><td>eval/rouge2</td><td>▂▃▄▄▅▆▆▆▆▆▃▇▇▇█████▁▄▄▅▅▅▅▅▅▅▅▅▅▄▄▃▅▆▆▆▆</td></tr><tr><td>eval/rougeL</td><td>▂▄▄▇▇▇▇▇▇▃▇█▇█████▁▁▆▆▇▇▇▇▇▂▃▃▇▇▇▇▂▄▇███</td></tr><tr><td>eval/runtime</td><td>▄▄▂▁▄▂▃▂█▄▂▂▁▄▅▃▄▄▂▄▅▃▄▄▅▄▃▄▄▅▄▁▅▄▄▁▁▂▄▂</td></tr><tr><td>eval/samples_per_second</td><td>▂█▇█▇█▇▇▆█▇▇█▅▇▇▇█▇█▁▇▇▅██▆▆▆▇▇▆▇▆▆███▆▆</td></tr><tr><td>eval/steps_per_second</td><td>▆█▆▆▆▆█▇█▆██▇██▇▁▇▆▆▄▆▆▇▆▆▆▇▅▆█▇▆▆████▆▆</td></tr><tr><td>test/bleu</td><td>▇█▆▁█</td></tr><tr><td>test/loss</td><td>█▇▁▄▄</td></tr><tr><td>test/meteor</td><td>▃█▅▁▆</td></tr><tr><td>test/rouge1</td><td>▃▇▂▁█</td></tr><tr><td>test/rouge2</td><td>▄█▄▁▄</td></tr><tr><td>test/rougeL</td><td>▂█▃▁█</td></tr><tr><td>test/runtime</td><td>▄▇▁█▆</td></tr><tr><td>test/samples_per_second</td><td>▅▂█▁▂</td></tr><tr><td>test/steps_per_second</td><td>▅▂█▁▂</td></tr><tr><td>train/epoch</td><td>▁▃▄▄▅▇█▁▃▃▅▅▇▇██▁▂▂▂▄▅▆▇▇▂▂▄▅▅▆▇▇▇█▄▄▅▅█</td></tr><tr><td>train/global_step</td><td>▂▃▄▅▅▅▆▇▁▂▅▅▆▇█▄▄▅▇▇█▁▄▄▅▅▆▇█▁▂▃▄▅▅▇▇▇██</td></tr><tr><td>train/grad_norm</td><td>▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆▄▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>█▆▆▅▅▃▃▂▁█▆▆▅▃▂▁██▇▇▅▅▄▃▂█▇▇▇▅██▇▆▅▅▄▃▃▂</td></tr><tr><td>train/loss</td><td>█▂▁▁▁▁▁▁▁▁▁▅▂▁▁▁▁▁▁▁▄▂▁▁▁▁▁▁▁▁▁▁▇▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Averages/epoch</td><td>30</td></tr><tr><td>Averages/eval_bleu</td><td>0.18264</td></tr><tr><td>Averages/eval_loss</td><td>0.49111</td></tr><tr><td>Averages/eval_meteor</td><td>0.43715</td></tr><tr><td>Averages/eval_rouge1</td><td>0.48551</td></tr><tr><td>Averages/eval_rouge2</td><td>0.23198</td></tr><tr><td>Averages/eval_rougeL</td><td>0.47581</td></tr><tr><td>Averages/eval_runtime</td><td>1.60926</td></tr><tr><td>Averages/eval_samples_per_second</td><td>77.734</td></tr><tr><td>Averages/eval_steps_per_second</td><td>2.5962</td></tr><tr><td>Fold_1/eval/epoch</td><td>30</td></tr><tr><td>Fold_1/eval/eval_bleu</td><td>0.18594</td></tr><tr><td>Fold_1/eval/eval_loss</td><td>0.51722</td></tr><tr><td>Fold_1/eval/eval_meteor</td><td>0.42583</td></tr><tr><td>Fold_1/eval/eval_rouge1</td><td>0.48086</td></tr><tr><td>Fold_1/eval/eval_rouge2</td><td>0.23151</td></tr><tr><td>Fold_1/eval/eval_rougeL</td><td>0.46704</td></tr><tr><td>Fold_1/eval/eval_runtime</td><td>1.5547</td></tr><tr><td>Fold_1/eval/eval_samples_per_second</td><td>77.185</td></tr><tr><td>Fold_1/eval/eval_steps_per_second</td><td>2.573</td></tr><tr><td>Fold_2/eval/epoch</td><td>30</td></tr><tr><td>Fold_2/eval/eval_bleu</td><td>0.19362</td></tr><tr><td>Fold_2/eval/eval_loss</td><td>0.50832</td></tr><tr><td>Fold_2/eval/eval_meteor</td><td>0.46069</td></tr><tr><td>Fold_2/eval/eval_rouge1</td><td>0.49703</td></tr><tr><td>Fold_2/eval/eval_rouge2</td><td>0.26188</td></tr><tr><td>Fold_2/eval/eval_rougeL</td><td>0.48803</td></tr><tr><td>Fold_2/eval/eval_runtime</td><td>1.2772</td></tr><tr><td>Fold_2/eval/eval_samples_per_second</td><td>93.957</td></tr><tr><td>Fold_2/eval/eval_steps_per_second</td><td>3.132</td></tr><tr><td>Fold_3/eval/epoch</td><td>30</td></tr><tr><td>Fold_3/eval/eval_bleu</td><td>0.18455</td></tr><tr><td>Fold_3/eval/eval_loss</td><td>0.45784</td></tr><tr><td>Fold_3/eval/eval_meteor</td><td>0.43916</td></tr><tr><td>Fold_3/eval/eval_rouge1</td><td>0.47714</td></tr><tr><td>Fold_3/eval/eval_rouge2</td><td>0.2304</td></tr><tr><td>Fold_3/eval/eval_rougeL</td><td>0.47107</td></tr><tr><td>Fold_3/eval/eval_runtime</td><td>2.2996</td></tr><tr><td>Fold_3/eval/eval_samples_per_second</td><td>52.182</td></tr><tr><td>Fold_3/eval/eval_steps_per_second</td><td>1.739</td></tr><tr><td>Fold_4/eval/epoch</td><td>30</td></tr><tr><td>Fold_4/eval/eval_bleu</td><td>0.15719</td></tr><tr><td>Fold_4/eval/eval_loss</td><td>0.48553</td></tr><tr><td>Fold_4/eval/eval_meteor</td><td>0.4115</td></tr><tr><td>Fold_4/eval/eval_rouge1</td><td>0.47283</td></tr><tr><td>Fold_4/eval/eval_rouge2</td><td>0.2051</td></tr><tr><td>Fold_4/eval/eval_rougeL</td><td>0.46377</td></tr><tr><td>Fold_4/eval/eval_runtime</td><td>1.5923</td></tr><tr><td>Fold_4/eval/eval_samples_per_second</td><td>75.363</td></tr><tr><td>Fold_4/eval/eval_steps_per_second</td><td>2.512</td></tr><tr><td>Fold_5/eval/epoch</td><td>30</td></tr><tr><td>Fold_5/eval/eval_bleu</td><td>0.19189</td></tr><tr><td>Fold_5/eval/eval_loss</td><td>0.48663</td></tr><tr><td>Fold_5/eval/eval_meteor</td><td>0.44856</td></tr><tr><td>Fold_5/eval/eval_rouge1</td><td>0.49969</td></tr><tr><td>Fold_5/eval/eval_rouge2</td><td>0.23101</td></tr><tr><td>Fold_5/eval/eval_rougeL</td><td>0.48914</td></tr><tr><td>Fold_5/eval/eval_runtime</td><td>1.3225</td></tr><tr><td>Fold_5/eval/eval_samples_per_second</td><td>89.983</td></tr><tr><td>Fold_5/eval/eval_steps_per_second</td><td>3.025</td></tr><tr><td>eval/bleu</td><td>0.19189</td></tr><tr><td>eval/loss</td><td>0.48663</td></tr><tr><td>eval/meteor</td><td>0.44856</td></tr><tr><td>eval/rouge1</td><td>0.49969</td></tr><tr><td>eval/rouge2</td><td>0.23101</td></tr><tr><td>eval/rougeL</td><td>0.48914</td></tr><tr><td>eval/runtime</td><td>1.3225</td></tr><tr><td>eval/samples_per_second</td><td>89.983</td></tr><tr><td>eval/steps_per_second</td><td>3.025</td></tr><tr><td>test/bleu</td><td>0.19189</td></tr><tr><td>test/loss</td><td>0.48663</td></tr><tr><td>test/meteor</td><td>0.44856</td></tr><tr><td>test/rouge1</td><td>0.49969</td></tr><tr><td>test/rouge2</td><td>0.23101</td></tr><tr><td>test/rougeL</td><td>0.48914</td></tr><tr><td>test/runtime</td><td>1.5865</td></tr><tr><td>test/samples_per_second</td><td>75.008</td></tr><tr><td>test/steps_per_second</td><td>2.521</td></tr><tr><td>total_flos</td><td>1948921941196800.0</td></tr><tr><td>train/epoch</td><td>30</td></tr><tr><td>train/global_step</td><td>450</td></tr><tr><td>train/grad_norm</td><td>0.78102</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4781</td></tr><tr><td>train_loss</td><td>1.16199</td></tr><tr><td>train_runtime</td><td>526.7421</td></tr><tr><td>train_samples_per_second</td><td>27.338</td></tr><tr><td>train_steps_per_second</td><td>0.854</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">T5_SMALL_TOP5DOCS_LDA</strong> at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/vru2jrkd' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/vru2jrkd</a><br> View project at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250511_232557-vru2jrkd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final predicitons for all folds foudn in crossval_T5_SMALL_TOP5DOCS_LDA.jsonl\n"
     ]
    }
   ],
   "source": [
    "cross_val_train(modelname=\"t5-small\", run_name=\"T5_SMALL_TOP5DOCS_LDA\",fold_filename=\"training_top5_qulac_en_LDA_rep_PREPROCESSED_FOR_MODEL.json\",filename=\"crossval_T5_SMALL_TOP5DOCS_LDA.jsonl\", \n",
    "                tokenizer_global=tokenizer_global, tokenized_dataset_global=tokenized_dataset_global, \n",
    "                folds=folds, nb_epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d469a66-bc2e-4d3e-9795-bdad4fd0e076",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc8a22e-03fa-4ca7-9403-6259e90cb227",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
