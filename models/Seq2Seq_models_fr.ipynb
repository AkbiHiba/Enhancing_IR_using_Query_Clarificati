{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b4e941a-06c8-4724-9c72-b8ce769ee276",
   "metadata": {},
   "source": [
    "# FRENCHTOP 20 QULAC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e702f7-925f-4747-ab37-83a85deebec0",
   "metadata": {},
   "source": [
    "First start with english dataset just to set up the runs, do it on top 20 docs. For now we dont put any parameters, we just use the docs and the query and try to predict 1 claridying query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ac16d0-c6ea-4eba-b467-98c4738ea304",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "966e332e-d77e-4323-8bf9-004ce71bff69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average token length: 86448.01502504174\n"
     ]
    }
   ],
   "source": [
    "def get_token_length(input_text):\n",
    "    tokenized_input = tokenizer(input_text, padding=False, truncation=False)\n",
    "    return len(tokenized_input[\"input_ids\"])\n",
    "\n",
    "# Calculate the average token length for all inputs\n",
    "token_lengths = [get_token_length(entry[\"input\"]) for entry in raw_data]\n",
    "\n",
    "# Calculate the average and print it\n",
    "average_token_length = sum(token_lengths) / len(token_lengths)\n",
    "print(f\"Average token length: {average_token_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d684ddb-d503-402e-b86f-fcec6420cdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08ce92c9-cad0-4cde-98c1-7867d02e5681",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /users/Etu0/21402600/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhibaakbi\u001b[0m (\u001b[33mhibaakbi-sorbonne-universit-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# Paste your API key directly here\n",
    "wandb.login(key=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eca98c-d837-4af0-8465-291d7362f22c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0f3abc-97b0-4a37-98f8-2fd7d7bf0969",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "085d12e2-cb0a-4511-bc09-560f309474f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24a83916-aab0-4c34-b54d-922ba0cb813f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 02:08:18.813475: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747094898.835604   62038 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747094898.842502   62038 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747094898.859634   62038 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747094898.859655   62038 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747094898.859658   62038 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747094898.859659   62038 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-13 02:08:18.865460: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /users/Etu0/21402600/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /users/Etu0/21402600/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /users/Etu0/21402600/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "\n",
    "bleu = load(\"bleu\")\n",
    "rouge = load(\"rouge\")\n",
    "meteor = load(\"meteor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9084fb-6dc9-4e79-a299-9b1efd656e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForSeq2SeqLM, Trainer, TrainingArguments,EarlyStoppingCallback\n",
    "# from datasets import Dataset\n",
    "# import os\n",
    "\n",
    "\n",
    "# model_name = \"t5-small\"\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# output_dir = \"./results\"\n",
    "# logging_dir = os.path.join(output_dir, \"logs\")\n",
    "# # Define training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",  # Output directory for logs and checkpoints (optional)\n",
    "#     eval_strategy=\"epoch\",  # Evaluate after every epoch\n",
    "#     save_strategy=\"epoch\",\n",
    "#     learning_rate=5e-5,  # Learning rate\n",
    "#     per_device_train_batch_size=16,  # Batch size per device for training\n",
    "#     per_device_eval_batch_size=16,  # Batch size per device for evaluation\n",
    "#     num_train_epochs=30,  # Number of training epochs\n",
    "#     logging_dir=logging_dir,\n",
    "#     use_cpu=True,\n",
    "#     logging_first_step=True,\n",
    "#     weight_decay=0.01,  # Weight decay for regularization\n",
    "#     logging_steps=50,\n",
    "#     load_best_model_at_end=True,\n",
    "#     metric_for_best_model=\"eval_loss\", \n",
    "#     greater_is_better=False\n",
    "# )\n",
    "\n",
    "# os.makedirs(logging_dir, exist_ok=True)\n",
    "\n",
    "# # Initialize the Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_train_dataset,\n",
    "#     eval_dataset=tokenized_eval_dataset,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     tokenizer=tokenizer,  # Pass the tokenizer for automatic tokenization during training\n",
    "#     callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "# )\n",
    "\n",
    "# # Start the training (without saving)\n",
    "# trainer.train()\n",
    "\n",
    "# # You can evaluate the model here if you need\n",
    "# # Example: Evaluate after training\n",
    "# evaluation_results = trainer.evaluate()\n",
    "# print(evaluation_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a7b5ace7-d9cc-4cfa-8a41-6b1441404838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final predictions saved to predicted_clarif_TINYT5_30EPOCH.jsonl\n"
     ]
    }
   ],
   "source": [
    "# predictions = trainer.predict(tokenized_eval_dataset)\n",
    "\n",
    "# pred_ids = np.argmax(predictions.predictions[0], axis=-1)\n",
    "# predicted_texts = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "# true_texts = tokenizer.batch_decode(predictions.label_ids, skip_special_tokens=True)\n",
    "\n",
    "# output_predictions_file = \"predicted_clarif_TINYT5_30EPOCH.jsonl\"\n",
    "# with open(output_predictions_file, \"w\", encoding=\"utf-8\") as writer:\n",
    "#     for true_text, predicted_text in zip(true_texts, predicted_texts):\n",
    "#         output_dict = {\"true\": true_text, \"predicted\": predicted_text}\n",
    "#         writer.write(json.dumps(output_dict) + \"\\n\")\n",
    "\n",
    "# print(f\"Final predictions saved to {output_predictions_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e312c7-1f22-48b7-892b-b22142021c16",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29e4133-8414-421c-abfd-f5a28e8d078b",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4a597e-50a1-4652-9925-ac74241f6f28",
   "metadata": {},
   "source": [
    "# T5 EFFICIENT -------------- TOP5 RAW DOCS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "577268e3-90af-41c5-a08c-b6a4dd48e205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "#fucntion to tokenize the dataset based on the model\n",
    "def tokenize_and_split_dataset(filename, modelname):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_data = json.load(f)\n",
    "        \n",
    "    # Choose your model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(modelname)\n",
    "\n",
    "    # Tokenization function\n",
    "    def preprocess(batch):\n",
    "        model_inputs = tokenizer(\n",
    "            batch[\"input\"],\n",
    "            max_length=512,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(\n",
    "                batch[\"output\"],\n",
    "                max_length=64,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True\n",
    "            )\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "    \n",
    "    # Convert raw data to Dataset object\n",
    "    dataset = Dataset.from_list(raw_data)\n",
    "    tokenized_dataset = dataset.map(preprocess, batched=True)\n",
    "\n",
    "    return tokenizer, tokenized_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84b05fa6-a374-45dc-ade1-8544fd37135c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#fucntion to split dataset into folds\n",
    "def generate_folds(tokenized_dataset, n_splits=5, seed=42):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    folds = list(kf.split(tokenized_dataset))\n",
    "    return folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4abe23d8-a598-474f-a764-1e28f14cee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from transformers import AutoModelForSeq2SeqLM, Trainer, TrainingArguments, EarlyStoppingCallback, TrainerCallback\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Custom callback to log metrics after each evaluation step\n",
    "class WandbLoggingCallback(TrainerCallback):\n",
    "    def __init__(self, fold_num):\n",
    "        self.fold_num = fold_num\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        if state.is_world_process_zero:\n",
    "            log_data = {f\"Fold_{self.fold_num}/eval/{key}\": value for key, value in metrics.items()}\n",
    "            wandb.log(log_data)\n",
    "\n",
    "def cross_val_train(modelname, filename, run_name, tokenizer, tokenized_dataset, folds, nb_epochs=30):\n",
    "    # Start a single WandB run for all folds\n",
    "    wandb.init(project=\"cross_val_T5\", name=run_name, config={\"epochs\": nb_epochs})\n",
    "    filenamebis= filename.split(\".jsonl\")[0]\n",
    "    fold_metrics = []\n",
    "\n",
    "    for fold_num, (train_idx, test_idx) in enumerate(folds):\n",
    "        print(f\"Processing Fold {fold_num + 1}\")\n",
    "\n",
    "        # Split the dataset\n",
    "        train_dataset = tokenized_dataset.select(train_idx)\n",
    "        test_dataset = tokenized_dataset.select(test_idx)\n",
    "\n",
    "        # Initialize the model\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(modelname, from_tf=True)\n",
    "\n",
    "        # Training arguments with early stopping\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"./results/{run_name}/fold_{fold_num+1}\",\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=5e-5,\n",
    "            per_device_train_batch_size=8,\n",
    "            per_device_eval_batch_size=8,\n",
    "            num_train_epochs=nb_epochs,\n",
    "            load_best_model_at_end=True,\n",
    "            logging_steps=20,\n",
    "            logging_first_step=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            seed=42,\n",
    "        )\n",
    "\n",
    "        # Initialize Trainer with the logging callback and early stopping\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=test_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            # compute_metrics=compute_metrics,\n",
    "            compute_metrics=lambda eval_pred: compute_metrics(eval_pred, tokenizer),\n",
    "            callbacks=[\n",
    "                WandbLoggingCallback(fold_num + 1),\n",
    "                EarlyStoppingCallback(early_stopping_patience=3)\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "\n",
    "        # Final evaluation for the fold\n",
    "        eval_results = trainer.evaluate()\n",
    "        print(f\"Fold {fold_num + 1} results: {eval_results}\")\n",
    "\n",
    "        # Store the final evaluation results for this fold\n",
    "        fold_metrics.append(eval_results)\n",
    "\n",
    "        # Predict on the test dataset\n",
    "        predictions = trainer.predict(test_dataset)\n",
    "\n",
    "    \n",
    "        # Decode predictions and save to file\n",
    "        pred_ids = np.argmax(predictions.predictions[0], axis=-1)\n",
    "        predicted_texts = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "        true_texts = tokenizer.batch_decode(predictions.label_ids, skip_special_tokens=True)\n",
    "        input_ids = test_dataset[\"input_ids\"]\n",
    "        input_texts = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Save predictions to a file\n",
    "\n",
    "        output_predictions_file = f\"{filenamebis}_fold_{fold_num+1}.jsonl\"\n",
    "        with open(output_predictions_file, \"w\", encoding=\"utf-8\") as writer:\n",
    "            for input_text, true_text, predicted_text in tqdm(zip(input_texts, true_texts, predicted_texts), total=len(true_texts)):\n",
    "                output_dict = {\n",
    "                    \"input\": input_text,\n",
    "                    \"true\": true_text,\n",
    "                    \"predicted\": predicted_text\n",
    "                }\n",
    "                writer.write(json.dumps(output_dict, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "        print(f\"Predictions for fold {fold_num + 1} saved to {output_predictions_file}\")\n",
    "\n",
    "    # Calculate and log average metrics\n",
    "    avg_metrics = {key: np.mean([fold[key] for fold in fold_metrics]) for key in fold_metrics[0]}\n",
    "    wandb.log({f\"Averages/{key}\": value for key, value in avg_metrics.items()})\n",
    "    print(\"Average metrics over all folds:\", avg_metrics)\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    #merge all to a single file for predcitions\n",
    "\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as merged_file:\n",
    "        for fold_num in range(len(folds)):\n",
    "            fold_file = f\"{filenamebis}_fold_{fold_num+1}.jsonl\"\n",
    "            if os.path.exists(fold_file):\n",
    "                with open(fold_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                    for line in f:\n",
    "                        merged_file.write(line)\n",
    "    print(f\"Final predicitons for all folds foudn in {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ac6395-d9cb-4f1d-8483-50c68ff46e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORKING VERSION OF CV WITHOUT PROPER LOGGING\n",
    "\n",
    "# import wandb\n",
    "# from transformers import AutoModelForSeq2SeqLM, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "# from datasets import Dataset\n",
    "# import os\n",
    "# from tqdm import tqdm\n",
    "# from sklearn.model_selection import KFold\n",
    "# import numpy as np\n",
    "\n",
    "# def cross_val_train(modelname, filename, run_name, tokenizer, tokenized_dataset, folds, nb_epochs=30):\n",
    "#     # Start a single WandB run for all folds\n",
    "#     wandb.init(project=\"cross_val_Test\", name=run_name, config={\"epochs\": nb_epochs})\n",
    "\n",
    "#     fold_metrics = []\n",
    "\n",
    "#     for fold_num, (train_idx, test_idx) in enumerate(folds):\n",
    "#         print(f\"Processing Fold {fold_num + 1}\")\n",
    "\n",
    "#         # Split the dataset\n",
    "#         train_dataset = tokenized_dataset.select(train_idx)\n",
    "#         test_dataset = tokenized_dataset.select(test_idx)\n",
    "\n",
    "#         # Initialize the model for each fold\n",
    "#         model = AutoModelForSeq2SeqLM.from_pretrained(modelname, from_tf=True)\n",
    "\n",
    "#         # Training arguments\n",
    "#         training_args = TrainingArguments(\n",
    "#             output_dir=f\"./results/fold_{fold_num+1}\",  # Different folder per fold\n",
    "#             eval_strategy=\"epoch\",\n",
    "#             save_strategy=\"epoch\",\n",
    "#             learning_rate=5e-5,\n",
    "#             per_device_train_batch_size=8,\n",
    "#             per_device_eval_batch_size=8,\n",
    "#             num_train_epochs=nb_epochs,\n",
    "#             logging_dir=f\"./results/logs/fold_{fold_num+1}\",\n",
    "#             load_best_model_at_end=True,\n",
    "#             metric_for_best_model=\"eval_loss\",\n",
    "#             greater_is_better=False,\n",
    "#             seed=42,  # Seed for reproducibility\n",
    "#         )\n",
    "\n",
    "#         # Initialize Trainer\n",
    "#         trainer = Trainer(\n",
    "#             model=model,\n",
    "#             args=training_args,\n",
    "#             train_dataset=train_dataset,\n",
    "#             eval_dataset=test_dataset,\n",
    "#             tokenizer=tokenizer,\n",
    "#             compute_metrics=compute_metrics,\n",
    "#         )\n",
    "\n",
    "#         # Train the model\n",
    "#         trainer.train()\n",
    "\n",
    "#         # Evaluate the model on this fold\n",
    "#         eval_results = trainer.evaluate()\n",
    "#         print(f\"Fold {fold_num + 1} results: {eval_results}\")\n",
    "\n",
    "#         # Store the fold metrics\n",
    "#         fold_metrics.append(eval_results)\n",
    "\n",
    "#     # Aggregate results over all folds\n",
    "#     avg_metrics = {key: np.mean([fold[key] for fold in fold_metrics]) for key in fold_metrics[0]}\n",
    "#     print(\"Average metrics over all folds:\", avg_metrics)\n",
    "\n",
    "#     # Log average metrics to WandB\n",
    "\n",
    "#     # End the WandB run\n",
    "#     wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e84200f6-9236-4dcb-8d78-e54b0a153f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# from transformers import AutoModelForSeq2SeqLM, Trainer, TrainingArguments, EarlyStoppingCallback, TrainerCallback\n",
    "# from datasets import Dataset\n",
    "# import numpy as np\n",
    "\n",
    "# # Custom callback to log metrics after each evaluation step\n",
    "# class WandbLoggingCallback(TrainerCallback):\n",
    "#     def __init__(self, fold_num):\n",
    "#         self.fold_num = fold_num\n",
    "\n",
    "#     def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "#         if state.is_world_process_zero:\n",
    "#             # Log metrics with fold-specific keys\n",
    "#             log_data = {f\"Fold_{self.fold_num}/eval/{key}\": value for key, value in metrics.items()}\n",
    "#             wandb.log(log_data)\n",
    "\n",
    "# def cross_val_train(modelname, filename, run_name, tokenizer, tokenized_dataset, folds, nb_epochs=30):\n",
    "#     # Start a single WandB run for all folds\n",
    "#     wandb.init(project=\"cross_val_T5\", name=run_name, config={\"epochs\": nb_epochs})\n",
    "\n",
    "#     fold_metrics = []\n",
    "\n",
    "#     for fold_num, (train_idx, test_idx) in enumerate(folds):\n",
    "#         print(f\"Processing Fold {fold_num + 1}\")\n",
    "\n",
    "#         # Split the dataset\n",
    "#         train_dataset = tokenized_dataset.select(train_idx)\n",
    "#         test_dataset = tokenized_dataset.select(test_idx)\n",
    "\n",
    "#         # Initialize the model\n",
    "#         model = AutoModelForSeq2SeqLM.from_pretrained(modelname, from_tf=True)\n",
    "\n",
    "#         # Training arguments with early stopping\n",
    "#         training_args = TrainingArguments(\n",
    "#             output_dir=f\"./results/fold_{fold_num+1}\",\n",
    "#             eval_strategy=\"epoch\",\n",
    "#             save_strategy=\"epoch\",\n",
    "#             learning_rate=5e-5,\n",
    "#             per_device_train_batch_size=8,\n",
    "#             per_device_eval_batch_size=8,\n",
    "#             num_train_epochs=nb_epochs,\n",
    "#             logging_dir=f\"./results/logs/fold_{fold_num+1}\",\n",
    "#             load_best_model_at_end=True,\n",
    "#             logging_steps=20,\n",
    "#             logging_first_step=True,\n",
    "#             metric_for_best_model=\"eval_loss\",\n",
    "#             greater_is_better=False,\n",
    "#             seed=42,\n",
    "#         )\n",
    "\n",
    "#         # Initialize Trainer with the logging callback and early stopping\n",
    "#         trainer = Trainer(\n",
    "#             model=model,\n",
    "#             args=training_args,\n",
    "#             train_dataset=train_dataset,\n",
    "#             eval_dataset=test_dataset,\n",
    "#             tokenizer=tokenizer,\n",
    "#             compute_metrics=compute_metrics,\n",
    "#             callbacks=[\n",
    "#                 WandbLoggingCallback(fold_num + 1),  # Log metrics for each fold\n",
    "#                 EarlyStoppingCallback(early_stopping_patience=3)  # Early stopping after 3 epochs of no improvement\n",
    "#             ],\n",
    "#         )\n",
    "\n",
    "#         # Train the model\n",
    "#         trainer.train()\n",
    "\n",
    "#         # Final evaluation for the fold\n",
    "#         eval_results = trainer.evaluate()\n",
    "#         print(f\"Fold {fold_num + 1} results: {eval_results}\")\n",
    "\n",
    "#         # Store the final evaluation results for this fold\n",
    "#         fold_metrics.append(eval_results)\n",
    "\n",
    "#     # Calculate and log average metrics\n",
    "#     avg_metrics = {key: np.mean([fold[key] for fold in fold_metrics]) for key in fold_metrics[0]}\n",
    "#     wandb.log({f\"Averages/{key}\": value for key, value in avg_metrics.items()})\n",
    "#     print(\"Average metrics over all folds:\", avg_metrics)\n",
    "\n",
    "#     wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55287291-74c3-4494-8a34-bb2bc8d5572f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# T5 SMALL ------------------- TO5 RAW DOCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e617d9-2b97-40ea-af98-77cb2d5dc9d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "19172d91-d925-4171-ad8c-3ed0b74a3eb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5ea4cd15aad4b678ede1bfc7e05b3a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/Etu0/21402600/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': '[QUERY] Find information on President Barack Obama\\'s family history, including genealogy, national origins, places and dates of birth, etc.\\n[DOCUMENTS]\\n[DOC 1] Family of Barack Obama - Wikipedia, the free encyclopedia\\\\nFamily of Barack Obama\\\\nFrom Wikipedia, the free encyclopedia\\\\n(Redirected from Sasha Obama)\\\\nJump to: navigation, search\\\\nObama Family\\\\nPresident Barack Obama, First Lady Michelle, and daughters Malia and Sasha wave to the crowd after his inaugural address Jan. 20, 2009, on the west steps of the U.S. Capitol.[1]\\\\nCurrent region\\\\nWashington, DC\\\\nInformation\\\\nPlace of origin\\\\nUnited States\\\\nNotable members\\\\nBarack Obama, Michelle Obama, Ann Dunham, Barack Obama, Sr., etc.\\\\nConnected families\\\\nRobinson, Dunham, Soetoro, Ng\\\\nThis article is part of a series about\\\\nBarack Obama\\\\nBackground \\xa0ýý Illinois Senate \\xa0ýý U.S. Senate\\\\nPolitical positions\\xa0ýý Public image\\xa0ýý Family\\\\n2008 primaries\\xa0ýý ObamaýýýBiden campaign\\\\nTransition\\xa0ýý Inauguration\\xa0ýý Electoral history\\\\nPresidency (Timeline, First 100 days)\\\\nMalia, Michelle and Sasha on stage at the 2008 Democratic National Convention\\\\nThe Family of Barack Obama is an extended clan of African American, English, Indonesian, and Kenyan (Luo) heritage known through the writings and political career of Barack Obama, the President of the United States of America,[2][3][4][5] and other reports. His immediate family is the First Family of the United States. The Obamas are the first First Family of African American descent in the United States and the youngest to enter the White House since the Kennedys. One columnist wrote, ýýýObama\\'s young, energetic family harks back to days of Camelot.ýýý[6]\\\\nContents\\\\n1 Immediate family\\\\n2 Extended family - maternal relations\\\\n3 Extended family - paternal relations\\\\n4 Michelle Robinson Obama\\'s extended family\\\\n5 Genealogical charts\\\\n5.1 Obama ancestry\\\\n5.2 Family trees\\\\n6 Distant relations\\\\n7 See also\\\\n8 References\\\\n9 External links\\\\nImmediate family\\\\nMichelle Obama\\\\nMichelle Obama, nýýe Robinson, the wife of Barack Obama, was born on January 17, 1964 in Chicago, Illinois. She is a lawyer and was a University of Chicago Hospital Vice-President. She is the First Lady of the United States.\\\\nMalia Obama and Sasha Obama\\\\nBarack and Michelle Obama have two daughters: Malia Ann (pronounced /mýýýýliýýýý/), born in 1998,[7] and Natasha (known as Sasha) /ýýsýýýýýýýý/), born in 2001. Sasha is the youngest child to reside in the White House since John F. Kennedy, Jr, arrived as an infant in 1961.[8]\\\\nBefore his inauguration, President Obama published an open letter to his daughters in Parade magazine, describing what he wants for them and every child in America: \\\\\"to grow up in a world with no limits on your dreams and no achievements beyond your reach, and to grow into compassionate, committed women who will help build that world.\\\\\"[9]\\\\nWhile living in Chicago, they kept busy schedules, as the Associated Press reports: \\\\\"soccer, dance and drama for Malia, gymnastics and tap for Sasha, piano and tennis for both.\\\\\"[10][11] In July 2008, the family gave an interview to the television series Access Hollywood; Obama later said they regretted allowing the children to be included.[12]\\\\nIn his victory speech on the night of his election, President Obama repeated his promise to Sasha and Malia to get a puppy to take with them to the White House.[13] However the selection of a dog has been slow because Malia is allergic to animal dander;[14] the president subsequently said that the choice has been narrowed down to either a labradoodle or Portuguese Water Dog, and they are hoping to find a shelter animal.[15]\\\\nMalia and Sasha attend the private Sidwell Friends School in Washington, DC, the same school as attended by Chelsea Clinton, Tricia Nixon Cox, and Archibald Roosevelt, and currently the grandchildren of Vice President Joe Biden.[16] The Obama girls began classes there on January 5, 2009.[17] While in Chicago, both attended the private University of Chicago Laboratory School.\\\\nMarian Shields Robinson\\\\nMichelle Obama\\'s mother (birthname Marian Shields, born July 1937), now widowed, married Michelle\\'s father, Fraser Robinson, in 1960.[18][19] Robinson was formerly a secretary at Spiegel catalog and a bank. While Michelle and Barack Obama were campaigning in 2008, Robinson tended the Obama\\'s young children and she intends to do the same while in Washington, DC. Robinson is currently living in the White House itself as part of the First Family;[20] she is the first live-in grandmother there since Elivera M. Doud during the Eisenhower administration.[21] Some media outlets have dubbed Robinson the \\\\\"First Granny\\\\\".[22][21]\\\\nExtended family - maternal relations\\\\nRight-to-left: Barack Obama and Maya Soetoro with their mother Ann Dunham and grandfather Stanley Dunham in Hawaii (early 1970s)\\\\nWikinews has related news:\\\\nBarack Obama elected 44th President of the United States\\\\nGrandmother of Barack Obama dies at 86\\\\nMadelyn Dunham with her daughter Ann\\\\nAccording to Barack Obama\\'s Dreams from My Father, his great-grandmother Leona McCurry was part Native American, which Obama believed Leona held as a \\\\\"source of considerable shame\\\\\" and \\\\\"blanched whenever someone mentioned the subject and hoped to carry the secret to her grave\\\\\"; whereas McCurry\\'s daughter (Obama\\'s maternal grandmother) \\\\\"would turn her head in profile to show off her beaked nose, which along with a pair of jet-black eyes, was offered as proof of Cherokee blood.\\\\\"[23] To date, no concrete evidence has surfaced of Cherokee heritage. Obama\\'s maternal heritage consists mostly of English ancestry, with much smaller amounts of German, Irish, Scottish, Welsh, Swiss, and French ancestry.[2]\\\\nAnn Dunham\\\\nMother of Barack Obama born in 1942, died in 1995. Birthname Stanley Ann Dunham. Anthropologist in Hawaii and Indonesia.\\\\nMadelyn Lee Payne Dunham\\\\nBarack Obama\\'s maternal grandmother, born in 1922 and died on November 2, 2008.[24] She was a bank vice president in Hawaii. Obama said that when he was a child, his grandmother \\\\\"read me the opening lines of the Declaration of Independence and told me about the men and women who marched for equality because they believed those words put to paper two centuries ago should mean something.\\\\\"[9]\\\\nStanley Armour Dunham\\\\nGrandfather of Barack Obama, born 1918, died 1992. World War II U.S. Army sergeant, furniture salesman in Hawaii.\\\\nCharles T. Payne\\\\nGreat-uncle of Barack Obama, younger brother of Madelyn Dunham, born 1925. Served during World War II in the U.S. Army 89th Infantry Division.[25] Obama has often described Payne\\'s role in liberating Ohrdruf forced labor camp.[26] There was brief media attention when Obama mistakenly identified the camp as Auschwitz during the campaign.[27] Payne appeared in the visitor\\'s gallery at the Democratic National Convention in Denver, Colorado, when his great-nephew was nominated for President.[28] He was the assistant director of the University of Chicago\\'s Library.[26]\\\\nMaya Soetoro-Ng\\\\nHalf-sister of Barack Obama, born August 15, 1970, in Jakarta, Indonesia.[29] She is married to Konrad Ng, with whom she has a daughter, Suhaila. Maya Soetoro-Ng is a teacher in Hawaii.\\\\nKonrad Ng\\\\nBrother-in-law of Barack Obama, born 1974. A Canadian whose parents are Malaysian Chinese immigrants, he is an assistant professor at the University of Hawaii\\'s Academy of Creative Media.[30] His parents are from Kudat and Sandakan, two small towns in Sabah, Malaysia, and he was born and raised in Burlington, Ontario.[31] He married Maya Soetoro-Ng at the end of 2003 in Hawaii.[32] They have one daughter, Suhaila.[33][34][35] Konrad Ng is now a US citizen.[36]\\\\nLolo Soetoro\\\\nStepfather of Barack Obama, born in Indonesia 1936, died 1987.\\\\nExtended family - paternal relations\\\\nThe Obamas are members of the Luo, Kenya\\'s third-largest ethnic group, which is part of a larger family of ethnic groups, collectively also known as Luo. This group belongs to the Eastern Sudanic branch of the Nilo-Saharan phylum. The Obama family is largely concentrated in the western province of Nyanza.\\\\nFront row (left to right): Auma Obama (Barack\\'s half-sister), Kezia Obama (Barack\\'s step-mother), Sarah Hussein Onyango Obama (third wife of Barack\\'s paternal grandfather), Zeituni Onyango (Barack\\'s aunt)\\\\nBack row (left to right): Said Obama (Barack\\'s uncle), Barack Obama, Abongo [Roy] Obama (Barack\\'s half-brother), unidentified woman, Bernard Obama (Barack\\'s half-brother), Abo Obama (Barack\\'s half-brother).\\\\nBarack Obama, Sr.\\\\nBarack Obama\\'s father, (1936ýýý1982). Government economist in Kenya. In addition to President Obama, Barack Obama Sr. fathered six other sons and a daughter.[37]\\\\nHussein Onyango Obama\\\\nBarack Obama\\'s paternal grandfather (c. 1895ýýý1979);[38] he worked as a mission cook. He joined the British Army during World War I. (One source gives 1870ýýý1975 as his dates of birth and death based on his tombstone reading \\\\\"Mzee Hussein Onyango Obama\\\\\" in his home village.[39] The term \\\\\"mzee\\\\\" is a Kenyan honorific meaning \\\\\"old man\\\\\" or \\\\\"elder.\\\\\") According to his third wife, Sarah, he originally converted to Catholicism, but took the name Hussein when he later converted to Islam; she said he passed the name, not the religion, on to his children.[40]\\\\nHabiba Akumu Obama\\\\nBarack Obama\\'s paternal grandmother, and the second wife of Hussein Onyango Obama. A photograph of her holding her son, Barack Sr, on her lap is on the cover of her grandson\\'s memoirs titled Dreams from my Father.[41]\\\\nSarah Obama\\\\nThird wife of Obama\\'s paternal grandfather, born 1922.[39] Also known, through the addition of her late husband\\'s name, as Sarah Onyango Obama,[42] and sometimes referred to as Sarah Ogwel, Sarah Hussein Obama or Sarah Anyango Obama,[43] she lives in Nyangýýýoma Kogelo village, 30 miles west of western Kenya\\'s main town, Kisumu, on the edge of Lake Victoria.[44][45]\\\\nAlthough not a blood relation, Barack Obama calls her \\\\\"Granny Sarah\\\\\".[43][46] Sarah, who speaks Luo and only a few words of English, communicates with President Obama through an interpreter.\\\\nOn July 4, 2008, she attended the United States Independence Day celebrations in Nairobi, hosted by Michael Ranneberger, the US ambassador in Kenya.[47]\\\\nDuring the campaign she protested attempts to portray Obama as a foreigner to the United States or a Muslim, saying that while Obama\\'s grandfather had been a Muslim, \\\\\"In the world of today, children have different religions from their parents.\\\\\"[40] Sarah Obama herself is \\\\\"a strong believer of the Islamic faith,ýýý in her words.[48]\\\\nKezia Obama\\\\nFirst wife of Barack Obama\\'s father, born c. 1940. She is Barack Obama Sr.\\'s first wife, whom he married in Kenya before studying abroad in the United States. Also known as Kezia Grace Obama.[49][50] She currently lives in Bracknell, Berkshire, England.[51][52] Her sister, Jane, is the \\'Auntie Jane\\' mentioned at the very start of Dreams from My Father when she telephoned President Obama to inform him that his father had been killed in a car accident.[53]\\\\nMalik Obama\\\\nBarack Obama\\'s half-brother, also known as Abongo or Roy, born c. March, 1958,[51] son of Barack Obama, Sr. with his first wife, Kezia.[54] Malik Obama was born and raised in Nairobi, Kenya.[55] He earned a degree in accounting from the University of Nairobi.[56] He met his half-brother for the first time in 1985[55] when Barack flew from Chicago to Washington, D.C. to visit him.[57] Malik and his half-brother Barack were best men at each other\\'s weddings.[55] Barack Obama brought his wife Michelle to Kenya three years later, and they met with Malik again while Barack was introducing Michelle to many other new relatives.[58]\\\\nAlthough much of the Obama family has dispersed throughout Kenya and overseas, most, including Malik Obama, still considered their rural village on the shores of Lake Victoria to be their true home, and feel that those who have left the village have become culturally \\\\\"lost\\\\\".[59] A frequent visitor to the United States,[58] and consultant in Washington, D.C. for several months per year,[55] he nevertheless settled in the Obamas\\' ancestral home, Nyangýýýoma Kogelo, a village of several hundred people that he prefers to the city for its slow pace.[55] He runs a small electronics shop a half hour drive outside of town.[55]\\\\nDuring his brother\\'s presidential campaign, Malik Obama was a spokesman for the extended Obama family in Kenya, dealing with safety and privacy concerns arising from increased attention from the press.[60]\\\\nAbo Obama\\\\nBarack Obama\\'s half-brother, born 1968. International telephone store manager in Kenya.\\\\nAuma Obama\\\\nBarack Obama\\'s half-sister, born c. 1960.[61] As of July 2008, development worker in Kenya.[62] She studied German at the University of Heidelberg from 1981 to 1987. After her graduation in Heidelberg she went on for graduate studies at the University of Bayreuth, which awarded her a PhD in 1996. Her dissertation was about the conception of labor in Germany and its literary reflections.[62] Auma Obama lives in London, and in 1996 married an Englishman, Ian Manners. They have a daughter named Akinyi (b. 1997).[62][verification needed]\\\\nBernard Obama\\\\nBarack Obama\\'s half-brother, born 1970, son of Barack Obama, Sr. and his first wife, Kezia. He had been an auto parts supplier in Nairobi, Kenya, and has one child. Bernard converted to Islam as an adult and has said: \\\\\"Iýýým a Muslim, I donýýýt deny it. My father was raised a Muslim. But itýýýs not an issue. I donýýýt know what all the hullabaloo is about.\\\\\"[63] He currently resides in Bracknell, England, with his mother Kezia.[63]\\\\nRuth Ndesandjo\\\\nBorn Ruth Nidesand, in US c. 1940s, Barack Obama Sr.\\'s third wife and a private kindergarten director in Kenya.[64] Ruth\\'s two sons with Barack Obama, Sr., are Mark and David Ndesandjo; her third son, Joseph Ndesandjo, was born c. 1980 from a subsequent marriage to a Tanzanian.[65][66]\\\\nMark Ndesandjo\\\\nBarack Obama\\'s half-brother, son of Ruth Nidesand and Barack Obama Sr.[67] He runs an Internet company called WorldNexus that advises Chinese corporations how best to reach international customers.[68] Mark graduated from Brown University, studied physics at Stanford University, received an MBA from Emory University, and has lived in Shenzhen, China, since 2002 and is married to a Chinese woman.[69] He is also an accomplished pianist.[70]\\\\nDavid Ndesandjo\\\\nDavid Ndesandjo\\\\nBarack Obama\\'s half-brother (also known as David Opiyo Obama), son of Ruth Nidesand and Barack Obama Sr. Killed in a motorcycle accident.[71]\\\\nGeorge Hussein Onyango Obama\\\\nYoungest half-brother of Barack Obama, born c.1982, son of Barack Obama Sr.[72] and Jael (now a resident of Atlanta, Georgia).[73][74] George was six months old when his father died in an automobile accident, after which he was raised in Nairobi by his mother and a French step-father. He later lived in South Korea for two years while his mother resided there for business reasons.[73] Returning to Kenya, George Obama \\\\\"slept rough for several years,\\\\\" until his aunt gave him a six-by-eight foot corrugated metal shack in the Nairobi, Kenya, slum of Huruma Flats.[73] As of August 2008, Obama was studying to become a mechanic.[73] George received little attention until being featured in an article in the Italian language edition of Vanity Fair in August 2008, which portrayed him as living in poverty, shame, and obscurity.[75] The article quoted Obama as saying that he lived \\\\\"on less than a dollar a month\\\\\" and stated that he \\\\\"does not mention his famous half-brother in conversation\\\\\" out of shame at his poverty.[76] In later interviews George contradicted this picture. In an interview with The Times, Obama \\\\\"said that he was furious at subsequent reports that he had been abandoned by the Obama family and that he was filled with shame about living in a slum.\\\\\"[74] He told The Times, \\\\\"Life in Huruma is good.\\\\\" Obama said that he expects no favors, that he was supported by relatives, and that reports he lived on a dollar a month were \\\\\"all lies by people who donýýýt want my brother to win.ýýý[74] He told The Telegraph that he was inspired by his half-brother.[73] According to Time, George \\\\\"has repeatedly denied...that he feels abandoned by Obama.\\\\\"[77] CNN quoted him as saying, \\\\\"I was brought up well. I live well even now. The magazines, they have exaggerated everything... I think I kind of like it here. There are some challenges, but maybe it is just like where you come from, there are the same challenges.\\\\\"[75] George\\'s reported poverty was seized on by conservative critics of Barack Obama. Columnist Dinesh D\\'Souza solicited donations for George Obama from his readers,[78] while Jerome Corsi planned to give him a $1,000 check during a trip to Kenya (Corsi was expelled from the country by immigration authorities).[77]\\\\nOmar Obama\\\\nHalf-uncle of Barack Obama,[79] born on June 3, 1944 in Nyangýýýoma Kogelo. Oldest son of Onyango and Sarah Obama, resides in Boston, Massachusetts.[citation needed]\\\\nZeituni Onyango\\\\nHalf-aunt of Barack Obama,[80] born May 29, 1952, in Kenya,[81] Onyango is referred to as \\\\\"Aunti Zeituni\\\\\" in President Obama\\'s memoir, Dreams from My Father.[82]\\\\nYusuf Obama\\\\nHalf-uncle of Barack Obama,[79] born c. 1950s in Nyangýýýoma Kogelo; son of Onyango and Sarah Obama.[citation needed]\\\\nSaid Obama\\\\nHalf-uncle of Barack Obama,[79][83] born c. 1950s in Nyangýýýoma Kogelo; son of Onyango and Sarah Obama.[citation needed]\\\\nMichelle Robinson Obama\\'s extended family\\\\nFraser Robinson, Sr. (1884ýýý1936) of South Carolina, shown in an old photo along with his wife, Rosella Cohen Robinson, in its background\\\\nBarack Obama has called his wife Michelle \\\\\"the most quintessentially American woman I know.\\\\\"[3] Her family is of African American heritage, descendents of Africans of the American Colonial Era.[3] Michelle Obama\\'s family history traces back from slavery to Reconstruction to the Great Migration North. Some of Michelle\\'s relatives still reside in South Carolina.\\\\nMichelle\\'s earliest known relative is her great-great grandfather Jim Robinson, born in the 1850s, who was an American slave on the Friendfield plantation in South Carolina. The family believes that after the Civil War he remained a Friendfield plantation sharecropper for the rest of his life and that he was buried there in an unmarked grave.[3]\\\\nJim had two sons, Gabriel and Fraser, Michelle Obama\\'s great-grandfather. Fraser had an arm amputated as a result of a boyhood injury. He worked as a shoemaker, newspaper salesman and in a lumber mill and was married to Rosella Cohen.[3] Carrie Nelson, Gabriel Robinson\\'s daughter, now 80, is the oldest living Robinson and the keeper of family lore.[3]\\\\nAt least three of Michelle Obama\\'s great-uncles served in the military of the United States. One aunt moved to Princeton, New Jersey, where she worked as a maid, and cooked Southern-style meals for Michelle and her brother, Craig, when they were students at Princeton University.\\\\nCraig Robinson\\\\nMichelle Obama\\'s brother, born 1962. He is currently head coach of men\\'s basketball at Oregon State University.[84]\\\\nFraser Robinson III\\\\nMichelle Obama\\'s father, born 1935, died 1991, married Michelle\\'s mother, Marian Shields, in 1960.[85][19] Robinson was a pump worker at the City of Chicago water plant.[3]\\\\nFraser Robinson, Jr.\\\\nMichelle Obama\\'s grandfather was born on August 24, 1912 in Georgetown, South Carolina, and died on November 9, 1996, aged 84. He was a good student and orator, but moved from South Carolina to Chicago to find better work than he could find at home, eventually becoming a worker for the United States Postal Service. He was married to LaVaughn Johnson. When he retired, they moved back to South Carolina.[3]\\\\nCapers C. Funnye Jr.\\\\nMichelle Obama\\'s first cousin once removed: Funnyeýýýs mother, Verdelle Robinson Funnye (born Verdelle Robinson; August 22, 1930 ýýý April 16, 2000) and Michelle Obamaýýýs paternal grandfather, Fraser Robinson Jr., were siblings. One of America\\'s most prominent African American Jews, known for acting as a bridge between mainstream Jewry and African Americans.[86]\\\\nGenealogical charts\\\\nObama ancestry\\\\n16. Opiyo\\\\n8. Obama\\\\n4. Hussein Onyango Obama\\\\n9. Nyaoke\\\\n2. Barack Hussein Obama, Sr.\\\\n5. Habiba Akumu\\\\n1. Barack Hussein Obama II\\\\n24. Jacob William Dunham\\\\n12. Ralph Waldo Emerson Dunham, Sr.\\\\n25. Mary Ann Kearney\\\\n6. Stanley Armour Dunham\\\\n26. Harry Ellington Armour\\\\n13. Ruth Lucille Armour\\\\n27. Gabriella Clark\\\\n3. Stanley Ann Dunham\\\\n28. Charles T. Payne\\\\n14. Rolla Charles Payne\\\\n29. Della L. Wolfley\\\\n7. Madelyn Lee Payne\\\\n30. Thomas Creekmore McCurry\\\\n15. Leona Belle McCurry\\\\n31. Margaret Belle Wright\\\\nFamily trees\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nBarack Obama\\\\nStanley\\\\nDunham\\\\n1918ýýý1992\\\\nMadelyn\\\\nPayne\\\\n1922ýýý2008\\\\nHabiba\\\\nAkumu\\\\nOnyango\\\\nObama\\\\nc.\\xa01895ýýý1979\\\\nSarah\\\\nOgwel\\\\n1922ýýý\\\\nLolo\\\\nSoetoro\\\\n1936ýýý1987\\\\nAnn\\\\nDunham\\\\n1942ýýý1995\\\\nBarack\\\\nObama, Sr. *\\\\n1936ýýý1982\\\\nKezia\\\\nGrace\\\\nc. 1940ýýý\\\\nOmar\\\\nObama\\\\n1944ýýý\\\\nZeituni\\\\nOnyango\\\\n1952ýýý\\\\nYusuf\\\\nObama\\\\nc. 1950sýýý\\\\nSaid\\\\nObama\\\\nc. 1950sýýý\\\\nKonrad\\\\nNg\\\\nc. 1974ýýý\\\\nMaya\\\\nSoetoro\\\\n1970ýýý\\\\nBarack\\\\nObama\\\\n1961ýýý\\\\nMichelle\\\\nRobinson\\\\n1964ýýý\\\\nM. Abongo\\\\nObama\\\\n1958ýýý\\\\nAuma\\\\nObama\\\\nc. 1960ýýý\\\\nAbo\\\\nObama\\\\n1968ýýý\\\\nBernard\\\\nObama\\\\n1970ýýý\\\\nSuhaila\\\\nNg\\\\nc. 2005ýýý\\\\nMalia Ann\\\\nObama\\\\n1998ýýý\\\\nSasha\\\\nObama\\\\n2001ýýý\\\\n* Barack\\\\nObama, Sr.\\'s\\\\nadditional\\\\nRuth\\\\nNidesandjo\\\\nc. 1940sýýý\\\\nJael\\\\nOtieno\\\\nrelationships:\\\\nMark\\\\nNdesandjo\\\\nDavid\\\\nNdesandjo\\\\ndied\\xa0c.\\xa01987\\\\nGeorge\\\\nObama\\\\nc. 1982ýýý\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nFirst Lady Michelle Obama\\\\nFraser\\\\nRobinson, Jr.\\\\nýýý\\\\nSouth Carolina\\\\n(1912ýýý1996)\\\\nSawmill worker\\\\nLaVaughn D.\\\\nJohnson\\\\nýýý\\\\nIllinois\\\\n(1915ýýý2002)\\\\nCapers C.\\\\nFunnye, Jr.\\\\nýýý\\\\n(born c. 1952;\\\\nnephew of\\\\nFraser\\\\nRobinson, Jr.)\\\\nRabbi in Chicago\\\\nFraser\\\\nRobinson III\\\\nýýý\\\\nIllinois\\\\n(1935ýýý1991)\\\\nEnjoyed boxing\\\\nin youth;\\\\nwater plant\\\\npump operator\\\\nin Chicago\\\\nMarian\\\\nShields\\\\nýýý\\\\nIllinois\\\\n(born 1937)\\\\nSecretary at\\\\nSpiegel catalog\\\\nin Chicago;\\\\nU.S.\\'s First\\\\nGrandmother\\\\nCraig\\\\nRobinson\\\\nýýý\\\\nIllinois\\\\n(born 1962)\\\\nHead coach of\\\\nOregon State\\\\nBeavers men\\'s\\\\nbasketball\\\\nMichelle\\\\nRobinson\\\\nýýý\\\\nIllinois\\\\n(born 1964)\\\\nFirst Lady\\\\nof the United States\\\\nDistant relations\\\\nSee also: List of United States Presidents by genealogical relationship\\\\nAccording to genealogists, Barack Obama\\'s distant cousins include the multitude of descendants of his maternal ancestors from all along the early-American Atlantic seaboard as well as paternal, Kenyan relations belonging to the Luo tribe, many descending from a 17th century ancestor named Owiny.[87][88] For example, George W. Bush, the 43rd U.S. president, is the eleventh cousin of Barack Obama.[89] The New York Times science writer Nicholas Wade argues that with eleven generations leading back to their common progenitor, Samuel Hinckley, the relationship between the 43rd President and the 44th President is \\\\\"genetically meaningless\\\\\".[90]\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nSelected genealogical relationships\\\\nBill Hickok\\\\nAccording to Barack Obama\\'s family lore (and confirmed by the New England Historic Genealogical Society), the President and Hickok are sixth cousins, six-times removed.[91]\\\\nRobert Duvall\\\\nGoodnight helped inspire Pulitzer Prize-winning author Larry McMurtry to create a protagonist for his novel series Lonesome Dove: Woodrow Call. In Dove\\'s television novela, Woodrow Call\\'s partner is Gus McCrae, portrayed by Obama\\'s eighth cousin, twice removed, actor Robert Duvall.[92]\\\\nJames Madison\\\\nObama is also distantly related to U.S. Presidents James Madison, Lyndon Johnson, Gerald Ford, and U.S. Vice President Dick Cheney, British Prime Minister Sir Winston Churchill, U.S. Civil War General Robert E. Lee, and actor Brad Pitt.[93][94][95]\\\\nCharles Goodnight\\\\nAccording to Chicago Sun-Times reporter Scott Fornek, another Obama progenitor, Catherine Goodnight, was the grandniece of George Goodnight, who was in turn great-grandfather of famed cattleman Charles Goodnight.[92]\\\\nHarry S. Truman\\\\nActor Duvall is distant cousins with United States President Harry Truman, who\\'s likewise a fourth cousin, four times removed, of Obama\\'s.[92]\\\\nGeorgia O\\'Keeffe\\\\nNotable women Obama is related to include painter Georgia OýýýKeeffe.[93]\\\\nSee also\\\\nU.S. Presidential genealogical relationships\\\\nNew England Historic Genealogical Society\\\\nGary Boyd Roberts\\\\nReferences\\\\n^ Keck, Kristi (4 June 2008). \\\\\"Obama crosses historic milestone\\\\\". CNN. http://news.yahoo.com/s/ap/20081122/ap_on_go_pr_wh/obama_school_12. Retrieved on 2008-11-21.\\\\n^ a b Reitwiesner, William Addams. \\\\\"Ancestry of Barack Obama\\\\\". http://www.wargs.com/political/obama.html. Retrieved on 2008-10-09.\\\\n^ a b c d e f g h Murray, Shailagh (2 October 2008). \\\\\"A Family Tree Rooted In American Soil: Michelle Obama Learns About Her Slave Ancestors, Herself and Her Country\\\\\". The Washington Post: p.\\xa0C01. http://www.washingtonpost.com/wp-dyn/content/article/2008/10/01/AR2008100103169.html. Retrieved on 2008-10-10.\\\\n^ Sheridan, Michael (5 February 2007). \\\\\"Secrets of Obama Family Unlocked\\\\\". Muslim Observer. http://news.newamericamedia.org/news/view_article.html?article_id=ae5895fc29971b172938790be94ab107. Retrieved on 2008-11-21.\\\\n^ RTE news report March, 2007:Obamas Irish family links discovered by ancestry.co.uk\\\\n^ Noveck, Jocelyn (2008-11-07). \\\\\"Deseret News | Obama\\'s young, energetic family harks back to days of Camelot\\\\\". Secure.deseretnews.com. https://secure.deseretnews.com/article/1,5143,705261276,00.html. Retrieved on 2009-01-31.\\\\n^ Liza Mundy, Michelle: A Biography (Simon and Schuster, 2008), p. 129.\\\\n^ \\\\\"Sasha Obama\\\\\". Baltimore Sun. http://www.baltimoresun.com/topic/politics/sasha-obama-PECLB004381.topic. Retrieved on 2009-01-31.\\\\n^ a b Obama, Barack (13 January 2009). \\\\\"\\'What I Want for You--And Every Child in America\\'\\\\\". http://www.parade.com/news/2009/01/barack-obama-letter-to-my-daughters.html.\\\\n^ Sobieraj Westfall, Sandra (23 June 2008). \\\\\"Barack Obama Gives Daughter $1 Allowance a Week\\\\\". People Magazine. http://www.people.com/people/article/0,,20214569_1,00.html. Retrieved on 2008-11-21.\\\\n^ Lester, Will (July 23, 2008). \\\\\"Obama daughters keep hectic schedules of their own\\\\\". Associated Press. http://elections.apnews.com/apelect/db_6911/contentdetail.htm;jsessionid=8314A43012AB5FF1D0697247362D8752?contentguid=H95QubFb&full=true. Retrieved on 2008-08-04.\\\\n^ Hiro, Anne. \\\\\"Obama regrets letting \\\\\"Access Hollywood\\\\\" interview daughters. Won\\'t do it again. MSNBC\\'s Dan Abrams gets the story behind the story. - Lynn Sweet\\\\\". Blogs.suntimes.com. http://blogs.suntimes.com/sweet/2008/07/obama_regrets_letting_access_h.html. Retrieved on 2009-01-31.\\\\n^ Ahmed, Saeed (5 November 2008). \\\\\"Move over Barney, new dog moving into White House\\\\\". CNN. http://www.cnn.com/2008/LIVING/wayoflife/11/05/presidential.pets/index.html. Retrieved on 2008-11-21.\\\\n^ \\\\\"Obama: Getting a dog isn\\'t easy\\\\\". Associated Press. 7 November 2008. http://www.mercurynews.com/ci_10927292. Retrieved on 2008-11-21.\\\\n^ Janice Lloyd (2009-01-12). \\\\\"Obamas down to Labradoodle or Portuguese water dog\\\\\". USA Today. http://www.usatoday.com/news/washington/2009-01-11-obama-dog_N.htm. Retrieved on 2009-01-28.\\\\n^ Swarns, Rachel (21 November 2008). \\\\\"And the Winner Is ýýý Sidwell Friends\\\\\". The New York Times. http://thecaucus.blogs.nytimes.com/2008/11/21/and-the-winner-is-sidwell-friends/. Retrieved on 2008-11-21.\\\\n^ Tolin, Lisa (2009-01-05). \\\\\"Obama girls start school with photographers in tow\\\\\". The Associated Press. http://www.google.com/hostednews/ap/article/ALeqM5g6mv_lkODQMmQdpyIEnr8Zpm5mogD95H8KA80. Retrieved on 2009-01-06.\\\\n^ Taylor Marsh (2008-08-25). \\\\\"Political Analysis, National Security and Breaking News\\\\\". Taylor Marsh. http://www.taylormarsh.com/archives_view.php?id=28286. Retrieved on 2009-01-31.\\\\n^ a b Lia LoBello (2008-01-02). \\\\\"First Families: Radar introduces you to the next president\\'s relatives\\\\\". Radar Online. http://www.radaronline.com/features/2008/07/john_mccain_barack_obama_michelle_cindy_dunham_roberta_wrigh_04.php. Retrieved on 2009-01-28.\\\\n^ Rachel L. Swarns (2009-01-09). \\\\\"Obamaýýýs Mother-in-Law to Move Into the White House\\\\\". The New York Times. http://thecaucus.blogs.nytimes.com/2009/01/09/obamas-mother-in-law-to-move-into-the-white-house/?hp. Retrieved on 2009-01-09.\\\\n^ a b \\\\\"Will Obama mum-in-law make it a family affair in the White House?\\\\\". Agence France Presse. 2008-11-22. http://www.google.com/hostednews/afp/article/ALeqM5gN_i2jrCVkJQgfMbSDRRrNk8U4Sw. Retrieved on 2009-01-09.\\\\n^ Philip Sherwell (2008-2008-11-09). \\\\\"Michelle Obama persuades First Granny to join new White House team\\\\\". The Telegraph (UK). http://www.telegraph.co.uk/news/3407525/Michelle-Obama-persuades-First-Granny-to-join-new-White-House-team.html. Retrieved on 2009-01-09.\\\\n^ \\\\\"ýýýTootýýý: Obama grandmother a force that shaped him\\\\\". via Associated Press. 2008-08-25. http://www.thekansan.com/news/x1311851415/-Toot-Obama-grandmother-a-force-that-shaped-him. Retrieved on 2008-08-29.\\\\n^ \\\\\"CNN: \\\\\"Obama\\'s grandmother dies after battle with cancer\\\\\"\\\\\". http://www.cnn.com/2008/POLITICS/11/03/obama.grandma/index.html. Retrieved on 2008-11-04.\\\\n^ The 89th Infantry Division, United States Holocaust Memorial Museum\\\\n^ a b Obama\\'s great-uncle recalls liberating Nazi camp, Boston.com, July 22, 2008\\\\n^ Major Garrett (2008-05-27). \\\\\"Obama Campaign Scrambles to Correct the Record on Uncle\\'s War Service\\\\\". FOXNews.com. http://elections.foxnews.com/2008/05/27/recollection-of-obama-familys-service-missing-key-details. Retrieved on 2009-01-31.\\\\n^ \\\\\"Democrats salute Obamaýýýs great uncle\\\\\". Jewish Telegraphic Agency. August 28, 2008. http://jta.org/news/article/2008/08/28/110123/obamapayne. Retrieved on 31 January 2009.\\\\n^ Obama Family Tree dgmweb.net\\\\n^ Chicago Sun Times article with her picture\\\\n^ Obama has links to Malaysia\\\\n^ Nolan, Daniel (2008-06-11). \\\\\"Relative: Obama\\'s got \\'a good handle on Canada\\'\\\\\". The Hamilton Spectator. http://www.thespec.com/burlingtonlife/article/384475. Retrieved on 2008-07-03.\\\\n^ Nolan, Daniel (June 11, 2008). \\\\\"Obama\\'s Burlington connection\\\\\". The Hamilton Spectator. http://www.thespec.com/article/384307. Retrieved on 2008-06-21.\\\\n^ Misner, Jason (2008-06-20). \\\\\"Barack Obama was here\\\\\". Burlington Post. http://www.burlingtonpost.com/printarticle/186215. Retrieved on 2008-07-03.\\\\n^ Fornek, Scott (2007-09-09). \\\\\"\\'He helped me find my voice\\'\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545473,BSX-News-wotreehh09.article.\\\\n^ Cooper, Tom (2009-01-20). \\\\\"Keep watch for Obama\\\\\". TheSpec.com. http://www.thespec.com/Opinions/article/499161. Retrieved on 2009-01-28.\\\\n^ Ancestry of Barack Obama\\\\n^ Dreams from My Father, p. 376\\\\n^ a b Kenya: Special Report: Sleepy Little Village Where Obama Traces His Own Roots (Page 2 of 2)\\\\n^ a b \\\\\"Obama\\'s grandma slams \\'untruths\\'\\\\\". Associated Press. 2008-03-05. http://www.usatoday.com/news/world/2008-03-05-obama-kin_N.htm.\\xa0 See also this correction.\\\\n^ \\\\\"Q&A ON THE NEWS\\\\\". Atlanta Journal-Constitution. 2009-02-25. http://www.ajc.com/services/content/metro/stories/2009/02/25/questi0225.html. Retrieved on 2009-02-27.\\\\n^ In Kenya, Barack Obamaýýýs family prays for end to conflict - Times Online\\\\n^ a b Crilly, Rob (February 27, 2008). \\\\\"Dreams from Obama\\'s Grandmother\\\\\". Time Magazine, Inc.. http://www.time.com/time/world/article/0,8599,1717590,00.html?xid=rss-topstories. Retrieved on 2008-07-03.\\\\n^ Pflanz, Mike (2008-01-11). \\\\\"Barack Obama\\'s Kenyan relatives keep faith\\\\\". Daily Telegraph. http://www.telegraph.co.uk/news/main.jhtml?xml=/news/2008/01/09/wuspols1009.xml.\\\\n^ Fornek, Scott (2007-09-09). \\\\\"Sarah Obama - \\'Sparkling, laughing eyes\\'\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545459,BSX-News-wotreeu09.article.\\\\n^ \\\\\"Barack Obama in Kenya\\\\\". CNN. http://www.youtube.com/watch?v=Ikg6gj71U9k.\\\\n^ Daily Nation, July 8, 2008: Obama granny\\'s day out with envoys and top politicians\\\\n^ \\\\\"A Candidate, His Minister and the Search for Faith\\\\\". New York Times. 2007-04-30. http://www.nytimes.com/2007/04/30/us/politics/30obama.html?_r=2&pagewanted=all&oref=slogin&oref=slogin.\\\\n^ \\\\\"Kenya: All Obama kin to spend voting day in Kogelo\\\\\". afrika.no. 2008-10-27. http://www.afrika.no/Detailed/17321.html. Retrieved on 2009-01-31.\\\\n^ Cohen, Roger (2008-03-06). \\\\\"The Obamas of the World - New York Times\\\\\". Nytimes.com. http://www.nytimes.com/2008/03/06/opinion/06cohen.html. Retrieved on 2009-01-31.\\\\n^ a b Sanderson, Elizabeth (2008-01-06). \\\\\"Barack Obama\\'s stepmother living in Bracknell reveals the close bond with him ... and his mother\\\\\". Daily Mail. http://www.dailymail.co.uk/news/article-506338/Barack-Obamas-stepmother-living-Bracknell-reveals-close-bond---mother.html.\\\\n^ Lindsay, Anna (2009-01-20). \\\\\"Barack\\'s bingo-loving stepmother\\\\\". BBC News. http://news.bbc.co.uk/1/hi/england/berkshire/7834368.stm.\\\\n^ http://www.newvision.co.ug/D/8/26/666733\\\\n^ jpt (2008-06-18). \\\\\"From the Fact Check Desk: What Did Obama\\'s Half-Brother Say About Obama\\'s Background\\\\\". ABC News. http://blogs.abcnews.com/politicalpunch/2008/06/from-the-fact-c.html.\\\\n^ a b c d e f Maliti, Tom (2004-10-26). \\\\\"Obama\\'s Brother Chooses Life in Slow Lane\\\\\". The Associated Press. http://www.msnbc.msn.com/id/6333496.\\\\n^ Obama, Dreams from my Father, 2004, p. 265.\\\\n^ Obama, Dreams from my Father, 2004, p. 262.\\\\n^ a b *Oywa, John (2004-08-15). \\\\\"Sleepy Little Village Where Obama Traces His Own Roots\\\\\". The Daily Nation. http://allafrica.com/stories/200408160533.html.\\\\n^ Philip Ochieng (2004-11-01). \\\\\"From Home Squared to the US Senate: How Barack Obama Was Lost and Found\\\\\". The East African. http://www.nationmedia.com/EastAfrican/01112004/Features/PA2-11.html. Retrieved on 2008-03-23.\\\\n^ Warah, Rasna (2008-06-09). \\\\\"We cannot lay claims on Obama; he\\'s not one of us - Obama in this world\\\\\". Daily Nation. http://www.nationmedia.com. Retrieved on 2008-07-10.\\\\n^ Scott Fornek (2007-09-09). \\\\\"AUMA OBAMA: \\'Her restlessness, her independence\\'\\\\\". Chicago Sun Times. http://www.suntimes.com/news/politics/obama/familytree/545465,BSX-News-wotreew09.article. Retrieved on 2008-03-23.\\\\n^ a b c Gathmann, Florian; Gregor Peter Schmitz, Jochen Schýýnmann (July 24, 2008). \\\\\"Studentin in der Bundesrepublik: Wie Auma Obama mit Deutschland haderte\\\\\" (in German). Spiegel Online. http://www.spiegel.de/politik/ausland/0,1518,567286,00.html. Retrieved on 2008-07-24.\\\\n^ a b Harvey, Oliver (07-26 2008). \\\\\"Obama\\'s brother is in Bracknell\\\\\". The Sun. http://www.thesun.co.uk/sol/homepage/news/the_real_american_idol/article1472877.ece. Retrieved on 2008-10-06.\\\\n^ \\\\\"Madari Kindergarten\\\\\". http://www.madarikindergarten.com/.\\\\n^ \\\\\"Welcome To MedWeek San Antonio 2007\\\\\". Medweeksa.org. http://www.medweeksa.org/awardwinners/techfirm.htm. Retrieved on 2009-01-31.\\\\n^ \\\\\"PIDE - Partners for International Development & Education Inc\\\\\". Pideafrica.org. http://pideafrica.org/aboutus.htm. Retrieved on 2009-01-31.\\\\n^ Barack Obamaýýýs brother pushes Chinese imports on US - Times Online\\\\n^ Obama half-brother runs Internet company in China\\\\n^ Roger Cohen (2008-03-17). \\\\\"Obama\\'s Brother in China\\\\\". The New York Times. http://www.nytimes.com/2008/03/17/opinion/29cohen.html. Retrieved on 2008-03-23.\\\\n^ \\\\\"Youku Buzz (daily)\\xa0ýý Blog Archive\\xa0ýý Barack Obamaýýýs Half-Brother in Concert\\\\\". Buzz.youku.com. 2009-01-18. http://buzz.youku.com/2009/01/18/barack-obamas-half-brother-in-concert/. Retrieved on 2009-01-31.\\\\n^ jaketapper (2008-07-28). \\\\\"Political Punch: Barack Obama\\'s Branch-y Family Tree\\\\\". Blogs.abcnews.com. http://blogs.abcnews.com/politicalpunch/2008/07/barack-obamas-1.html. Retrieved on 2009-01-31.\\\\n^ Fornek, Scott (September 9, 2007). \\\\\"HALF-BROTHER GEORGE: \\'I would be there for him\\'\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545447,BSX-News-wotreecc09.stng. Retrieved on 2008-08-04.\\\\n^ a b c d e Pflanz, Mike (August 21, 2008). \\\\\"Barack Obama is my inspiration, says lost brother\\\\\". The Daily Telegraph. http://www.telegraph.co.uk/news/newstopics/uselection2008/barackobama/2595688/Barack-Obama-is-my-inspiration-says-lost-brother.html. Retrieved on 2008-08-23.\\\\n^ a b c Crilly, Rob (August 22, 2008). \\\\\"Life is good in my Nairobi slum, says Barack Obama\\'s younger brother\\\\\". The Times. http://www.timesonline.co.uk/tol/news/world/us_and_americas/us_elections/article4583353.ece. Retrieved on 2008-08-23.\\\\n^ a b McKenzie, David (2008-08-23). \\\\\"Behind the Scenes: Meet George Obama\\\\\". CNN. http://www.cnn.com/2008/POLITICS/08/22/bts.obama.brother/. Retrieved on 2008-10-26.\\\\n^ Pisa, Nick (August 20, 2008). \\\\\"Barack Obama\\'s \\'lost\\' brother found in Kenya\\\\\". Daily Telegraph. http://www.telegraph.co.uk/news/newstopics/uselection2008/barackobama/2590614/Barack-Obamas-lost-brother-found-in-Kenya.html. Retrieved on 2008-08-20.\\\\n^ a b Wadhams, Nick (2008-10-07). \\\\\"Corsi in Kenya: Obama\\'s Nation Boots Obama Nation Author\\\\\". TIME. http://www.time.com/time/world/article/0,8599,1847965,00.html?imw=Y. Retrieved on 2009-01-31.\\\\n^ by Dinesh D\\'Souza. \\\\\"Dinesh D\\'Souza\\xa0: George Obama, Start Packing\\\\\". Townhall.com. http://townhall.com/columnists/DineshDSouza/2008/09/22/george_obama,_start_packing. Retrieved on 2009-01-31.\\\\n^ a b c \\\\\"The Obama Family Tree\\\\\" (PDF). Chicago Sun-Times. September 9, 2007. http://www.suntimes.com/images/cds/MP3/obamatree.pdf. Retrieved on 2008-11-23.\\\\n^ First read, MSNBC\\\\n^ \\\\\"Barack Obama\\'s aunt found living in rundown public housing estate | The Australian\\\\\". Theaustralian.news.com.au. 2008-10-31. http://www.theaustralian.news.com.au/story/0,25197,24578185-5017121,00.html. Retrieved on 2009-01-31.\\\\n^ Boston Housing Authority ýýýflabbergasteredýýý Barack Obamaýýýs aunt living in Southie\\\\n^ Kilner, Derek (2008-11-05). \\\\\"Kenya Celebrates President Obama as Native Son\\\\\". Voice Of America. http://www.voanews.com/english/archive/2008-11/2008-11-05-voa45.cfm. Retrieved on 2008-12-24.\\\\n^ \\\\\"Oregon State University Beavers: Craig Robinson bio\\\\\". http://www.osubeavers.com/ViewArticle.dbml?SPSID=106239&SPID=1954&DB_OEM_ID=4700&ATCLID=1436883&Q_SEASON=2008. Retrieved on 2008-08-21.\\\\n^ \\\\\"RootsWeb\\'s WorldConnect Project: Dowling Family Genealogy\\\\\". Wc.rootsweb.ancestry.com. http://wc.rootsweb.ancestry.com/cgi-bin/igm.cgi?op=GET&db=dowfam3&id=I105855. Retrieved on 2009-01-31.\\\\n^ Weiss, Anthony (September 2, 2008). \\\\\"Michelle Obama Has a Rabbi in Her Family\\\\\". The Forward. http://www.forward.com/articles/14121/. Retrieved on 2008-10-09.\\\\n^ Gary Boyd Roberts. \\\\\"Notes on the Ancestry of Senator Barack Hussein Obama, Jr\\\\\". New England Historic Genealogical Society. http://www.newenglandancestors.org/research/services/articles_ancestry_barack_obama.asp. Retrieved on 2009-01-31.\\\\n^ Scott Fornek (2007-09-09). \\\\\"Obama Family Tree\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545463,BSX-News-wotreer09.stng. Retrieved on 2009-01-31.\\\\n^ \\\\\"Obama-Bush (family tree)\\\\\" (PDF). New England Historic Genealogical Society. http://www.newenglandancestors.org/pdfs/obama_bush.pdf. Retrieved on 2009-01-31.\\\\n^ Wade, Nicholas (2007-10-21). \\\\\"Barack Obama - Dick Cheney - Ancestry and Genealogy - Washington - New York Times\\\\\". Nytimes.com. http://www.nytimes.com/2007/10/21/weekinreview/21basic.html. Retrieved on 2009-01-31.\\\\n^ Eastman, Dick (2008-07-30). \\\\\"Barack Obama is Related to Wild Bill Hickok\\\\\". Blog.eogn.com. http://blog.eogn.com/eastmans_online_genealogy/2008/07/barack-obama-is.html. Retrieved on 2009-01-31.\\\\n^ a b c Scott Fornek (2007-09-09). \\\\\"Obama Family Tree\\\\\". Suntimes.com. http://www.suntimes.com/news/politics/obama/familytree/545441,BSX-News-wotreec09.stng. Retrieved on 2009-01-31.\\\\n^ a b \\\\\"Notes on the Ancestry of Senator Barack Hussein Obama, Jr\\\\\". New England Historic Genealogical Society. 2002-08-14. http://www.newenglandancestors.org/research/services/articles_ancestry_barack_obama.asp. Retrieved on 2009-01-31.\\\\n^ \\\\\"Obama, Clinton and McCain have some famous relations\\\\\". HeraldNet - AP. 2008-03-26. http://www.heraldnet.com/article/20080326/NEWS02/151343661. Retrieved on 2009-01-31.\\\\n^ \\\\\"Barack Obama and Joe Biden: The Change We Need\\\\\". My.barackobama.com. 2008-07-31. http://my.barackobama.com/page/community/post/williambrehm/gG5TVR. Retrieved on 2009-01-31.\\\\nExternal links\\\\nBarack Obama\\'s Family Tree - Photo Essays - TIME\\\\n\\\\\"Though Obama Had to Leave to Find Himself, It Is Hawaii That Made His Rise Possible,\\\\\" by David Maraniss\\\\nBarack Obama\\'s Branch-y Family Tree by Jake Tapper\\\\n\\\\\"Obama Family Tree\\\\\" series, by Scott Fornek\\\\n\\\\\"Six Degrees of Barack Obama\\\\\"\\\\n\\\\\"Notes on the Ancestry of Senator Barack Hussein Obama, Jr.,\\\\\" by Gary Boyd Roberts\\\\n\\\\\"Obama, Clinton and McCain have some famous relations,\\\\\" by The Associated Press\\\\n\\\\\"Obama\\'s Patriotic Family Tree,\\\\\" by Bill Brehm\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nBarack Obama\\\\nPresidency\\\\nTransition\\xa0ýý Inauguration\\xa0ýý Timeline\\xa0ýý Cabinet\\xa0ýý Judiciary\\xa0ýý Foreign policy\\xa0ýý First 100 days\\\\nEarly life and\\\\npolitical career\\\\nPublic image\\xa0ýý Illinois Senate career\\xa0ýý 2004 Democratic National Convention\\xa0ýý U.S. Senate election in Illinois\\xa0ýý U.S. Senate career\\xa0ýý Presidential primary campaign\\xa0ýý ObamaýýýBiden 2008\\xa0ýý Electoral history\\xa0ýý Political positions\\\\nBooks\\\\nDreams from My Father\\xa0ýý The Audacity of Hope\\\\nSpeeches\\\\nThe Audacity of Hope\\xa0ýý A More Perfect Union\\xa0ýý Change Has Come to America\\xa0ýý 2009 speech to joint session of Congress\\\\nFamily\\\\nMichelle Obama\\xa0ýý Barack Obama, Sr.\\xa0ýý Ann Dunham\\xa0ýý Lolo Soetoro (stepfather)\\xa0ýý Maya Soetoro-Ng (half-sister)\\xa0ýý Marian Robinson (mother-in-law)\\xa0ýý Stanley Armour Dunham (grandfather)\\xa0ýý Madelyn Dunham (grandmother)\\xa0ýý Extended family\\xa0ýý Family tree\\\\nRetrieved from \\\\\"http://en.wikipedia.org/wiki/Family_of_Barack_Obama#Malia_and_Sasha_Obama\\\\\"\\\\nCategories: Obama family | African American history | African American families | Luo Kenyans | People of mixed Black African-European ethnicity | African Americans | Asian Americans | Dutch Americans | English Americans | French Americans | German-Americans | Irish-Americans | Indonesian Americans | Kenyan-Americans | Scottish-Americans | Chinese Canadians | People of mixed Asian-European ethnicity | American families | First Families of the United States | Family treesHidden categories: Wikipedia semi-protected pages | Wikipedia indefinitely move-protected pages | All pages needing cleanup | Wikipedia articles needing factual verification since October 2008 | All pages needing factual verification | All articles with unsourced statements | Articles with unsourced statements since November 2008\\\\nViews\\\\nArticle\\\\nDiscussion\\\\nView source\\\\nHistory\\\\nPersonal tools\\\\nLog in / create account\\\\nNavigation\\\\nMain page\\\\nContents\\\\nFeatured content\\\\nCurrent events\\\\nRandom article\\\\nSearch\\\\nInteraction\\\\nAbout Wikipedia\\\\nCommunity portal\\\\nRecent changes\\\\nContact Wikipedia\\\\nDonate to Wikipedia\\\\nHelp\\\\nToolbox\\\\nWhat links here\\\\nRelated changes\\\\nUpload file\\\\nSpecial pages\\\\nPrintable version Permanent linkCite this page\\\\nLanguages\\\\nBahasa Indonesia\\\\nSvenska\\\\nýýýýýý\\\\nýýýýýý\\\\nThis page was last modified on 14 March 2009, at 05:50.\\\\nAll text is available under the terms of the GNU Free Documentation License. (See Copyrights for details.)\\\\nWikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a U.S. registered 501(c)(3) tax-deductible nonprofit charity.\\\\nPrivacy policy\\\\nAbout Wikipedia\\\\nDisclaimers\\n[DOC 2] Family of Barack Obama - Wikipedia, the free encyclopedia\\\\nFamily of Barack Obama\\\\nFrom Wikipedia, the free encyclopedia\\\\n(Redirected from Ruth Obama)\\\\nJump to: navigation, search\\\\nObama Family\\\\nPresident Barack Obama, First Lady Michelle, and daughters Malia and Sasha wave to the crowd after his inaugural address Jan. 20, 2009, on the west steps of the U.S. Capitol.[1]\\\\nCurrent region\\\\nWashington, DC\\\\nInformation\\\\nPlace of origin\\\\nUnited States\\\\nNotable members\\\\nBarack Obama, Michelle Obama, Ann Dunham, Barack Obama, Sr., etc.\\\\nConnected families\\\\nRobinson, Dunham, Soetoro, Ng\\\\nThis article is part of a series about\\\\nBarack Obama\\\\nBackground \\xa0ýý Illinois Senate \\xa0ýý U.S. Senate\\\\nPolitical positions\\xa0ýý Public image\\xa0ýý Family\\\\n2008 primaries\\xa0ýý ObamaýýýBiden campaign\\\\nTransition\\xa0ýý Inauguration\\xa0ýý Electoral history\\\\nPresidency (Timeline, First 100 days)\\\\nMalia, Michelle and Sasha on stage at the 2008 Democratic National Convention\\\\nThe Family of Barack Obama is an extended clan of African American, English, Indonesian, and Kenyan (Luo) heritage known through the writings and political career of Barack Obama, the President of the United States of America,[2][3][4][5] and other reports. His immediate family is the First Family of the United States. The Obamas are the first First Family of African American descent in the United States and the youngest to enter the White House since the Kennedys. One columnist wrote, ýýýObama\\'s young, energetic family harks back to days of Camelot.ýýý[6]\\\\nContents\\\\n1 Immediate family\\\\n2 Extended family - maternal relations\\\\n3 Extended family - paternal relations\\\\n4 Michelle Robinson Obama\\'s extended family\\\\n5 Genealogical charts\\\\n5.1 Obama ancestry\\\\n5.2 Family trees\\\\n6 Distant relations\\\\n7 See also\\\\n8 References\\\\n9 External links\\\\nImmediate family\\\\nMichelle Obama\\\\nMichelle Obama, nýýe Robinson, the wife of Barack Obama, was born on January 17, 1964 in Chicago, Illinois. She is a lawyer and was a University of Chicago Hospital Vice-President. She is the First Lady of the United States.\\\\nMalia Obama and Sasha Obama\\\\nBarack and Michelle Obama have two daughters: Malia Ann (pronounced /mýýýýliýýýý/), born in 1998,[7] and Natasha (known as Sasha) /ýýsýýýýýýýý/), born in 2001. Sasha is the youngest child to reside in the White House since John F. Kennedy, Jr, arrived as an infant in 1961.[8]\\\\nBefore his inauguration, President Obama published an open letter to his daughters in Parade magazine, describing what he wants for them and every child in America: \\\\\"to grow up in a world with no limits on your dreams and no achievements beyond your reach, and to grow into compassionate, committed women who will help build that world.\\\\\"[9]\\\\nWhile living in Chicago, they kept busy schedules, as the Associated Press reports: \\\\\"soccer, dance and drama for Malia, gymnastics and tap for Sasha, piano and tennis for both.\\\\\"[10][11] In July 2008, the family gave an interview to the television series Access Hollywood; Obama later said they regretted allowing the children to be included.[12]\\\\nIn his victory speech on the night of his election, President Obama repeated his promise to Sasha and Malia to get a puppy to take with them to the White House.[13] However the selection of a dog has been slow because Malia is allergic to animal dander;[14] the president subsequently said that the choice has been narrowed down to either a labradoodle or Portuguese Water Dog, and they are hoping to find a shelter animal.[15]\\\\nMalia and Sasha attend the private Sidwell Friends School in Washington, DC, the same school as attended by Chelsea Clinton, Tricia Nixon Cox, and Archibald Roosevelt, and currently the grandchildren of Vice President Joe Biden.[16] The Obama girls began classes there on January 5, 2009.[17] While in Chicago, both attended the private University of Chicago Laboratory School.\\\\nMarian Shields Robinson\\\\nMichelle Obama\\'s mother (birthname Marian Shields, born July 1937), now widowed, married Michelle\\'s father, Fraser Robinson, in 1960.[18][19] Robinson was formerly a secretary at Spiegel catalog and a bank. While Michelle and Barack Obama were campaigning in 2008, Robinson tended the Obama\\'s young children and she intends to do the same while in Washington, DC. Robinson is currently living in the White House itself as part of the First Family;[20] she is the first live-in grandmother there since Elivera M. Doud during the Eisenhower administration.[21] Some media outlets have dubbed Robinson the \\\\\"First Granny\\\\\".[22][21]\\\\nExtended family - maternal relations\\\\nRight-to-left: Barack Obama and Maya Soetoro with their mother Ann Dunham and grandfather Stanley Dunham in Hawaii (early 1970s)\\\\nWikinews has related news:\\\\nBarack Obama elected 44th President of the United States\\\\nGrandmother of Barack Obama dies at 86\\\\nMadelyn Dunham with her daughter Ann\\\\nAccording to Barack Obama\\'s Dreams from My Father, his great-grandmother Leona McCurry was part Native American, which Obama believed Leona held as a \\\\\"source of considerable shame\\\\\" and \\\\\"blanched whenever someone mentioned the subject and hoped to carry the secret to her grave\\\\\"; whereas McCurry\\'s daughter (Obama\\'s maternal grandmother) \\\\\"would turn her head in profile to show off her beaked nose, which along with a pair of jet-black eyes, was offered as proof of Cherokee blood.\\\\\"[23] To date, no concrete evidence has surfaced of Cherokee heritage. Obama\\'s maternal heritage consists mostly of English ancestry, with much smaller amounts of German, Irish, Scottish, Welsh, Swiss, and French ancestry.[2]\\\\nAnn Dunham\\\\nMother of Barack Obama born in 1942, died in 1995. Birthname Stanley Ann Dunham. Anthropologist in Hawaii and Indonesia.\\\\nMadelyn Lee Payne Dunham\\\\nBarack Obama\\'s maternal grandmother, born in 1922 and died on November 2, 2008.[24] She was a bank vice president in Hawaii. Obama said that when he was a child, his grandmother \\\\\"read me the opening lines of the Declaration of Independence and told me about the men and women who marched for equality because they believed those words put to paper two centuries ago should mean something.\\\\\"[9]\\\\nStanley Armour Dunham\\\\nGrandfather of Barack Obama, born 1918, died 1992. World War II U.S. Army sergeant, furniture salesman in Hawaii.\\\\nCharles T. Payne\\\\nGreat-uncle of Barack Obama, younger brother of Madelyn Dunham, born 1925. Served during World War II in the U.S. Army 89th Infantry Division.[25] Obama has often described Payne\\'s role in liberating Ohrdruf forced labor camp.[26] There was brief media attention when Obama mistakenly identified the camp as Auschwitz during the campaign.[27] Payne appeared in the visitor\\'s gallery at the Democratic National Convention in Denver, Colorado, when his great-nephew was nominated for President.[28] He was the assistant director of the University of Chicago\\'s Library.[26]\\\\nMaya Soetoro-Ng\\\\nHalf-sister of Barack Obama, born August 15, 1970, in Jakarta, Indonesia.[29] She is married to Konrad Ng, with whom she has a daughter, Suhaila. Maya Soetoro-Ng is a teacher in Hawaii.\\\\nKonrad Ng\\\\nBrother-in-law of Barack Obama, born 1974. A Canadian whose parents are Malaysian Chinese immigrants, he is an assistant professor at the University of Hawaii\\'s Academy of Creative Media.[30] His parents are from Kudat and Sandakan, two small towns in Sabah, Malaysia, and he was born and raised in Burlington, Ontario.[31] He married Maya Soetoro-Ng at the end of 2003 in Hawaii.[32] They have one daughter, Suhaila.[33][34][35] Konrad Ng is now a US citizen.[36]\\\\nLolo Soetoro\\\\nStepfather of Barack Obama, born in Indonesia 1936, died 1987.\\\\nExtended family - paternal relations\\\\nThe Obamas are members of the Luo, Kenya\\'s third-largest ethnic group, which is part of a larger family of ethnic groups, collectively also known as Luo. This group belongs to the Eastern Sudanic branch of the Nilo-Saharan phylum. The Obama family is largely concentrated in the western province of Nyanza.\\\\nFront row (left to right): Auma Obama (Barack\\'s half-sister), Kezia Obama (Barack\\'s step-mother), Sarah Hussein Onyango Obama (third wife of Barack\\'s paternal grandfather), Zeituni Onyango (Barack\\'s aunt)\\\\nBack row (left to right): Said Obama (Barack\\'s uncle), Barack Obama, Abongo [Roy] Obama (Barack\\'s half-brother), unidentified woman, Bernard Obama (Barack\\'s half-brother), Abo Obama (Barack\\'s half-brother).\\\\nBarack Obama, Sr.\\\\nBarack Obama\\'s father, (1936ýýý1982). Government economist in Kenya. In addition to President Obama, Barack Obama Sr. fathered six other sons and a daughter.[37]\\\\nHussein Onyango Obama\\\\nBarack Obama\\'s paternal grandfather (c. 1895ýýý1979);[38] he worked as a mission cook. He joined the British Army during World War I. (One source gives 1870ýýý1975 as his dates of birth and death based on his tombstone reading \\\\\"Mzee Hussein Onyango Obama\\\\\" in his home village.[39] The term \\\\\"mzee\\\\\" is a Kenyan honorific meaning \\\\\"old man\\\\\" or \\\\\"elder.\\\\\") According to his third wife, Sarah, he originally converted to Catholicism, but took the name Hussein when he later converted to Islam; she said he passed the name, not the religion, on to his children.[40]\\\\nHabiba Akumu Obama\\\\nBarack Obama\\'s paternal grandmother, and the second wife of Hussein Onyango Obama. A photograph of her holding her son, Barack Sr, on her lap is on the cover of her grandson\\'s memoirs titled Dreams from my Father.[41]\\\\nSarah Obama\\\\nThird wife of Obama\\'s paternal grandfather, born 1922.[39] Also known, through the addition of her late husband\\'s name, as Sarah Onyango Obama,[42] and sometimes referred to as Sarah Ogwel, Sarah Hussein Obama or Sarah Anyango Obama,[43] she lives in Nyangýýýoma Kogelo village, 30 miles west of western Kenya\\'s main town, Kisumu, on the edge of Lake Victoria.[44][45]\\\\nAlthough not a blood relation, Barack Obama calls her \\\\\"Granny Sarah\\\\\".[43][46] Sarah, who speaks Luo and only a few words of English, communicates with President Obama through an interpreter.\\\\nOn July 4, 2008, she attended the United States Independence Day celebrations in Nairobi, hosted by Michael Ranneberger, the US ambassador in Kenya.[47]\\\\nDuring the campaign she protested attempts to portray Obama as a foreigner to the United States or a Muslim, saying that while Obama\\'s grandfather had been a Muslim, \\\\\"In the world of today, children have different religions from their parents.\\\\\"[40] Sarah Obama herself is \\\\\"a strong believer of the Islamic faith,ýýý in her words.[48]\\\\nKezia Obama\\\\nFirst wife of Barack Obama\\'s father, born c. 1940. She is Barack Obama Sr.\\'s first wife, whom he married in Kenya before studying abroad in the United States. Also known as Kezia Grace Obama.[49][50] She currently lives in Bracknell, Berkshire, England.[51][52] Her sister, Jane, is the \\'Auntie Jane\\' mentioned at the very start of Dreams from My Father when she telephoned President Obama to inform him that his father had been killed in a car accident.[53]\\\\nMalik Obama\\\\nBarack Obama\\'s half-brother, also known as Abongo or Roy, born c. March, 1958,[51] son of Barack Obama, Sr. with his first wife, Kezia.[54] Malik Obama was born and raised in Nairobi, Kenya.[55] He earned a degree in accounting from the University of Nairobi.[56] He met his half-brother for the first time in 1985[55] when Barack flew from Chicago to Washington, D.C. to visit him.[57] Malik and his half-brother Barack were best men at each other\\'s weddings.[55] Barack Obama brought his wife Michelle to Kenya three years later, and they met with Malik again while Barack was introducing Michelle to many other new relatives.[58]\\\\nAlthough much of the Obama family has dispersed throughout Kenya and overseas, most, including Malik Obama, still considered their rural village on the shores of Lake Victoria to be their true home, and feel that those who have left the village have become culturally \\\\\"lost\\\\\".[59] A frequent visitor to the United States,[58] and consultant in Washington, D.C. for several months per year,[55] he nevertheless settled in the Obamas\\' ancestral home, Nyangýýýoma Kogelo, a village of several hundred people that he prefers to the city for its slow pace.[55] He runs a small electronics shop a half hour drive outside of town.[55]\\\\nDuring his brother\\'s presidential campaign, Malik Obama was a spokesman for the extended Obama family in Kenya, dealing with safety and privacy concerns arising from increased attention from the press.[60]\\\\nAbo Obama\\\\nBarack Obama\\'s half-brother, born 1968. International telephone store manager in Kenya.\\\\nAuma Obama\\\\nBarack Obama\\'s half-sister, born c. 1960.[61] As of July 2008, development worker in Kenya.[62] She studied German at the University of Heidelberg from 1981 to 1987. After her graduation in Heidelberg she went on for graduate studies at the University of Bayreuth, which awarded her a PhD in 1996. Her dissertation was about the conception of labor in Germany and its literary reflections.[62] Auma Obama lives in London, and in 1996 married an Englishman, Ian Manners. They have a daughter named Akinyi (b. 1997).[62][verification needed]\\\\nBernard Obama\\\\nBarack Obama\\'s half-brother, born 1970, son of Barack Obama, Sr. and his first wife, Kezia. He had been an auto parts supplier in Nairobi, Kenya, and has one child. Bernard converted to Islam as an adult and has said: \\\\\"Iýýým a Muslim, I donýýýt deny it. My father was raised a Muslim. But itýýýs not an issue. I donýýýt know what all the hullabaloo is about.\\\\\"[63] He currently resides in Bracknell, England, with his mother Kezia.[63]\\\\nRuth Ndesandjo\\\\nBorn Ruth Nidesand, in US c. 1940s, Barack Obama Sr.\\'s third wife and a private kindergarten director in Kenya.[64] Ruth\\'s two sons with Barack Obama, Sr., are Mark and David Ndesandjo; her third son, Joseph Ndesandjo, was born c. 1980 from a subsequent marriage to a Tanzanian.[65][66]\\\\nMark Ndesandjo\\\\nBarack Obama\\'s half-brother, son of Ruth Nidesand and Barack Obama Sr.[67] He runs an Internet company called WorldNexus that advises Chinese corporations how best to reach international customers.[68] Mark graduated from Brown University, studied physics at Stanford University, received an MBA from Emory University, and has lived in Shenzhen, China, since 2002 and is married to a Chinese woman.[69] He is also an accomplished pianist.[70]\\\\nDavid Ndesandjo\\\\nDavid Ndesandjo\\\\nBarack Obama\\'s half-brother (also known as David Opiyo Obama), son of Ruth Nidesand and Barack Obama Sr. Killed in a motorcycle accident.[71]\\\\nGeorge Hussein Onyango Obama\\\\nYoungest half-brother of Barack Obama, born c.1982, son of Barack Obama Sr.[72] and Jael (now a resident of Atlanta, Georgia).[73][74] George was six months old when his father died in an automobile accident, after which he was raised in Nairobi by his mother and a French step-father. He later lived in South Korea for two years while his mother resided there for business reasons.[73] Returning to Kenya, George Obama \\\\\"slept rough for several years,\\\\\" until his aunt gave him a six-by-eight foot corrugated metal shack in the Nairobi, Kenya, slum of Huruma Flats.[73] As of August 2008, Obama was studying to become a mechanic.[73] George received little attention until being featured in an article in the Italian language edition of Vanity Fair in August 2008, which portrayed him as living in poverty, shame, and obscurity.[75] The article quoted Obama as saying that he lived \\\\\"on less than a dollar a month\\\\\" and stated that he \\\\\"does not mention his famous half-brother in conversation\\\\\" out of shame at his poverty.[76] In later interviews George contradicted this picture. In an interview with The Times, Obama \\\\\"said that he was furious at subsequent reports that he had been abandoned by the Obama family and that he was filled with shame about living in a slum.\\\\\"[74] He told The Times, \\\\\"Life in Huruma is good.\\\\\" Obama said that he expects no favors, that he was supported by relatives, and that reports he lived on a dollar a month were \\\\\"all lies by people who donýýýt want my brother to win.ýýý[74] He told The Telegraph that he was inspired by his half-brother.[73] According to Time, George \\\\\"has repeatedly denied...that he feels abandoned by Obama.\\\\\"[77] CNN quoted him as saying, \\\\\"I was brought up well. I live well even now. The magazines, they have exaggerated everything... I think I kind of like it here. There are some challenges, but maybe it is just like where you come from, there are the same challenges.\\\\\"[75] George\\'s reported poverty was seized on by conservative critics of Barack Obama. Columnist Dinesh D\\'Souza solicited donations for George Obama from his readers,[78] while Jerome Corsi planned to give him a $1,000 check during a trip to Kenya (Corsi was expelled from the country by immigration authorities).[77]\\\\nOmar Obama\\\\nHalf-uncle of Barack Obama,[79] born on June 3, 1944 in Nyangýýýoma Kogelo. Oldest son of Onyango and Sarah Obama, resides in Boston, Massachusetts.[citation needed]\\\\nZeituni Onyango\\\\nHalf-aunt of Barack Obama,[80] born May 29, 1952, in Kenya,[81] Onyango is referred to as \\\\\"Aunti Zeituni\\\\\" in President Obama\\'s memoir, Dreams from My Father.[82]\\\\nYusuf Obama\\\\nHalf-uncle of Barack Obama,[79] born c. 1950s in Nyangýýýoma Kogelo; son of Onyango and Sarah Obama.[citation needed]\\\\nSaid Obama\\\\nHalf-uncle of Barack Obama,[79][83] born c. 1950s in Nyangýýýoma Kogelo; son of Onyango and Sarah Obama.[citation needed]\\\\nMichelle Robinson Obama\\'s extended family\\\\nFraser Robinson, Sr. (1884ýýý1936) of South Carolina, shown in an old photo along with his wife, Rosella Cohen Robinson, in its background\\\\nBarack Obama has called his wife Michelle \\\\\"the most quintessentially American woman I know.\\\\\"[3] Her family is of African American heritage, descendents of Africans of the American Colonial Era.[3] Michelle Obama\\'s family history traces back from slavery to Reconstruction to the Great Migration North. Some of Michelle\\'s relatives still reside in South Carolina.\\\\nMichelle\\'s earliest known relative is her great-great grandfather Jim Robinson, born in the 1850s, who was an American slave on the Friendfield plantation in South Carolina. The family believes that after the Civil War he remained a Friendfield plantation sharecropper for the rest of his life and that he was buried there in an unmarked grave.[3]\\\\nJim had two sons, Gabriel and Fraser, Michelle Obama\\'s great-grandfather. Fraser had an arm amputated as a result of a boyhood injury. He worked as a shoemaker, newspaper salesman and in a lumber mill and was married to Rosella Cohen.[3] Carrie Nelson, Gabriel Robinson\\'s daughter, now 80, is the oldest living Robinson and the keeper of family lore.[3]\\\\nAt least three of Michelle Obama\\'s great-uncles served in the military of the United States. One aunt moved to Princeton, New Jersey, where she worked as a maid, and cooked Southern-style meals for Michelle and her brother, Craig, when they were students at Princeton University.\\\\nCraig Robinson\\\\nMichelle Obama\\'s brother, born 1962. He is currently head coach of men\\'s basketball at Oregon State University.[84]\\\\nFraser Robinson III\\\\nMichelle Obama\\'s father, born 1935, died 1991, married Michelle\\'s mother, Marian Shields, in 1960.[85][19] Robinson was a pump worker at the City of Chicago water plant.[3]\\\\nFraser Robinson, Jr.\\\\nMichelle Obama\\'s grandfather was born on August 24, 1912 in Georgetown, South Carolina, and died on November 9, 1996, aged 84. He was a good student and orator, but moved from South Carolina to Chicago to find better work than he could find at home, eventually becoming a worker for the United States Postal Service. He was married to LaVaughn Johnson. When he retired, they moved back to South Carolina.[3]\\\\nCapers C. Funnye Jr.\\\\nMichelle Obama\\'s first cousin once removed: Funnyeýýýs mother, Verdelle Robinson Funnye (born Verdelle Robinson; August 22, 1930 ýýý April 16, 2000) and Michelle Obamaýýýs paternal grandfather, Fraser Robinson Jr., were siblings. One of America\\'s most prominent African American Jews, known for acting as a bridge between mainstream Jewry and African Americans.[86]\\\\nGenealogical charts\\\\nObama ancestry\\\\n16. Opiyo\\\\n8. Obama\\\\n4. Hussein Onyango Obama\\\\n9. Nyaoke\\\\n2. Barack Hussein Obama, Sr.\\\\n5. Habiba Akumu\\\\n1. Barack Hussein Obama II\\\\n24. Jacob William Dunham\\\\n12. Ralph Waldo Emerson Dunham, Sr.\\\\n25. Mary Ann Kearney\\\\n6. Stanley Armour Dunham\\\\n26. Harry Ellington Armour\\\\n13. Ruth Lucille Armour\\\\n27. Gabriella Clark\\\\n3. Stanley Ann Dunham\\\\n28. Charles T. Payne\\\\n14. Rolla Charles Payne\\\\n29. Della L. Wolfley\\\\n7. Madelyn Lee Payne\\\\n30. Thomas Creekmore McCurry\\\\n15. Leona Belle McCurry\\\\n31. Margaret Belle Wright\\\\nFamily trees\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nBarack Obama\\\\nStanley\\\\nDunham\\\\n1918ýýý1992\\\\nMadelyn\\\\nPayne\\\\n1922ýýý2008\\\\nHabiba\\\\nAkumu\\\\nOnyango\\\\nObama\\\\nc.\\xa01895ýýý1979\\\\nSarah\\\\nOgwel\\\\n1922ýýý\\\\nLolo\\\\nSoetoro\\\\n1936ýýý1987\\\\nAnn\\\\nDunham\\\\n1942ýýý1995\\\\nBarack\\\\nObama, Sr. *\\\\n1936ýýý1982\\\\nKezia\\\\nGrace\\\\nc. 1940ýýý\\\\nOmar\\\\nObama\\\\n1944ýýý\\\\nZeituni\\\\nOnyango\\\\n1952ýýý\\\\nYusuf\\\\nObama\\\\nc. 1950sýýý\\\\nSaid\\\\nObama\\\\nc. 1950sýýý\\\\nKonrad\\\\nNg\\\\nc. 1974ýýý\\\\nMaya\\\\nSoetoro\\\\n1970ýýý\\\\nBarack\\\\nObama\\\\n1961ýýý\\\\nMichelle\\\\nRobinson\\\\n1964ýýý\\\\nM. Abongo\\\\nObama\\\\n1958ýýý\\\\nAuma\\\\nObama\\\\nc. 1960ýýý\\\\nAbo\\\\nObama\\\\n1968ýýý\\\\nBernard\\\\nObama\\\\n1970ýýý\\\\nSuhaila\\\\nNg\\\\nc. 2005ýýý\\\\nMalia Ann\\\\nObama\\\\n1998ýýý\\\\nSasha\\\\nObama\\\\n2001ýýý\\\\n* Barack\\\\nObama, Sr.\\'s\\\\nadditional\\\\nRuth\\\\nNidesandjo\\\\nc. 1940sýýý\\\\nJael\\\\nOtieno\\\\nrelationships:\\\\nMark\\\\nNdesandjo\\\\nDavid\\\\nNdesandjo\\\\ndied\\xa0c.\\xa01987\\\\nGeorge\\\\nObama\\\\nc. 1982ýýý\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nFirst Lady Michelle Obama\\\\nFraser\\\\nRobinson, Jr.\\\\nýýý\\\\nSouth Carolina\\\\n(1912ýýý1996)\\\\nSawmill worker\\\\nLaVaughn D.\\\\nJohnson\\\\nýýý\\\\nIllinois\\\\n(1915ýýý2002)\\\\nCapers C.\\\\nFunnye, Jr.\\\\nýýý\\\\n(born c. 1952;\\\\nnephew of\\\\nFraser\\\\nRobinson, Jr.)\\\\nRabbi in Chicago\\\\nFraser\\\\nRobinson III\\\\nýýý\\\\nIllinois\\\\n(1935ýýý1991)\\\\nEnjoyed boxing\\\\nin youth;\\\\nwater plant\\\\npump operator\\\\nin Chicago\\\\nMarian\\\\nShields\\\\nýýý\\\\nIllinois\\\\n(born 1937)\\\\nSecretary at\\\\nSpiegel catalog\\\\nin Chicago;\\\\nU.S.\\'s First\\\\nGrandmother\\\\nCraig\\\\nRobinson\\\\nýýý\\\\nIllinois\\\\n(born 1962)\\\\nHead coach of\\\\nOregon State\\\\nBeavers men\\'s\\\\nbasketball\\\\nMichelle\\\\nRobinson\\\\nýýý\\\\nIllinois\\\\n(born 1964)\\\\nFirst Lady\\\\nof the United States\\\\nDistant relations\\\\nSee also: List of United States Presidents by genealogical relationship\\\\nAccording to genealogists, Barack Obama\\'s distant cousins include the multitude of descendants of his maternal ancestors from all along the early-American Atlantic seaboard as well as paternal, Kenyan relations belonging to the Luo tribe, many descending from a 17th century ancestor named Owiny.[87][88] For example, George W. Bush, the 43rd U.S. president, is the eleventh cousin of Barack Obama.[89] The New York Times science writer Nicholas Wade argues that with eleven generations leading back to their common progenitor, Samuel Hinckley, the relationship between the 43rd President and the 44th President is \\\\\"genetically meaningless\\\\\".[90]\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nSelected genealogical relationships\\\\nBill Hickok\\\\nAccording to Barack Obama\\'s family lore (and confirmed by the New England Historic Genealogical Society), the President and Hickok are sixth cousins, six-times removed.[91]\\\\nRobert Duvall\\\\nGoodnight helped inspire Pulitzer Prize-winning author Larry McMurtry to create a protagonist for his novel series Lonesome Dove: Woodrow Call. In Dove\\'s television novela, Woodrow Call\\'s partner is Gus McCrae, portrayed by Obama\\'s eighth cousin, twice removed, actor Robert Duvall.[92]\\\\nJames Madison\\\\nObama is also distantly related to U.S. Presidents James Madison, Lyndon Johnson, Gerald Ford, and U.S. Vice President Dick Cheney, British Prime Minister Sir Winston Churchill, U.S. Civil War General Robert E. Lee, and actor Brad Pitt.[93][94][95]\\\\nCharles Goodnight\\\\nAccording to Chicago Sun-Times reporter Scott Fornek, another Obama progenitor, Catherine Goodnight, was the grandniece of George Goodnight, who was in turn great-grandfather of famed cattleman Charles Goodnight.[92]\\\\nHarry S. Truman\\\\nActor Duvall is distant cousins with United States President Harry Truman, who\\'s likewise a fourth cousin, four times removed, of Obama\\'s.[92]\\\\nGeorgia O\\'Keeffe\\\\nNotable women Obama is related to include painter Georgia OýýýKeeffe.[93]\\\\nSee also\\\\nU.S. Presidential genealogical relationships\\\\nNew England Historic Genealogical Society\\\\nGary Boyd Roberts\\\\nReferences\\\\n^ Keck, Kristi (4 June 2008). \\\\\"Obama crosses historic milestone\\\\\". CNN. http://news.yahoo.com/s/ap/20081122/ap_on_go_pr_wh/obama_school_12. Retrieved on 2008-11-21.\\\\n^ a b Reitwiesner, William Addams. \\\\\"Ancestry of Barack Obama\\\\\". http://www.wargs.com/political/obama.html. Retrieved on 2008-10-09.\\\\n^ a b c d e f g h Murray, Shailagh (2 October 2008). \\\\\"A Family Tree Rooted In American Soil: Michelle Obama Learns About Her Slave Ancestors, Herself and Her Country\\\\\". The Washington Post: p.\\xa0C01. http://www.washingtonpost.com/wp-dyn/content/article/2008/10/01/AR2008100103169.html. Retrieved on 2008-10-10.\\\\n^ Sheridan, Michael (5 February 2007). \\\\\"Secrets of Obama Family Unlocked\\\\\". Muslim Observer. http://news.newamericamedia.org/news/view_article.html?article_id=ae5895fc29971b172938790be94ab107. Retrieved on 2008-11-21.\\\\n^ RTE news report March, 2007:Obamas Irish family links discovered by ancestry.co.uk\\\\n^ Noveck, Jocelyn (2008-11-07). \\\\\"Deseret News | Obama\\'s young, energetic family harks back to days of Camelot\\\\\". Secure.deseretnews.com. https://secure.deseretnews.com/article/1,5143,705261276,00.html. Retrieved on 2009-01-31.\\\\n^ Liza Mundy, Michelle: A Biography (Simon and Schuster, 2008), p. 129.\\\\n^ \\\\\"Sasha Obama\\\\\". Baltimore Sun. http://www.baltimoresun.com/topic/politics/sasha-obama-PECLB004381.topic. Retrieved on 2009-01-31.\\\\n^ a b Obama, Barack (13 January 2009). \\\\\"\\'What I Want for You--And Every Child in America\\'\\\\\". http://www.parade.com/news/2009/01/barack-obama-letter-to-my-daughters.html.\\\\n^ Sobieraj Westfall, Sandra (23 June 2008). \\\\\"Barack Obama Gives Daughter $1 Allowance a Week\\\\\". People Magazine. http://www.people.com/people/article/0,,20214569_1,00.html. Retrieved on 2008-11-21.\\\\n^ Lester, Will (July 23, 2008). \\\\\"Obama daughters keep hectic schedules of their own\\\\\". Associated Press. http://elections.apnews.com/apelect/db_6911/contentdetail.htm;jsessionid=8314A43012AB5FF1D0697247362D8752?contentguid=H95QubFb&full=true. Retrieved on 2008-08-04.\\\\n^ Hiro, Anne. \\\\\"Obama regrets letting \\\\\"Access Hollywood\\\\\" interview daughters. Won\\'t do it again. MSNBC\\'s Dan Abrams gets the story behind the story. - Lynn Sweet\\\\\". Blogs.suntimes.com. http://blogs.suntimes.com/sweet/2008/07/obama_regrets_letting_access_h.html. Retrieved on 2009-01-31.\\\\n^ Ahmed, Saeed (5 November 2008). \\\\\"Move over Barney, new dog moving into White House\\\\\". CNN. http://www.cnn.com/2008/LIVING/wayoflife/11/05/presidential.pets/index.html. Retrieved on 2008-11-21.\\\\n^ \\\\\"Obama: Getting a dog isn\\'t easy\\\\\". Associated Press. 7 November 2008. http://www.mercurynews.com/ci_10927292. Retrieved on 2008-11-21.\\\\n^ Janice Lloyd (2009-01-12). \\\\\"Obamas down to Labradoodle or Portuguese water dog\\\\\". USA Today. http://www.usatoday.com/news/washington/2009-01-11-obama-dog_N.htm. Retrieved on 2009-01-28.\\\\n^ Swarns, Rachel (21 November 2008). \\\\\"And the Winner Is ýýý Sidwell Friends\\\\\". The New York Times. http://thecaucus.blogs.nytimes.com/2008/11/21/and-the-winner-is-sidwell-friends/. Retrieved on 2008-11-21.\\\\n^ Tolin, Lisa (2009-01-05). \\\\\"Obama girls start school with photographers in tow\\\\\". The Associated Press. http://www.google.com/hostednews/ap/article/ALeqM5g6mv_lkODQMmQdpyIEnr8Zpm5mogD95H8KA80. Retrieved on 2009-01-06.\\\\n^ Taylor Marsh (2008-08-25). \\\\\"Political Analysis, National Security and Breaking News\\\\\". Taylor Marsh. http://www.taylormarsh.com/archives_view.php?id=28286. Retrieved on 2009-01-31.\\\\n^ a b Lia LoBello (2008-01-02). \\\\\"First Families: Radar introduces you to the next president\\'s relatives\\\\\". Radar Online. http://www.radaronline.com/features/2008/07/john_mccain_barack_obama_michelle_cindy_dunham_roberta_wrigh_04.php. Retrieved on 2009-01-28.\\\\n^ Rachel L. Swarns (2009-01-09). \\\\\"Obamaýýýs Mother-in-Law to Move Into the White House\\\\\". The New York Times. http://thecaucus.blogs.nytimes.com/2009/01/09/obamas-mother-in-law-to-move-into-the-white-house/?hp. Retrieved on 2009-01-09.\\\\n^ a b \\\\\"Will Obama mum-in-law make it a family affair in the White House?\\\\\". Agence France Presse. 2008-11-22. http://www.google.com/hostednews/afp/article/ALeqM5gN_i2jrCVkJQgfMbSDRRrNk8U4Sw. Retrieved on 2009-01-09.\\\\n^ Philip Sherwell (2008-2008-11-09). \\\\\"Michelle Obama persuades First Granny to join new White House team\\\\\". The Telegraph (UK). http://www.telegraph.co.uk/news/3407525/Michelle-Obama-persuades-First-Granny-to-join-new-White-House-team.html. Retrieved on 2009-01-09.\\\\n^ \\\\\"ýýýTootýýý: Obama grandmother a force that shaped him\\\\\". via Associated Press. 2008-08-25. http://www.thekansan.com/news/x1311851415/-Toot-Obama-grandmother-a-force-that-shaped-him. Retrieved on 2008-08-29.\\\\n^ \\\\\"CNN: \\\\\"Obama\\'s grandmother dies after battle with cancer\\\\\"\\\\\". http://www.cnn.com/2008/POLITICS/11/03/obama.grandma/index.html. Retrieved on 2008-11-04.\\\\n^ The 89th Infantry Division, United States Holocaust Memorial Museum\\\\n^ a b Obama\\'s great-uncle recalls liberating Nazi camp, Boston.com, July 22, 2008\\\\n^ Major Garrett (2008-05-27). \\\\\"Obama Campaign Scrambles to Correct the Record on Uncle\\'s War Service\\\\\". FOXNews.com. http://elections.foxnews.com/2008/05/27/recollection-of-obama-familys-service-missing-key-details. Retrieved on 2009-01-31.\\\\n^ \\\\\"Democrats salute Obamaýýýs great uncle\\\\\". Jewish Telegraphic Agency. August 28, 2008. http://jta.org/news/article/2008/08/28/110123/obamapayne. Retrieved on 31 January 2009.\\\\n^ Obama Family Tree dgmweb.net\\\\n^ Chicago Sun Times article with her picture\\\\n^ Obama has links to Malaysia\\\\n^ Nolan, Daniel (2008-06-11). \\\\\"Relative: Obama\\'s got \\'a good handle on Canada\\'\\\\\". The Hamilton Spectator. http://www.thespec.com/burlingtonlife/article/384475. Retrieved on 2008-07-03.\\\\n^ Nolan, Daniel (June 11, 2008). \\\\\"Obama\\'s Burlington connection\\\\\". The Hamilton Spectator. http://www.thespec.com/article/384307. Retrieved on 2008-06-21.\\\\n^ Misner, Jason (2008-06-20). \\\\\"Barack Obama was here\\\\\". Burlington Post. http://www.burlingtonpost.com/printarticle/186215. Retrieved on 2008-07-03.\\\\n^ Fornek, Scott (2007-09-09). \\\\\"\\'He helped me find my voice\\'\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545473,BSX-News-wotreehh09.article.\\\\n^ Cooper, Tom (2009-01-20). \\\\\"Keep watch for Obama\\\\\". TheSpec.com. http://www.thespec.com/Opinions/article/499161. Retrieved on 2009-01-28.\\\\n^ Ancestry of Barack Obama\\\\n^ Dreams from My Father, p. 376\\\\n^ a b Kenya: Special Report: Sleepy Little Village Where Obama Traces His Own Roots (Page 2 of 2)\\\\n^ a b \\\\\"Obama\\'s grandma slams \\'untruths\\'\\\\\". Associated Press. 2008-03-05. http://www.usatoday.com/news/world/2008-03-05-obama-kin_N.htm.\\xa0 See also this correction.\\\\n^ \\\\\"Q&A ON THE NEWS\\\\\". Atlanta Journal-Constitution. 2009-02-25. http://www.ajc.com/services/content/metro/stories/2009/02/25/questi0225.html. Retrieved on 2009-02-27.\\\\n^ In Kenya, Barack Obamaýýýs family prays for end to conflict - Times Online\\\\n^ a b Crilly, Rob (February 27, 2008). \\\\\"Dreams from Obama\\'s Grandmother\\\\\". Time Magazine, Inc.. http://www.time.com/time/world/article/0,8599,1717590,00.html?xid=rss-topstories. Retrieved on 2008-07-03.\\\\n^ Pflanz, Mike (2008-01-11). \\\\\"Barack Obama\\'s Kenyan relatives keep faith\\\\\". Daily Telegraph. http://www.telegraph.co.uk/news/main.jhtml?xml=/news/2008/01/09/wuspols1009.xml.\\\\n^ Fornek, Scott (2007-09-09). \\\\\"Sarah Obama - \\'Sparkling, laughing eyes\\'\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545459,BSX-News-wotreeu09.article.\\\\n^ \\\\\"Barack Obama in Kenya\\\\\". CNN. http://www.youtube.com/watch?v=Ikg6gj71U9k.\\\\n^ Daily Nation, July 8, 2008: Obama granny\\'s day out with envoys and top politicians\\\\n^ \\\\\"A Candidate, His Minister and the Search for Faith\\\\\". New York Times. 2007-04-30. http://www.nytimes.com/2007/04/30/us/politics/30obama.html?_r=2&pagewanted=all&oref=slogin&oref=slogin.\\\\n^ \\\\\"Kenya: All Obama kin to spend voting day in Kogelo\\\\\". afrika.no. 2008-10-27. http://www.afrika.no/Detailed/17321.html. Retrieved on 2009-01-31.\\\\n^ Cohen, Roger (2008-03-06). \\\\\"The Obamas of the World - New York Times\\\\\". Nytimes.com. http://www.nytimes.com/2008/03/06/opinion/06cohen.html. Retrieved on 2009-01-31.\\\\n^ a b Sanderson, Elizabeth (2008-01-06). \\\\\"Barack Obama\\'s stepmother living in Bracknell reveals the close bond with him ... and his mother\\\\\". Daily Mail. http://www.dailymail.co.uk/news/article-506338/Barack-Obamas-stepmother-living-Bracknell-reveals-close-bond---mother.html.\\\\n^ Lindsay, Anna (2009-01-20). \\\\\"Barack\\'s bingo-loving stepmother\\\\\". BBC News. http://news.bbc.co.uk/1/hi/england/berkshire/7834368.stm.\\\\n^ http://www.newvision.co.ug/D/8/26/666733\\\\n^ jpt (2008-06-18). \\\\\"From the Fact Check Desk: What Did Obama\\'s Half-Brother Say About Obama\\'s Background\\\\\". ABC News. http://blogs.abcnews.com/politicalpunch/2008/06/from-the-fact-c.html.\\\\n^ a b c d e f Maliti, Tom (2004-10-26). \\\\\"Obama\\'s Brother Chooses Life in Slow Lane\\\\\". The Associated Press. http://www.msnbc.msn.com/id/6333496.\\\\n^ Obama, Dreams from my Father, 2004, p. 265.\\\\n^ Obama, Dreams from my Father, 2004, p. 262.\\\\n^ a b *Oywa, John (2004-08-15). \\\\\"Sleepy Little Village Where Obama Traces His Own Roots\\\\\". The Daily Nation. http://allafrica.com/stories/200408160533.html.\\\\n^ Philip Ochieng (2004-11-01). \\\\\"From Home Squared to the US Senate: How Barack Obama Was Lost and Found\\\\\". The East African. http://www.nationmedia.com/EastAfrican/01112004/Features/PA2-11.html. Retrieved on 2008-03-23.\\\\n^ Warah, Rasna (2008-06-09). \\\\\"We cannot lay claims on Obama; he\\'s not one of us - Obama in this world\\\\\". Daily Nation. http://www.nationmedia.com. Retrieved on 2008-07-10.\\\\n^ Scott Fornek (2007-09-09). \\\\\"AUMA OBAMA: \\'Her restlessness, her independence\\'\\\\\". Chicago Sun Times. http://www.suntimes.com/news/politics/obama/familytree/545465,BSX-News-wotreew09.article. Retrieved on 2008-03-23.\\\\n^ a b c Gathmann, Florian; Gregor Peter Schmitz, Jochen Schýýnmann (July 24, 2008). \\\\\"Studentin in der Bundesrepublik: Wie Auma Obama mit Deutschland haderte\\\\\" (in German). Spiegel Online. http://www.spiegel.de/politik/ausland/0,1518,567286,00.html. Retrieved on 2008-07-24.\\\\n^ a b Harvey, Oliver (07-26 2008). \\\\\"Obama\\'s brother is in Bracknell\\\\\". The Sun. http://www.thesun.co.uk/sol/homepage/news/the_real_american_idol/article1472877.ece. Retrieved on 2008-10-06.\\\\n^ \\\\\"Madari Kindergarten\\\\\". http://www.madarikindergarten.com/.\\\\n^ \\\\\"Welcome To MedWeek San Antonio 2007\\\\\". Medweeksa.org. http://www.medweeksa.org/awardwinners/techfirm.htm. Retrieved on 2009-01-31.\\\\n^ \\\\\"PIDE - Partners for International Development & Education Inc\\\\\". Pideafrica.org. http://pideafrica.org/aboutus.htm. Retrieved on 2009-01-31.\\\\n^ Barack Obamaýýýs brother pushes Chinese imports on US - Times Online\\\\n^ Obama half-brother runs Internet company in China\\\\n^ Roger Cohen (2008-03-17). \\\\\"Obama\\'s Brother in China\\\\\". The New York Times. http://www.nytimes.com/2008/03/17/opinion/29cohen.html. Retrieved on 2008-03-23.\\\\n^ \\\\\"Youku Buzz (daily)\\xa0ýý Blog Archive\\xa0ýý Barack Obamaýýýs Half-Brother in Concert\\\\\". Buzz.youku.com. 2009-01-18. http://buzz.youku.com/2009/01/18/barack-obamas-half-brother-in-concert/. Retrieved on 2009-01-31.\\\\n^ jaketapper (2008-07-28). \\\\\"Political Punch: Barack Obama\\'s Branch-y Family Tree\\\\\". Blogs.abcnews.com. http://blogs.abcnews.com/politicalpunch/2008/07/barack-obamas-1.html. Retrieved on 2009-01-31.\\\\n^ Fornek, Scott (September 9, 2007). \\\\\"HALF-BROTHER GEORGE: \\'I would be there for him\\'\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545447,BSX-News-wotreecc09.stng. Retrieved on 2008-08-04.\\\\n^ a b c d e Pflanz, Mike (August 21, 2008). \\\\\"Barack Obama is my inspiration, says lost brother\\\\\". The Daily Telegraph. http://www.telegraph.co.uk/news/newstopics/uselection2008/barackobama/2595688/Barack-Obama-is-my-inspiration-says-lost-brother.html. Retrieved on 2008-08-23.\\\\n^ a b c Crilly, Rob (August 22, 2008). \\\\\"Life is good in my Nairobi slum, says Barack Obama\\'s younger brother\\\\\". The Times. http://www.timesonline.co.uk/tol/news/world/us_and_americas/us_elections/article4583353.ece. Retrieved on 2008-08-23.\\\\n^ a b McKenzie, David (2008-08-23). \\\\\"Behind the Scenes: Meet George Obama\\\\\". CNN. http://www.cnn.com/2008/POLITICS/08/22/bts.obama.brother/. Retrieved on 2008-10-26.\\\\n^ Pisa, Nick (August 20, 2008). \\\\\"Barack Obama\\'s \\'lost\\' brother found in Kenya\\\\\". Daily Telegraph. http://www.telegraph.co.uk/news/newstopics/uselection2008/barackobama/2590614/Barack-Obamas-lost-brother-found-in-Kenya.html. Retrieved on 2008-08-20.\\\\n^ a b Wadhams, Nick (2008-10-07). \\\\\"Corsi in Kenya: Obama\\'s Nation Boots Obama Nation Author\\\\\". TIME. http://www.time.com/time/world/article/0,8599,1847965,00.html?imw=Y. Retrieved on 2009-01-31.\\\\n^ by Dinesh D\\'Souza. \\\\\"Dinesh D\\'Souza\\xa0: George Obama, Start Packing\\\\\". Townhall.com. http://townhall.com/columnists/DineshDSouza/2008/09/22/george_obama,_start_packing. Retrieved on 2009-01-31.\\\\n^ a b c \\\\\"The Obama Family Tree\\\\\" (PDF). Chicago Sun-Times. September 9, 2007. http://www.suntimes.com/images/cds/MP3/obamatree.pdf. Retrieved on 2008-11-23.\\\\n^ First read, MSNBC\\\\n^ \\\\\"Barack Obama\\'s aunt found living in rundown public housing estate | The Australian\\\\\". Theaustralian.news.com.au. 2008-10-31. http://www.theaustralian.news.com.au/story/0,25197,24578185-5017121,00.html. Retrieved on 2009-01-31.\\\\n^ Boston Housing Authority ýýýflabbergasteredýýý Barack Obamaýýýs aunt living in Southie\\\\n^ Kilner, Derek (2008-11-05). \\\\\"Kenya Celebrates President Obama as Native Son\\\\\". Voice Of America. http://www.voanews.com/english/archive/2008-11/2008-11-05-voa45.cfm. Retrieved on 2008-12-24.\\\\n^ \\\\\"Oregon State University Beavers: Craig Robinson bio\\\\\". http://www.osubeavers.com/ViewArticle.dbml?SPSID=106239&SPID=1954&DB_OEM_ID=4700&ATCLID=1436883&Q_SEASON=2008. Retrieved on 2008-08-21.\\\\n^ \\\\\"RootsWeb\\'s WorldConnect Project: Dowling Family Genealogy\\\\\". Wc.rootsweb.ancestry.com. http://wc.rootsweb.ancestry.com/cgi-bin/igm.cgi?op=GET&db=dowfam3&id=I105855. Retrieved on 2009-01-31.\\\\n^ Weiss, Anthony (September 2, 2008). \\\\\"Michelle Obama Has a Rabbi in Her Family\\\\\". The Forward. http://www.forward.com/articles/14121/. Retrieved on 2008-10-09.\\\\n^ Gary Boyd Roberts. \\\\\"Notes on the Ancestry of Senator Barack Hussein Obama, Jr\\\\\". New England Historic Genealogical Society. http://www.newenglandancestors.org/research/services/articles_ancestry_barack_obama.asp. Retrieved on 2009-01-31.\\\\n^ Scott Fornek (2007-09-09). \\\\\"Obama Family Tree\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545463,BSX-News-wotreer09.stng. Retrieved on 2009-01-31.\\\\n^ \\\\\"Obama-Bush (family tree)\\\\\" (PDF). New England Historic Genealogical Society. http://www.newenglandancestors.org/pdfs/obama_bush.pdf. Retrieved on 2009-01-31.\\\\n^ Wade, Nicholas (2007-10-21). \\\\\"Barack Obama - Dick Cheney - Ancestry and Genealogy - Washington - New York Times\\\\\". Nytimes.com. http://www.nytimes.com/2007/10/21/weekinreview/21basic.html. Retrieved on 2009-01-31.\\\\n^ Eastman, Dick (2008-07-30). \\\\\"Barack Obama is Related to Wild Bill Hickok\\\\\". Blog.eogn.com. http://blog.eogn.com/eastmans_online_genealogy/2008/07/barack-obama-is.html. Retrieved on 2009-01-31.\\\\n^ a b c Scott Fornek (2007-09-09). \\\\\"Obama Family Tree\\\\\". Suntimes.com. http://www.suntimes.com/news/politics/obama/familytree/545441,BSX-News-wotreec09.stng. Retrieved on 2009-01-31.\\\\n^ a b \\\\\"Notes on the Ancestry of Senator Barack Hussein Obama, Jr\\\\\". New England Historic Genealogical Society. 2002-08-14. http://www.newenglandancestors.org/research/services/articles_ancestry_barack_obama.asp. Retrieved on 2009-01-31.\\\\n^ \\\\\"Obama, Clinton and McCain have some famous relations\\\\\". HeraldNet - AP. 2008-03-26. http://www.heraldnet.com/article/20080326/NEWS02/151343661. Retrieved on 2009-01-31.\\\\n^ \\\\\"Barack Obama and Joe Biden: The Change We Need\\\\\". My.barackobama.com. 2008-07-31. http://my.barackobama.com/page/community/post/williambrehm/gG5TVR. Retrieved on 2009-01-31.\\\\nExternal links\\\\nBarack Obama\\'s Family Tree - Photo Essays - TIME\\\\n\\\\\"Though Obama Had to Leave to Find Himself, It Is Hawaii That Made His Rise Possible,\\\\\" by David Maraniss\\\\nBarack Obama\\'s Branch-y Family Tree by Jake Tapper\\\\n\\\\\"Obama Family Tree\\\\\" series, by Scott Fornek\\\\n\\\\\"Six Degrees of Barack Obama\\\\\"\\\\n\\\\\"Notes on the Ancestry of Senator Barack Hussein Obama, Jr.,\\\\\" by Gary Boyd Roberts\\\\n\\\\\"Obama, Clinton and McCain have some famous relations,\\\\\" by The Associated Press\\\\n\\\\\"Obama\\'s Patriotic Family Tree,\\\\\" by Bill Brehm\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nBarack Obama\\\\nPresidency\\\\nTransition\\xa0ýý Inauguration\\xa0ýý Timeline\\xa0ýý Cabinet\\xa0ýý Judiciary\\xa0ýý Foreign policy\\xa0ýý First 100 days\\\\nEarly life and\\\\npolitical career\\\\nPublic image\\xa0ýý Illinois Senate career\\xa0ýý 2004 Democratic National Convention\\xa0ýý U.S. Senate election in Illinois\\xa0ýý U.S. Senate career\\xa0ýý Presidential primary campaign\\xa0ýý ObamaýýýBiden 2008\\xa0ýý Electoral history\\xa0ýý Political positions\\\\nBooks\\\\nDreams from My Father\\xa0ýý The Audacity of Hope\\\\nSpeeches\\\\nThe Audacity of Hope\\xa0ýý A More Perfect Union\\xa0ýý Change Has Come to America\\xa0ýý 2009 speech to joint session of Congress\\\\nFamily\\\\nMichelle Obama\\xa0ýý Barack Obama, Sr.\\xa0ýý Ann Dunham\\xa0ýý Lolo Soetoro (stepfather)\\xa0ýý Maya Soetoro-Ng (half-sister)\\xa0ýý Marian Robinson (mother-in-law)\\xa0ýý Stanley Armour Dunham (grandfather)\\xa0ýý Madelyn Dunham (grandmother)\\xa0ýý Extended family\\xa0ýý Family tree\\\\nRetrieved from \\\\\"http://en.wikipedia.org/wiki/Family_of_Barack_Obama#Ruth_Ndesandjo\\\\\"\\\\nCategories: Obama family | African American history | African American families | Luo Kenyans | People of mixed Black African-European ethnicity | African Americans | Asian Americans | Dutch Americans | English Americans | French Americans | German-Americans | Irish-Americans | Indonesian Americans | Kenyan-Americans | Scottish-Americans | Chinese Canadians | People of mixed Asian-European ethnicity | American families | First Families of the United States | Family treesHidden categories: Wikipedia semi-protected pages | Wikipedia indefinitely move-protected pages | All pages needing cleanup | Wikipedia articles needing factual verification since October 2008 | All pages needing factual verification | All articles with unsourced statements | Articles with unsourced statements since November 2008\\\\nViews\\\\nArticle\\\\nDiscussion\\\\nView source\\\\nHistory\\\\nPersonal tools\\\\nLog in / create account\\\\nNavigation\\\\nMain page\\\\nContents\\\\nFeatured content\\\\nCurrent events\\\\nRandom article\\\\nSearch\\\\nInteraction\\\\nAbout Wikipedia\\\\nCommunity portal\\\\nRecent changes\\\\nContact Wikipedia\\\\nDonate to Wikipedia\\\\nHelp\\\\nToolbox\\\\nWhat links here\\\\nRelated changes\\\\nUpload file\\\\nSpecial pages\\\\nPrintable version Permanent linkCite this page\\\\nLanguages\\\\nBahasa Indonesia\\\\nSvenska\\\\nýýýýýý\\\\nýýýýýý\\\\nThis page was last modified on 14 March 2009, at 05:50.\\\\nAll text is available under the terms of the GNU Free Documentation License. (See Copyrights for details.)\\\\nWikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a U.S. registered 501(c)(3) tax-deductible nonprofit charity.\\\\nPrivacy policy\\\\nAbout Wikipedia\\\\nDisclaimers\\n[DOC 3] Family of Barack Obama - Wikipedia, the free encyclopedia\\\\nFamily of Barack Obama\\\\nFrom Wikipedia, the free encyclopedia\\\\n(Redirected from Soetoro)\\\\nJump to: navigation, search\\\\nObama Family\\\\nPresident Barack Obama, First Lady Michelle, and daughters Malia and Sasha wave to the crowd after his inaugural address Jan. 20, 2009, on the west steps of the U.S. Capitol.[1]\\\\nCurrent region\\\\nWashington, DC\\\\nInformation\\\\nPlace of origin\\\\nUnited States\\\\nNotable members\\\\nBarack Obama, Michelle Obama, Ann Dunham, Barack Obama, Sr., etc.\\\\nConnected families\\\\nRobinson, Dunham, Soetoro, Ng\\\\nThis article is part of a series about\\\\nBarack Obama\\\\nBackground \\xa0ýý Illinois Senate \\xa0ýý U.S. Senate\\\\nPolitical positions\\xa0ýý Public image\\xa0ýý Family\\\\n2008 primaries\\xa0ýý ObamaýýýBiden campaign\\\\nTransition\\xa0ýý Inauguration\\xa0ýý Electoral history\\\\nPresidency (Timeline, First 100 days)\\\\nMalia, Michelle and Sasha on stage at the 2008 Democratic National Convention\\\\nThe Family of Barack Obama is an extended clan of African American, English, Indonesian, and Kenyan (Luo) heritage known through the writings and political career of Barack Obama, the President of the United States of America,[2][3][4][5] and other reports. His immediate family is the First Family of the United States. The Obamas are the first First Family of African American descent in the United States and the youngest to enter the White House since the Kennedys. One columnist wrote, ýýýObama\\'s young, energetic family harks back to days of Camelot.ýýý[6]\\\\nContents\\\\n1 Immediate family\\\\n2 Extended family - maternal relations\\\\n3 Extended family - paternal relations\\\\n4 Michelle Robinson Obama\\'s extended family\\\\n5 Genealogical charts\\\\n5.1 Obama ancestry\\\\n5.2 Family trees\\\\n6 Distant relations\\\\n7 See also\\\\n8 References\\\\n9 External links\\\\nImmediate family\\\\nMichelle Obama\\\\nMichelle Obama, nýýe Robinson, the wife of Barack Obama, was born on January 17, 1964 in Chicago, Illinois. She is a lawyer and was a University of Chicago Hospital Vice-President. She is the First Lady of the United States.\\\\nMalia Obama and Sasha Obama\\\\nBarack and Michelle Obama have two daughters: Malia Ann (pronounced /mýýýýliýýýý/), born in 1998,[7] and Natasha (known as Sasha) /ýýsýýýýýýýý/), born in 2001. Sasha is the youngest child to reside in the White House since John F. Kennedy, Jr, arrived as an infant in 1961.[8] Sasha is also the first White House resident born in the 21st century.\\\\nBefore his inauguration, President Obama published an open letter to his daughters in Parade magazine, describing what he wants for them and every child in America: \\\\\"to grow up in a world with no limits on your dreams and no achievements beyond your reach, and to grow into compassionate, committed women who will help build that world.\\\\\"[9]\\\\nWhile living in Chicago, they kept busy schedules, as the Associated Press reports: \\\\\"soccer, dance and drama for Malia, gymnastics and tap for Sasha, piano and tennis for both.\\\\\"[10][11] In July 2008, the family gave an interview to the television series Access Hollywood; Obama later said they regretted allowing the children to be included.[12]\\\\nIn his victory speech on the night of his election, President Obama repeated his promise to Sasha and Malia to get a puppy to take with them to the White House.[13] However the selection of a dog has been slow because Malia is allergic to animal dander;[14] the president subsequently said that the choice has been narrowed down to either a labradoodle or Portuguese Water Dog, and they are hoping to find a shelter animal.[15]\\\\nMalia and Sasha attend the private Sidwell Friends School in Washington, DC, the same school as attended by Chelsea Clinton, Tricia Nixon Cox, and Archibald Roosevelt, and currently the grandchildren of Vice President Joe Biden.[16] The Obama girls began classes there on January 5, 2009.[17] While in Chicago, both attended the private University of Chicago Laboratory School.\\\\nMarian Shields Robinson\\\\nMichelle Obama\\'s mother (birthname Marian Shields, born July 1937), now widowed, married Michelle\\'s father, Fraser Robinson, in 1960.[18][19] Robinson was formerly a secretary at Spiegel catalog and a bank. While Michelle and Barack Obama were campaigning in 2008, Robinson tended the Obama\\'s young children and she intends to do the same while in Washington, DC. Robinson is currently living in the White House itself as part of the First Family;[20] she is the first live-in grandmother there since Elivera M. Doud during the Eisenhower administration.[21] Some media outlets have dubbed Robinson the \\\\\"First Granny\\\\\".[22][21]\\\\nExtended family - maternal relations\\\\nRight-to-left: Barack Obama and Maya Soetoro with their mother Ann Dunham and grandfather Stanley Dunham in Hawaii (early 1970s)\\\\nWikinews has related news:\\\\nBarack Obama elected 44th President of the United States\\\\nGrandmother of Barack Obama dies at 86\\\\nMadelyn Dunham with her daughter Ann\\\\nAccording to Barack Obama\\'s Dreams from My Father, his great-grandmother Leona McCurry was part Native American, which Obama believed Leona held as a \\\\\"source of considerable shame\\\\\" and \\\\\"blanched whenever someone mentioned the subject and hoped to carry the secret to her grave\\\\\"; whereas McCurry\\'s daughter (Obama\\'s maternal grandmother) \\\\\"would turn her head in profile to show off her beaked nose, which along with a pair of jet-black eyes, was offered as proof of Cherokee blood.\\\\\"[23] To date, no concrete evidence has surfaced of Cherokee heritage. Obama\\'s maternal heritage consists mostly of English ancestry, with much smaller amounts of German, Irish, Scottish, Welsh, Swiss, and French ancestry.[2]\\\\nAnn Dunham\\\\nMother of Barack Obama born in 1942, died in 1995. Birthname Stanley Ann Dunham. Anthropologist in Hawaii and Indonesia.\\\\nMadelyn Lee Payne Dunham\\\\nBarack Obama\\'s maternal grandmother, born in 1922 and died on November 2, 2008.[24] She was a bank vice president in Hawaii. Obama said that when he was a child, his grandmother \\\\\"read me the opening lines of the Declaration of Independence and told me about the men and women who marched for equality because they believed those words put to paper two centuries ago should mean something.\\\\\"[9]\\\\nStanley Armour Dunham\\\\nGrandfather of Barack Obama, born 1918, died 1992. World War II U.S. Army sergeant, furniture salesman in Hawaii.\\\\nCharles T. Payne\\\\nGreat-uncle of Barack Obama, younger brother of Madelyn Dunham, born 1925. Served during World War II in the U.S. Army 89th Infantry Division.[25] Obama has often described Payne\\'s role in liberating Ohrdruf forced labor camp.[26] There was brief media attention when Obama mistakenly identified the camp as Auschwitz during the campaign.[27] Payne appeared in the visitor\\'s gallery at the Democratic National Convention in Denver, Colorado, when his great-nephew was nominated for President.[28] He was the assistant director of the University of Chicago\\'s Library.[26]\\\\nMaya Soetoro-Ng\\\\nHalf-sister of Barack Obama, born August 15, 1970, in Jakarta, Indonesia.[29] She is married to Konrad Ng, with whom she has a daughter, Suhaila. Maya Soetoro-Ng is a teacher in Hawaii.\\\\nKonrad Ng\\\\nBrother-in-law of Barack Obama, born 1974. A Canadian whose parents are Malaysian Chinese immigrants, he is an assistant professor at the University of Hawaii\\'s Academy of Creative Media.[30] His parents are from Kudat and Sandakan, two small towns in Sabah, Malaysia, and he was born and raised in Burlington, Ontario.[31] He married Maya Soetoro-Ng at the end of 2003 in Hawaii.[32] They have one daughter, Suhaila.[33][34][35] Konrad Ng is now a US citizen.[36]\\\\nLolo Soetoro\\\\nStepfather of Barack Obama, born in Indonesia 1936, died 1987.\\\\nExtended family - paternal relations\\\\nThe Obamas are members of the Luo, Kenya\\'s third-largest ethnic group, which is part of a larger family of ethnic groups, collectively also known as Luo. This group belongs to the Eastern Sudanic branch of the Nilo-Saharan phylum. The Obama family is largely concentrated in the western province of Nyanza.\\\\nFront row (left to right): Auma Obama (Barack\\'s half-sister), Kezia Obama (Barack\\'s step-mother), Sarah Hussein Onyango Obama (third wife of Barack\\'s paternal grandfather), Zeituni Onyango (Barack\\'s aunt)\\\\nBack row (left to right): Said Obama (Barack\\'s uncle), Barack Obama, Abongo [Roy] Obama (Barack\\'s half-brother), unidentified woman, Bernard Obama (Barack\\'s half-brother), Abo Obama (Barack\\'s half-brother).\\\\nBarack Obama, Sr.\\\\nBarack Obama\\'s father, (1936ýýý1982). Government economist in Kenya. In addition to President Obama, Barack Obama Sr. fathered six other sons and a daughter.[37]\\\\nHussein Onyango Obama\\\\nBarack Obama\\'s paternal grandfather (c. 1895ýýý1979);[38] he worked as a mission cook. He joined the British Army during World War I. (One source gives 1870ýýý1975 as his dates of birth and death based on his tombstone reading \\\\\"Mzee Hussein Onyango Obama\\\\\" in his home village.[39] The term \\\\\"mzee\\\\\" is a Kenyan honorific meaning \\\\\"old man\\\\\" or \\\\\"elder.\\\\\") According to his third wife, Sarah, he originally converted to Catholicism, but took the name Hussein when he later converted to Islam; she said he passed the name, not the religion, on to his children.[40]\\\\nHabiba Akumu Obama\\\\nBarack Obama\\'s paternal grandmother, and the second wife of Hussein Onyango Obama. A photograph of her holding her son, Barack Sr, on her lap is on the cover of her grandson\\'s memoirs titled Dreams from my Father.[41]\\\\nSarah Obama\\\\nThird wife of Obama\\'s paternal grandfather, born 1922.[39] Also known, through the addition of her late husband\\'s name, as Sarah Onyango Obama,[42] and sometimes referred to as Sarah Ogwel, Sarah Hussein Obama or Sarah Anyango Obama,[43] she lives in Nyangýýýoma Kogelo village, 30 miles west of western Kenya\\'s main town, Kisumu, on the edge of Lake Victoria.[44][45]\\\\nAlthough not a blood relation, Barack Obama calls her \\\\\"Granny Sarah\\\\\".[43][46] Sarah, who speaks Luo and only a few words of English, communicates with President Obama through an interpreter.\\\\nOn July 4, 2008, she attended the United States Independence Day celebrations in Nairobi, hosted by Michael Ranneberger, the US ambassador in Kenya.[47]\\\\nDuring the campaign she protested attempts to portray Obama as a foreigner to the United States or a Muslim, saying that while Obama\\'s grandfather had been a Muslim, \\\\\"In the world of today, children have different religions from their parents.\\\\\"[40] Sarah Obama herself is \\\\\"a strong believer of the Islamic faith,ýýý in her words.[48]\\\\nKezia Obama\\\\nFirst wife of Barack Obama\\'s father, born c. 1940. She is Barack Obama Sr.\\'s first wife, whom he married in Kenya before studying abroad in the United States. Also known as Kezia Grace Obama.[49][50] She currently lives in Bracknell, Berkshire, England.[51][52] Her sister, Jane, is the \\'Auntie Jane\\' mentioned at the very start of Dreams from My Father when she telephoned President Obama to inform him that his father had been killed in a car accident.[53]\\\\nMalik Obama\\\\nBarack Obama\\'s half-brother, also known as Abongo or Roy, born c. March, 1958,[51] son of Barack Obama, Sr. with his first wife, Kezia.[54] Malik Obama was born and raised in Nairobi, Kenya.[55] He earned a degree in accounting from the University of Nairobi.[56] He met his half-brother for the first time in 1985[55] when Barack flew from Chicago to Washington, D.C. to visit him.[57] Malik and his half-brother Barack were best men at each other\\'s weddings.[55] Barack Obama brought his wife Michelle to Kenya three years later, and they met with Malik again while Barack was introducing Michelle to many other new relatives.[58]\\\\nAlthough much of the Obama family has dispersed throughout Kenya and overseas, most, including Malik Obama, still considered their rural village on the shores of Lake Victoria to be their true home, and feel that those who have left the village have become culturally \\\\\"lost\\\\\".[59] A frequent visitor to the United States,[58] and consultant in Washington, D.C. for several months per year,[55] he nevertheless settled in the Obamas\\' ancestral home, Nyangýýýoma Kogelo, a village of several hundred people that he prefers to the city for its slow pace.[55] He runs a small electronics shop a half hour drive outside of town.[55]\\\\nDuring his brother\\'s presidential campaign, Malik Obama was a spokesman for the extended Obama family in Kenya, dealing with safety and privacy concerns arising from increased attention from the press.[60]\\\\nAbo Obama\\\\nBarack Obama\\'s half-brother, born 1968. International telephone store manager in Kenya.\\\\nAuma Obama\\\\nBarack Obama\\'s half-sister, born c. 1960.[61] As of July 2008, development worker in Kenya.[62] She studied German at the University of Heidelberg from 1981 to 1987. After her graduation in Heidelberg she went on for graduate studies at the University of Bayreuth, which awarded her a PhD in 1996. Her dissertation was about the conception of labor in Germany and its literary reflections.[62] Auma Obama lives in London, and in 1996 married an Englishman, Ian Manners. They have a daughter named Akinyi (b. 1997).[62][verification needed]\\\\nBernard Obama\\\\nBarack Obama\\'s half-brother, born 1970, son of Barack Obama, Sr. and his first wife, Kezia. He had been an auto parts supplier in Nairobi, Kenya, and has one child. Bernard converted to Islam as an adult and has said: \\\\\"Iýýým a Muslim, I donýýýt deny it. My father was raised a Muslim. But itýýýs not an issue. I donýýýt know what all the hullabaloo is about.\\\\\"[63] He currently resides in Bracknell, England, with his mother Kezia.[63]\\\\nRuth Ndesandjo\\\\nBorn Ruth Nidesand, in US c. 1940s, Barack Obama Sr.\\'s third wife and a private kindergarten director in Kenya.[64] Ruth\\'s two sons with Barack Obama, Sr., are Mark and David Ndesandjo; her third son, Joseph Ndesandjo, was born c. 1980 from a subsequent marriage to a Tanzanian.[65][66]\\\\nMark Ndesandjo\\\\nBarack Obama\\'s half-brother, son of Ruth Nidesand and Barack Obama Sr.[67] He runs an Internet company called WorldNexus that advises Chinese corporations how best to reach international customers.[68] Mark graduated from Brown University, studied physics at Stanford University, received an MBA from Emory University, and has lived in Shenzhen, China, since 2002 and is married to a Chinese woman.[69] He is also an accomplished pianist.[70]\\\\nDavid Ndesandjo\\\\nDavid Ndesandjo\\\\nBarack Obama\\'s half-brother (also known as David Opiyo Obama), son of Ruth Nidesand and Barack Obama Sr. Killed in a motorcycle accident.[71]\\\\nGeorge Hussein Onyango Obama\\\\nYoungest half-brother of Barack Obama, born c.1982, son of Barack Obama Sr.[72] and Jael (now a resident of Atlanta, Georgia).[73][74] George was six months old when his father died in an automobile accident, after which he was raised in Nairobi by his mother and a French step-father. He later lived in South Korea for two years while his mother resided there for business reasons.[73] Returning to Kenya, George Obama \\\\\"slept rough for several years,\\\\\" until his aunt gave him a six-by-eight foot corrugated metal shack in the Nairobi, Kenya, slum of Huruma Flats.[73] As of August 2008, Obama was studying to become a mechanic.[73] George received little attention until being featured in an article in the Italian language edition of Vanity Fair in August 2008, which portrayed him as living in poverty, shame, and obscurity.[75] The article quoted Obama as saying that he lived \\\\\"on less than a dollar a month\\\\\" and stated that he \\\\\"does not mention his famous half-brother in conversation\\\\\" out of shame at his poverty.[76] In later interviews George contradicted this picture. In an interview with The Times, Obama \\\\\"said that he was furious at subsequent reports that he had been abandoned by the Obama family and that he was filled with shame about living in a slum.\\\\\"[74] He told The Times, \\\\\"Life in Huruma is good.\\\\\" Obama said that he expects no favors, that he was supported by relatives, and that reports he lived on a dollar a month were \\\\\"all lies by people who donýýýt want my brother to win.ýýý[74] He told The Telegraph that he was inspired by his half-brother.[73] According to Time, George \\\\\"has repeatedly denied...that he feels abandoned by Obama.\\\\\"[77] CNN quoted him as saying, \\\\\"I was brought up well. I live well even now. The magazines, they have exaggerated everything... I think I kind of like it here. There are some challenges, but maybe it is just like where you come from, there are the same challenges.\\\\\"[75] George\\'s reported poverty was seized on by conservative critics of Barack Obama. Columnist Dinesh D\\'Souza solicited donations for George Obama from his readers,[78] while Jerome Corsi planned to give him a $1,000 check during a trip to Kenya (Corsi was expelled from the country by immigration authorities).[77]\\\\nOmar Obama\\\\nHalf-uncle of Barack Obama,[79] born on June 3, 1944 in Nyangýýýoma Kogelo. Oldest son of Onyango and Sarah Obama, resides in Boston, Massachusetts.[citation needed]\\\\nZeituni Onyango\\\\nHalf-aunt of Barack Obama,[80] born May 29, 1952, in Kenya,[81] Onyango is referred to as \\\\\"Aunti Zeituni\\\\\" in President Obama\\'s memoir, Dreams from My Father.[82]\\\\nYusuf Obama\\\\nHalf-uncle of Barack Obama,[79] born c. 1950s in Nyangýýýoma Kogelo; son of Onyango and Sarah Obama.[citation needed]\\\\nSaid Obama\\\\nHalf-uncle of Barack Obama,[79][83] born c. 1950s in Nyangýýýoma Kogelo; son of Onyango and Sarah Obama.[citation needed]\\\\nMichelle Robinson Obama\\'s extended family\\\\nFraser Robinson, Sr. (1884ýýý1936) of South Carolina, shown in an old photo along with his wife, Rosella Cohen Robinson, in its background\\\\nBarack Obama has called his wife Michelle \\\\\"the most quintessentially American woman I know.\\\\\"[3] Her family is of African American heritage, descendents of Africans of the American Colonial Era.[3] Michelle Obama\\'s family history traces back from slavery to Reconstruction to the Great Migration North. Some of Michelle\\'s relatives still reside in South Carolina.\\\\nMichelle\\'s earliest known relative is her great-great grandfather Jim Robinson, born in the 1850s, who was an American slave on the Friendfield plantation in South Carolina. The family believes that after the Civil War he remained a Friendfield plantation sharecropper for the rest of his life and that he was buried there in an unmarked grave.[3]\\\\nJim had two sons, Gabriel and Fraser, Michelle Obama\\'s great-grandfather. Fraser had an arm amputated as a result of a boyhood injury. He worked as a shoemaker, newspaper salesman and in a lumber mill and was married to Rosella Cohen.[3] Carrie Nelson, Gabriel Robinson\\'s daughter, now 80, is the oldest living Robinson and the keeper of family lore.[3]\\\\nAt least three of Michelle Obama\\'s great-uncles served in the military of the United States. One aunt moved to Princeton, New Jersey, where she worked as a maid, and cooked Southern-style meals for Michelle and her brother, Craig, when they were students at Princeton University.\\\\nCraig Robinson\\\\nMichelle Obama\\'s brother, born 1962. He is currently head coach of men\\'s basketball at Oregon State University.[84]\\\\nFraser Robinson III\\\\nMichelle Obama\\'s father, born 1935, died 1991, married Michelle\\'s mother, Marian Shields, in 1960.[85][19] Robinson was a pump worker at the City of Chicago water plant.[3]\\\\nFraser Robinson, Jr.\\\\nMichelle Obama\\'s grandfather was born on August 24, 1912 in Georgetown, South Carolina, and died on November 9, 1996, aged 84. He was a good student and orator, but moved from South Carolina to Chicago to find better work than he could find at home, eventually becoming a worker for the United States Postal Service. He was married to LaVaughn Johnson. When he retired, they moved back to South Carolina.[3]\\\\nCapers C. Funnye Jr.\\\\nMichelle Obama\\'s first cousin once removed: Funnyeýýýs mother, Verdelle Robinson Funnye (born Verdelle Robinson; August 22, 1930 ýýý April 16, 2000) and Michelle Obamaýýýs paternal grandfather, Fraser Robinson Jr., were siblings. One of America\\'s most prominent African American Jews, known for acting as a bridge between mainstream Jewry and African Americans.[86]\\\\nGenealogical charts\\\\nObama ancestry\\\\n16. Opiyo\\\\n8. Obama\\\\n4. Hussein Onyango Obama\\\\n9. Nyaoke\\\\n2. Barack Hussein Obama, Sr.\\\\n5. Habiba Akumu\\\\n1. Barack Hussein Obama II\\\\n24. Jacob William Dunham\\\\n12. Ralph Waldo Emerson Dunham, Sr.\\\\n25. Mary Ann Kearney\\\\n6. Stanley Armour Dunham\\\\n26. Harry Ellington Armour\\\\n13. Ruth Lucille Armour\\\\n27. Gabriella Clark\\\\n3. Stanley Ann Dunham\\\\n28. Charles T. Payne\\\\n14. Rolla Charles Payne\\\\n29. Della L. Wolfley\\\\n7. Madelyn Lee Payne\\\\n30. Thomas Creekmore McCurry\\\\n15. Leona Belle McCurry\\\\n31. Margaret Belle Wright\\\\nFamily trees\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nBarack Obama\\\\nStanley\\\\nDunham\\\\n1918ýýý1992\\\\nMadelyn\\\\nPayne\\\\n1922ýýý2008\\\\nHabiba\\\\nAkumu\\\\nOnyango\\\\nObama\\\\nc.\\xa01895ýýý1979\\\\nSarah\\\\nOgwel\\\\n1922ýýý\\\\nLolo\\\\nSoetoro\\\\n1936ýýý1987\\\\nAnn\\\\nDunham\\\\n1942ýýý1995\\\\nBarack\\\\nObama, Sr. *\\\\n1936ýýý1982\\\\nKezia\\\\nGrace\\\\nc. 1940ýýý\\\\nOmar\\\\nObama\\\\n1944ýýý\\\\nZeituni\\\\nOnyango\\\\n1952ýýý\\\\nYusuf\\\\nObama\\\\nc. 1950sýýý\\\\nSaid\\\\nObama\\\\nc. 1950sýýý\\\\nKonrad\\\\nNg\\\\nc. 1974ýýý\\\\nMaya\\\\nSoetoro\\\\n1970ýýý\\\\nBarack\\\\nObama\\\\n1961ýýý\\\\nMichelle\\\\nRobinson\\\\n1964ýýý\\\\nM. Abongo\\\\nObama\\\\n1958ýýý\\\\nAuma\\\\nObama\\\\nc. 1960ýýý\\\\nAbo\\\\nObama\\\\n1968ýýý\\\\nBernard\\\\nObama\\\\n1970ýýý\\\\nSuhaila\\\\nNg\\\\nc. 2005ýýý\\\\nMalia Ann\\\\nObama\\\\n1998ýýý\\\\nSasha\\\\nObama\\\\n2001ýýý\\\\n* Barack\\\\nObama, Sr.\\'s\\\\nadditional\\\\nRuth\\\\nNidesandjo\\\\nc. 1940sýýý\\\\nJael\\\\nOtieno\\\\nrelationships:\\\\nMark\\\\nNdesandjo\\\\nDavid\\\\nNdesandjo\\\\ndied\\xa0c.\\xa01987\\\\nGeorge\\\\nObama\\\\nc. 1982ýýý\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nMichelle Obama\\\\nFraser\\\\nRobinson, Jr.\\\\nýýý\\\\nSouth Carolina\\\\n(1912ýýý1996)\\\\nSawmill worker\\\\nLaVaughn D.\\\\nJohnson\\\\nýýý\\\\nIllinois\\\\n(1915ýýý2002)\\\\nCapers C.\\\\nFunnye, Jr.\\\\nýýý\\\\n(born c. 1952;\\\\nnephew of\\\\nFraser\\\\nRobinson, Jr.)\\\\nRabbi in Chicago\\\\nFraser\\\\nRobinson III\\\\nýýý\\\\nIllinois\\\\n(1935ýýý1991)\\\\nEnjoyed boxing\\\\nin youth;\\\\nwater plant\\\\npump operator\\\\nin Chicago\\\\nMarian\\\\nShields\\\\nýýý\\\\nIllinois\\\\n(born 1937)\\\\nSecretary at\\\\nSpiegel catalog\\\\nin Chicago;\\\\nU.S.\\'s First\\\\nGrandmother\\\\nCraig\\\\nRobinson\\\\nýýý\\\\nIllinois\\\\n(born 1962)\\\\nHead coach of\\\\nOregon State\\\\nBeavers men\\'s\\\\nbasketball\\\\nMichelle\\\\nRobinson\\\\nýýý\\\\nIllinois\\\\n(born 1964)\\\\nFirst Lady\\\\nof the United States\\\\nDistant relations\\\\nSee also: List of United States Presidents by genealogical relationship\\\\nAccording to genealogists, Barack Obama\\'s distant cousins include the multitude of descendants of his maternal ancestors from all along the early-American Atlantic seaboard as well as paternal, Kenyan relations belonging to the Luo tribe, many descending from a 17th century ancestor named Owiny.[87][88] For example, George W. Bush, the 43rd U.S. president, is the eleventh cousin of Barack Obama.[89] The New York Times science writer Nicholas Wade argues that with eleven generations leading back to their common progenitor, Samuel Hinckley, the relationship between the 43rd President and the 44th President is \\\\\"genetically meaningless\\\\\".[90]\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nSelected genealogical relationships\\\\nBill Hickok\\\\nAccording to Barack Obama\\'s family lore (and confirmed by the New England Historic Genealogical Society), the President and Hickok are sixth cousins, six-times removed.[91]\\\\nRobert Duvall\\\\nGoodnight helped inspire Pulitzer Prize-winning author Larry McMurtry to create a protagonist for his novel series Lonesome Dove: Woodrow Call. In Dove\\'s television novela, Woodrow Call\\'s partner is Gus McCrae, portrayed by Obama\\'s eighth cousin, twice removed, actor Robert Duvall.[92]\\\\nJames Madison\\\\nObama is also distantly related to U.S. Presidents James Madison, Lyndon Johnson, Gerald Ford, and U.S. Vice President Dick Cheney, British Prime Minister Sir Winston Churchill, U.S. Civil War General Robert E. Lee, and actor Brad Pitt.[93][94][95]\\\\nCharles Goodnight\\\\nAccording to Chicago Sun-Times reporter Scott Fornek, another Obama progenitor, Catherine Goodnight, was the grandniece of George Goodnight, who was in turn great-grandfather of famed cattleman Charles Goodnight.[92]\\\\nHarry S. Truman\\\\nActor Duvall is distant cousins with United States President Harry Truman, who\\'s likewise a fourth cousin, four times removed, of Obama\\'s.[92]\\\\nGeorgia O\\'Keeffe\\\\nNotable women Obama is related to include painter Georgia OýýýKeeffe.[93]\\\\nSee also\\\\nU.S. Presidential genealogical relationships\\\\nNew England Historic Genealogical Society\\\\nGary Boyd Roberts\\\\nReferences\\\\n^ Keck, Kristi (4 June 2008). \\\\\"Obama crosses historic milestone\\\\\". CNN. http://news.yahoo.com/s/ap/20081122/ap_on_go_pr_wh/obama_school_12. Retrieved on 2008-11-21.\\\\n^ a b Reitwiesner, William Addams. \\\\\"Ancestry of Barack Obama\\\\\". http://www.wargs.com/political/obama.html. Retrieved on 2008-10-09.\\\\n^ a b c d e f g h Murray, Shailagh (2 October 2008). \\\\\"A Family Tree Rooted In American Soil: Michelle Obama Learns About Her Slave Ancestors, Herself and Her Country\\\\\". The Washington Post: p.\\xa0C01. http://www.washingtonpost.com/wp-dyn/content/article/2008/10/01/AR2008100103169.html. Retrieved on 2008-10-10.\\\\n^ Sheridan, Michael (5 February 2007). \\\\\"Secrets of Obama Family Unlocked\\\\\". Muslim Observer. http://news.newamericamedia.org/news/view_article.html?article_id=ae5895fc29971b172938790be94ab107. Retrieved on 2008-11-21.\\\\n^ RTE news report March, 2007:Obamas Irish family links discovered by ancestry.co.uk\\\\n^ Noveck, Jocelyn (2008-11-07). \\\\\"Deseret News | Obama\\'s young, energetic family harks back to days of Camelot\\\\\". Secure.deseretnews.com. https://secure.deseretnews.com/article/1,5143,705261276,00.html. Retrieved on 2009-01-31.\\\\n^ Liza Mundy, Michelle: A Biography (Simon and Schuster, 2008), p. 129.\\\\n^ \\\\\"Sasha Obama\\\\\". Baltimore Sun. http://www.baltimoresun.com/topic/politics/sasha-obama-PECLB004381.topic. Retrieved on 2009-01-31.\\\\n^ a b Obama, Barack (13 January 2009). \\\\\"\\'What I Want for You--And Every Child in America\\'\\\\\". http://www.parade.com/news/2009/01/barack-obama-letter-to-my-daughters.html.\\\\n^ Sobieraj Westfall, Sandra (23 June 2008). \\\\\"Barack Obama Gives Daughter $1 Allowance a Week\\\\\". People Magazine. http://www.people.com/people/article/0,,20214569_1,00.html. Retrieved on 2008-11-21.\\\\n^ Lester, Will (July 23, 2008). \\\\\"Obama daughters keep hectic schedules of their own\\\\\". Associated Press. http://elections.apnews.com/apelect/db_6911/contentdetail.htm;jsessionid=8314A43012AB5FF1D0697247362D8752?contentguid=H95QubFb&full=true. Retrieved on 2008-08-04.\\\\n^ Hiro, Anne. \\\\\"Obama regrets letting \\\\\"Access Hollywood\\\\\" interview daughters. Won\\'t do it again. MSNBC\\'s Dan Abrams gets the story behind the story. - Lynn Sweet\\\\\". Blogs.suntimes.com. http://blogs.suntimes.com/sweet/2008/07/obama_regrets_letting_access_h.html. Retrieved on 2009-01-31.\\\\n^ Ahmed, Saeed (5 November 2008). \\\\\"Move over Barney, new dog moving into White House\\\\\". CNN. http://www.cnn.com/2008/LIVING/wayoflife/11/05/presidential.pets/index.html. Retrieved on 2008-11-21.\\\\n^ \\\\\"Obama: Getting a dog isn\\'t easy\\\\\". Associated Press. 7 November 2008. http://www.mercurynews.com/ci_10927292. Retrieved on 2008-11-21.\\\\n^ Janice Lloyd (2009-01-12). \\\\\"Obamas down to Labradoodle or Portuguese water dog\\\\\". USA Today. http://www.usatoday.com/news/washington/2009-01-11-obama-dog_N.htm. Retrieved on 2009-01-28.\\\\n^ Swarns, Rachel (21 November 2008). \\\\\"And the Winner Is ýýý Sidwell Friends\\\\\". The New York Times. http://thecaucus.blogs.nytimes.com/2008/11/21/and-the-winner-is-sidwell-friends/. Retrieved on 2008-11-21.\\\\n^ Tolin, Lisa (2009-01-05). \\\\\"Obama girls start school with photographers in tow\\\\\". The Associated Press. http://www.google.com/hostednews/ap/article/ALeqM5g6mv_lkODQMmQdpyIEnr8Zpm5mogD95H8KA80. Retrieved on 2009-01-06.\\\\n^ Taylor Marsh (2008-08-25). \\\\\"Political Analysis, National Security and Breaking News\\\\\". Taylor Marsh. http://www.taylormarsh.com/archives_view.php?id=28286. Retrieved on 2009-01-31.\\\\n^ a b Lia LoBello (2008-01-02). \\\\\"First Families: Radar introduces you to the next president\\'s relatives\\\\\". Radar Online. http://www.radaronline.com/features/2008/07/john_mccain_barack_obama_michelle_cindy_dunham_roberta_wrigh_04.php. Retrieved on 2009-01-28.\\\\n^ Rachel L. Swarns (2009-01-09). \\\\\"Obamaýýýs Mother-in-Law to Move Into the White House\\\\\". The New York Times. http://thecaucus.blogs.nytimes.com/2009/01/09/obamas-mother-in-law-to-move-into-the-white-house/?hp. Retrieved on 2009-01-09.\\\\n^ a b \\\\\"Will Obama mum-in-law make it a family affair in the White House?\\\\\". Agence France Presse. 2008-11-22. http://www.google.com/hostednews/afp/article/ALeqM5gN_i2jrCVkJQgfMbSDRRrNk8U4Sw. Retrieved on 2009-01-09.\\\\n^ Philip Sherwell (2008-2008-11-09). \\\\\"Michelle Obama persuades First Granny to join new White House team\\\\\". The Telegraph (UK). http://www.telegraph.co.uk/news/3407525/Michelle-Obama-persuades-First-Granny-to-join-new-White-House-team.html. Retrieved on 2009-01-09.\\\\n^ \\\\\"ýýýTootýýý: Obama grandmother a force that shaped him\\\\\". via Associated Press. 2008-08-25. http://www.thekansan.com/news/x1311851415/-Toot-Obama-grandmother-a-force-that-shaped-him. Retrieved on 2008-08-29.\\\\n^ \\\\\"CNN: \\\\\"Obama\\'s grandmother dies after battle with cancer\\\\\"\\\\\". http://www.cnn.com/2008/POLITICS/11/03/obama.grandma/index.html. Retrieved on 2008-11-04.\\\\n^ The 89th Infantry Division, United States Holocaust Memorial Museum\\\\n^ a b Obama\\'s great-uncle recalls liberating Nazi camp, Boston.com, July 22, 2008\\\\n^ Major Garrett (2008-05-27). \\\\\"Obama Campaign Scrambles to Correct the Record on Uncle\\'s War Service\\\\\". FOXNews.com. http://elections.foxnews.com/2008/05/27/recollection-of-obama-familys-service-missing-key-details. Retrieved on 2009-01-31.\\\\n^ \\\\\"Democrats salute Obamaýýýs great uncle\\\\\". Jewish Telegraphic Agency. August 28, 2008. http://jta.org/news/article/2008/08/28/110123/obamapayne. Retrieved on 31 January 2009.\\\\n^ Obama Family Tree dgmweb.net\\\\n^ Chicago Sun Times article with her picture\\\\n^ Obama has links to Malaysia\\\\n^ Nolan, Daniel (2008-06-11). \\\\\"Relative: Obama\\'s got \\'a good handle on Canada\\'\\\\\". The Hamilton Spectator. http://www.thespec.com/burlingtonlife/article/384475. Retrieved on 2008-07-03.\\\\n^ Nolan, Daniel (June 11, 2008). \\\\\"Obama\\'s Burlington connection\\\\\". The Hamilton Spectator. http://www.thespec.com/article/384307. Retrieved on 2008-06-21.\\\\n^ Misner, Jason (2008-06-20). \\\\\"Barack Obama was here\\\\\". Burlington Post. http://www.burlingtonpost.com/printarticle/186215. Retrieved on 2008-07-03.\\\\n^ Fornek, Scott (2007-09-09). \\\\\"\\'He helped me find my voice\\'\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545473,BSX-News-wotreehh09.article.\\\\n^ Cooper, Tom (2009-01-20). \\\\\"Keep watch for Obama\\\\\". TheSpec.com. http://www.thespec.com/Opinions/article/499161. Retrieved on 2009-01-28.\\\\n^ Ancestry of Barack Obama\\\\n^ Dreams from My Father, p. 376\\\\n^ a b Kenya: Special Report: Sleepy Little Village Where Obama Traces His Own Roots (Page 2 of 2)\\\\n^ a b \\\\\"Obama\\'s grandma slams \\'untruths\\'\\\\\". Associated Press. 2008-03-05. http://www.usatoday.com/news/world/2008-03-05-obama-kin_N.htm.\\xa0 See also this correction.\\\\n^ \\\\\"Q&A ON THE NEWS\\\\\". Atlanta Journal-Constitution. 2009-02-25. http://www.ajc.com/services/content/metro/stories/2009/02/25/questi0225.html. Retrieved on 2009-02-27.\\\\n^ In Kenya, Barack Obamaýýýs family prays for end to conflict - Times Online\\\\n^ a b Crilly, Rob (February 27, 2008). \\\\\"Dreams from Obama\\'s Grandmother\\\\\". Time Magazine, Inc.. http://www.time.com/time/world/article/0,8599,1717590,00.html?xid=rss-topstories. Retrieved on 2008-07-03.\\\\n^ Pflanz, Mike (2008-01-11). \\\\\"Barack Obama\\'s Kenyan relatives keep faith\\\\\". Daily Telegraph. http://www.telegraph.co.uk/news/main.jhtml?xml=/news/2008/01/09/wuspols1009.xml.\\\\n^ Fornek, Scott (2007-09-09). \\\\\"Sarah Obama - \\'Sparkling, laughing eyes\\'\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545459,BSX-News-wotreeu09.article.\\\\n^ \\\\\"Barack Obama in Kenya\\\\\". CNN. http://www.youtube.com/watch?v=Ikg6gj71U9k.\\\\n^ Daily Nation, July 8, 2008: Obama granny\\'s day out with envoys and top politicians\\\\n^ \\\\\"A Candidate, His Minister and the Search for Faith\\\\\". New York Times. 2007-04-30. http://www.nytimes.com/2007/04/30/us/politics/30obama.html?_r=2&pagewanted=all&oref=slogin&oref=slogin.\\\\n^ \\\\\"Kenya: All Obama kin to spend voting day in Kogelo\\\\\". afrika.no. 2008-10-27. http://www.afrika.no/Detailed/17321.html. Retrieved on 2009-01-31.\\\\n^ Cohen, Roger (2008-03-06). \\\\\"The Obamas of the World - New York Times\\\\\". Nytimes.com. http://www.nytimes.com/2008/03/06/opinion/06cohen.html. Retrieved on 2009-01-31.\\\\n^ a b Sanderson, Elizabeth (2008-01-06). \\\\\"Barack Obama\\'s stepmother living in Bracknell reveals the close bond with him ... and his mother\\\\\". Daily Mail. http://www.dailymail.co.uk/news/article-506338/Barack-Obamas-stepmother-living-Bracknell-reveals-close-bond---mother.html.\\\\n^ Lindsay, Anna (2009-01-20). \\\\\"Barack\\'s bingo-loving stepmother\\\\\". BBC News. http://news.bbc.co.uk/1/hi/england/berkshire/7834368.stm.\\\\n^ http://www.newvision.co.ug/D/8/26/666733\\\\n^ jpt (2008-06-18). \\\\\"From the Fact Check Desk: What Did Obama\\'s Half-Brother Say About Obama\\'s Background\\\\\". ABC News. http://blogs.abcnews.com/politicalpunch/2008/06/from-the-fact-c.html.\\\\n^ a b c d e f Maliti, Tom (2004-10-26). \\\\\"Obama\\'s Brother Chooses Life in Slow Lane\\\\\". The Associated Press. http://www.msnbc.msn.com/id/6333496.\\\\n^ Obama, Dreams from my Father, 2004, p. 265.\\\\n^ Obama, Dreams from my Father, 2004, p. 262.\\\\n^ a b *Oywa, John (2004-08-15). \\\\\"Sleepy Little Village Where Obama Traces His Own Roots\\\\\". The Daily Nation. http://allafrica.com/stories/200408160533.html.\\\\n^ Philip Ochieng (2004-11-01). \\\\\"From Home Squared to the US Senate: How Barack Obama Was Lost and Found\\\\\". The East African. http://www.nationmedia.com/EastAfrican/01112004/Features/PA2-11.html. Retrieved on 2008-03-23.\\\\n^ Warah, Rasna (2008-06-09). \\\\\"We cannot lay claims on Obama; he\\'s not one of us - Obama in this world\\\\\". Daily Nation. http://www.nationmedia.com. Retrieved on 2008-07-10.\\\\n^ Scott Fornek (2007-09-09). \\\\\"AUMA OBAMA: \\'Her restlessness, her independence\\'\\\\\". Chicago Sun Times. http://www.suntimes.com/news/politics/obama/familytree/545465,BSX-News-wotreew09.article. Retrieved on 2008-03-23.\\\\n^ a b c Gathmann, Florian; Gregor Peter Schmitz, Jochen Schýýnmann (July 24, 2008). \\\\\"Studentin in der Bundesrepublik: Wie Auma Obama mit Deutschland haderte\\\\\" (in German). Spiegel Online. http://www.spiegel.de/politik/ausland/0,1518,567286,00.html. Retrieved on 2008-07-24.\\\\n^ a b Harvey, Oliver (07-26 2008). \\\\\"Obama\\'s brother is in Bracknell\\\\\". The Sun. http://www.thesun.co.uk/sol/homepage/news/the_real_american_idol/article1472877.ece. Retrieved on 2008-10-06.\\\\n^ \\\\\"Madari Kindergarten\\\\\". http://www.madarikindergarten.com/.\\\\n^ \\\\\"Welcome To MedWeek San Antonio 2007\\\\\". Medweeksa.org. http://www.medweeksa.org/awardwinners/techfirm.htm. Retrieved on 2009-01-31.\\\\n^ \\\\\"PIDE - Partners for International Development & Education Inc\\\\\". Pideafrica.org. http://pideafrica.org/aboutus.htm. Retrieved on 2009-01-31.\\\\n^ Barack Obamaýýýs brother pushes Chinese imports on US - Times Online\\\\n^ Obama half-brother runs Internet company in China\\\\n^ Roger Cohen (2008-03-17). \\\\\"Obama\\'s Brother in China\\\\\". The New York Times. http://www.nytimes.com/2008/03/17/opinion/29cohen.html. Retrieved on 2008-03-23.\\\\n^ \\\\\"Youku Buzz (daily)\\xa0ýý Blog Archive\\xa0ýý Barack Obamaýýýs Half-Brother in Concert\\\\\". Buzz.youku.com. 2009-01-18. http://buzz.youku.com/2009/01/18/barack-obamas-half-brother-in-concert/. Retrieved on 2009-01-31.\\\\n^ jaketapper (2008-07-28). \\\\\"Political Punch: Barack Obama\\'s Branch-y Family Tree\\\\\". Blogs.abcnews.com. http://blogs.abcnews.com/politicalpunch/2008/07/barack-obamas-1.html. Retrieved on 2009-01-31.\\\\n^ Fornek, Scott (September 9, 2007). \\\\\"HALF-BROTHER GEORGE: \\'I would be there for him\\'\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545447,BSX-News-wotreecc09.stng. Retrieved on 2008-08-04.\\\\n^ a b c d e Pflanz, Mike (August 21, 2008). \\\\\"Barack Obama is my inspiration, says lost brother\\\\\". The Daily Telegraph. http://www.telegraph.co.uk/news/newstopics/uselection2008/barackobama/2595688/Barack-Obama-is-my-inspiration-says-lost-brother.html. Retrieved on 2008-08-23.\\\\n^ a b c Crilly, Rob (August 22, 2008). \\\\\"Life is good in my Nairobi slum, says Barack Obama\\'s younger brother\\\\\". The Times. http://www.timesonline.co.uk/tol/news/world/us_and_americas/us_elections/article4583353.ece. Retrieved on 2008-08-23.\\\\n^ a b McKenzie, David (2008-08-23). \\\\\"Behind the Scenes: Meet George Obama\\\\\". CNN. http://www.cnn.com/2008/POLITICS/08/22/bts.obama.brother/. Retrieved on 2008-10-26.\\\\n^ Pisa, Nick (August 20, 2008). \\\\\"Barack Obama\\'s \\'lost\\' brother found in Kenya\\\\\". Daily Telegraph. http://www.telegraph.co.uk/news/newstopics/uselection2008/barackobama/2590614/Barack-Obamas-lost-brother-found-in-Kenya.html. Retrieved on 2008-08-20.\\\\n^ a b Wadhams, Nick (2008-10-07). \\\\\"Corsi in Kenya: Obama\\'s Nation Boots Obama Nation Author\\\\\". TIME. http://www.time.com/time/world/article/0,8599,1847965,00.html?imw=Y. Retrieved on 2009-01-31.\\\\n^ by Dinesh D\\'Souza. \\\\\"Dinesh D\\'Souza\\xa0: George Obama, Start Packing\\\\\". Townhall.com. http://townhall.com/columnists/DineshDSouza/2008/09/22/george_obama,_start_packing. Retrieved on 2009-01-31.\\\\n^ a b c \\\\\"The Obama Family Tree\\\\\" (PDF). Chicago Sun-Times. September 9, 2007. http://www.suntimes.com/images/cds/MP3/obamatree.pdf. Retrieved on 2008-11-23.\\\\n^ First read, MSNBC\\\\n^ \\\\\"Barack Obama\\'s aunt found living in rundown public housing estate | The Australian\\\\\". Theaustralian.news.com.au. 2008-10-31. http://www.theaustralian.news.com.au/story/0,25197,24578185-5017121,00.html. Retrieved on 2009-01-31.\\\\n^ Boston Housing Authority ýýýflabbergasteredýýý Barack Obamaýýýs aunt living in Southie\\\\n^ Kilner, Derek (2008-11-05). \\\\\"Kenya Celebrates President Obama as Native Son\\\\\". Voice Of America. http://www.voanews.com/english/archive/2008-11/2008-11-05-voa45.cfm. Retrieved on 2008-12-24.\\\\n^ \\\\\"Oregon State University Beavers: Craig Robinson bio\\\\\". http://www.osubeavers.com/ViewArticle.dbml?SPSID=106239&SPID=1954&DB_OEM_ID=4700&ATCLID=1436883&Q_SEASON=2008. Retrieved on 2008-08-21.\\\\n^ \\\\\"RootsWeb\\'s WorldConnect Project: Dowling Family Genealogy\\\\\". Wc.rootsweb.ancestry.com. http://wc.rootsweb.ancestry.com/cgi-bin/igm.cgi?op=GET&db=dowfam3&id=I105855. Retrieved on 2009-01-31.\\\\n^ Weiss, Anthony (September 2, 2008). \\\\\"Michelle Obama Has a Rabbi in Her Family\\\\\". The Forward. http://www.forward.com/articles/14121/. Retrieved on 2008-10-09.\\\\n^ Gary Boyd Roberts. \\\\\"Notes on the Ancestry of Senator Barack Hussein Obama, Jr\\\\\". New England Historic Genealogical Society. http://www.newenglandancestors.org/research/services/articles_ancestry_barack_obama.asp. Retrieved on 2009-01-31.\\\\n^ Scott Fornek (2007-09-09). \\\\\"Obama Family Tree\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545463,BSX-News-wotreer09.stng. Retrieved on 2009-01-31.\\\\n^ \\\\\"Obama-Bush (family tree)\\\\\" (PDF). New England Historic Genealogical Society. http://www.newenglandancestors.org/pdfs/obama_bush.pdf. Retrieved on 2009-01-31.\\\\n^ Wade, Nicholas (2007-10-21). \\\\\"Barack Obama - Dick Cheney - Ancestry and Genealogy - Washington - New York Times\\\\\". Nytimes.com. http://www.nytimes.com/2007/10/21/weekinreview/21basic.html. Retrieved on 2009-01-31.\\\\n^ Eastman, Dick (2008-07-30). \\\\\"Barack Obama is Related to Wild Bill Hickok\\\\\". Blog.eogn.com. http://blog.eogn.com/eastmans_online_genealogy/2008/07/barack-obama-is.html. Retrieved on 2009-01-31.\\\\n^ a b c Scott Fornek (2007-09-09). \\\\\"Obama Family Tree\\\\\". Suntimes.com. http://www.suntimes.com/news/politics/obama/familytree/545441,BSX-News-wotreec09.stng. Retrieved on 2009-01-31.\\\\n^ a b \\\\\"Notes on the Ancestry of Senator Barack Hussein Obama, Jr\\\\\". New England Historic Genealogical Society. 2002-08-14. http://www.newenglandancestors.org/research/services/articles_ancestry_barack_obama.asp. Retrieved on 2009-01-31.\\\\n^ \\\\\"Obama, Clinton and McCain have some famous relations\\\\\". HeraldNet - AP. 2008-03-26. http://www.heraldnet.com/article/20080326/NEWS02/151343661. Retrieved on 2009-01-31.\\\\n^ \\\\\"Barack Obama and Joe Biden: The Change We Need\\\\\". My.barackobama.com. 2008-07-31. http://my.barackobama.com/page/community/post/williambrehm/gG5TVR. Retrieved on 2009-01-31.\\\\nExternal links\\\\nBarack Obama\\'s Family Tree - Photo Essays - TIME\\\\n\\\\\"Though Obama Had to Leave to Find Himself, It Is Hawaii That Made His Rise Possible,\\\\\" by David Maraniss\\\\nBarack Obama\\'s Branch-y Family Tree by Jake Tapper\\\\n\\\\\"Obama Family Tree\\\\\" series, by Scott Fornek\\\\n\\\\\"Six Degrees of Barack Obama\\\\\"\\\\n\\\\\"Notes on the Ancestry of Senator Barack Hussein Obama, Jr.,\\\\\" by Gary Boyd Roberts\\\\n\\\\\"Obama, Clinton and McCain have some famous relations,\\\\\" by The Associated Press\\\\n\\\\\"Obama\\'s Patriotic Family Tree,\\\\\" by Bill Brehm\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nBarack Obama\\\\nPresidency\\\\nTransition\\xa0ýý Inauguration\\xa0ýý Timeline\\xa0ýý Cabinet\\xa0ýý Judiciary\\xa0ýý Foreign policy\\xa0ýý First 100 days\\\\nEarly life and\\\\npolitical career\\\\nPublic image\\xa0ýý Illinois Senate career\\xa0ýý 2004 Democratic National Convention\\xa0ýý U.S. Senate election in Illinois\\xa0ýý U.S. Senate career\\xa0ýý Presidential primary campaign\\xa0ýý ObamaýýýBiden 2008\\xa0ýý Electoral history\\xa0ýý Political positions\\\\nBooks\\\\nDreams from My Father\\xa0ýý The Audacity of Hope\\\\nSpeeches\\\\nThe Audacity of Hope\\xa0ýý A More Perfect Union\\xa0ýý Change Has Come to America\\xa0ýý 2009 speech to joint session of Congress\\\\nFamily\\\\nMichelle Obama\\xa0ýý Barack Obama, Sr.\\xa0ýý Ann Dunham\\xa0ýý Lolo Soetoro (stepfather)\\xa0ýý Maya Soetoro-Ng (half-sister)\\xa0ýý Marian Robinson (mother-in-law)\\xa0ýý Stanley Armour Dunham (grandfather)\\xa0ýý Madelyn Dunham (grandmother)\\xa0ýý Extended family\\xa0ýý Family tree\\\\nRetrieved from \\\\\"http://en.wikipedia.org/wiki/Family_of_Barack_Obama\\\\\"\\\\nCategories: Obama family | African American history | African American families | Luo Kenyans | People of mixed Black African-European ethnicity | African Americans | Asian Americans | Dutch Americans | English Americans | French Americans | German-Americans | Irish-Americans | Indonesian Americans | Kenyan-Americans | Scottish-Americans | Chinese Canadians | People of mixed Asian-European ethnicity | American families | First Families of the United States | Family treesHidden categories: Wikipedia semi-protected pages | Wikipedia indefinitely move-protected pages | All pages needing cleanup | Wikipedia articles needing factual verification since October 2008 | All pages needing factual verification | All articles with unsourced statements | Articles with unsourced statements since November 2008\\\\nViews\\\\nArticle\\\\nDiscussion\\\\nView source\\\\nHistory\\\\nPersonal tools\\\\nLog in / create account\\\\nNavigation\\\\nMain page\\\\nContents\\\\nFeatured content\\\\nCurrent events\\\\nRandom article\\\\nSearch\\\\nInteraction\\\\nAbout Wikipedia\\\\nCommunity portal\\\\nRecent changes\\\\nContact Wikipedia\\\\nDonate to Wikipedia\\\\nHelp\\\\nToolbox\\\\nWhat links here\\\\nRelated changes\\\\nUpload file\\\\nSpecial pages\\\\nPrintable version Permanent linkCite this page\\\\nLanguages\\\\nBahasa Indonesia\\\\nSvenska\\\\nýýýýýý\\\\nýýýýýý\\\\nThis page was last modified on 16 March 2009, at 03:22.\\\\nAll text is available under the terms of the GNU Free Documentation License. (See Copyrights for details.)\\\\nWikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a U.S. registered 501(c)(3) tax-deductible nonprofit charity.\\\\nPrivacy policy\\\\nAbout Wikipedia\\\\nDisclaimers\\n[DOC 4] Family of Barack Obama - Wikipedia, the free encyclopedia\\\\nFamily of Barack Obama\\\\nFrom Wikipedia, the free encyclopedia\\\\n(Redirected from Sarah Ogwel)\\\\nJump to: navigation, search\\\\nObama Family\\\\nPresident Barack Obama, First Lady Michelle, and daughters Malia and Sasha wave to the crowd after his inaugural address Jan. 20, 2009, on the west steps of the U.S. Capitol.[1]\\\\nCurrent region\\\\nWashington, DC\\\\nInformation\\\\nPlace of origin\\\\nUnited States\\\\nNotable members\\\\nBarack Obama, Michelle Obama, Ann Dunham, Barack Obama, Sr., etc.\\\\nConnected families\\\\nRobinson, Dunham, Soetoro, Ng\\\\nThis article is part of a series about\\\\nBarack Obama\\\\nBackground \\xa0ýý Illinois Senate \\xa0ýý U.S. Senate\\\\nPolitical positions\\xa0ýý Public image\\xa0ýý Family\\\\n2008 primaries\\xa0ýý ObamaýýýBiden campaign\\\\nTransition\\xa0ýý Inauguration\\xa0ýý Electoral history\\\\nPresidency (Timeline, First 100 days)\\\\nMalia, Michelle and Sasha on stage at the 2008 Democratic National Convention\\\\nThe Family of Barack Obama is an extended clan of African American, English, Indonesian, and Kenyan (Luo) heritage known through the writings and political career of Barack Obama, the President of the United States of America,[2][3][4][5] and other reports. His immediate family is the First Family of the United States. The Obamas are the first First Family of African American descent in the United States and the youngest to enter the White House since the Kennedys. One columnist wrote, ýýýObama\\'s young, energetic family harks back to days of Camelot.ýýý[6]\\\\nContents\\\\n1 Immediate family\\\\n2 Extended family - maternal relations\\\\n3 Extended family - paternal relations\\\\n4 Michelle Robinson Obama\\'s extended family\\\\n5 Genealogical charts\\\\n5.1 Obama ancestry\\\\n5.2 Family trees\\\\n6 Distant relations\\\\n7 See also\\\\n8 References\\\\n9 External links\\\\nImmediate family\\\\nMichelle Obama\\\\nMichelle Obama, nýýe Robinson, the wife of Barack Obama, was born on January 17, 1964 in Chicago, Illinois. She is a lawyer and was a University of Chicago Hospital Vice-President. She is the First Lady of the United States.\\\\nMalia Obama and Sasha Obama\\\\nBarack and Michelle Obama have two daughters: Malia Ann (pronounced /mýýýýliýýýý/), born in 1998,[7] and Natasha (known as Sasha) /ýýsýýýýýýýý/), born in 2001. Sasha is the youngest child to reside in the White House since John F. Kennedy, Jr, arrived as an infant in 1961.[8]\\\\nBefore his inauguration, President Obama published an open letter to his daughters in Parade magazine, describing what he wants for them and every child in America: \\\\\"to grow up in a world with no limits on your dreams and no achievements beyond your reach, and to grow into compassionate, committed women who will help build that world.\\\\\"[9]\\\\nWhile living in Chicago, they kept busy schedules, as the Associated Press reports: \\\\\"soccer, dance and drama for Malia, gymnastics and tap for Sasha, piano and tennis for both.\\\\\"[10][11] In July 2008, the family gave an interview to the television series Access Hollywood; Obama later said they regretted allowing the children to be included.[12]\\\\nIn his victory speech on the night of his election, President Obama repeated his promise to Sasha and Malia to get a puppy to take with them to the White House.[13] However the selection of a dog has been slow because Malia is allergic to animal dander;[14] the president subsequently said that the choice has been narrowed down to either a labradoodle or Portuguese Water Dog, and they are hoping to find a shelter animal.[15]\\\\nMalia and Sasha attend the private Sidwell Friends School in Washington, DC, the same school as attended by Chelsea Clinton, Tricia Nixon Cox, and Archibald Roosevelt, and currently the grandchildren of Vice President Joe Biden.[16] The Obama girls began classes there on January 5, 2009.[17] While in Chicago, both attended the private University of Chicago Laboratory School.\\\\nMarian Shields Robinson\\\\nMichelle Obama\\'s mother (birthname Marian Shields, born July 1937), now widowed, married Michelle\\'s father, Fraser Robinson, in 1960.[18][19] Robinson was formerly a secretary at Spiegel catalog and a bank. While Michelle and Barack Obama were campaigning in 2008, Robinson tended the Obama\\'s young children and she intends to do the same while in Washington, DC. Robinson is currently living in the White House itself as part of the First Family;[20] she is the first live-in grandmother there since Elivera M. Doud during the Eisenhower administration.[21] Some media outlets have dubbed Robinson the \\\\\"First Granny\\\\\".[22][21]\\\\nExtended family - maternal relations\\\\nRight-to-left: Barack Obama and Maya Soetoro with their mother Ann Dunham and grandfather Stanley Dunham in Hawaii (early 1970s)\\\\nWikinews has related news:\\\\nBarack Obama elected 44th President of the United States\\\\nGrandmother of Barack Obama dies at 86\\\\nMadelyn Dunham with her daughter Ann\\\\nAccording to Barack Obama\\'s Dreams from My Father, his great-grandmother Leona McCurry was part Native American, which Obama believed Leona held as a \\\\\"source of considerable shame\\\\\" and \\\\\"blanched whenever someone mentioned the subject and hoped to carry the secret to her grave\\\\\"; whereas McCurry\\'s daughter (Obama\\'s maternal grandmother) \\\\\"would turn her head in profile to show off her beaked nose, which along with a pair of jet-black eyes, was offered as proof of Cherokee blood.\\\\\"[23] To date, no concrete evidence has surfaced of Cherokee heritage. Obama\\'s maternal heritage consists mostly of English ancestry, with much smaller amounts of German, Irish, Scottish, Welsh, Swiss, and French ancestry.[2]\\\\nAnn Dunham\\\\nMother of Barack Obama born in 1942, died in 1995. Birthname Stanley Ann Dunham. Anthropologist in Hawaii and Indonesia.\\\\nMadelyn Lee Payne Dunham\\\\nBarack Obama\\'s maternal grandmother, born in 1922 and died on November 2, 2008.[24] She was a bank vice president in Hawaii. Obama said that when he was a child, his grandmother \\\\\"read me the opening lines of the Declaration of Independence and told me about the men and women who marched for equality because they believed those words put to paper two centuries ago should mean something.\\\\\"[9]\\\\nStanley Armour Dunham\\\\nGrandfather of Barack Obama, born 1918, died 1992. World War II U.S. Army sergeant, furniture salesman in Hawaii.\\\\nCharles T. Payne\\\\nGreat-uncle of Barack Obama, younger brother of Madelyn Dunham, born 1925. Served during World War II in the U.S. Army 89th Infantry Division.[25] Obama has often described Payne\\'s role in liberating Ohrdruf forced labor camp.[26] There was brief media attention when Obama mistakenly identified the camp as Auschwitz during the campaign.[27] Payne appeared in the visitor\\'s gallery at the Democratic National Convention in Denver, Colorado, when his great-nephew was nominated for President.[28] He was the assistant director of the University of Chicago\\'s Library.[26]\\\\nMaya Soetoro-Ng\\\\nHalf-sister of Barack Obama, born August 15, 1970, in Jakarta, Indonesia.[29] She is married to Konrad Ng, with whom she has a daughter, Suhaila. Maya Soetoro-Ng is a teacher in Hawaii.\\\\nKonrad Ng\\\\nBrother-in-law of Barack Obama, born 1974. A Canadian whose parents are Malaysian Chinese immigrants, he is an assistant professor at the University of Hawaii\\'s Academy of Creative Media.[30] His parents are from Kudat and Sandakan, two small towns in Sabah, Malaysia, and he was born and raised in Burlington, Ontario.[31] He married Maya Soetoro-Ng at the end of 2003 in Hawaii.[32] They have one daughter, Suhaila.[33][34][35] Konrad Ng is now a US citizen.[36]\\\\nLolo Soetoro\\\\nStepfather of Barack Obama, born in Indonesia 1936, died 1987.\\\\nExtended family - paternal relations\\\\nThe Obamas are members of the Luo, Kenya\\'s third-largest ethnic group, which is part of a larger family of ethnic groups, collectively also known as Luo. This group belongs to the Eastern Sudanic branch of the Nilo-Saharan phylum. The Obama family is largely concentrated in the western province of Nyanza.\\\\nFront row (left to right): Auma Obama (Barack\\'s half-sister), Kezia Obama (Barack\\'s step-mother), Sarah Hussein Onyango Obama (third wife of Barack\\'s paternal grandfather), Zeituni Onyango (Barack\\'s aunt)\\\\nBack row (left to right): Said Obama (Barack\\'s uncle), Barack Obama, Abongo [Roy] Obama (Barack\\'s half-brother), unidentified woman, Bernard Obama (Barack\\'s half-brother), Abo Obama (Barack\\'s half-brother).\\\\nBarack Obama, Sr.\\\\nBarack Obama\\'s father, (1936ýýý1982). Government economist in Kenya. In addition to President Obama, Barack Obama Sr. fathered six other sons and a daughter.[37]\\\\nHussein Onyango Obama\\\\nBarack Obama\\'s paternal grandfather (c. 1895ýýý1979);[38] he worked as a mission cook. He joined the British Army during World War I. (One source gives 1870ýýý1975 as his dates of birth and death based on his tombstone reading \\\\\"Mzee Hussein Onyango Obama\\\\\" in his home village.[39] The term \\\\\"mzee\\\\\" is a Kenyan honorific meaning \\\\\"old man\\\\\" or \\\\\"elder.\\\\\") According to his third wife, Sarah, he originally converted to Catholicism, but took the name Hussein when he later converted to Islam; she said he passed the name, not the religion, on to his children.[40]\\\\nHabiba Akumu Obama\\\\nBarack Obama\\'s paternal grandmother, and the second wife of Hussein Onyango Obama. A photograph of her holding her son, Barack Sr, on her lap is on the cover of her grandson\\'s memoirs titled Dreams from my Father.[41]\\\\nSarah Obama\\\\nThird wife of Obama\\'s paternal grandfather, born 1922.[39] Also known, through the addition of her late husband\\'s name, as Sarah Onyango Obama,[42] and sometimes referred to as Sarah Ogwel, Sarah Hussein Obama or Sarah Anyango Obama,[43] she lives in Nyangýýýoma Kogelo village, 30 miles west of western Kenya\\'s main town, Kisumu, on the edge of Lake Victoria.[44][45]\\\\nAlthough not a blood relation, Barack Obama calls her \\\\\"Granny Sarah\\\\\".[43][46] Sarah, who speaks Luo and only a few words of English, communicates with President Obama through an interpreter.\\\\nOn July 4, 2008, she attended the United States Independence Day celebrations in Nairobi, hosted by Michael Ranneberger, the US ambassador in Kenya.[47]\\\\nDuring the campaign she protested attempts to portray Obama as a foreigner to the United States or a Muslim, saying that while Obama\\'s grandfather had been a Muslim, \\\\\"In the world of today, children have different religions from their parents.\\\\\"[40] Sarah Obama herself is \\\\\"a strong believer of the Islamic faith,ýýý in her words.[48]\\\\nKezia Obama\\\\nFirst wife of Barack Obama\\'s father, born c. 1940. She is Barack Obama Sr.\\'s first wife, whom he married in Kenya before studying abroad in the United States. Also known as Kezia Grace Obama.[49][50] She currently lives in Bracknell, Berkshire, England.[51][52] Her sister, Jane, is the \\'Auntie Jane\\' mentioned at the very start of Dreams from My Father when she telephoned President Obama to inform him that his father had been killed in a car accident.[53]\\\\nMalik Obama\\\\nBarack Obama\\'s half-brother, also known as Abongo or Roy, born c. March, 1958,[51] son of Barack Obama, Sr. with his first wife, Kezia.[54] Malik Obama was born and raised in Nairobi, Kenya.[55] He earned a degree in accounting from the University of Nairobi.[56] He met his half-brother for the first time in 1985[55] when Barack flew from Chicago to Washington, D.C. to visit him.[57] Malik and his half-brother Barack were best men at each other\\'s weddings.[55] Barack Obama brought his wife Michelle to Kenya three years later, and they met with Malik again while Barack was introducing Michelle to many other new relatives.[58]\\\\nAlthough much of the Obama family has dispersed throughout Kenya and overseas, most, including Malik Obama, still considered their rural village on the shores of Lake Victoria to be their true home, and feel that those who have left the village have become culturally \\\\\"lost\\\\\".[59] A frequent visitor to the United States,[58] and consultant in Washington, D.C. for several months per year,[55] he nevertheless settled in the Obamas\\' ancestral home, Nyangýýýoma Kogelo, a village of several hundred people that he prefers to the city for its slow pace.[55] He runs a small electronics shop a half hour drive outside of town.[55]\\\\nDuring his brother\\'s presidential campaign, Malik Obama was a spokesman for the extended Obama family in Kenya, dealing with safety and privacy concerns arising from increased attention from the press.[60]\\\\nAbo Obama\\\\nBarack Obama\\'s half-brother, born 1968. International telephone store manager in Kenya.\\\\nAuma Obama\\\\nBarack Obama\\'s half-sister, born c. 1960.[61] As of July 2008, development worker in Kenya.[62] She studied German at the University of Heidelberg from 1981 to 1987. After her graduation in Heidelberg she went on for graduate studies at the University of Bayreuth, which awarded her a PhD in 1996. Her dissertation was about the conception of labor in Germany and its literary reflections.[62] Auma Obama lives in London, and in 1996 married an Englishman, Ian Manners. They have a daughter named Akinyi (b. 1997).[62][verification needed]\\\\nBernard Obama\\\\nBarack Obama\\'s half-brother, born 1970, son of Barack Obama, Sr. and his first wife, Kezia. He had been an auto parts supplier in Nairobi, Kenya, and has one child. Bernard converted to Islam as an adult and has said: \\\\\"Iýýým a Muslim, I donýýýt deny it. My father was raised a Muslim. But itýýýs not an issue. I donýýýt know what all the hullabaloo is about.\\\\\"[63] He currently resides in Bracknell, England, with his mother Kezia.[63]\\\\nRuth Ndesandjo\\\\nBorn Ruth Nidesand, in US c. 1940s, Barack Obama Sr.\\'s third wife and a private kindergarten director in Kenya.[64] Ruth\\'s two sons with Barack Obama, Sr., are Mark and David Ndesandjo; her third son, Joseph Ndesandjo, was born c. 1980 from a subsequent marriage to a Tanzanian.[65][66]\\\\nMark Ndesandjo\\\\nBarack Obama\\'s half-brother, son of Ruth Nidesand and Barack Obama Sr.[67] He runs an Internet company called WorldNexus that advises Chinese corporations how best to reach international customers.[68] Mark graduated from Brown University, studied physics at Stanford University, received an MBA from Emory University, and has lived in Shenzhen, China, since 2002 and is married to a Chinese woman.[69] He is also an accomplished pianist.[70]\\\\nDavid Ndesandjo\\\\nDavid Ndesandjo\\\\nBarack Obama\\'s half-brother (also known as David Opiyo Obama), son of Ruth Nidesand and Barack Obama Sr. Killed in a motorcycle accident.[71]\\\\nGeorge Hussein Onyango Obama\\\\nYoungest half-brother of Barack Obama, born c.1982, son of Barack Obama Sr.[72] and Jael (now a resident of Atlanta, Georgia).[73][74] George was six months old when his father died in an automobile accident, after which he was raised in Nairobi by his mother and a French step-father. He later lived in South Korea for two years while his mother resided there for business reasons.[73] Returning to Kenya, George Obama \\\\\"slept rough for several years,\\\\\" until his aunt gave him a six-by-eight foot corrugated metal shack in the Nairobi, Kenya, slum of Huruma Flats.[73] As of August 2008, Obama was studying to become a mechanic.[73] George received little attention until being featured in an article in the Italian language edition of Vanity Fair in August 2008, which portrayed him as living in poverty, shame, and obscurity.[75] The article quoted Obama as saying that he lived \\\\\"on less than a dollar a month\\\\\" and stated that he \\\\\"does not mention his famous half-brother in conversation\\\\\" out of shame at his poverty.[76] In later interviews George contradicted this picture. In an interview with The Times, Obama \\\\\"said that he was furious at subsequent reports that he had been abandoned by the Obama family and that he was filled with shame about living in a slum.\\\\\"[74] He told The Times, \\\\\"Life in Huruma is good.\\\\\" Obama said that he expects no favors, that he was supported by relatives, and that reports he lived on a dollar a month were \\\\\"all lies by people who donýýýt want my brother to win.ýýý[74] He told The Telegraph that he was inspired by his half-brother.[73] According to Time, George \\\\\"has repeatedly denied...that he feels abandoned by Obama.\\\\\"[77] CNN quoted him as saying, \\\\\"I was brought up well. I live well even now. The magazines, they have exaggerated everything... I think I kind of like it here. There are some challenges, but maybe it is just like where you come from, there are the same challenges.\\\\\"[75] George\\'s reported poverty was seized on by conservative critics of Barack Obama. Columnist Dinesh D\\'Souza solicited donations for George Obama from his readers,[78] while Jerome Corsi planned to give him a $1,000 check during a trip to Kenya (Corsi was expelled from the country by immigration authorities).[77]\\\\nOmar Obama\\\\nHalf-uncle of Barack Obama,[79] born on June 3, 1944 in Nyangýýýoma Kogelo. Oldest son of Onyango and Sarah Obama, resides in Boston, Massachusetts.[citation needed]\\\\nZeituni Onyango\\\\nHalf-aunt of Barack Obama,[80] born May 29, 1952, in Kenya,[81] Onyango is referred to as \\\\\"Aunti Zeituni\\\\\" in President Obama\\'s memoir, Dreams from My Father.[82]\\\\nYusuf Obama\\\\nHalf-uncle of Barack Obama,[79] born c. 1950s in Nyangýýýoma Kogelo; son of Onyango and Sarah Obama.[citation needed]\\\\nSaid Obama\\\\nHalf-uncle of Barack Obama,[79][83] born c. 1950s in Nyangýýýoma Kogelo; son of Onyango and Sarah Obama.[citation needed]\\\\nMichelle Robinson Obama\\'s extended family\\\\nFraser Robinson, Sr. (1884ýýý1936) of South Carolina, shown in an old photo along with his wife, Rosella Cohen Robinson, in its background\\\\nBarack Obama has called his wife Michelle \\\\\"the most quintessentially American woman I know.\\\\\"[3] Her family is of African American heritage, descendents of Africans of the American Colonial Era.[3] Michelle Obama\\'s family history traces back from slavery to Reconstruction to the Great Migration North. Some of Michelle\\'s relatives still reside in South Carolina.\\\\nMichelle\\'s earliest known relative is her great-great grandfather Jim Robinson, born in the 1850s, who was an American slave on the Friendfield plantation in South Carolina. The family believes that after the Civil War he remained a Friendfield plantation sharecropper for the rest of his life and that he was buried there in an unmarked grave.[3]\\\\nJim had two sons, Gabriel and Fraser, Michelle Obama\\'s great-grandfather. Fraser had an arm amputated as a result of a boyhood injury. He worked as a shoemaker, newspaper salesman and in a lumber mill and was married to Rosella Cohen.[3] Carrie Nelson, Gabriel Robinson\\'s daughter, now 80, is the oldest living Robinson and the keeper of family lore.[3]\\\\nAt least three of Michelle Obama\\'s great-uncles served in the military of the United States. One aunt moved to Princeton, New Jersey, where she worked as a maid, and cooked Southern-style meals for Michelle and her brother, Craig, when they were students at Princeton University.\\\\nCraig Robinson\\\\nMichelle Obama\\'s brother, born 1962. He is currently head coach of men\\'s basketball at Oregon State University.[84]\\\\nFraser Robinson III\\\\nMichelle Obama\\'s father, born 1935, died 1991, married Michelle\\'s mother, Marian Shields, in 1960.[85][19] Robinson was a pump worker at the City of Chicago water plant.[3]\\\\nFraser Robinson, Jr.\\\\nMichelle Obama\\'s grandfather was born on August 24, 1912 in Georgetown, South Carolina, and died on November 9, 1996, aged 84. He was a good student and orator, but moved from South Carolina to Chicago to find better work than he could find at home, eventually becoming a worker for the United States Postal Service. He was married to LaVaughn Johnson. When he retired, they moved back to South Carolina.[3]\\\\nCapers C. Funnye Jr.\\\\nMichelle Obama\\'s first cousin once removed: Funnyeýýýs mother, Verdelle Robinson Funnye (born Verdelle Robinson; August 22, 1930 ýýý April 16, 2000) and Michelle Obamaýýýs paternal grandfather, Fraser Robinson Jr., were siblings. One of America\\'s most prominent African American Jews, known for acting as a bridge between mainstream Jewry and African Americans.[86]\\\\nGenealogical charts\\\\nObama ancestry\\\\n16. Opiyo\\\\n8. Obama\\\\n4. Hussein Onyango Obama\\\\n9. Nyaoke\\\\n2. Barack Hussein Obama, Sr.\\\\n5. Habiba Akumu\\\\n1. Barack Hussein Obama II\\\\n24. Jacob William Dunham\\\\n12. Ralph Waldo Emerson Dunham, Sr.\\\\n25. Mary Ann Kearney\\\\n6. Stanley Armour Dunham\\\\n26. Harry Ellington Armour\\\\n13. Ruth Lucille Armour\\\\n27. Gabriella Clark\\\\n3. Stanley Ann Dunham\\\\n28. Charles T. Payne\\\\n14. Rolla Charles Payne\\\\n29. Della L. Wolfley\\\\n7. Madelyn Lee Payne\\\\n30. Thomas Creekmore McCurry\\\\n15. Leona Belle McCurry\\\\n31. Margaret Belle Wright\\\\nFamily trees\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nBarack Obama\\\\nStanley\\\\nDunham\\\\n1918ýýý1992\\\\nMadelyn\\\\nPayne\\\\n1922ýýý2008\\\\nHabiba\\\\nAkumu\\\\nOnyango\\\\nObama\\\\nc.\\xa01895ýýý1979\\\\nSarah\\\\nOgwel\\\\n1922ýýý\\\\nLolo\\\\nSoetoro\\\\n1936ýýý1987\\\\nAnn\\\\nDunham\\\\n1942ýýý1995\\\\nBarack\\\\nObama, Sr. *\\\\n1936ýýý1982\\\\nKezia\\\\nGrace\\\\nc. 1940ýýý\\\\nOmar\\\\nObama\\\\n1944ýýý\\\\nZeituni\\\\nOnyango\\\\n1952ýýý\\\\nYusuf\\\\nObama\\\\nc. 1950sýýý\\\\nSaid\\\\nObama\\\\nc. 1950sýýý\\\\nKonrad\\\\nNg\\\\nc. 1974ýýý\\\\nMaya\\\\nSoetoro\\\\n1970ýýý\\\\nBarack\\\\nObama\\\\n1961ýýý\\\\nMichelle\\\\nRobinson\\\\n1964ýýý\\\\nM. Abongo\\\\nObama\\\\n1958ýýý\\\\nAuma\\\\nObama\\\\nc. 1960ýýý\\\\nAbo\\\\nObama\\\\n1968ýýý\\\\nBernard\\\\nObama\\\\n1970ýýý\\\\nSuhaila\\\\nNg\\\\nc. 2005ýýý\\\\nMalia Ann\\\\nObama\\\\n1998ýýý\\\\nSasha\\\\nObama\\\\n2001ýýý\\\\n* Barack\\\\nObama, Sr.\\'s\\\\nadditional\\\\nRuth\\\\nNidesandjo\\\\nc. 1940sýýý\\\\nJael\\\\nOtieno\\\\nrelationships:\\\\nMark\\\\nNdesandjo\\\\nDavid\\\\nNdesandjo\\\\ndied\\xa0c.\\xa01987\\\\nGeorge\\\\nObama\\\\nc. 1982ýýý\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nMichelle Obama\\\\nFraser\\\\nRobinson, Jr.\\\\nýýý\\\\nSouth Carolina\\\\n(1912ýýý1996)\\\\nSawmill worker\\\\nLaVaughn D.\\\\nJohnson\\\\nýýý\\\\nIllinois\\\\n(1915ýýý2002)\\\\nCapers C.\\\\nFunnye, Jr.\\\\nýýý\\\\n(born c. 1952;\\\\nnephew of\\\\nFraser\\\\nRobinson, Jr.)\\\\nRabbi in Chicago\\\\nFraser\\\\nRobinson III\\\\nýýý\\\\nIllinois\\\\n(1935ýýý1991)\\\\nEnjoyed boxing\\\\nin youth;\\\\nwater plant\\\\npump operator\\\\nin Chicago\\\\nMarian\\\\nShields\\\\nýýý\\\\nIllinois\\\\n(born 1937)\\\\nSecretary at\\\\nSpiegel catalog\\\\nin Chicago;\\\\nU.S.\\'s First\\\\nGrandmother\\\\nCraig\\\\nRobinson\\\\nýýý\\\\nIllinois\\\\n(born 1962)\\\\nHead coach of\\\\nOregon State\\\\nBeavers men\\'s\\\\nbasketball\\\\nMichelle\\\\nRobinson\\\\nýýý\\\\nIllinois\\\\n(born 1964)\\\\nFirst Lady\\\\nof the United States\\\\nDistant relations\\\\nSee also: List of United States Presidents by genealogical relationship\\\\nAccording to genealogists, Barack Obama\\'s distant cousins include the multitude of descendants of his maternal ancestors from all along the early-American Atlantic seaboard as well as paternal, Kenyan relations belonging to the Luo tribe, many descending from a 17th century ancestor named Owiny.[87][88] For example, George W. Bush, the 43rd U.S. president, is the eleventh cousin of Barack Obama.[89] The New York Times science writer Nicholas Wade argues that with eleven generations leading back to their common progenitor, Samuel Hinckley, the relationship between the 43rd President and the 44th President is \\\\\"genetically meaningless\\\\\".[90]\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nSelected genealogical relationships\\\\nBill Hickok\\\\nAccording to Barack Obama\\'s family lore (and confirmed by the New England Historic Genealogical Society), the President and Hickok are sixth cousins, six-times removed.[91]\\\\nRobert Duvall\\\\nGoodnight helped inspire Pulitzer Prize-winning author Larry McMurtry to create a protagonist for his novel series Lonesome Dove: Woodrow Call. In Dove\\'s television novela, Woodrow Call\\'s partner is Gus McCrae, portrayed by Obama\\'s eighth cousin, twice removed, actor Robert Duvall.[92]\\\\nJames Madison\\\\nObama is also distantly related to U.S. Presidents James Madison, Lyndon Johnson, Gerald Ford, and U.S. Vice President Dick Cheney, British Prime Minister Sir Winston Churchill, U.S. Civil War General Robert E. Lee, and actor Brad Pitt.[93][94][95]\\\\nCharles Goodnight\\\\nAccording to Chicago Sun-Times reporter Scott Fornek, another Obama progenitor, Catherine Goodnight, was the grandniece of George Goodnight, who was in turn great-grandfather of famed cattleman Charles Goodnight.[92]\\\\nHarry S. Truman\\\\nActor Duvall is distant cousins with United States President Harry Truman, who\\'s likewise a fourth cousin, four times removed, of Obama\\'s.[92]\\\\nGeorgia O\\'Keeffe\\\\nNotable women Obama is related to include painter Georgia OýýýKeeffe.[93]\\\\nSee also\\\\nU.S. Presidential genealogical relationships\\\\nNew England Historic Genealogical Society\\\\nGary Boyd Roberts\\\\nReferences\\\\n^ Keck, Kristi (4 June 2008). \\\\\"Obama crosses historic milestone\\\\\". CNN. http://news.yahoo.com/s/ap/20081122/ap_on_go_pr_wh/obama_school_12. Retrieved on 2008-11-21.\\\\n^ a b Reitwiesner, William Addams. \\\\\"Ancestry of Barack Obama\\\\\". http://www.wargs.com/political/obama.html. Retrieved on 2008-10-09.\\\\n^ a b c d e f g h Murray, Shailagh (2 October 2008). \\\\\"A Family Tree Rooted In American Soil: Michelle Obama Learns About Her Slave Ancestors, Herself and Her Country\\\\\". The Washington Post: p.\\xa0C01. http://www.washingtonpost.com/wp-dyn/content/article/2008/10/01/AR2008100103169.html. Retrieved on 2008-10-10.\\\\n^ Sheridan, Michael (5 February 2007). \\\\\"Secrets of Obama Family Unlocked\\\\\". Muslim Observer. http://news.newamericamedia.org/news/view_article.html?article_id=ae5895fc29971b172938790be94ab107. Retrieved on 2008-11-21.\\\\n^ RTE news report March, 2007:Obamas Irish family links discovered by ancestry.co.uk\\\\n^ Noveck, Jocelyn (2008-11-07). \\\\\"Deseret News | Obama\\'s young, energetic family harks back to days of Camelot\\\\\". Secure.deseretnews.com. https://secure.deseretnews.com/article/1,5143,705261276,00.html. Retrieved on 2009-01-31.\\\\n^ Liza Mundy, Michelle: A Biography (Simon and Schuster, 2008), p. 129.\\\\n^ \\\\\"Sasha Obama\\\\\". Baltimore Sun. http://www.baltimoresun.com/topic/politics/sasha-obama-PECLB004381.topic. Retrieved on 2009-01-31.\\\\n^ a b Obama, Barack (13 January 2009). \\\\\"\\'What I Want for You--And Every Child in America\\'\\\\\". http://www.parade.com/news/2009/01/barack-obama-letter-to-my-daughters.html.\\\\n^ Sobieraj Westfall, Sandra (23 June 2008). \\\\\"Barack Obama Gives Daughter $1 Allowance a Week\\\\\". People Magazine. http://www.people.com/people/article/0,,20214569_1,00.html. Retrieved on 2008-11-21.\\\\n^ Lester, Will (July 23, 2008). \\\\\"Obama daughters keep hectic schedules of their own\\\\\". Associated Press. http://elections.apnews.com/apelect/db_6911/contentdetail.htm;jsessionid=8314A43012AB5FF1D0697247362D8752?contentguid=H95QubFb&full=true. Retrieved on 2008-08-04.\\\\n^ Hiro, Anne. \\\\\"Obama regrets letting \\\\\"Access Hollywood\\\\\" interview daughters. Won\\'t do it again. MSNBC\\'s Dan Abrams gets the story behind the story. - Lynn Sweet\\\\\". Blogs.suntimes.com. http://blogs.suntimes.com/sweet/2008/07/obama_regrets_letting_access_h.html. Retrieved on 2009-01-31.\\\\n^ Ahmed, Saeed (5 November 2008). \\\\\"Move over Barney, new dog moving into White House\\\\\". CNN. http://www.cnn.com/2008/LIVING/wayoflife/11/05/presidential.pets/index.html. Retrieved on 2008-11-21.\\\\n^ \\\\\"Obama: Getting a dog isn\\'t easy\\\\\". Associated Press. 7 November 2008. http://www.mercurynews.com/ci_10927292. Retrieved on 2008-11-21.\\\\n^ Janice Lloyd (2009-01-12). \\\\\"Obamas down to Labradoodle or Portuguese water dog\\\\\". USA Today. http://www.usatoday.com/news/washington/2009-01-11-obama-dog_N.htm. Retrieved on 2009-01-28.\\\\n^ Swarns, Rachel (21 November 2008). \\\\\"And the Winner Is ýýý Sidwell Friends\\\\\". The New York Times. http://thecaucus.blogs.nytimes.com/2008/11/21/and-the-winner-is-sidwell-friends/. Retrieved on 2008-11-21.\\\\n^ Tolin, Lisa (2009-01-05). \\\\\"Obama girls start school with photographers in tow\\\\\". The Associated Press. http://www.google.com/hostednews/ap/article/ALeqM5g6mv_lkODQMmQdpyIEnr8Zpm5mogD95H8KA80. Retrieved on 2009-01-06.\\\\n^ Taylor Marsh (2008-08-25). \\\\\"Political Analysis, National Security and Breaking News\\\\\". Taylor Marsh. http://www.taylormarsh.com/archives_view.php?id=28286. Retrieved on 2009-01-31.\\\\n^ a b Lia LoBello (2008-01-02). \\\\\"First Families: Radar introduces you to the next president\\'s relatives\\\\\". Radar Online. http://www.radaronline.com/features/2008/07/john_mccain_barack_obama_michelle_cindy_dunham_roberta_wrigh_04.php. Retrieved on 2009-01-28.\\\\n^ Rachel L. Swarns (2009-01-09). \\\\\"Obamaýýýs Mother-in-Law to Move Into the White House\\\\\". The New York Times. http://thecaucus.blogs.nytimes.com/2009/01/09/obamas-mother-in-law-to-move-into-the-white-house/?hp. Retrieved on 2009-01-09.\\\\n^ a b \\\\\"Will Obama mum-in-law make it a family affair in the White House?\\\\\". Agence France Presse. 2008-11-22. http://www.google.com/hostednews/afp/article/ALeqM5gN_i2jrCVkJQgfMbSDRRrNk8U4Sw. Retrieved on 2009-01-09.\\\\n^ Philip Sherwell (2008-2008-11-09). \\\\\"Michelle Obama persuades First Granny to join new White House team\\\\\". The Telegraph (UK). http://www.telegraph.co.uk/news/3407525/Michelle-Obama-persuades-First-Granny-to-join-new-White-House-team.html. Retrieved on 2009-01-09.\\\\n^ \\\\\"ýýýTootýýý: Obama grandmother a force that shaped him\\\\\". via Associated Press. 2008-08-25. http://www.thekansan.com/news/x1311851415/-Toot-Obama-grandmother-a-force-that-shaped-him. Retrieved on 2008-08-29.\\\\n^ \\\\\"CNN: \\\\\"Obama\\'s grandmother dies after battle with cancer\\\\\"\\\\\". http://www.cnn.com/2008/POLITICS/11/03/obama.grandma/index.html. Retrieved on 2008-11-04.\\\\n^ The 89th Infantry Division, United States Holocaust Memorial Museum\\\\n^ a b Obama\\'s great-uncle recalls liberating Nazi camp, Boston.com, July 22, 2008\\\\n^ Major Garrett (2008-05-27). \\\\\"Obama Campaign Scrambles to Correct the Record on Uncle\\'s War Service\\\\\". FOXNews.com. http://elections.foxnews.com/2008/05/27/recollection-of-obama-familys-service-missing-key-details. Retrieved on 2009-01-31.\\\\n^ \\\\\"Democrats salute Obamaýýýs great uncle\\\\\". Jewish Telegraphic Agency. August 28, 2008. http://jta.org/news/article/2008/08/28/110123/obamapayne. Retrieved on 31 January 2009.\\\\n^ Obama Family Tree dgmweb.net\\\\n^ Chicago Sun Times article with her picture\\\\n^ Obama has links to Malaysia\\\\n^ Nolan, Daniel (2008-06-11). \\\\\"Relative: Obama\\'s got \\'a good handle on Canada\\'\\\\\". The Hamilton Spectator. http://www.thespec.com/burlingtonlife/article/384475. Retrieved on 2008-07-03.\\\\n^ Nolan, Daniel (June 11, 2008). \\\\\"Obama\\'s Burlington connection\\\\\". The Hamilton Spectator. http://www.thespec.com/article/384307. Retrieved on 2008-06-21.\\\\n^ Misner, Jason (2008-06-20). \\\\\"Barack Obama was here\\\\\". Burlington Post. http://www.burlingtonpost.com/printarticle/186215. Retrieved on 2008-07-03.\\\\n^ Fornek, Scott (2007-09-09). \\\\\"\\'He helped me find my voice\\'\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545473,BSX-News-wotreehh09.article.\\\\n^ Cooper, Tom (2009-01-20). \\\\\"Keep watch for Obama\\\\\". TheSpec.com. http://www.thespec.com/Opinions/article/499161. Retrieved on 2009-01-28.\\\\n^ Ancestry of Barack Obama\\\\n^ Dreams from My Father, p. 376\\\\n^ a b Kenya: Special Report: Sleepy Little Village Where Obama Traces His Own Roots (Page 2 of 2)\\\\n^ a b \\\\\"Obama\\'s grandma slams \\'untruths\\'\\\\\". Associated Press. 2008-03-05. http://www.usatoday.com/news/world/2008-03-05-obama-kin_N.htm.\\xa0 See also this correction.\\\\n^ \\\\\"Q&A ON THE NEWS\\\\\". Atlanta Journal-Constitution. 2009-02-25. http://www.ajc.com/services/content/metro/stories/2009/02/25/questi0225.html. Retrieved on 2009-02-27.\\\\n^ In Kenya, Barack Obamaýýýs family prays for end to conflict - Times Online\\\\n^ a b Crilly, Rob (February 27, 2008). \\\\\"Dreams from Obama\\'s Grandmother\\\\\". Time Magazine, Inc.. http://www.time.com/time/world/article/0,8599,1717590,00.html?xid=rss-topstories. Retrieved on 2008-07-03.\\\\n^ Pflanz, Mike (2008-01-11). \\\\\"Barack Obama\\'s Kenyan relatives keep faith\\\\\". Daily Telegraph. http://www.telegraph.co.uk/news/main.jhtml?xml=/news/2008/01/09/wuspols1009.xml.\\\\n^ Fornek, Scott (2007-09-09). \\\\\"Sarah Obama - \\'Sparkling, laughing eyes\\'\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545459,BSX-News-wotreeu09.article.\\\\n^ \\\\\"Barack Obama in Kenya\\\\\". CNN. http://www.youtube.com/watch?v=Ikg6gj71U9k.\\\\n^ Daily Nation, July 8, 2008: Obama granny\\'s day out with envoys and top politicians\\\\n^ \\\\\"A Candidate, His Minister and the Search for Faith\\\\\". New York Times. 2007-04-30. http://www.nytimes.com/2007/04/30/us/politics/30obama.html?_r=2&pagewanted=all&oref=slogin&oref=slogin.\\\\n^ \\\\\"Kenya: All Obama kin to spend voting day in Kogelo\\\\\". afrika.no. 2008-10-27. http://www.afrika.no/Detailed/17321.html. Retrieved on 2009-01-31.\\\\n^ Cohen, Roger (2008-03-06). \\\\\"The Obamas of the World - New York Times\\\\\". Nytimes.com. http://www.nytimes.com/2008/03/06/opinion/06cohen.html. Retrieved on 2009-01-31.\\\\n^ a b Sanderson, Elizabeth (2008-01-06). \\\\\"Barack Obama\\'s stepmother living in Bracknell reveals the close bond with him ... and his mother\\\\\". Daily Mail. http://www.dailymail.co.uk/news/article-506338/Barack-Obamas-stepmother-living-Bracknell-reveals-close-bond---mother.html.\\\\n^ Lindsay, Anna (2009-01-20). \\\\\"Barack\\'s bingo-loving stepmother\\\\\". BBC News. http://news.bbc.co.uk/1/hi/england/berkshire/7834368.stm.\\\\n^ http://www.newvision.co.ug/D/8/26/666733\\\\n^ jpt (2008-06-18). \\\\\"From the Fact Check Desk: What Did Obama\\'s Half-Brother Say About Obama\\'s Background\\\\\". ABC News. http://blogs.abcnews.com/politicalpunch/2008/06/from-the-fact-c.html.\\\\n^ a b c d e f Maliti, Tom (2004-10-26). \\\\\"Obama\\'s Brother Chooses Life in Slow Lane\\\\\". The Associated Press. http://www.msnbc.msn.com/id/6333496.\\\\n^ Obama, Dreams from my Father, 2004, p. 265.\\\\n^ Obama, Dreams from my Father, 2004, p. 262.\\\\n^ a b *Oywa, John (2004-08-15). \\\\\"Sleepy Little Village Where Obama Traces His Own Roots\\\\\". The Daily Nation. http://allafrica.com/stories/200408160533.html.\\\\n^ Philip Ochieng (2004-11-01). \\\\\"From Home Squared to the US Senate: How Barack Obama Was Lost and Found\\\\\". The East African. http://www.nationmedia.com/EastAfrican/01112004/Features/PA2-11.html. Retrieved on 2008-03-23.\\\\n^ Warah, Rasna (2008-06-09). \\\\\"We cannot lay claims on Obama; he\\'s not one of us - Obama in this world\\\\\". Daily Nation. http://www.nationmedia.com. Retrieved on 2008-07-10.\\\\n^ Scott Fornek (2007-09-09). \\\\\"AUMA OBAMA: \\'Her restlessness, her independence\\'\\\\\". Chicago Sun Times. http://www.suntimes.com/news/politics/obama/familytree/545465,BSX-News-wotreew09.article. Retrieved on 2008-03-23.\\\\n^ a b c Gathmann, Florian; Gregor Peter Schmitz, Jochen Schýýnmann (July 24, 2008). \\\\\"Studentin in der Bundesrepublik: Wie Auma Obama mit Deutschland haderte\\\\\" (in German). Spiegel Online. http://www.spiegel.de/politik/ausland/0,1518,567286,00.html. Retrieved on 2008-07-24.\\\\n^ a b Harvey, Oliver (07-26 2008). \\\\\"Obama\\'s brother is in Bracknell\\\\\". The Sun. http://www.thesun.co.uk/sol/homepage/news/the_real_american_idol/article1472877.ece. Retrieved on 2008-10-06.\\\\n^ \\\\\"Madari Kindergarten\\\\\". http://www.madarikindergarten.com/.\\\\n^ \\\\\"Welcome To MedWeek San Antonio 2007\\\\\". Medweeksa.org. http://www.medweeksa.org/awardwinners/techfirm.htm. Retrieved on 2009-01-31.\\\\n^ \\\\\"PIDE - Partners for International Development & Education Inc\\\\\". Pideafrica.org. http://pideafrica.org/aboutus.htm. Retrieved on 2009-01-31.\\\\n^ Barack Obamaýýýs brother pushes Chinese imports on US - Times Online\\\\n^ Obama half-brother runs Internet company in China\\\\n^ Roger Cohen (2008-03-17). \\\\\"Obama\\'s Brother in China\\\\\". The New York Times. http://www.nytimes.com/2008/03/17/opinion/29cohen.html. Retrieved on 2008-03-23.\\\\n^ \\\\\"Youku Buzz (daily)\\xa0ýý Blog Archive\\xa0ýý Barack Obamaýýýs Half-Brother in Concert\\\\\". Buzz.youku.com. 2009-01-18. http://buzz.youku.com/2009/01/18/barack-obamas-half-brother-in-concert/. Retrieved on 2009-01-31.\\\\n^ jaketapper (2008-07-28). \\\\\"Political Punch: Barack Obama\\'s Branch-y Family Tree\\\\\". Blogs.abcnews.com. http://blogs.abcnews.com/politicalpunch/2008/07/barack-obamas-1.html. Retrieved on 2009-01-31.\\\\n^ Fornek, Scott (September 9, 2007). \\\\\"HALF-BROTHER GEORGE: \\'I would be there for him\\'\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545447,BSX-News-wotreecc09.stng. Retrieved on 2008-08-04.\\\\n^ a b c d e Pflanz, Mike (August 21, 2008). \\\\\"Barack Obama is my inspiration, says lost brother\\\\\". The Daily Telegraph. http://www.telegraph.co.uk/news/newstopics/uselection2008/barackobama/2595688/Barack-Obama-is-my-inspiration-says-lost-brother.html. Retrieved on 2008-08-23.\\\\n^ a b c Crilly, Rob (August 22, 2008). \\\\\"Life is good in my Nairobi slum, says Barack Obama\\'s younger brother\\\\\". The Times. http://www.timesonline.co.uk/tol/news/world/us_and_americas/us_elections/article4583353.ece. Retrieved on 2008-08-23.\\\\n^ a b McKenzie, David (2008-08-23). \\\\\"Behind the Scenes: Meet George Obama\\\\\". CNN. http://www.cnn.com/2008/POLITICS/08/22/bts.obama.brother/. Retrieved on 2008-10-26.\\\\n^ Pisa, Nick (August 20, 2008). \\\\\"Barack Obama\\'s \\'lost\\' brother found in Kenya\\\\\". Daily Telegraph. http://www.telegraph.co.uk/news/newstopics/uselection2008/barackobama/2590614/Barack-Obamas-lost-brother-found-in-Kenya.html. Retrieved on 2008-08-20.\\\\n^ a b Wadhams, Nick (2008-10-07). \\\\\"Corsi in Kenya: Obama\\'s Nation Boots Obama Nation Author\\\\\". TIME. http://www.time.com/time/world/article/0,8599,1847965,00.html?imw=Y. Retrieved on 2009-01-31.\\\\n^ by Dinesh D\\'Souza. \\\\\"Dinesh D\\'Souza\\xa0: George Obama, Start Packing\\\\\". Townhall.com. http://townhall.com/columnists/DineshDSouza/2008/09/22/george_obama,_start_packing. Retrieved on 2009-01-31.\\\\n^ a b c \\\\\"The Obama Family Tree\\\\\" (PDF). Chicago Sun-Times. September 9, 2007. http://www.suntimes.com/images/cds/MP3/obamatree.pdf. Retrieved on 2008-11-23.\\\\n^ First read, MSNBC\\\\n^ \\\\\"Barack Obama\\'s aunt found living in rundown public housing estate | The Australian\\\\\". Theaustralian.news.com.au. 2008-10-31. http://www.theaustralian.news.com.au/story/0,25197,24578185-5017121,00.html. Retrieved on 2009-01-31.\\\\n^ Boston Housing Authority ýýýflabbergasteredýýý Barack Obamaýýýs aunt living in Southie\\\\n^ Kilner, Derek (2008-11-05). \\\\\"Kenya Celebrates President Obama as Native Son\\\\\". Voice Of America. http://www.voanews.com/english/archive/2008-11/2008-11-05-voa45.cfm. Retrieved on 2008-12-24.\\\\n^ \\\\\"Oregon State University Beavers: Craig Robinson bio\\\\\". http://www.osubeavers.com/ViewArticle.dbml?SPSID=106239&SPID=1954&DB_OEM_ID=4700&ATCLID=1436883&Q_SEASON=2008. Retrieved on 2008-08-21.\\\\n^ \\\\\"RootsWeb\\'s WorldConnect Project: Dowling Family Genealogy\\\\\". Wc.rootsweb.ancestry.com. http://wc.rootsweb.ancestry.com/cgi-bin/igm.cgi?op=GET&db=dowfam3&id=I105855. Retrieved on 2009-01-31.\\\\n^ Weiss, Anthony (September 2, 2008). \\\\\"Michelle Obama Has a Rabbi in Her Family\\\\\". The Forward. http://www.forward.com/articles/14121/. Retrieved on 2008-10-09.\\\\n^ Gary Boyd Roberts. \\\\\"Notes on the Ancestry of Senator Barack Hussein Obama, Jr\\\\\". New England Historic Genealogical Society. http://www.newenglandancestors.org/research/services/articles_ancestry_barack_obama.asp. Retrieved on 2009-01-31.\\\\n^ Scott Fornek (2007-09-09). \\\\\"Obama Family Tree\\\\\". Chicago Sun-Times. http://www.suntimes.com/news/politics/obama/familytree/545463,BSX-News-wotreer09.stng. Retrieved on 2009-01-31.\\\\n^ \\\\\"Obama-Bush (family tree)\\\\\" (PDF). New England Historic Genealogical Society. http://www.newenglandancestors.org/pdfs/obama_bush.pdf. Retrieved on 2009-01-31.\\\\n^ Wade, Nicholas (2007-10-21). \\\\\"Barack Obama - Dick Cheney - Ancestry and Genealogy - Washington - New York Times\\\\\". Nytimes.com. http://www.nytimes.com/2007/10/21/weekinreview/21basic.html. Retrieved on 2009-01-31.\\\\n^ Eastman, Dick (2008-07-30). \\\\\"Barack Obama is Related to Wild Bill Hickok\\\\\". Blog.eogn.com. http://blog.eogn.com/eastmans_online_genealogy/2008/07/barack-obama-is.html. Retrieved on 2009-01-31.\\\\n^ a b c Scott Fornek (2007-09-09). \\\\\"Obama Family Tree\\\\\". Suntimes.com. http://www.suntimes.com/news/politics/obama/familytree/545441,BSX-News-wotreec09.stng. Retrieved on 2009-01-31.\\\\n^ a b \\\\\"Notes on the Ancestry of Senator Barack Hussein Obama, Jr\\\\\". New England Historic Genealogical Society. 2002-08-14. http://www.newenglandancestors.org/research/services/articles_ancestry_barack_obama.asp. Retrieved on 2009-01-31.\\\\n^ \\\\\"Obama, Clinton and McCain have some famous relations\\\\\". HeraldNet - AP. 2008-03-26. http://www.heraldnet.com/article/20080326/NEWS02/151343661. Retrieved on 2009-01-31.\\\\n^ \\\\\"Barack Obama and Joe Biden: The Change We Need\\\\\". My.barackobama.com. 2008-07-31. http://my.barackobama.com/page/community/post/williambrehm/gG5TVR. Retrieved on 2009-01-31.\\\\nExternal links\\\\nBarack Obama\\'s Family Tree - Photo Essays - TIME\\\\n\\\\\"Though Obama Had to Leave to Find Himself, It Is Hawaii That Made His Rise Possible,\\\\\" by David Maraniss\\\\nBarack Obama\\'s Branch-y Family Tree by Jake Tapper\\\\n\\\\\"Obama Family Tree\\\\\" series, by Scott Fornek\\\\n\\\\\"Six Degrees of Barack Obama\\\\\"\\\\n\\\\\"Notes on the Ancestry of Senator Barack Hussein Obama, Jr.,\\\\\" by Gary Boyd Roberts\\\\n\\\\\"Obama, Clinton and McCain have some famous relations,\\\\\" by The Associated Press\\\\n\\\\\"Obama\\'s Patriotic Family Tree,\\\\\" by Bill Brehm\\\\nv\\xa0ýýý\\xa0d\\xa0ýýý\\xa0e\\\\nBarack Obama\\\\nPresidency\\\\nTransition\\xa0ýý Inauguration\\xa0ýý Timeline\\xa0ýý Cabinet\\xa0ýý Judiciary\\xa0ýý Foreign policy\\xa0ýý First 100 days\\\\nEarly life and\\\\npolitical career\\\\nPublic image\\xa0ýý Illinois Senate career\\xa0ýý 2004 Democratic National Convention\\xa0ýý U.S. Senate election in Illinois\\xa0ýý U.S. Senate career\\xa0ýý Presidential primary campaign\\xa0ýý ObamaýýýBiden 2008\\xa0ýý Electoral history\\xa0ýý Political positions\\\\nBooks\\\\nDreams from My Father\\xa0ýý The Audacity of Hope\\\\nSpeeches\\\\nThe Audacity of Hope\\xa0ýý A More Perfect Union\\xa0ýý Change Has Come to America\\xa0ýý 2009 speech to joint session of Congress\\\\nFamily\\\\nMichelle Obama\\xa0ýý Barack Obama, Sr.\\xa0ýý Ann Dunham\\xa0ýý Lolo Soetoro (stepfather)\\xa0ýý Maya Soetoro-Ng (half-sister)\\xa0ýý Marian Robinson (mother-in-law)\\xa0ýý Stanley Armour Dunham (grandfather)\\xa0ýý Madelyn Dunham (grandmother)\\xa0ýý Extended family\\xa0ýý Family tree\\\\nRetrieved from \\\\\"http://en.wikipedia.org/wiki/Family_of_Barack_Obama#Sarah_Obama\\\\\"\\\\nCategories: Obama family | African American history | African American families | Luo Kenyans | People of mixed Black African-European ethnicity | African Americans | Asian Americans | Dutch Americans | English Americans | French Americans | German-Americans | Irish-Americans | Indonesian Americans | Kenyan-Americans | Scottish-Americans | Chinese Canadians | People of mixed Asian-European ethnicity | American families | First Families of the United States | Family treesHidden categories: Wikipedia semi-protected pages | Wikipedia indefinitely move-protected pages | All pages needing cleanup | Wikipedia articles needing factual verification since October 2008 | All pages needing factual verification | All articles with unsourced statements | Articles with unsourced statements since November 2008\\\\nViews\\\\nArticle\\\\nDiscussion\\\\nView source\\\\nHistory\\\\nPersonal tools\\\\nLog in / create account\\\\nNavigation\\\\nMain page\\\\nContents\\\\nFeatured content\\\\nCurrent events\\\\nRandom article\\\\nSearch\\\\nInteraction\\\\nAbout Wikipedia\\\\nCommunity portal\\\\nRecent changes\\\\nContact Wikipedia\\\\nDonate to Wikipedia\\\\nHelp\\\\nToolbox\\\\nWhat links here\\\\nRelated changes\\\\nUpload file\\\\nSpecial pages\\\\nPrintable version Permanent linkCite this page\\\\nLanguages\\\\nBahasa Indonesia\\\\nSvenska\\\\nýýýýýý\\\\nýýýýýý\\\\nThis page was last modified on 14 March 2009, at 05:50.\\\\nAll text is available under the terms of the GNU Free Documentation License. (See Copyrights for details.)\\\\nWikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a U.S. registered 501(c)(3) tax-deductible nonprofit charity.\\\\nPrivacy policy\\\\nAbout Wikipedia\\\\nDisclaimers', 'output': \"Where did Barack Obama's parents and grandparents come from?\", 'input_ids': [784, 15367, 11824, 908, 2588, 251, 30, 1661, 20653, 4534, 31, 7, 384, 892, 6, 379, 31960, 6, 1157, 5233, 7, 6, 1747, 11, 5128, 13, 3879, 6, 672, 5, 784, 9857, 5211, 11810, 134, 908, 784, 26472, 209, 908, 3712, 13, 20653, 4534, 3, 18, 16885, 6, 8, 339, 3, 4392, 3903, 24477, 2, 29, 371, 3690, 120, 13, 20653, 4534, 2, 29, 22674, 16885, 6, 8, 339, 3, 4392, 3903, 24477, 2, 29, 599, 1649, 22955, 45, 180, 3198, 9, 4534, 61, 2, 29, 683, 440, 102, 12, 10, 8789, 6, 960, 2, 29, 667, 115, 265, 9, 3712, 2, 29, 345, 15704, 20653, 4534, 6, 1485, 8571, 15275, 6, 11, 16649, 283, 5434, 11, 180, 3198, 9, 6772, 12, 8, 4374, 227, 112, 22145, 1115, 3049, 5, 16047, 2464, 6, 30, 8, 4653, 2245, 13, 8, 412, 5, 134, 5, 18808, 5, 6306, 536, 908, 2, 29, 254, 450, 5320, 1719, 2, 29, 518, 3198, 6029, 6, 5795, 2, 29, 1570, 14678, 2, 29, 345, 11706, 13, 5233, 2, 29, 5110, 23, 1054, 1323, 2, 29, 10358, 179, 724, 2, 29, 14851, 4365, 4534, 6, 15275, 4534, 6, 6206, 6393, 1483, 6, 20653, 4534, 6, 180, 52, 5, 6, 672, 5, 2, 29, 25772, 15, 26, 1791, 2, 29, 24372, 77, 739, 6, 6393, 1483, 6, 264, 15, 17, 127, 32, 6, 445, 122, 2, 29, 3713, 1108, 19, 294, 13, 3, 9, 939, 81, 2, 29, 14851, 4365, 4534, 2, 29, 21106, 9232, 3, 2, 7659, 7819, 3, 2, 412, 5, 134, 5, 7819, 2, 29, 8931, 155, 1950, 4655, 3, 2, 2575, 1023, 3, 2, 3712, 2, 29, 16128, 3778, 2593, 3, 2, 4534, 2, 279, 23, 537, 2066, 2, 29, 18474, 4749, 3, 2, 86, 402, 7840, 257, 3, 2, 3, 21543, 8563, 892, 2, 29, 10572, 1583, 11298, 41, 13368, 747, 6, 1485, 910, 477, 61, 2, 29, 329, 5434, 6, 15275, 11, 180, 3198, 9, 30, 1726, 44, 8, 2628, 10021, 868, 11347, 2, 29, 634, 3712, 13, 20653, 4534, 19, 46, 4760, 18297, 13, 3850, 797, 6, 1566, 6, 9995, 29, 6, 11, 12605, 29, 41, 434, 76, 32, 61, 8681, 801, 190, 8, 913, 7, 11, 1827, 1415, 13, 20653, 4534, 6, 8, 1661, 13, 8, 907, 1323, 13, 1371, 6, 6306, 357, 908, 6306, 519, 908, 6306, 591, 908, 6306, 755, 908, 11, 119, 2279, 5, 978, 5299, 384, 19, 8, 1485, 3712, 13, 8, 907, 1323, 5, 37, 4534, 7, 33, 8, 166, 1485, 3712, 13, 3850, 797, 19991, 16, 8, 907, 1323, 11, 8, 19147, 12, 2058, 8, 1945, 1384, 437, 8, 14532, 7, 5, 555, 6710, 343, 2832, 6, 3, 2, 667, 115, 265, 9, 31, 7, 1021, 6, 11273, 384, 3, 3272, 157, 7, 223, 12, 477, 13, 5184, 15, 3171, 5, 2, 6306, 948, 908, 2, 29, 4302, 4669, 7, 2, 29, 536, 1318, 5700, 342, 384, 2, 29, 357, 27944, 384, 3, 18, 28574, 5836, 2, 29, 519, 27944, 384, 3, 18, 2576, 2947, 138, 5836, 2, 29, 591, 15275, 17461, 4534, 31, 7, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [2840, 410, 20653, 4534, 31, 7, 1362, 11, 22229, 369, 45, 58, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "tokenizer,tokenized_dataset= tokenize_and_split_dataset(\"training_top5_qulac_PREPROCESSED_FOR_MODEL.json\",\"t5-small\")\n",
    "print(tokenized_dataset[0])  # Optional: check one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe93aef1-844a-4a6f-b041-2eaccd8852a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds= generate_folds(tokenized_dataset, n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1d89eb52-b327-4743-8997-6121fb993b0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/users/nfs/Etu0/21402600/wandb/run-20250509_013816-uhtrlok0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/uhtrlok0' target=\"_blank\">T5_SMALL_TOP_5_DOCS</a></strong> to <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/uhtrlok0' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/uhtrlok0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1981337/3761292519.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [450/450 07:18, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>24.110600</td>\n",
       "      <td>7.909437</td>\n",
       "      <td>0.047588</td>\n",
       "      <td>0.374766</td>\n",
       "      <td>0.157747</td>\n",
       "      <td>0.363523</td>\n",
       "      <td>0.275237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>12.190700</td>\n",
       "      <td>1.011101</td>\n",
       "      <td>0.103273</td>\n",
       "      <td>0.386857</td>\n",
       "      <td>0.166034</td>\n",
       "      <td>0.377508</td>\n",
       "      <td>0.301269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.538600</td>\n",
       "      <td>0.872277</td>\n",
       "      <td>0.114142</td>\n",
       "      <td>0.398919</td>\n",
       "      <td>0.178434</td>\n",
       "      <td>0.387180</td>\n",
       "      <td>0.319090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.164700</td>\n",
       "      <td>0.828989</td>\n",
       "      <td>0.117945</td>\n",
       "      <td>0.409622</td>\n",
       "      <td>0.173707</td>\n",
       "      <td>0.393014</td>\n",
       "      <td>0.330334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.164700</td>\n",
       "      <td>0.721032</td>\n",
       "      <td>0.135739</td>\n",
       "      <td>0.427961</td>\n",
       "      <td>0.195236</td>\n",
       "      <td>0.414530</td>\n",
       "      <td>0.358241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.847300</td>\n",
       "      <td>0.673097</td>\n",
       "      <td>0.143557</td>\n",
       "      <td>0.433076</td>\n",
       "      <td>0.206606</td>\n",
       "      <td>0.422220</td>\n",
       "      <td>0.369737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.747500</td>\n",
       "      <td>0.636529</td>\n",
       "      <td>0.148427</td>\n",
       "      <td>0.436883</td>\n",
       "      <td>0.212886</td>\n",
       "      <td>0.428279</td>\n",
       "      <td>0.378551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.657900</td>\n",
       "      <td>0.590409</td>\n",
       "      <td>0.159183</td>\n",
       "      <td>0.450980</td>\n",
       "      <td>0.225189</td>\n",
       "      <td>0.442551</td>\n",
       "      <td>0.391678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.657900</td>\n",
       "      <td>0.556572</td>\n",
       "      <td>0.167652</td>\n",
       "      <td>0.474435</td>\n",
       "      <td>0.229413</td>\n",
       "      <td>0.466709</td>\n",
       "      <td>0.418650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.583200</td>\n",
       "      <td>0.544981</td>\n",
       "      <td>0.171319</td>\n",
       "      <td>0.479601</td>\n",
       "      <td>0.226465</td>\n",
       "      <td>0.472042</td>\n",
       "      <td>0.423932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.538300</td>\n",
       "      <td>0.537315</td>\n",
       "      <td>0.169903</td>\n",
       "      <td>0.484318</td>\n",
       "      <td>0.226112</td>\n",
       "      <td>0.474042</td>\n",
       "      <td>0.427499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.504800</td>\n",
       "      <td>0.533065</td>\n",
       "      <td>0.169556</td>\n",
       "      <td>0.482238</td>\n",
       "      <td>0.220682</td>\n",
       "      <td>0.473084</td>\n",
       "      <td>0.421973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.504800</td>\n",
       "      <td>0.530522</td>\n",
       "      <td>0.177291</td>\n",
       "      <td>0.485835</td>\n",
       "      <td>0.231536</td>\n",
       "      <td>0.477122</td>\n",
       "      <td>0.427798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.486300</td>\n",
       "      <td>0.527241</td>\n",
       "      <td>0.175608</td>\n",
       "      <td>0.483763</td>\n",
       "      <td>0.228803</td>\n",
       "      <td>0.474618</td>\n",
       "      <td>0.425666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.489400</td>\n",
       "      <td>0.523926</td>\n",
       "      <td>0.177900</td>\n",
       "      <td>0.485893</td>\n",
       "      <td>0.230996</td>\n",
       "      <td>0.475810</td>\n",
       "      <td>0.429522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.479100</td>\n",
       "      <td>0.521356</td>\n",
       "      <td>0.180087</td>\n",
       "      <td>0.486335</td>\n",
       "      <td>0.232589</td>\n",
       "      <td>0.476741</td>\n",
       "      <td>0.430383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.479100</td>\n",
       "      <td>0.519820</td>\n",
       "      <td>0.180061</td>\n",
       "      <td>0.486199</td>\n",
       "      <td>0.232381</td>\n",
       "      <td>0.476313</td>\n",
       "      <td>0.430324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.466400</td>\n",
       "      <td>0.518212</td>\n",
       "      <td>0.184195</td>\n",
       "      <td>0.488098</td>\n",
       "      <td>0.234597</td>\n",
       "      <td>0.478191</td>\n",
       "      <td>0.432997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.463500</td>\n",
       "      <td>0.517498</td>\n",
       "      <td>0.182706</td>\n",
       "      <td>0.489047</td>\n",
       "      <td>0.236792</td>\n",
       "      <td>0.479070</td>\n",
       "      <td>0.435227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.460900</td>\n",
       "      <td>0.516440</td>\n",
       "      <td>0.180808</td>\n",
       "      <td>0.487784</td>\n",
       "      <td>0.235710</td>\n",
       "      <td>0.478534</td>\n",
       "      <td>0.432403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.460900</td>\n",
       "      <td>0.515080</td>\n",
       "      <td>0.183027</td>\n",
       "      <td>0.488974</td>\n",
       "      <td>0.236652</td>\n",
       "      <td>0.480782</td>\n",
       "      <td>0.433855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.453600</td>\n",
       "      <td>0.514179</td>\n",
       "      <td>0.186609</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.240421</td>\n",
       "      <td>0.482084</td>\n",
       "      <td>0.438209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.452700</td>\n",
       "      <td>0.513694</td>\n",
       "      <td>0.186710</td>\n",
       "      <td>0.492348</td>\n",
       "      <td>0.240421</td>\n",
       "      <td>0.483541</td>\n",
       "      <td>0.438477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.448200</td>\n",
       "      <td>0.513186</td>\n",
       "      <td>0.186737</td>\n",
       "      <td>0.492244</td>\n",
       "      <td>0.240041</td>\n",
       "      <td>0.483362</td>\n",
       "      <td>0.438496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.448200</td>\n",
       "      <td>0.512549</td>\n",
       "      <td>0.187207</td>\n",
       "      <td>0.491146</td>\n",
       "      <td>0.241812</td>\n",
       "      <td>0.482542</td>\n",
       "      <td>0.439854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.435900</td>\n",
       "      <td>0.512018</td>\n",
       "      <td>0.188984</td>\n",
       "      <td>0.491524</td>\n",
       "      <td>0.242821</td>\n",
       "      <td>0.483351</td>\n",
       "      <td>0.441620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.456100</td>\n",
       "      <td>0.511792</td>\n",
       "      <td>0.188808</td>\n",
       "      <td>0.491524</td>\n",
       "      <td>0.241501</td>\n",
       "      <td>0.482099</td>\n",
       "      <td>0.441620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.442400</td>\n",
       "      <td>0.511576</td>\n",
       "      <td>0.189059</td>\n",
       "      <td>0.493813</td>\n",
       "      <td>0.243877</td>\n",
       "      <td>0.484650</td>\n",
       "      <td>0.443585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.442400</td>\n",
       "      <td>0.511359</td>\n",
       "      <td>0.189310</td>\n",
       "      <td>0.494954</td>\n",
       "      <td>0.244892</td>\n",
       "      <td>0.485735</td>\n",
       "      <td>0.444662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.444400</td>\n",
       "      <td>0.511338</td>\n",
       "      <td>0.189310</td>\n",
       "      <td>0.494954</td>\n",
       "      <td>0.244892</td>\n",
       "      <td>0.485735</td>\n",
       "      <td>0.444662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 results: {'eval_loss': 0.5113382935523987, 'eval_bleu': 0.1893099536860164, 'eval_rouge1': 0.4949540176339151, 'eval_rouge2': 0.24489164019572845, 'eval_rougeL': 0.4857354966359395, 'eval_meteor': 0.44466199798519823, 'eval_runtime': 1.6267, 'eval_samples_per_second': 73.77, 'eval_steps_per_second': 2.459, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 46410.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 1 saved to crossval_T5_SMALL_TOP5DOCS_fold_1.jsonl\n",
      "Processing Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1981337/3761292519.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [450/450 07:18, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>23.505700</td>\n",
       "      <td>7.940170</td>\n",
       "      <td>0.050934</td>\n",
       "      <td>0.370665</td>\n",
       "      <td>0.158977</td>\n",
       "      <td>0.362204</td>\n",
       "      <td>0.280213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>12.294400</td>\n",
       "      <td>1.066327</td>\n",
       "      <td>0.114260</td>\n",
       "      <td>0.382475</td>\n",
       "      <td>0.171940</td>\n",
       "      <td>0.376345</td>\n",
       "      <td>0.302115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.605700</td>\n",
       "      <td>0.861255</td>\n",
       "      <td>0.116127</td>\n",
       "      <td>0.394917</td>\n",
       "      <td>0.182430</td>\n",
       "      <td>0.388872</td>\n",
       "      <td>0.319728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.151400</td>\n",
       "      <td>0.817595</td>\n",
       "      <td>0.123737</td>\n",
       "      <td>0.406307</td>\n",
       "      <td>0.186882</td>\n",
       "      <td>0.399597</td>\n",
       "      <td>0.344140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.151400</td>\n",
       "      <td>0.708440</td>\n",
       "      <td>0.138450</td>\n",
       "      <td>0.420978</td>\n",
       "      <td>0.203982</td>\n",
       "      <td>0.412892</td>\n",
       "      <td>0.369331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.846600</td>\n",
       "      <td>0.644693</td>\n",
       "      <td>0.147756</td>\n",
       "      <td>0.433454</td>\n",
       "      <td>0.213056</td>\n",
       "      <td>0.423685</td>\n",
       "      <td>0.385310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.744400</td>\n",
       "      <td>0.608788</td>\n",
       "      <td>0.149544</td>\n",
       "      <td>0.436275</td>\n",
       "      <td>0.218419</td>\n",
       "      <td>0.426298</td>\n",
       "      <td>0.389329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.569370</td>\n",
       "      <td>0.155394</td>\n",
       "      <td>0.451408</td>\n",
       "      <td>0.230495</td>\n",
       "      <td>0.440874</td>\n",
       "      <td>0.405376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.534808</td>\n",
       "      <td>0.162999</td>\n",
       "      <td>0.466417</td>\n",
       "      <td>0.232215</td>\n",
       "      <td>0.457027</td>\n",
       "      <td>0.424069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.583700</td>\n",
       "      <td>0.523624</td>\n",
       "      <td>0.166071</td>\n",
       "      <td>0.474904</td>\n",
       "      <td>0.235152</td>\n",
       "      <td>0.465789</td>\n",
       "      <td>0.436950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.542800</td>\n",
       "      <td>0.517126</td>\n",
       "      <td>0.177888</td>\n",
       "      <td>0.483728</td>\n",
       "      <td>0.239014</td>\n",
       "      <td>0.472722</td>\n",
       "      <td>0.443699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.509000</td>\n",
       "      <td>0.513670</td>\n",
       "      <td>0.178287</td>\n",
       "      <td>0.486572</td>\n",
       "      <td>0.241704</td>\n",
       "      <td>0.475245</td>\n",
       "      <td>0.448656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.509000</td>\n",
       "      <td>0.509130</td>\n",
       "      <td>0.174522</td>\n",
       "      <td>0.488581</td>\n",
       "      <td>0.239806</td>\n",
       "      <td>0.477077</td>\n",
       "      <td>0.449860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.500500</td>\n",
       "      <td>0.506172</td>\n",
       "      <td>0.177143</td>\n",
       "      <td>0.491805</td>\n",
       "      <td>0.241926</td>\n",
       "      <td>0.479809</td>\n",
       "      <td>0.452243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.485600</td>\n",
       "      <td>0.503882</td>\n",
       "      <td>0.179849</td>\n",
       "      <td>0.491332</td>\n",
       "      <td>0.244030</td>\n",
       "      <td>0.480109</td>\n",
       "      <td>0.453904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.470900</td>\n",
       "      <td>0.502146</td>\n",
       "      <td>0.184786</td>\n",
       "      <td>0.492644</td>\n",
       "      <td>0.247601</td>\n",
       "      <td>0.481743</td>\n",
       "      <td>0.455765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.470900</td>\n",
       "      <td>0.499719</td>\n",
       "      <td>0.187418</td>\n",
       "      <td>0.491806</td>\n",
       "      <td>0.249472</td>\n",
       "      <td>0.481358</td>\n",
       "      <td>0.454218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.478800</td>\n",
       "      <td>0.498231</td>\n",
       "      <td>0.187443</td>\n",
       "      <td>0.491943</td>\n",
       "      <td>0.249667</td>\n",
       "      <td>0.480979</td>\n",
       "      <td>0.454235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.458200</td>\n",
       "      <td>0.497519</td>\n",
       "      <td>0.188977</td>\n",
       "      <td>0.493430</td>\n",
       "      <td>0.251425</td>\n",
       "      <td>0.482318</td>\n",
       "      <td>0.455434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.461700</td>\n",
       "      <td>0.496796</td>\n",
       "      <td>0.189515</td>\n",
       "      <td>0.498480</td>\n",
       "      <td>0.256435</td>\n",
       "      <td>0.487493</td>\n",
       "      <td>0.459725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.461700</td>\n",
       "      <td>0.495910</td>\n",
       "      <td>0.185162</td>\n",
       "      <td>0.490145</td>\n",
       "      <td>0.252433</td>\n",
       "      <td>0.479651</td>\n",
       "      <td>0.453973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.460600</td>\n",
       "      <td>0.495084</td>\n",
       "      <td>0.185357</td>\n",
       "      <td>0.492714</td>\n",
       "      <td>0.252948</td>\n",
       "      <td>0.482635</td>\n",
       "      <td>0.455166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.446900</td>\n",
       "      <td>0.494512</td>\n",
       "      <td>0.186733</td>\n",
       "      <td>0.493394</td>\n",
       "      <td>0.254413</td>\n",
       "      <td>0.483003</td>\n",
       "      <td>0.456316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.450600</td>\n",
       "      <td>0.493823</td>\n",
       "      <td>0.189892</td>\n",
       "      <td>0.495249</td>\n",
       "      <td>0.257201</td>\n",
       "      <td>0.485099</td>\n",
       "      <td>0.458948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.450600</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>0.189866</td>\n",
       "      <td>0.495249</td>\n",
       "      <td>0.257201</td>\n",
       "      <td>0.485099</td>\n",
       "      <td>0.458944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.450600</td>\n",
       "      <td>0.492913</td>\n",
       "      <td>0.188639</td>\n",
       "      <td>0.495863</td>\n",
       "      <td>0.256036</td>\n",
       "      <td>0.485552</td>\n",
       "      <td>0.457832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.445900</td>\n",
       "      <td>0.492713</td>\n",
       "      <td>0.185757</td>\n",
       "      <td>0.494164</td>\n",
       "      <td>0.253238</td>\n",
       "      <td>0.483836</td>\n",
       "      <td>0.455620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.441400</td>\n",
       "      <td>0.492383</td>\n",
       "      <td>0.187019</td>\n",
       "      <td>0.494306</td>\n",
       "      <td>0.253492</td>\n",
       "      <td>0.484150</td>\n",
       "      <td>0.456667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.441400</td>\n",
       "      <td>0.492238</td>\n",
       "      <td>0.187019</td>\n",
       "      <td>0.494306</td>\n",
       "      <td>0.253492</td>\n",
       "      <td>0.484150</td>\n",
       "      <td>0.456667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.448500</td>\n",
       "      <td>0.492179</td>\n",
       "      <td>0.187093</td>\n",
       "      <td>0.495323</td>\n",
       "      <td>0.254196</td>\n",
       "      <td>0.484847</td>\n",
       "      <td>0.457071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 results: {'eval_loss': 0.492179274559021, 'eval_bleu': 0.18709251573004337, 'eval_rouge1': 0.4953230691007793, 'eval_rouge2': 0.25419599837399587, 'eval_rougeL': 0.48484664268120403, 'eval_meteor': 0.45707138727979757, 'eval_runtime': 2.3616, 'eval_samples_per_second': 50.814, 'eval_steps_per_second': 1.694, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 46495.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 2 saved to crossval_T5_SMALL_TOP5DOCS_fold_2.jsonl\n",
      "Processing Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1981337/3761292519.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [450/450 07:18, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>23.328100</td>\n",
       "      <td>8.096430</td>\n",
       "      <td>0.055385</td>\n",
       "      <td>0.357908</td>\n",
       "      <td>0.161374</td>\n",
       "      <td>0.350420</td>\n",
       "      <td>0.269925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>12.432700</td>\n",
       "      <td>1.002345</td>\n",
       "      <td>0.103265</td>\n",
       "      <td>0.360592</td>\n",
       "      <td>0.153494</td>\n",
       "      <td>0.351926</td>\n",
       "      <td>0.284345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.649600</td>\n",
       "      <td>0.861524</td>\n",
       "      <td>0.114531</td>\n",
       "      <td>0.365002</td>\n",
       "      <td>0.158698</td>\n",
       "      <td>0.357462</td>\n",
       "      <td>0.294214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.178800</td>\n",
       "      <td>0.815570</td>\n",
       "      <td>0.116965</td>\n",
       "      <td>0.380148</td>\n",
       "      <td>0.162180</td>\n",
       "      <td>0.373489</td>\n",
       "      <td>0.314761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.178800</td>\n",
       "      <td>0.696713</td>\n",
       "      <td>0.136258</td>\n",
       "      <td>0.398439</td>\n",
       "      <td>0.187520</td>\n",
       "      <td>0.393987</td>\n",
       "      <td>0.347386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.870100</td>\n",
       "      <td>0.631138</td>\n",
       "      <td>0.151025</td>\n",
       "      <td>0.415786</td>\n",
       "      <td>0.204817</td>\n",
       "      <td>0.409936</td>\n",
       "      <td>0.368813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.745600</td>\n",
       "      <td>0.584815</td>\n",
       "      <td>0.151458</td>\n",
       "      <td>0.420237</td>\n",
       "      <td>0.203879</td>\n",
       "      <td>0.413175</td>\n",
       "      <td>0.376094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.668800</td>\n",
       "      <td>0.543954</td>\n",
       "      <td>0.155305</td>\n",
       "      <td>0.432323</td>\n",
       "      <td>0.207701</td>\n",
       "      <td>0.425633</td>\n",
       "      <td>0.385146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.668800</td>\n",
       "      <td>0.508514</td>\n",
       "      <td>0.165670</td>\n",
       "      <td>0.453704</td>\n",
       "      <td>0.214391</td>\n",
       "      <td>0.448437</td>\n",
       "      <td>0.406393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.597600</td>\n",
       "      <td>0.491800</td>\n",
       "      <td>0.171525</td>\n",
       "      <td>0.468051</td>\n",
       "      <td>0.220034</td>\n",
       "      <td>0.463571</td>\n",
       "      <td>0.418190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.536200</td>\n",
       "      <td>0.486961</td>\n",
       "      <td>0.171678</td>\n",
       "      <td>0.470743</td>\n",
       "      <td>0.221525</td>\n",
       "      <td>0.464735</td>\n",
       "      <td>0.424185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.525600</td>\n",
       "      <td>0.482558</td>\n",
       "      <td>0.175600</td>\n",
       "      <td>0.472647</td>\n",
       "      <td>0.226655</td>\n",
       "      <td>0.466910</td>\n",
       "      <td>0.427843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.525600</td>\n",
       "      <td>0.479175</td>\n",
       "      <td>0.177658</td>\n",
       "      <td>0.473605</td>\n",
       "      <td>0.228753</td>\n",
       "      <td>0.468578</td>\n",
       "      <td>0.430152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.503700</td>\n",
       "      <td>0.474252</td>\n",
       "      <td>0.178200</td>\n",
       "      <td>0.476048</td>\n",
       "      <td>0.227129</td>\n",
       "      <td>0.471031</td>\n",
       "      <td>0.435318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.496800</td>\n",
       "      <td>0.470813</td>\n",
       "      <td>0.181982</td>\n",
       "      <td>0.477998</td>\n",
       "      <td>0.229432</td>\n",
       "      <td>0.473335</td>\n",
       "      <td>0.438164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.485600</td>\n",
       "      <td>0.468495</td>\n",
       "      <td>0.183291</td>\n",
       "      <td>0.475471</td>\n",
       "      <td>0.230393</td>\n",
       "      <td>0.471108</td>\n",
       "      <td>0.436664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.485600</td>\n",
       "      <td>0.466696</td>\n",
       "      <td>0.187776</td>\n",
       "      <td>0.476889</td>\n",
       "      <td>0.232445</td>\n",
       "      <td>0.472287</td>\n",
       "      <td>0.438028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.478700</td>\n",
       "      <td>0.465300</td>\n",
       "      <td>0.188352</td>\n",
       "      <td>0.479397</td>\n",
       "      <td>0.233941</td>\n",
       "      <td>0.474502</td>\n",
       "      <td>0.438870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.480100</td>\n",
       "      <td>0.464052</td>\n",
       "      <td>0.190570</td>\n",
       "      <td>0.478730</td>\n",
       "      <td>0.235110</td>\n",
       "      <td>0.473532</td>\n",
       "      <td>0.439404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.468700</td>\n",
       "      <td>0.462676</td>\n",
       "      <td>0.190112</td>\n",
       "      <td>0.479239</td>\n",
       "      <td>0.234368</td>\n",
       "      <td>0.474054</td>\n",
       "      <td>0.439060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.468700</td>\n",
       "      <td>0.461321</td>\n",
       "      <td>0.190019</td>\n",
       "      <td>0.480006</td>\n",
       "      <td>0.235213</td>\n",
       "      <td>0.474533</td>\n",
       "      <td>0.440007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.461600</td>\n",
       "      <td>0.460510</td>\n",
       "      <td>0.189858</td>\n",
       "      <td>0.476114</td>\n",
       "      <td>0.235256</td>\n",
       "      <td>0.471264</td>\n",
       "      <td>0.438729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.470600</td>\n",
       "      <td>0.459875</td>\n",
       "      <td>0.190803</td>\n",
       "      <td>0.479170</td>\n",
       "      <td>0.236593</td>\n",
       "      <td>0.473830</td>\n",
       "      <td>0.441150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.458200</td>\n",
       "      <td>0.458973</td>\n",
       "      <td>0.190993</td>\n",
       "      <td>0.480079</td>\n",
       "      <td>0.237786</td>\n",
       "      <td>0.475146</td>\n",
       "      <td>0.441865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.458200</td>\n",
       "      <td>0.458433</td>\n",
       "      <td>0.190279</td>\n",
       "      <td>0.478203</td>\n",
       "      <td>0.236856</td>\n",
       "      <td>0.473564</td>\n",
       "      <td>0.440277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.450900</td>\n",
       "      <td>0.458117</td>\n",
       "      <td>0.190372</td>\n",
       "      <td>0.478578</td>\n",
       "      <td>0.236856</td>\n",
       "      <td>0.473799</td>\n",
       "      <td>0.440402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.460400</td>\n",
       "      <td>0.457962</td>\n",
       "      <td>0.190894</td>\n",
       "      <td>0.478166</td>\n",
       "      <td>0.237714</td>\n",
       "      <td>0.473780</td>\n",
       "      <td>0.441347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.455100</td>\n",
       "      <td>0.457800</td>\n",
       "      <td>0.190925</td>\n",
       "      <td>0.478450</td>\n",
       "      <td>0.237714</td>\n",
       "      <td>0.474168</td>\n",
       "      <td>0.440856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.455100</td>\n",
       "      <td>0.457713</td>\n",
       "      <td>0.190814</td>\n",
       "      <td>0.477161</td>\n",
       "      <td>0.237714</td>\n",
       "      <td>0.472879</td>\n",
       "      <td>0.440236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.450200</td>\n",
       "      <td>0.457697</td>\n",
       "      <td>0.190845</td>\n",
       "      <td>0.477421</td>\n",
       "      <td>0.237714</td>\n",
       "      <td>0.473177</td>\n",
       "      <td>0.440259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 results: {'eval_loss': 0.4576970934867859, 'eval_bleu': 0.19084476001215522, 'eval_rouge1': 0.4774208557361214, 'eval_rouge2': 0.2377144272562019, 'eval_rougeL': 0.473177172964063, 'eval_meteor': 0.440258519080202, 'eval_runtime': 1.7084, 'eval_samples_per_second': 70.242, 'eval_steps_per_second': 2.341, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 46120.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 3 saved to crossval_T5_SMALL_TOP5DOCS_fold_3.jsonl\n",
      "Processing Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1981337/3761292519.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [450/450 07:21, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>24.423100</td>\n",
       "      <td>7.922876</td>\n",
       "      <td>0.041999</td>\n",
       "      <td>0.346055</td>\n",
       "      <td>0.140593</td>\n",
       "      <td>0.333109</td>\n",
       "      <td>0.252670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>12.217200</td>\n",
       "      <td>1.003843</td>\n",
       "      <td>0.090906</td>\n",
       "      <td>0.357478</td>\n",
       "      <td>0.147780</td>\n",
       "      <td>0.346236</td>\n",
       "      <td>0.271056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.661100</td>\n",
       "      <td>0.850357</td>\n",
       "      <td>0.090823</td>\n",
       "      <td>0.367325</td>\n",
       "      <td>0.152041</td>\n",
       "      <td>0.356209</td>\n",
       "      <td>0.290821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.166300</td>\n",
       "      <td>0.800059</td>\n",
       "      <td>0.102207</td>\n",
       "      <td>0.381367</td>\n",
       "      <td>0.163857</td>\n",
       "      <td>0.371026</td>\n",
       "      <td>0.313867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.166300</td>\n",
       "      <td>0.693316</td>\n",
       "      <td>0.130541</td>\n",
       "      <td>0.401187</td>\n",
       "      <td>0.197060</td>\n",
       "      <td>0.388240</td>\n",
       "      <td>0.343843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.858300</td>\n",
       "      <td>0.641209</td>\n",
       "      <td>0.146986</td>\n",
       "      <td>0.413773</td>\n",
       "      <td>0.209488</td>\n",
       "      <td>0.401735</td>\n",
       "      <td>0.362795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.745400</td>\n",
       "      <td>0.597495</td>\n",
       "      <td>0.149567</td>\n",
       "      <td>0.422369</td>\n",
       "      <td>0.212035</td>\n",
       "      <td>0.411369</td>\n",
       "      <td>0.367227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.683300</td>\n",
       "      <td>0.558066</td>\n",
       "      <td>0.159802</td>\n",
       "      <td>0.431925</td>\n",
       "      <td>0.222289</td>\n",
       "      <td>0.422825</td>\n",
       "      <td>0.375573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.683300</td>\n",
       "      <td>0.520543</td>\n",
       "      <td>0.170670</td>\n",
       "      <td>0.460920</td>\n",
       "      <td>0.234290</td>\n",
       "      <td>0.450344</td>\n",
       "      <td>0.409113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.600500</td>\n",
       "      <td>0.505494</td>\n",
       "      <td>0.173857</td>\n",
       "      <td>0.479401</td>\n",
       "      <td>0.231259</td>\n",
       "      <td>0.470351</td>\n",
       "      <td>0.427682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.546500</td>\n",
       "      <td>0.499390</td>\n",
       "      <td>0.178529</td>\n",
       "      <td>0.480072</td>\n",
       "      <td>0.231448</td>\n",
       "      <td>0.471608</td>\n",
       "      <td>0.430866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.518600</td>\n",
       "      <td>0.494980</td>\n",
       "      <td>0.184645</td>\n",
       "      <td>0.485061</td>\n",
       "      <td>0.237826</td>\n",
       "      <td>0.475602</td>\n",
       "      <td>0.432729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.518600</td>\n",
       "      <td>0.493064</td>\n",
       "      <td>0.188728</td>\n",
       "      <td>0.486628</td>\n",
       "      <td>0.237777</td>\n",
       "      <td>0.477574</td>\n",
       "      <td>0.435741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.500300</td>\n",
       "      <td>0.491406</td>\n",
       "      <td>0.187123</td>\n",
       "      <td>0.485110</td>\n",
       "      <td>0.235997</td>\n",
       "      <td>0.477504</td>\n",
       "      <td>0.433575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.488600</td>\n",
       "      <td>0.488767</td>\n",
       "      <td>0.187318</td>\n",
       "      <td>0.485629</td>\n",
       "      <td>0.237108</td>\n",
       "      <td>0.477516</td>\n",
       "      <td>0.435809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.489600</td>\n",
       "      <td>0.486822</td>\n",
       "      <td>0.184655</td>\n",
       "      <td>0.483884</td>\n",
       "      <td>0.236642</td>\n",
       "      <td>0.476340</td>\n",
       "      <td>0.434463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.489600</td>\n",
       "      <td>0.485068</td>\n",
       "      <td>0.187001</td>\n",
       "      <td>0.483923</td>\n",
       "      <td>0.236826</td>\n",
       "      <td>0.477634</td>\n",
       "      <td>0.435505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.481800</td>\n",
       "      <td>0.483874</td>\n",
       "      <td>0.187180</td>\n",
       "      <td>0.483739</td>\n",
       "      <td>0.235001</td>\n",
       "      <td>0.476480</td>\n",
       "      <td>0.434896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.469800</td>\n",
       "      <td>0.482829</td>\n",
       "      <td>0.183125</td>\n",
       "      <td>0.483011</td>\n",
       "      <td>0.232648</td>\n",
       "      <td>0.475366</td>\n",
       "      <td>0.433911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.461900</td>\n",
       "      <td>0.482136</td>\n",
       "      <td>0.182606</td>\n",
       "      <td>0.480064</td>\n",
       "      <td>0.231963</td>\n",
       "      <td>0.472757</td>\n",
       "      <td>0.431149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.461900</td>\n",
       "      <td>0.481369</td>\n",
       "      <td>0.181217</td>\n",
       "      <td>0.478464</td>\n",
       "      <td>0.230801</td>\n",
       "      <td>0.470618</td>\n",
       "      <td>0.429056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.467900</td>\n",
       "      <td>0.480239</td>\n",
       "      <td>0.184131</td>\n",
       "      <td>0.481912</td>\n",
       "      <td>0.233621</td>\n",
       "      <td>0.474171</td>\n",
       "      <td>0.432449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.455500</td>\n",
       "      <td>0.479722</td>\n",
       "      <td>0.183880</td>\n",
       "      <td>0.481281</td>\n",
       "      <td>0.233586</td>\n",
       "      <td>0.473445</td>\n",
       "      <td>0.431538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.452800</td>\n",
       "      <td>0.479376</td>\n",
       "      <td>0.182859</td>\n",
       "      <td>0.476567</td>\n",
       "      <td>0.230610</td>\n",
       "      <td>0.469202</td>\n",
       "      <td>0.427598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.452800</td>\n",
       "      <td>0.478994</td>\n",
       "      <td>0.182859</td>\n",
       "      <td>0.476567</td>\n",
       "      <td>0.230610</td>\n",
       "      <td>0.468723</td>\n",
       "      <td>0.427566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.451000</td>\n",
       "      <td>0.478794</td>\n",
       "      <td>0.185422</td>\n",
       "      <td>0.477169</td>\n",
       "      <td>0.232401</td>\n",
       "      <td>0.469391</td>\n",
       "      <td>0.429274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.455600</td>\n",
       "      <td>0.478751</td>\n",
       "      <td>0.185422</td>\n",
       "      <td>0.477169</td>\n",
       "      <td>0.232401</td>\n",
       "      <td>0.469898</td>\n",
       "      <td>0.428616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.452200</td>\n",
       "      <td>0.478639</td>\n",
       "      <td>0.182784</td>\n",
       "      <td>0.476062</td>\n",
       "      <td>0.230610</td>\n",
       "      <td>0.468814</td>\n",
       "      <td>0.425997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.452200</td>\n",
       "      <td>0.478499</td>\n",
       "      <td>0.182784</td>\n",
       "      <td>0.476018</td>\n",
       "      <td>0.230610</td>\n",
       "      <td>0.468743</td>\n",
       "      <td>0.425997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.451000</td>\n",
       "      <td>0.478491</td>\n",
       "      <td>0.182784</td>\n",
       "      <td>0.476018</td>\n",
       "      <td>0.230610</td>\n",
       "      <td>0.468743</td>\n",
       "      <td>0.425997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 results: {'eval_loss': 0.4784913957118988, 'eval_bleu': 0.18278447619229612, 'eval_rouge1': 0.4760184309750496, 'eval_rouge2': 0.23060972344865766, 'eval_rougeL': 0.46874332718886214, 'eval_meteor': 0.42599713806917544, 'eval_runtime': 1.7334, 'eval_samples_per_second': 69.228, 'eval_steps_per_second': 2.308, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 47617.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 4 saved to crossval_T5_SMALL_TOP5DOCS_fold_4.jsonl\n",
      "Processing Fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1981337/3761292519.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [450/450 07:16, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>23.898800</td>\n",
       "      <td>7.936031</td>\n",
       "      <td>0.066495</td>\n",
       "      <td>0.386179</td>\n",
       "      <td>0.166075</td>\n",
       "      <td>0.378805</td>\n",
       "      <td>0.283774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>12.000200</td>\n",
       "      <td>1.004953</td>\n",
       "      <td>0.115424</td>\n",
       "      <td>0.389591</td>\n",
       "      <td>0.166647</td>\n",
       "      <td>0.383703</td>\n",
       "      <td>0.296994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.493300</td>\n",
       "      <td>0.854488</td>\n",
       "      <td>0.114650</td>\n",
       "      <td>0.397639</td>\n",
       "      <td>0.170023</td>\n",
       "      <td>0.389756</td>\n",
       "      <td>0.307615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.150700</td>\n",
       "      <td>0.798124</td>\n",
       "      <td>0.136465</td>\n",
       "      <td>0.402984</td>\n",
       "      <td>0.181164</td>\n",
       "      <td>0.395077</td>\n",
       "      <td>0.331188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.150700</td>\n",
       "      <td>0.696073</td>\n",
       "      <td>0.148521</td>\n",
       "      <td>0.419650</td>\n",
       "      <td>0.193164</td>\n",
       "      <td>0.409715</td>\n",
       "      <td>0.358577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.865200</td>\n",
       "      <td>0.623379</td>\n",
       "      <td>0.162405</td>\n",
       "      <td>0.433229</td>\n",
       "      <td>0.206757</td>\n",
       "      <td>0.423072</td>\n",
       "      <td>0.378308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.753400</td>\n",
       "      <td>0.601476</td>\n",
       "      <td>0.163957</td>\n",
       "      <td>0.440597</td>\n",
       "      <td>0.209819</td>\n",
       "      <td>0.429251</td>\n",
       "      <td>0.385870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.668800</td>\n",
       "      <td>0.555373</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.450529</td>\n",
       "      <td>0.216583</td>\n",
       "      <td>0.439000</td>\n",
       "      <td>0.395253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.668800</td>\n",
       "      <td>0.521964</td>\n",
       "      <td>0.178880</td>\n",
       "      <td>0.469579</td>\n",
       "      <td>0.218129</td>\n",
       "      <td>0.457868</td>\n",
       "      <td>0.413532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.601900</td>\n",
       "      <td>0.501299</td>\n",
       "      <td>0.194098</td>\n",
       "      <td>0.493574</td>\n",
       "      <td>0.228036</td>\n",
       "      <td>0.482472</td>\n",
       "      <td>0.439482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.556100</td>\n",
       "      <td>0.494871</td>\n",
       "      <td>0.193544</td>\n",
       "      <td>0.492536</td>\n",
       "      <td>0.224180</td>\n",
       "      <td>0.481533</td>\n",
       "      <td>0.439459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.521500</td>\n",
       "      <td>0.490976</td>\n",
       "      <td>0.193569</td>\n",
       "      <td>0.494187</td>\n",
       "      <td>0.224108</td>\n",
       "      <td>0.483831</td>\n",
       "      <td>0.439251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.521500</td>\n",
       "      <td>0.487548</td>\n",
       "      <td>0.194720</td>\n",
       "      <td>0.493827</td>\n",
       "      <td>0.225753</td>\n",
       "      <td>0.484353</td>\n",
       "      <td>0.439916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.501300</td>\n",
       "      <td>0.485152</td>\n",
       "      <td>0.193190</td>\n",
       "      <td>0.493221</td>\n",
       "      <td>0.223426</td>\n",
       "      <td>0.484174</td>\n",
       "      <td>0.439310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.500800</td>\n",
       "      <td>0.482798</td>\n",
       "      <td>0.195626</td>\n",
       "      <td>0.492662</td>\n",
       "      <td>0.223118</td>\n",
       "      <td>0.483120</td>\n",
       "      <td>0.441149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.485300</td>\n",
       "      <td>0.481393</td>\n",
       "      <td>0.198885</td>\n",
       "      <td>0.498433</td>\n",
       "      <td>0.231465</td>\n",
       "      <td>0.490268</td>\n",
       "      <td>0.448057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.485300</td>\n",
       "      <td>0.479589</td>\n",
       "      <td>0.198409</td>\n",
       "      <td>0.498507</td>\n",
       "      <td>0.230808</td>\n",
       "      <td>0.489090</td>\n",
       "      <td>0.447488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.482800</td>\n",
       "      <td>0.478611</td>\n",
       "      <td>0.198103</td>\n",
       "      <td>0.496737</td>\n",
       "      <td>0.229203</td>\n",
       "      <td>0.486980</td>\n",
       "      <td>0.445704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>0.477555</td>\n",
       "      <td>0.199607</td>\n",
       "      <td>0.499189</td>\n",
       "      <td>0.231592</td>\n",
       "      <td>0.489438</td>\n",
       "      <td>0.448880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.469700</td>\n",
       "      <td>0.476650</td>\n",
       "      <td>0.200720</td>\n",
       "      <td>0.498243</td>\n",
       "      <td>0.231959</td>\n",
       "      <td>0.488733</td>\n",
       "      <td>0.448702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.469700</td>\n",
       "      <td>0.475661</td>\n",
       "      <td>0.199624</td>\n",
       "      <td>0.499279</td>\n",
       "      <td>0.232948</td>\n",
       "      <td>0.489996</td>\n",
       "      <td>0.451779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.462700</td>\n",
       "      <td>0.474794</td>\n",
       "      <td>0.198420</td>\n",
       "      <td>0.499327</td>\n",
       "      <td>0.232750</td>\n",
       "      <td>0.489671</td>\n",
       "      <td>0.451110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.460600</td>\n",
       "      <td>0.474110</td>\n",
       "      <td>0.200847</td>\n",
       "      <td>0.500236</td>\n",
       "      <td>0.233577</td>\n",
       "      <td>0.490441</td>\n",
       "      <td>0.452001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.459200</td>\n",
       "      <td>0.473614</td>\n",
       "      <td>0.200847</td>\n",
       "      <td>0.500361</td>\n",
       "      <td>0.233806</td>\n",
       "      <td>0.490706</td>\n",
       "      <td>0.452001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.459200</td>\n",
       "      <td>0.473165</td>\n",
       "      <td>0.200073</td>\n",
       "      <td>0.499145</td>\n",
       "      <td>0.230996</td>\n",
       "      <td>0.489643</td>\n",
       "      <td>0.450486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.450100</td>\n",
       "      <td>0.472845</td>\n",
       "      <td>0.200277</td>\n",
       "      <td>0.499350</td>\n",
       "      <td>0.232247</td>\n",
       "      <td>0.489661</td>\n",
       "      <td>0.450504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.464400</td>\n",
       "      <td>0.472548</td>\n",
       "      <td>0.202948</td>\n",
       "      <td>0.498756</td>\n",
       "      <td>0.233691</td>\n",
       "      <td>0.489082</td>\n",
       "      <td>0.450799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.446800</td>\n",
       "      <td>0.472454</td>\n",
       "      <td>0.202948</td>\n",
       "      <td>0.498756</td>\n",
       "      <td>0.233691</td>\n",
       "      <td>0.489082</td>\n",
       "      <td>0.450799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.446800</td>\n",
       "      <td>0.472351</td>\n",
       "      <td>0.202868</td>\n",
       "      <td>0.497877</td>\n",
       "      <td>0.233691</td>\n",
       "      <td>0.487846</td>\n",
       "      <td>0.450149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.450300</td>\n",
       "      <td>0.472312</td>\n",
       "      <td>0.205260</td>\n",
       "      <td>0.498378</td>\n",
       "      <td>0.235239</td>\n",
       "      <td>0.488540</td>\n",
       "      <td>0.450927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 results: {'eval_loss': 0.47231248021125793, 'eval_bleu': 0.205260004896688, 'eval_rouge1': 0.49837764631903186, 'eval_rouge2': 0.23523881251568848, 'eval_rougeL': 0.4885402364015746, 'eval_meteor': 0.45092694757338664, 'eval_runtime': 1.7126, 'eval_samples_per_second': 69.486, 'eval_steps_per_second': 2.336, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:00<00:00, 44648.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 5 saved to crossval_T5_SMALL_TOP5DOCS_fold_5.jsonl\n",
      "Average metrics over all folds: {'eval_loss': 0.48240370750427247, 'eval_bleu': 0.19105834210343983, 'eval_rouge1': 0.48841880395297943, 'eval_rouge2': 0.2405301203580545, 'eval_rougeL': 0.48020857517432863, 'eval_meteor': 0.44378319799755195, 'eval_runtime': 1.8285399999999998, 'eval_samples_per_second': 66.708, 'eval_steps_per_second': 2.2276, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Averages/epoch</td><td>▁</td></tr><tr><td>Averages/eval_bleu</td><td>▁</td></tr><tr><td>Averages/eval_loss</td><td>▁</td></tr><tr><td>Averages/eval_meteor</td><td>▁</td></tr><tr><td>Averages/eval_rouge1</td><td>▁</td></tr><tr><td>Averages/eval_rouge2</td><td>▁</td></tr><tr><td>Averages/eval_rougeL</td><td>▁</td></tr><tr><td>Averages/eval_runtime</td><td>▁</td></tr><tr><td>Averages/eval_samples_per_second</td><td>▁</td></tr><tr><td>Averages/eval_steps_per_second</td><td>▁</td></tr><tr><td>Fold_1/eval/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>Fold_1/eval/eval_bleu</td><td>▁▄▄▄▅▆▆▇▇▇▇▇▇▇▇████████████████</td></tr><tr><td>Fold_1/eval/eval_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_1/eval/eval_meteor</td><td>▁▂▃▃▄▅▅▆▇▇▇▇▇▇▇▇▇██▇███████████</td></tr><tr><td>Fold_1/eval/eval_rouge1</td><td>▁▂▂▃▄▄▅▅▇▇▇▇▇▇▇▇▇██████████████</td></tr><tr><td>Fold_1/eval/eval_rouge2</td><td>▁▂▃▂▄▅▅▆▇▇▆▆▇▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>Fold_1/eval/eval_rougeL</td><td>▁▂▂▃▄▄▅▆▇▇▇▇█▇▇▇▇██████████████</td></tr><tr><td>Fold_1/eval/eval_runtime</td><td>▂▂▁▄▁▃▁▂▂▂▂▁▁▁▁▂▁█▁▁▁▁▁▂▁▁▁▁▁▁▁</td></tr><tr><td>Fold_1/eval/eval_samples_per_second</td><td>▇▆█▅▇▅█▇▇▇▇████▇█▁▇█▇█▇▇▇▇▇██▇█</td></tr><tr><td>Fold_1/eval/eval_steps_per_second</td><td>▇▆█▅▇▅█▇▇▇▇████▇█▁▇█▇█▇▇▇▇▇██▇█</td></tr><tr><td>Fold_2/eval/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>Fold_2/eval/eval_bleu</td><td>▁▄▄▅▅▆▆▆▇▇▇▇▇▇▇████████████████</td></tr><tr><td>Fold_2/eval/eval_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_2/eval/eval_meteor</td><td>▁▂▃▃▄▅▅▆▇▇▇████████████████████</td></tr><tr><td>Fold_2/eval/eval_rouge1</td><td>▁▂▂▃▄▄▅▅▆▇▇▇▇██████████████████</td></tr><tr><td>Fold_2/eval/eval_rouge2</td><td>▁▂▃▃▄▅▅▆▆▆▇▇▇▇▇▇▇▇█████████████</td></tr><tr><td>Fold_2/eval/eval_rougeL</td><td>▁▂▂▃▄▄▅▅▆▇▇▇▇██████████████████</td></tr><tr><td>Fold_2/eval/eval_runtime</td><td>▂▁▂▁▁▁▁▁▂▁▁▁▇▁▁▁▂▁▁▂▂▂▁▁▂█▂▂▂▁█</td></tr><tr><td>Fold_2/eval/eval_samples_per_second</td><td>▇█▇█▇███▇███▂█▇█▇▇█▇▇▆██▇▁▇▇▇█▁</td></tr><tr><td>Fold_2/eval/eval_steps_per_second</td><td>▇█▇█▇███▇███▁█▇█▇▇█▇▇▆██▇▁▇▇▇█▁</td></tr><tr><td>Fold_3/eval/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>Fold_3/eval/eval_bleu</td><td>▁▃▄▄▅▆▆▆▇▇▇▇▇▇█████████████████</td></tr><tr><td>Fold_3/eval/eval_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_3/eval/eval_meteor</td><td>▁▂▂▃▄▅▅▆▇▇▇▇███████████████████</td></tr><tr><td>Fold_3/eval/eval_rouge1</td><td>▁▁▁▂▃▄▅▅▆▇▇████████████████████</td></tr><tr><td>Fold_3/eval/eval_rouge2</td><td>▂▁▁▂▄▅▅▆▆▇▇▇▇▇▇▇███████████████</td></tr><tr><td>Fold_3/eval/eval_rougeL</td><td>▁▁▁▂▃▄▅▅▇▇▇████████████████████</td></tr><tr><td>Fold_3/eval/eval_runtime</td><td>▁▁▁▂▇▁▂▂▂█▁▂▁▂▁▁▁▂▂▂▂▂▁▂▁▂▂▂▁▁▂</td></tr><tr><td>Fold_3/eval/eval_samples_per_second</td><td>▇██▇▁█▇▇▇▁█▇▇▇█▇█▆▇▆▇▇█▇█▇▇▇█▇▇</td></tr><tr><td>Fold_3/eval/eval_steps_per_second</td><td>▇██▇▁█▇▇▇▁█▇▇▇█▇█▆▇▆▇▇█▇█▇▇▇█▇▇</td></tr><tr><td>Fold_4/eval/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>Fold_4/eval/eval_bleu</td><td>▁▃▃▄▅▆▆▇▇▇█████████████████████</td></tr><tr><td>Fold_4/eval/eval_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_4/eval/eval_meteor</td><td>▁▂▂▃▄▅▅▆▇██████████████████████</td></tr><tr><td>Fold_4/eval/eval_rouge1</td><td>▁▂▂▃▄▄▅▅▇██████████████▇▇██▇▇▇▇</td></tr><tr><td>Fold_4/eval/eval_rouge2</td><td>▁▂▂▃▅▆▆▇████████████▇██▇▇██▇▇▇▇</td></tr><tr><td>Fold_4/eval/eval_rougeL</td><td>▁▂▂▃▄▄▅▅▇██████████████████████</td></tr><tr><td>Fold_4/eval/eval_runtime</td><td>▂▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▂</td></tr><tr><td>Fold_4/eval/eval_samples_per_second</td><td>▇███▇▇█▇███▇██▇▇████▇█▁███████▆</td></tr><tr><td>Fold_4/eval/eval_steps_per_second</td><td>▇███▇▇█▇███▇██▇▇████▇█▁███████▆</td></tr><tr><td>Fold_5/eval/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>Fold_5/eval/eval_bleu</td><td>▁▃▃▅▅▆▆▆▇▇▇▇▇▇█████████████████</td></tr><tr><td>Fold_5/eval/eval_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_5/eval/eval_meteor</td><td>▁▂▂▃▄▅▅▆▆▇▇▇▇▇█████████████████</td></tr><tr><td>Fold_5/eval/eval_rouge1</td><td>▁▁▂▂▃▄▄▅▆██████████████████████</td></tr><tr><td>Fold_5/eval/eval_rouge2</td><td>▁▁▁▃▄▅▅▆▆▇▇▇▇▇▇██▇█████████████</td></tr><tr><td>Fold_5/eval/eval_rougeL</td><td>▁▁▂▂▃▄▄▅▆▇▇████████████████████</td></tr><tr><td>Fold_5/eval/eval_runtime</td><td>▆▆▅▅▆▅▅▆█▆▆▅▆▇▅▅▁▆▅▅▅█▅▄▄▄▂▃▅▅█</td></tr><tr><td>Fold_5/eval/eval_samples_per_second</td><td>▃▃▄▃▃▄▄▃▁▂▃▃▃▂▄▄█▃▄▄▄▁▄▅▅▅▆▅▄▄▁</td></tr><tr><td>Fold_5/eval/eval_steps_per_second</td><td>▃▃▄▃▃▄▄▃▁▂▃▃▃▂▄▄█▃▄▄▄▁▄▅▅▅▆▅▄▄▁</td></tr><tr><td>eval/bleu</td><td>▄▄▄▆▇▇██▁▆▆▇██▇▄▄▆▆▆████▁▆▇▇█▇▇▇▇▇▄▇████</td></tr><tr><td>eval/loss</td><td>▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/meteor</td><td>▂▅▇▇▇▇▂███████▁▄▅▇▇▇▇▇▇▂▅▇▇▇▇▇▇▇▁▃▅█████</td></tr><tr><td>eval/rouge1</td><td>▁▄██████▁▄███▇█▄▆▆▇▇▇▇▇▇▇▄▆▇▇▇▇▇▂▂▃▆████</td></tr><tr><td>eval/rouge2</td><td>▂▅▅▆▆▇▇▇▇▇▃▆█████▆▇▇▇▇▇▁▂▇▇▇▇▇▇▇▃▃▄▇▇▇▇▇</td></tr><tr><td>eval/rougeL</td><td>▁▃▇▇▇▇█▁▃▄▆▇▇▇▇█████▃▆▇▇▇▇▇▇▄▇▇▇▇▇▇█████</td></tr><tr><td>eval/runtime</td><td>▄▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂█▂▃▂▂▂▂▂▁▂▂▁▂▂▁▁▂▁▁▁</td></tr><tr><td>eval/samples_per_second</td><td>▅▇▆▇▇▇▆▇▇▇▇▂▇▆▆▆▇█▇▇▆▇▇▇▇▇▇▇▇▁▇▇▇▆▇▆▇██▇</td></tr><tr><td>eval/steps_per_second</td><td>▇▇▆▇▇▇▇▇█▇▇▇▇▇▇▇▆▁▆▇█▆▁▇▇▇▇▇▇▇▇▆▇▇▇▇█▆█▇</td></tr><tr><td>test/bleu</td><td>▃▂▄▁█</td></tr><tr><td>test/loss</td><td>█▅▁▄▃</td></tr><tr><td>test/meteor</td><td>▅█▄▁▇</td></tr><tr><td>test/rouge1</td><td>▇▇▁▁█</td></tr><tr><td>test/rouge2</td><td>▅█▃▁▂</td></tr><tr><td>test/rougeL</td><td>▇▇▃▁█</td></tr><tr><td>test/runtime</td><td>▁██▄▄</td></tr><tr><td>test/samples_per_second</td><td>█▁▁▅▄</td></tr><tr><td>test/steps_per_second</td><td>█▁▁▅▅</td></tr><tr><td>train/epoch</td><td>▂▂▃▃▄▅▆▆██▃▄▄▄▅██▁▃▃▃▄▅▇▇▁▂▂▃▄▇███▁▄▄▄▆▆</td></tr><tr><td>train/global_step</td><td>▂▂▂▃▃▃▃▅▅▅▇▁▂▂▄▅▅▇▇█▅▅▆▆▇█▂▂▃▄▅▅▆██▃▄▅▇█</td></tr><tr><td>train/grad_norm</td><td>▃▃▂▁▁▁▁▂▂▂▂▁▁▂▁▃▂▂▁▁▁▄▁█▃▂▂▂▁▁▂▁▁▁▄▃▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▇▇▇▆▆▃▂▁▁█▆▅▅▅▄▃▂██▇▅▄▃▂█▇▆▅▅▅▄▄▁█▇▅▄▄▃▁</td></tr><tr><td>train/loss</td><td>█▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Averages/epoch</td><td>30</td></tr><tr><td>Averages/eval_bleu</td><td>0.19106</td></tr><tr><td>Averages/eval_loss</td><td>0.4824</td></tr><tr><td>Averages/eval_meteor</td><td>0.44378</td></tr><tr><td>Averages/eval_rouge1</td><td>0.48842</td></tr><tr><td>Averages/eval_rouge2</td><td>0.24053</td></tr><tr><td>Averages/eval_rougeL</td><td>0.48021</td></tr><tr><td>Averages/eval_runtime</td><td>1.82854</td></tr><tr><td>Averages/eval_samples_per_second</td><td>66.708</td></tr><tr><td>Averages/eval_steps_per_second</td><td>2.2276</td></tr><tr><td>Fold_1/eval/epoch</td><td>30</td></tr><tr><td>Fold_1/eval/eval_bleu</td><td>0.18931</td></tr><tr><td>Fold_1/eval/eval_loss</td><td>0.51134</td></tr><tr><td>Fold_1/eval/eval_meteor</td><td>0.44466</td></tr><tr><td>Fold_1/eval/eval_rouge1</td><td>0.49495</td></tr><tr><td>Fold_1/eval/eval_rouge2</td><td>0.24489</td></tr><tr><td>Fold_1/eval/eval_rougeL</td><td>0.48574</td></tr><tr><td>Fold_1/eval/eval_runtime</td><td>1.6267</td></tr><tr><td>Fold_1/eval/eval_samples_per_second</td><td>73.77</td></tr><tr><td>Fold_1/eval/eval_steps_per_second</td><td>2.459</td></tr><tr><td>Fold_2/eval/epoch</td><td>30</td></tr><tr><td>Fold_2/eval/eval_bleu</td><td>0.18709</td></tr><tr><td>Fold_2/eval/eval_loss</td><td>0.49218</td></tr><tr><td>Fold_2/eval/eval_meteor</td><td>0.45707</td></tr><tr><td>Fold_2/eval/eval_rouge1</td><td>0.49532</td></tr><tr><td>Fold_2/eval/eval_rouge2</td><td>0.2542</td></tr><tr><td>Fold_2/eval/eval_rougeL</td><td>0.48485</td></tr><tr><td>Fold_2/eval/eval_runtime</td><td>2.3616</td></tr><tr><td>Fold_2/eval/eval_samples_per_second</td><td>50.814</td></tr><tr><td>Fold_2/eval/eval_steps_per_second</td><td>1.694</td></tr><tr><td>Fold_3/eval/epoch</td><td>30</td></tr><tr><td>Fold_3/eval/eval_bleu</td><td>0.19084</td></tr><tr><td>Fold_3/eval/eval_loss</td><td>0.4577</td></tr><tr><td>Fold_3/eval/eval_meteor</td><td>0.44026</td></tr><tr><td>Fold_3/eval/eval_rouge1</td><td>0.47742</td></tr><tr><td>Fold_3/eval/eval_rouge2</td><td>0.23771</td></tr><tr><td>Fold_3/eval/eval_rougeL</td><td>0.47318</td></tr><tr><td>Fold_3/eval/eval_runtime</td><td>1.7084</td></tr><tr><td>Fold_3/eval/eval_samples_per_second</td><td>70.242</td></tr><tr><td>Fold_3/eval/eval_steps_per_second</td><td>2.341</td></tr><tr><td>Fold_4/eval/epoch</td><td>30</td></tr><tr><td>Fold_4/eval/eval_bleu</td><td>0.18278</td></tr><tr><td>Fold_4/eval/eval_loss</td><td>0.47849</td></tr><tr><td>Fold_4/eval/eval_meteor</td><td>0.426</td></tr><tr><td>Fold_4/eval/eval_rouge1</td><td>0.47602</td></tr><tr><td>Fold_4/eval/eval_rouge2</td><td>0.23061</td></tr><tr><td>Fold_4/eval/eval_rougeL</td><td>0.46874</td></tr><tr><td>Fold_4/eval/eval_runtime</td><td>1.7334</td></tr><tr><td>Fold_4/eval/eval_samples_per_second</td><td>69.228</td></tr><tr><td>Fold_4/eval/eval_steps_per_second</td><td>2.308</td></tr><tr><td>Fold_5/eval/epoch</td><td>30</td></tr><tr><td>Fold_5/eval/eval_bleu</td><td>0.20526</td></tr><tr><td>Fold_5/eval/eval_loss</td><td>0.47231</td></tr><tr><td>Fold_5/eval/eval_meteor</td><td>0.45093</td></tr><tr><td>Fold_5/eval/eval_rouge1</td><td>0.49838</td></tr><tr><td>Fold_5/eval/eval_rouge2</td><td>0.23524</td></tr><tr><td>Fold_5/eval/eval_rougeL</td><td>0.48854</td></tr><tr><td>Fold_5/eval/eval_runtime</td><td>1.7126</td></tr><tr><td>Fold_5/eval/eval_samples_per_second</td><td>69.486</td></tr><tr><td>Fold_5/eval/eval_steps_per_second</td><td>2.336</td></tr><tr><td>eval/bleu</td><td>0.20526</td></tr><tr><td>eval/loss</td><td>0.47231</td></tr><tr><td>eval/meteor</td><td>0.45093</td></tr><tr><td>eval/rouge1</td><td>0.49838</td></tr><tr><td>eval/rouge2</td><td>0.23524</td></tr><tr><td>eval/rougeL</td><td>0.48854</td></tr><tr><td>eval/runtime</td><td>1.7126</td></tr><tr><td>eval/samples_per_second</td><td>69.486</td></tr><tr><td>eval/steps_per_second</td><td>2.336</td></tr><tr><td>test/bleu</td><td>0.20526</td></tr><tr><td>test/loss</td><td>0.47231</td></tr><tr><td>test/meteor</td><td>0.45093</td></tr><tr><td>test/rouge1</td><td>0.49838</td></tr><tr><td>test/rouge2</td><td>0.23524</td></tr><tr><td>test/rougeL</td><td>0.48854</td></tr><tr><td>test/runtime</td><td>1.6318</td></tr><tr><td>test/samples_per_second</td><td>72.927</td></tr><tr><td>test/steps_per_second</td><td>2.451</td></tr><tr><td>total_flos</td><td>1948921941196800.0</td></tr><tr><td>train/epoch</td><td>30</td></tr><tr><td>train/global_step</td><td>450</td></tr><tr><td>train/grad_norm</td><td>0.64161</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4503</td></tr><tr><td>train_loss</td><td>1.17942</td></tr><tr><td>train_runtime</td><td>437.3954</td></tr><tr><td>train_samples_per_second</td><td>32.922</td></tr><tr><td>train_steps_per_second</td><td>1.029</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">T5_SMALL_TOP_5_DOCS</strong> at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/uhtrlok0' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/uhtrlok0</a><br> View project at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250509_013816-uhtrlok0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final predicitons for all folds foudn in crossval_T5_SMALL_TOP5DOCS.jsonl\n"
     ]
    }
   ],
   "source": [
    "cross_val_train(modelname=\"t5-small\", run_name=\"T5_SMALL_TOP_5_DOCS\",filename=\"crossval_T5_SMALL_TOP5DOCS.jsonl\", \n",
    "                tokenizer=tokenizer, tokenized_dataset=tokenized_dataset, \n",
    "                folds=folds, nb_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de42d12c-1bf5-4928-9394-081f9ea5fd7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d665bc2-895a-4922-878b-9ecc1b0a08e4",
   "metadata": {},
   "source": [
    "# T5 SMALL----------------- TOP5 QUERY ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c97720a2-1ed6-4275-bd08-00920a1ec630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a7791b2ba3f4813a459d5492c8c155e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/596 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/Etu0/21402600/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer,tokenized_dataset= tokenize_and_split_dataset(\"FR_training_queryonly_qulac_PREPROCESSED_FOR_MODEL.json\",\"t5-small\")\n",
    "folds= generate_folds(tokenized_dataset, n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52f6996f-2cac-4005-bf08-e41832ffe26d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/users/nfs/Etu0/21402600/wandb/run-20250513_020948-rpai6d3d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/rpai6d3d' target=\"_blank\">FR_T5_SMALL_QUERY_ONLY</a></strong> to <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/rpai6d3d' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/rpai6d3d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1747094990.048300   62038 gpu_device.cc:2341] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_62038/3145572364.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1740' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1740/1800 10:57 < 00:22, 2.64 it/s, Epoch 29/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.730600</td>\n",
       "      <td>1.043109</td>\n",
       "      <td>0.097236</td>\n",
       "      <td>0.352985</td>\n",
       "      <td>0.160334</td>\n",
       "      <td>0.347635</td>\n",
       "      <td>0.274443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.022800</td>\n",
       "      <td>0.805679</td>\n",
       "      <td>0.106297</td>\n",
       "      <td>0.383997</td>\n",
       "      <td>0.171593</td>\n",
       "      <td>0.377809</td>\n",
       "      <td>0.316586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.769100</td>\n",
       "      <td>0.668976</td>\n",
       "      <td>0.148166</td>\n",
       "      <td>0.439469</td>\n",
       "      <td>0.219237</td>\n",
       "      <td>0.428556</td>\n",
       "      <td>0.377078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.691700</td>\n",
       "      <td>0.632927</td>\n",
       "      <td>0.160757</td>\n",
       "      <td>0.460511</td>\n",
       "      <td>0.240838</td>\n",
       "      <td>0.451540</td>\n",
       "      <td>0.408357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.643600</td>\n",
       "      <td>0.617701</td>\n",
       "      <td>0.170637</td>\n",
       "      <td>0.464162</td>\n",
       "      <td>0.250486</td>\n",
       "      <td>0.455936</td>\n",
       "      <td>0.419589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.633800</td>\n",
       "      <td>0.608882</td>\n",
       "      <td>0.179901</td>\n",
       "      <td>0.474578</td>\n",
       "      <td>0.256818</td>\n",
       "      <td>0.466017</td>\n",
       "      <td>0.429737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.606700</td>\n",
       "      <td>0.602614</td>\n",
       "      <td>0.180354</td>\n",
       "      <td>0.482062</td>\n",
       "      <td>0.258170</td>\n",
       "      <td>0.472393</td>\n",
       "      <td>0.434164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.582900</td>\n",
       "      <td>0.597470</td>\n",
       "      <td>0.176307</td>\n",
       "      <td>0.483201</td>\n",
       "      <td>0.253594</td>\n",
       "      <td>0.472649</td>\n",
       "      <td>0.435478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.610300</td>\n",
       "      <td>0.593227</td>\n",
       "      <td>0.177802</td>\n",
       "      <td>0.492510</td>\n",
       "      <td>0.259799</td>\n",
       "      <td>0.481075</td>\n",
       "      <td>0.443780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.563400</td>\n",
       "      <td>0.589826</td>\n",
       "      <td>0.183627</td>\n",
       "      <td>0.494818</td>\n",
       "      <td>0.265113</td>\n",
       "      <td>0.485611</td>\n",
       "      <td>0.450342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.540900</td>\n",
       "      <td>0.587052</td>\n",
       "      <td>0.182447</td>\n",
       "      <td>0.503006</td>\n",
       "      <td>0.270131</td>\n",
       "      <td>0.492578</td>\n",
       "      <td>0.452510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.516500</td>\n",
       "      <td>0.584335</td>\n",
       "      <td>0.194316</td>\n",
       "      <td>0.507792</td>\n",
       "      <td>0.278262</td>\n",
       "      <td>0.498677</td>\n",
       "      <td>0.459179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>0.582733</td>\n",
       "      <td>0.192806</td>\n",
       "      <td>0.505994</td>\n",
       "      <td>0.274109</td>\n",
       "      <td>0.498459</td>\n",
       "      <td>0.459697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.480400</td>\n",
       "      <td>0.582419</td>\n",
       "      <td>0.191419</td>\n",
       "      <td>0.503881</td>\n",
       "      <td>0.275337</td>\n",
       "      <td>0.496104</td>\n",
       "      <td>0.457780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.509000</td>\n",
       "      <td>0.580737</td>\n",
       "      <td>0.194496</td>\n",
       "      <td>0.506566</td>\n",
       "      <td>0.278444</td>\n",
       "      <td>0.498237</td>\n",
       "      <td>0.461443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.506700</td>\n",
       "      <td>0.581078</td>\n",
       "      <td>0.204297</td>\n",
       "      <td>0.507696</td>\n",
       "      <td>0.279130</td>\n",
       "      <td>0.501080</td>\n",
       "      <td>0.463833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.488800</td>\n",
       "      <td>0.579646</td>\n",
       "      <td>0.204686</td>\n",
       "      <td>0.510673</td>\n",
       "      <td>0.281508</td>\n",
       "      <td>0.503103</td>\n",
       "      <td>0.469159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.510100</td>\n",
       "      <td>0.578952</td>\n",
       "      <td>0.205884</td>\n",
       "      <td>0.507436</td>\n",
       "      <td>0.282570</td>\n",
       "      <td>0.499815</td>\n",
       "      <td>0.468558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.465200</td>\n",
       "      <td>0.579127</td>\n",
       "      <td>0.207026</td>\n",
       "      <td>0.509872</td>\n",
       "      <td>0.283458</td>\n",
       "      <td>0.501881</td>\n",
       "      <td>0.469329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.499100</td>\n",
       "      <td>0.578378</td>\n",
       "      <td>0.205020</td>\n",
       "      <td>0.509270</td>\n",
       "      <td>0.281457</td>\n",
       "      <td>0.500819</td>\n",
       "      <td>0.467587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.471800</td>\n",
       "      <td>0.577569</td>\n",
       "      <td>0.206655</td>\n",
       "      <td>0.511752</td>\n",
       "      <td>0.283283</td>\n",
       "      <td>0.503191</td>\n",
       "      <td>0.468702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.478800</td>\n",
       "      <td>0.577760</td>\n",
       "      <td>0.209218</td>\n",
       "      <td>0.513397</td>\n",
       "      <td>0.284108</td>\n",
       "      <td>0.502786</td>\n",
       "      <td>0.469321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.484000</td>\n",
       "      <td>0.577761</td>\n",
       "      <td>0.209704</td>\n",
       "      <td>0.512078</td>\n",
       "      <td>0.281802</td>\n",
       "      <td>0.501677</td>\n",
       "      <td>0.469631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.484400</td>\n",
       "      <td>0.577550</td>\n",
       "      <td>0.208879</td>\n",
       "      <td>0.510614</td>\n",
       "      <td>0.282647</td>\n",
       "      <td>0.500688</td>\n",
       "      <td>0.466619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.485300</td>\n",
       "      <td>0.577388</td>\n",
       "      <td>0.208450</td>\n",
       "      <td>0.509787</td>\n",
       "      <td>0.281699</td>\n",
       "      <td>0.499930</td>\n",
       "      <td>0.466600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.460100</td>\n",
       "      <td>0.577197</td>\n",
       "      <td>0.210637</td>\n",
       "      <td>0.511453</td>\n",
       "      <td>0.283027</td>\n",
       "      <td>0.501204</td>\n",
       "      <td>0.468321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.474800</td>\n",
       "      <td>0.577431</td>\n",
       "      <td>0.210470</td>\n",
       "      <td>0.510596</td>\n",
       "      <td>0.283232</td>\n",
       "      <td>0.501207</td>\n",
       "      <td>0.467286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.466800</td>\n",
       "      <td>0.577391</td>\n",
       "      <td>0.211591</td>\n",
       "      <td>0.510521</td>\n",
       "      <td>0.282691</td>\n",
       "      <td>0.501103</td>\n",
       "      <td>0.467921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.459500</td>\n",
       "      <td>0.577240</td>\n",
       "      <td>0.211730</td>\n",
       "      <td>0.510336</td>\n",
       "      <td>0.282922</td>\n",
       "      <td>0.500907</td>\n",
       "      <td>0.467318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 results: {'eval_loss': 0.5771968960762024, 'eval_bleu': 0.21063748371946756, 'eval_rouge1': 0.5114530881982768, 'eval_rouge2': 0.2830265907342715, 'eval_rougeL': 0.5012040037233638, 'eval_meteor': 0.46832069446197144, 'eval_runtime': 2.1001, 'eval_samples_per_second': 57.139, 'eval_steps_per_second': 7.142, 'epoch': 29.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 99469.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 1 saved to FR_crossval_T5_SMALL_QUERY_ONLY_fold_1.jsonl\n",
      "Processing Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_62038/3145572364.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1800' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1800/1800 11:25, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.590000</td>\n",
       "      <td>0.984963</td>\n",
       "      <td>0.104152</td>\n",
       "      <td>0.369764</td>\n",
       "      <td>0.168902</td>\n",
       "      <td>0.362460</td>\n",
       "      <td>0.288262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.971500</td>\n",
       "      <td>0.800221</td>\n",
       "      <td>0.112861</td>\n",
       "      <td>0.397801</td>\n",
       "      <td>0.180564</td>\n",
       "      <td>0.389904</td>\n",
       "      <td>0.327236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.743300</td>\n",
       "      <td>0.650644</td>\n",
       "      <td>0.150557</td>\n",
       "      <td>0.454435</td>\n",
       "      <td>0.226202</td>\n",
       "      <td>0.443302</td>\n",
       "      <td>0.400376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.669400</td>\n",
       "      <td>0.612646</td>\n",
       "      <td>0.161612</td>\n",
       "      <td>0.471398</td>\n",
       "      <td>0.242491</td>\n",
       "      <td>0.461094</td>\n",
       "      <td>0.417903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.652000</td>\n",
       "      <td>0.598475</td>\n",
       "      <td>0.174848</td>\n",
       "      <td>0.484182</td>\n",
       "      <td>0.257435</td>\n",
       "      <td>0.477503</td>\n",
       "      <td>0.430944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.607600</td>\n",
       "      <td>0.587920</td>\n",
       "      <td>0.168886</td>\n",
       "      <td>0.479129</td>\n",
       "      <td>0.250363</td>\n",
       "      <td>0.473210</td>\n",
       "      <td>0.429513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.600300</td>\n",
       "      <td>0.578481</td>\n",
       "      <td>0.178026</td>\n",
       "      <td>0.485021</td>\n",
       "      <td>0.257961</td>\n",
       "      <td>0.478025</td>\n",
       "      <td>0.435310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.609900</td>\n",
       "      <td>0.571393</td>\n",
       "      <td>0.183119</td>\n",
       "      <td>0.489370</td>\n",
       "      <td>0.262221</td>\n",
       "      <td>0.482928</td>\n",
       "      <td>0.440170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.600800</td>\n",
       "      <td>0.569210</td>\n",
       "      <td>0.189481</td>\n",
       "      <td>0.488440</td>\n",
       "      <td>0.265196</td>\n",
       "      <td>0.482012</td>\n",
       "      <td>0.444791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.576100</td>\n",
       "      <td>0.563668</td>\n",
       "      <td>0.193912</td>\n",
       "      <td>0.494252</td>\n",
       "      <td>0.272927</td>\n",
       "      <td>0.487106</td>\n",
       "      <td>0.447936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.523500</td>\n",
       "      <td>0.561166</td>\n",
       "      <td>0.190075</td>\n",
       "      <td>0.494761</td>\n",
       "      <td>0.272952</td>\n",
       "      <td>0.487962</td>\n",
       "      <td>0.450700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.542500</td>\n",
       "      <td>0.557981</td>\n",
       "      <td>0.194791</td>\n",
       "      <td>0.493412</td>\n",
       "      <td>0.272821</td>\n",
       "      <td>0.486596</td>\n",
       "      <td>0.453502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.566200</td>\n",
       "      <td>0.556508</td>\n",
       "      <td>0.193859</td>\n",
       "      <td>0.490108</td>\n",
       "      <td>0.270361</td>\n",
       "      <td>0.484046</td>\n",
       "      <td>0.449637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.558500</td>\n",
       "      <td>0.555793</td>\n",
       "      <td>0.192870</td>\n",
       "      <td>0.493675</td>\n",
       "      <td>0.272906</td>\n",
       "      <td>0.487075</td>\n",
       "      <td>0.450688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.515300</td>\n",
       "      <td>0.554450</td>\n",
       "      <td>0.196961</td>\n",
       "      <td>0.496730</td>\n",
       "      <td>0.274409</td>\n",
       "      <td>0.489213</td>\n",
       "      <td>0.455961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.503400</td>\n",
       "      <td>0.552512</td>\n",
       "      <td>0.198670</td>\n",
       "      <td>0.497841</td>\n",
       "      <td>0.274597</td>\n",
       "      <td>0.490260</td>\n",
       "      <td>0.460850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.512700</td>\n",
       "      <td>0.551666</td>\n",
       "      <td>0.199303</td>\n",
       "      <td>0.499523</td>\n",
       "      <td>0.274629</td>\n",
       "      <td>0.491800</td>\n",
       "      <td>0.460347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.523400</td>\n",
       "      <td>0.550138</td>\n",
       "      <td>0.197142</td>\n",
       "      <td>0.500586</td>\n",
       "      <td>0.274093</td>\n",
       "      <td>0.492847</td>\n",
       "      <td>0.461864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.500600</td>\n",
       "      <td>0.548406</td>\n",
       "      <td>0.198598</td>\n",
       "      <td>0.500906</td>\n",
       "      <td>0.273922</td>\n",
       "      <td>0.493409</td>\n",
       "      <td>0.460151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.477200</td>\n",
       "      <td>0.547476</td>\n",
       "      <td>0.199473</td>\n",
       "      <td>0.499004</td>\n",
       "      <td>0.274604</td>\n",
       "      <td>0.491565</td>\n",
       "      <td>0.461011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.504700</td>\n",
       "      <td>0.548025</td>\n",
       "      <td>0.199333</td>\n",
       "      <td>0.501336</td>\n",
       "      <td>0.277163</td>\n",
       "      <td>0.493472</td>\n",
       "      <td>0.460612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.444800</td>\n",
       "      <td>0.547085</td>\n",
       "      <td>0.200445</td>\n",
       "      <td>0.502258</td>\n",
       "      <td>0.276898</td>\n",
       "      <td>0.494695</td>\n",
       "      <td>0.461936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.478100</td>\n",
       "      <td>0.547084</td>\n",
       "      <td>0.199964</td>\n",
       "      <td>0.502111</td>\n",
       "      <td>0.276725</td>\n",
       "      <td>0.494672</td>\n",
       "      <td>0.461889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.464500</td>\n",
       "      <td>0.546558</td>\n",
       "      <td>0.200498</td>\n",
       "      <td>0.500819</td>\n",
       "      <td>0.277462</td>\n",
       "      <td>0.493269</td>\n",
       "      <td>0.461023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.500300</td>\n",
       "      <td>0.546273</td>\n",
       "      <td>0.200932</td>\n",
       "      <td>0.503774</td>\n",
       "      <td>0.279110</td>\n",
       "      <td>0.496623</td>\n",
       "      <td>0.463561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.483100</td>\n",
       "      <td>0.545982</td>\n",
       "      <td>0.201159</td>\n",
       "      <td>0.503915</td>\n",
       "      <td>0.280511</td>\n",
       "      <td>0.496463</td>\n",
       "      <td>0.464014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.479100</td>\n",
       "      <td>0.545769</td>\n",
       "      <td>0.200338</td>\n",
       "      <td>0.502941</td>\n",
       "      <td>0.277317</td>\n",
       "      <td>0.495084</td>\n",
       "      <td>0.462689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.483800</td>\n",
       "      <td>0.545655</td>\n",
       "      <td>0.200170</td>\n",
       "      <td>0.502436</td>\n",
       "      <td>0.275967</td>\n",
       "      <td>0.494548</td>\n",
       "      <td>0.462758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.486700</td>\n",
       "      <td>0.545748</td>\n",
       "      <td>0.200612</td>\n",
       "      <td>0.503752</td>\n",
       "      <td>0.277453</td>\n",
       "      <td>0.495929</td>\n",
       "      <td>0.463469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.447400</td>\n",
       "      <td>0.545713</td>\n",
       "      <td>0.200612</td>\n",
       "      <td>0.503752</td>\n",
       "      <td>0.277453</td>\n",
       "      <td>0.495929</td>\n",
       "      <td>0.463469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 results: {'eval_loss': 0.5456554889678955, 'eval_bleu': 0.20016987450842874, 'eval_rouge1': 0.5024361170134526, 'eval_rouge2': 0.27596679499406407, 'eval_rougeL': 0.4945484830097166, 'eval_meteor': 0.4627575483927283, 'eval_runtime': 2.0822, 'eval_samples_per_second': 57.151, 'eval_steps_per_second': 7.204, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 119/119 [00:00<00:00, 117440.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 2 saved to FR_crossval_T5_SMALL_QUERY_ONLY_fold_2.jsonl\n",
      "Processing Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_62038/3145572364.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1800' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1800/1800 11:16, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.720200</td>\n",
       "      <td>1.027429</td>\n",
       "      <td>0.087239</td>\n",
       "      <td>0.334964</td>\n",
       "      <td>0.130466</td>\n",
       "      <td>0.326955</td>\n",
       "      <td>0.250884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.027600</td>\n",
       "      <td>0.807834</td>\n",
       "      <td>0.091854</td>\n",
       "      <td>0.357029</td>\n",
       "      <td>0.141245</td>\n",
       "      <td>0.347519</td>\n",
       "      <td>0.288244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.798900</td>\n",
       "      <td>0.656663</td>\n",
       "      <td>0.110715</td>\n",
       "      <td>0.415569</td>\n",
       "      <td>0.175313</td>\n",
       "      <td>0.404209</td>\n",
       "      <td>0.354541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.708600</td>\n",
       "      <td>0.615450</td>\n",
       "      <td>0.121410</td>\n",
       "      <td>0.428965</td>\n",
       "      <td>0.188861</td>\n",
       "      <td>0.417565</td>\n",
       "      <td>0.376650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.662100</td>\n",
       "      <td>0.597737</td>\n",
       "      <td>0.124153</td>\n",
       "      <td>0.436831</td>\n",
       "      <td>0.199020</td>\n",
       "      <td>0.427327</td>\n",
       "      <td>0.391012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.620300</td>\n",
       "      <td>0.585033</td>\n",
       "      <td>0.133614</td>\n",
       "      <td>0.445630</td>\n",
       "      <td>0.210892</td>\n",
       "      <td>0.433999</td>\n",
       "      <td>0.408733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.613300</td>\n",
       "      <td>0.575713</td>\n",
       "      <td>0.141361</td>\n",
       "      <td>0.457907</td>\n",
       "      <td>0.221013</td>\n",
       "      <td>0.445119</td>\n",
       "      <td>0.422547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.573200</td>\n",
       "      <td>0.570218</td>\n",
       "      <td>0.136585</td>\n",
       "      <td>0.457873</td>\n",
       "      <td>0.215128</td>\n",
       "      <td>0.445643</td>\n",
       "      <td>0.406249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.615100</td>\n",
       "      <td>0.563452</td>\n",
       "      <td>0.145447</td>\n",
       "      <td>0.467876</td>\n",
       "      <td>0.223663</td>\n",
       "      <td>0.455903</td>\n",
       "      <td>0.426882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.546500</td>\n",
       "      <td>0.558994</td>\n",
       "      <td>0.155767</td>\n",
       "      <td>0.472258</td>\n",
       "      <td>0.227769</td>\n",
       "      <td>0.461495</td>\n",
       "      <td>0.434748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.538100</td>\n",
       "      <td>0.556492</td>\n",
       "      <td>0.168366</td>\n",
       "      <td>0.472748</td>\n",
       "      <td>0.234440</td>\n",
       "      <td>0.462109</td>\n",
       "      <td>0.435791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.553000</td>\n",
       "      <td>0.552911</td>\n",
       "      <td>0.160860</td>\n",
       "      <td>0.475557</td>\n",
       "      <td>0.231623</td>\n",
       "      <td>0.463664</td>\n",
       "      <td>0.434054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.558400</td>\n",
       "      <td>0.551354</td>\n",
       "      <td>0.168547</td>\n",
       "      <td>0.477644</td>\n",
       "      <td>0.237849</td>\n",
       "      <td>0.464771</td>\n",
       "      <td>0.443564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.543600</td>\n",
       "      <td>0.548482</td>\n",
       "      <td>0.167381</td>\n",
       "      <td>0.473987</td>\n",
       "      <td>0.234574</td>\n",
       "      <td>0.461417</td>\n",
       "      <td>0.434974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.532500</td>\n",
       "      <td>0.547171</td>\n",
       "      <td>0.176021</td>\n",
       "      <td>0.473922</td>\n",
       "      <td>0.243585</td>\n",
       "      <td>0.462725</td>\n",
       "      <td>0.445495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.522000</td>\n",
       "      <td>0.545682</td>\n",
       "      <td>0.179001</td>\n",
       "      <td>0.477708</td>\n",
       "      <td>0.246079</td>\n",
       "      <td>0.467408</td>\n",
       "      <td>0.447762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.484700</td>\n",
       "      <td>0.544570</td>\n",
       "      <td>0.180809</td>\n",
       "      <td>0.480192</td>\n",
       "      <td>0.245836</td>\n",
       "      <td>0.468540</td>\n",
       "      <td>0.446507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.529300</td>\n",
       "      <td>0.543472</td>\n",
       "      <td>0.182884</td>\n",
       "      <td>0.483153</td>\n",
       "      <td>0.250101</td>\n",
       "      <td>0.471890</td>\n",
       "      <td>0.449745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.497400</td>\n",
       "      <td>0.542357</td>\n",
       "      <td>0.178119</td>\n",
       "      <td>0.484334</td>\n",
       "      <td>0.248519</td>\n",
       "      <td>0.472082</td>\n",
       "      <td>0.448010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.474900</td>\n",
       "      <td>0.541522</td>\n",
       "      <td>0.179776</td>\n",
       "      <td>0.486049</td>\n",
       "      <td>0.249664</td>\n",
       "      <td>0.473579</td>\n",
       "      <td>0.449442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.521600</td>\n",
       "      <td>0.541657</td>\n",
       "      <td>0.181948</td>\n",
       "      <td>0.484791</td>\n",
       "      <td>0.250239</td>\n",
       "      <td>0.473099</td>\n",
       "      <td>0.448796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.489200</td>\n",
       "      <td>0.541493</td>\n",
       "      <td>0.182640</td>\n",
       "      <td>0.488671</td>\n",
       "      <td>0.253727</td>\n",
       "      <td>0.476965</td>\n",
       "      <td>0.451908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.492700</td>\n",
       "      <td>0.540788</td>\n",
       "      <td>0.185544</td>\n",
       "      <td>0.487901</td>\n",
       "      <td>0.253186</td>\n",
       "      <td>0.476673</td>\n",
       "      <td>0.451400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.496500</td>\n",
       "      <td>0.540264</td>\n",
       "      <td>0.185850</td>\n",
       "      <td>0.488052</td>\n",
       "      <td>0.252880</td>\n",
       "      <td>0.476750</td>\n",
       "      <td>0.449423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.476700</td>\n",
       "      <td>0.540031</td>\n",
       "      <td>0.185312</td>\n",
       "      <td>0.485485</td>\n",
       "      <td>0.252555</td>\n",
       "      <td>0.474108</td>\n",
       "      <td>0.447909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.501500</td>\n",
       "      <td>0.540332</td>\n",
       "      <td>0.186254</td>\n",
       "      <td>0.490207</td>\n",
       "      <td>0.254688</td>\n",
       "      <td>0.479032</td>\n",
       "      <td>0.451802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.486900</td>\n",
       "      <td>0.540004</td>\n",
       "      <td>0.185952</td>\n",
       "      <td>0.486801</td>\n",
       "      <td>0.253786</td>\n",
       "      <td>0.475734</td>\n",
       "      <td>0.451041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.444500</td>\n",
       "      <td>0.539703</td>\n",
       "      <td>0.185939</td>\n",
       "      <td>0.487313</td>\n",
       "      <td>0.252962</td>\n",
       "      <td>0.476834</td>\n",
       "      <td>0.449925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.449600</td>\n",
       "      <td>0.539491</td>\n",
       "      <td>0.184799</td>\n",
       "      <td>0.488818</td>\n",
       "      <td>0.253246</td>\n",
       "      <td>0.477736</td>\n",
       "      <td>0.448964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.461000</td>\n",
       "      <td>0.539498</td>\n",
       "      <td>0.183711</td>\n",
       "      <td>0.486025</td>\n",
       "      <td>0.250502</td>\n",
       "      <td>0.475536</td>\n",
       "      <td>0.447892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 results: {'eval_loss': 0.539490818977356, 'eval_bleu': 0.18479930837530748, 'eval_rouge1': 0.4888183600467273, 'eval_rouge2': 0.2532455647369434, 'eval_rougeL': 0.4777355534311596, 'eval_meteor': 0.44896350359845366, 'eval_runtime': 2.4337, 'eval_samples_per_second': 48.896, 'eval_steps_per_second': 6.163, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 119/119 [00:00<00:00, 102363.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 3 saved to FR_crossval_T5_SMALL_QUERY_ONLY_fold_3.jsonl\n",
      "Processing Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_62038/3145572364.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1800' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1800/1800 11:25, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.675900</td>\n",
       "      <td>1.095617</td>\n",
       "      <td>0.100144</td>\n",
       "      <td>0.356948</td>\n",
       "      <td>0.160359</td>\n",
       "      <td>0.348472</td>\n",
       "      <td>0.271349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.985400</td>\n",
       "      <td>0.823321</td>\n",
       "      <td>0.115836</td>\n",
       "      <td>0.388467</td>\n",
       "      <td>0.174531</td>\n",
       "      <td>0.383476</td>\n",
       "      <td>0.305789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.748300</td>\n",
       "      <td>0.676182</td>\n",
       "      <td>0.137950</td>\n",
       "      <td>0.436727</td>\n",
       "      <td>0.210195</td>\n",
       "      <td>0.430071</td>\n",
       "      <td>0.364396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.687400</td>\n",
       "      <td>0.644548</td>\n",
       "      <td>0.143299</td>\n",
       "      <td>0.445550</td>\n",
       "      <td>0.216909</td>\n",
       "      <td>0.439303</td>\n",
       "      <td>0.382487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.644500</td>\n",
       "      <td>0.628919</td>\n",
       "      <td>0.149139</td>\n",
       "      <td>0.466141</td>\n",
       "      <td>0.235637</td>\n",
       "      <td>0.456910</td>\n",
       "      <td>0.406039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.609300</td>\n",
       "      <td>0.617657</td>\n",
       "      <td>0.164737</td>\n",
       "      <td>0.469171</td>\n",
       "      <td>0.242309</td>\n",
       "      <td>0.460659</td>\n",
       "      <td>0.413693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.604600</td>\n",
       "      <td>0.610327</td>\n",
       "      <td>0.168704</td>\n",
       "      <td>0.470663</td>\n",
       "      <td>0.242935</td>\n",
       "      <td>0.461756</td>\n",
       "      <td>0.417808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.595100</td>\n",
       "      <td>0.604523</td>\n",
       "      <td>0.175701</td>\n",
       "      <td>0.474735</td>\n",
       "      <td>0.245546</td>\n",
       "      <td>0.466063</td>\n",
       "      <td>0.421950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.583900</td>\n",
       "      <td>0.600142</td>\n",
       "      <td>0.178141</td>\n",
       "      <td>0.477450</td>\n",
       "      <td>0.248992</td>\n",
       "      <td>0.469842</td>\n",
       "      <td>0.429259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.549000</td>\n",
       "      <td>0.596893</td>\n",
       "      <td>0.177303</td>\n",
       "      <td>0.476622</td>\n",
       "      <td>0.247180</td>\n",
       "      <td>0.469444</td>\n",
       "      <td>0.426369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.544500</td>\n",
       "      <td>0.593892</td>\n",
       "      <td>0.181537</td>\n",
       "      <td>0.481160</td>\n",
       "      <td>0.249866</td>\n",
       "      <td>0.473127</td>\n",
       "      <td>0.431511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.552500</td>\n",
       "      <td>0.591136</td>\n",
       "      <td>0.181682</td>\n",
       "      <td>0.487668</td>\n",
       "      <td>0.249731</td>\n",
       "      <td>0.477615</td>\n",
       "      <td>0.436747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.529600</td>\n",
       "      <td>0.589715</td>\n",
       "      <td>0.184433</td>\n",
       "      <td>0.479500</td>\n",
       "      <td>0.249453</td>\n",
       "      <td>0.472798</td>\n",
       "      <td>0.433877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>0.587271</td>\n",
       "      <td>0.183255</td>\n",
       "      <td>0.484161</td>\n",
       "      <td>0.250476</td>\n",
       "      <td>0.476180</td>\n",
       "      <td>0.436948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.529100</td>\n",
       "      <td>0.586496</td>\n",
       "      <td>0.186039</td>\n",
       "      <td>0.485385</td>\n",
       "      <td>0.251142</td>\n",
       "      <td>0.476094</td>\n",
       "      <td>0.439544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.510400</td>\n",
       "      <td>0.586037</td>\n",
       "      <td>0.192639</td>\n",
       "      <td>0.486183</td>\n",
       "      <td>0.255095</td>\n",
       "      <td>0.477526</td>\n",
       "      <td>0.444983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.486400</td>\n",
       "      <td>0.584455</td>\n",
       "      <td>0.190749</td>\n",
       "      <td>0.486300</td>\n",
       "      <td>0.255480</td>\n",
       "      <td>0.478687</td>\n",
       "      <td>0.442701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.519800</td>\n",
       "      <td>0.584004</td>\n",
       "      <td>0.192805</td>\n",
       "      <td>0.486326</td>\n",
       "      <td>0.255601</td>\n",
       "      <td>0.478184</td>\n",
       "      <td>0.441104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.488100</td>\n",
       "      <td>0.583819</td>\n",
       "      <td>0.190855</td>\n",
       "      <td>0.484782</td>\n",
       "      <td>0.253330</td>\n",
       "      <td>0.477263</td>\n",
       "      <td>0.440616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.476000</td>\n",
       "      <td>0.583232</td>\n",
       "      <td>0.191987</td>\n",
       "      <td>0.488156</td>\n",
       "      <td>0.254892</td>\n",
       "      <td>0.479691</td>\n",
       "      <td>0.445164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.483500</td>\n",
       "      <td>0.581924</td>\n",
       "      <td>0.193947</td>\n",
       "      <td>0.487574</td>\n",
       "      <td>0.255166</td>\n",
       "      <td>0.478819</td>\n",
       "      <td>0.444189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.492100</td>\n",
       "      <td>0.581812</td>\n",
       "      <td>0.191404</td>\n",
       "      <td>0.485700</td>\n",
       "      <td>0.255064</td>\n",
       "      <td>0.478191</td>\n",
       "      <td>0.444497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.469100</td>\n",
       "      <td>0.581850</td>\n",
       "      <td>0.190305</td>\n",
       "      <td>0.486585</td>\n",
       "      <td>0.249403</td>\n",
       "      <td>0.478245</td>\n",
       "      <td>0.444669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.449200</td>\n",
       "      <td>0.581774</td>\n",
       "      <td>0.193269</td>\n",
       "      <td>0.483045</td>\n",
       "      <td>0.253751</td>\n",
       "      <td>0.477570</td>\n",
       "      <td>0.443182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.467800</td>\n",
       "      <td>0.581773</td>\n",
       "      <td>0.193449</td>\n",
       "      <td>0.486910</td>\n",
       "      <td>0.254065</td>\n",
       "      <td>0.479006</td>\n",
       "      <td>0.443956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.491000</td>\n",
       "      <td>0.581890</td>\n",
       "      <td>0.193400</td>\n",
       "      <td>0.487727</td>\n",
       "      <td>0.254425</td>\n",
       "      <td>0.480171</td>\n",
       "      <td>0.444174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.487900</td>\n",
       "      <td>0.581318</td>\n",
       "      <td>0.193414</td>\n",
       "      <td>0.485856</td>\n",
       "      <td>0.253816</td>\n",
       "      <td>0.479879</td>\n",
       "      <td>0.443744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.463000</td>\n",
       "      <td>0.581067</td>\n",
       "      <td>0.193414</td>\n",
       "      <td>0.485691</td>\n",
       "      <td>0.254624</td>\n",
       "      <td>0.479273</td>\n",
       "      <td>0.445193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.467000</td>\n",
       "      <td>0.581186</td>\n",
       "      <td>0.193476</td>\n",
       "      <td>0.486905</td>\n",
       "      <td>0.254624</td>\n",
       "      <td>0.480697</td>\n",
       "      <td>0.445784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.430500</td>\n",
       "      <td>0.581105</td>\n",
       "      <td>0.193476</td>\n",
       "      <td>0.486905</td>\n",
       "      <td>0.254624</td>\n",
       "      <td>0.480697</td>\n",
       "      <td>0.445784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 results: {'eval_loss': 0.5810667872428894, 'eval_bleu': 0.19341375106856712, 'eval_rouge1': 0.4856906146362601, 'eval_rouge2': 0.25462368573488264, 'eval_rougeL': 0.47927273500561296, 'eval_meteor': 0.4451933872341262, 'eval_runtime': 2.5209, 'eval_samples_per_second': 47.206, 'eval_steps_per_second': 5.95, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 119/119 [00:00<00:00, 98816.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 4 saved to FR_crossval_T5_SMALL_QUERY_ONLY_fold_4.jsonl\n",
      "Processing Fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_62038/3145572364.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1800' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1800/1800 11:18, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.682100</td>\n",
       "      <td>1.022716</td>\n",
       "      <td>0.096612</td>\n",
       "      <td>0.367964</td>\n",
       "      <td>0.162053</td>\n",
       "      <td>0.360536</td>\n",
       "      <td>0.277890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.008800</td>\n",
       "      <td>0.813950</td>\n",
       "      <td>0.105544</td>\n",
       "      <td>0.394671</td>\n",
       "      <td>0.169591</td>\n",
       "      <td>0.385573</td>\n",
       "      <td>0.317014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.764200</td>\n",
       "      <td>0.671448</td>\n",
       "      <td>0.129467</td>\n",
       "      <td>0.438608</td>\n",
       "      <td>0.202908</td>\n",
       "      <td>0.431682</td>\n",
       "      <td>0.367188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.688800</td>\n",
       "      <td>0.634203</td>\n",
       "      <td>0.138759</td>\n",
       "      <td>0.449687</td>\n",
       "      <td>0.206684</td>\n",
       "      <td>0.441191</td>\n",
       "      <td>0.387230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.621900</td>\n",
       "      <td>0.615843</td>\n",
       "      <td>0.150359</td>\n",
       "      <td>0.466917</td>\n",
       "      <td>0.224346</td>\n",
       "      <td>0.458120</td>\n",
       "      <td>0.402671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.611100</td>\n",
       "      <td>0.606081</td>\n",
       "      <td>0.152796</td>\n",
       "      <td>0.474058</td>\n",
       "      <td>0.229991</td>\n",
       "      <td>0.467336</td>\n",
       "      <td>0.409726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.612400</td>\n",
       "      <td>0.601305</td>\n",
       "      <td>0.164946</td>\n",
       "      <td>0.482813</td>\n",
       "      <td>0.242127</td>\n",
       "      <td>0.475748</td>\n",
       "      <td>0.422971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.575700</td>\n",
       "      <td>0.592598</td>\n",
       "      <td>0.173437</td>\n",
       "      <td>0.486818</td>\n",
       "      <td>0.244484</td>\n",
       "      <td>0.478862</td>\n",
       "      <td>0.427685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.590100</td>\n",
       "      <td>0.587815</td>\n",
       "      <td>0.176372</td>\n",
       "      <td>0.491122</td>\n",
       "      <td>0.254516</td>\n",
       "      <td>0.487531</td>\n",
       "      <td>0.431451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.565000</td>\n",
       "      <td>0.582984</td>\n",
       "      <td>0.174021</td>\n",
       "      <td>0.487234</td>\n",
       "      <td>0.249818</td>\n",
       "      <td>0.482481</td>\n",
       "      <td>0.431655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.558300</td>\n",
       "      <td>0.579725</td>\n",
       "      <td>0.176211</td>\n",
       "      <td>0.494685</td>\n",
       "      <td>0.253260</td>\n",
       "      <td>0.487881</td>\n",
       "      <td>0.434707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.523000</td>\n",
       "      <td>0.576939</td>\n",
       "      <td>0.180672</td>\n",
       "      <td>0.495620</td>\n",
       "      <td>0.256450</td>\n",
       "      <td>0.489314</td>\n",
       "      <td>0.435090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.576400</td>\n",
       "      <td>0.576566</td>\n",
       "      <td>0.183330</td>\n",
       "      <td>0.501299</td>\n",
       "      <td>0.261485</td>\n",
       "      <td>0.495018</td>\n",
       "      <td>0.442150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.516100</td>\n",
       "      <td>0.574436</td>\n",
       "      <td>0.186990</td>\n",
       "      <td>0.501705</td>\n",
       "      <td>0.262588</td>\n",
       "      <td>0.494817</td>\n",
       "      <td>0.443288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.518800</td>\n",
       "      <td>0.570731</td>\n",
       "      <td>0.185417</td>\n",
       "      <td>0.500050</td>\n",
       "      <td>0.261869</td>\n",
       "      <td>0.493655</td>\n",
       "      <td>0.440925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.522400</td>\n",
       "      <td>0.569509</td>\n",
       "      <td>0.186437</td>\n",
       "      <td>0.500153</td>\n",
       "      <td>0.260541</td>\n",
       "      <td>0.494400</td>\n",
       "      <td>0.442853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.504200</td>\n",
       "      <td>0.568984</td>\n",
       "      <td>0.188402</td>\n",
       "      <td>0.504861</td>\n",
       "      <td>0.264899</td>\n",
       "      <td>0.497714</td>\n",
       "      <td>0.448197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.535100</td>\n",
       "      <td>0.569228</td>\n",
       "      <td>0.190914</td>\n",
       "      <td>0.504807</td>\n",
       "      <td>0.268728</td>\n",
       "      <td>0.498407</td>\n",
       "      <td>0.450361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.513100</td>\n",
       "      <td>0.568545</td>\n",
       "      <td>0.191902</td>\n",
       "      <td>0.507279</td>\n",
       "      <td>0.271707</td>\n",
       "      <td>0.500897</td>\n",
       "      <td>0.455019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.463700</td>\n",
       "      <td>0.568546</td>\n",
       "      <td>0.191449</td>\n",
       "      <td>0.507334</td>\n",
       "      <td>0.272305</td>\n",
       "      <td>0.500893</td>\n",
       "      <td>0.455582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.487800</td>\n",
       "      <td>0.567355</td>\n",
       "      <td>0.190891</td>\n",
       "      <td>0.506017</td>\n",
       "      <td>0.271586</td>\n",
       "      <td>0.499542</td>\n",
       "      <td>0.453253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.461400</td>\n",
       "      <td>0.567477</td>\n",
       "      <td>0.191191</td>\n",
       "      <td>0.507387</td>\n",
       "      <td>0.271096</td>\n",
       "      <td>0.501193</td>\n",
       "      <td>0.455459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.496900</td>\n",
       "      <td>0.566469</td>\n",
       "      <td>0.190841</td>\n",
       "      <td>0.504776</td>\n",
       "      <td>0.270104</td>\n",
       "      <td>0.498248</td>\n",
       "      <td>0.453437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.473100</td>\n",
       "      <td>0.565537</td>\n",
       "      <td>0.190924</td>\n",
       "      <td>0.505042</td>\n",
       "      <td>0.268883</td>\n",
       "      <td>0.498735</td>\n",
       "      <td>0.452185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.480400</td>\n",
       "      <td>0.565665</td>\n",
       "      <td>0.194431</td>\n",
       "      <td>0.508028</td>\n",
       "      <td>0.273719</td>\n",
       "      <td>0.501867</td>\n",
       "      <td>0.457665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.491200</td>\n",
       "      <td>0.565468</td>\n",
       "      <td>0.196295</td>\n",
       "      <td>0.508906</td>\n",
       "      <td>0.274133</td>\n",
       "      <td>0.503716</td>\n",
       "      <td>0.459454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.456100</td>\n",
       "      <td>0.565475</td>\n",
       "      <td>0.194831</td>\n",
       "      <td>0.505553</td>\n",
       "      <td>0.273829</td>\n",
       "      <td>0.500872</td>\n",
       "      <td>0.457567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.479000</td>\n",
       "      <td>0.565377</td>\n",
       "      <td>0.198480</td>\n",
       "      <td>0.508672</td>\n",
       "      <td>0.279310</td>\n",
       "      <td>0.503699</td>\n",
       "      <td>0.460811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.446600</td>\n",
       "      <td>0.565150</td>\n",
       "      <td>0.198420</td>\n",
       "      <td>0.508699</td>\n",
       "      <td>0.278285</td>\n",
       "      <td>0.503346</td>\n",
       "      <td>0.458379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.475900</td>\n",
       "      <td>0.565190</td>\n",
       "      <td>0.197400</td>\n",
       "      <td>0.508134</td>\n",
       "      <td>0.277834</td>\n",
       "      <td>0.503306</td>\n",
       "      <td>0.460329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 results: {'eval_loss': 0.5651502013206482, 'eval_bleu': 0.1984203652019145, 'eval_rouge1': 0.5086993385029102, 'eval_rouge2': 0.2782854137652523, 'eval_rougeL': 0.503345975184885, 'eval_meteor': 0.45837926472671403, 'eval_runtime': 2.4502, 'eval_samples_per_second': 48.567, 'eval_steps_per_second': 6.122, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 119/119 [00:00<00:00, 99189.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 5 saved to FR_crossval_T5_SMALL_QUERY_ONLY_fold_5.jsonl\n",
      "Average metrics over all folds: {'eval_loss': 0.5617120385169982, 'eval_bleu': 0.19748815657473706, 'eval_rouge1': 0.4994195036795254, 'eval_rouge2': 0.2690296099930828, 'eval_rougeL': 0.4912213500709476, 'eval_meteor': 0.4567228796827988, 'eval_runtime': 2.3174200000000003, 'eval_samples_per_second': 51.7918, 'eval_steps_per_second': 6.5162, 'epoch': 29.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Averages/epoch</td><td>▁</td></tr><tr><td>Averages/eval_bleu</td><td>▁</td></tr><tr><td>Averages/eval_loss</td><td>▁</td></tr><tr><td>Averages/eval_meteor</td><td>▁</td></tr><tr><td>Averages/eval_rouge1</td><td>▁</td></tr><tr><td>Averages/eval_rouge2</td><td>▁</td></tr><tr><td>Averages/eval_rougeL</td><td>▁</td></tr><tr><td>Averages/eval_runtime</td><td>▁</td></tr><tr><td>Averages/eval_samples_per_second</td><td>▁</td></tr><tr><td>Averages/eval_steps_per_second</td><td>▁</td></tr><tr><td>Fold_1/eval/epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇███</td></tr><tr><td>Fold_1/eval/eval_bleu</td><td>▁▂▄▅▅▆▆▆▆▆▆▇▇▇▇███████████████</td></tr><tr><td>Fold_1/eval/eval_loss</td><td>█▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_1/eval/eval_meteor</td><td>▁▃▅▆▆▇▇▇▇▇▇███████████████████</td></tr><tr><td>Fold_1/eval/eval_rouge1</td><td>▁▂▅▆▆▆▇▇▇▇████████████████████</td></tr><tr><td>Fold_1/eval/eval_rouge2</td><td>▁▂▄▆▆▆▇▆▇▇▇█▇█████████████████</td></tr><tr><td>Fold_1/eval/eval_rougeL</td><td>▁▂▅▆▆▆▇▇▇▇████████████████████</td></tr><tr><td>Fold_1/eval/eval_runtime</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_1/eval/eval_samples_per_second</td><td>▁█████████████████████████████</td></tr><tr><td>Fold_1/eval/eval_steps_per_second</td><td>▁█████████████████████████████</td></tr><tr><td>Fold_2/eval/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>Fold_2/eval/eval_bleu</td><td>▁▂▄▅▆▆▆▇▇▇▇█▇▇█████████████████</td></tr><tr><td>Fold_2/eval/eval_loss</td><td>█▅▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_2/eval/eval_meteor</td><td>▁▃▅▆▇▇▇▇▇▇▇█▇▇█████████████████</td></tr><tr><td>Fold_2/eval/eval_rouge1</td><td>▁▂▅▆▇▇▇▇▇▇█▇▇▇█████████████████</td></tr><tr><td>Fold_2/eval/eval_rouge2</td><td>▁▂▅▆▇▆▇▇▇███▇██████████████████</td></tr><tr><td>Fold_2/eval/eval_rougeL</td><td>▁▂▅▆▇▇▇▇▇██▇▇██████████████████</td></tr><tr><td>Fold_2/eval/eval_runtime</td><td>▁▇▆▆▆▆▆▆▆▆▆▆▅▇▅▆▆▆▅▆▇▆▆▅▆▅▅▅█▅▆</td></tr><tr><td>Fold_2/eval/eval_samples_per_second</td><td>█▂▃▃▃▃▃▃▃▃▃▃▄▂▄▃▃▃▄▃▂▃▃▄▃▄▄▄▁▄▃</td></tr><tr><td>Fold_2/eval/eval_steps_per_second</td><td>█▂▃▃▃▃▃▃▃▃▃▃▄▂▄▃▃▃▄▃▂▃▃▄▃▄▄▄▁▄▃</td></tr><tr><td>Fold_3/eval/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>Fold_3/eval/eval_bleu</td><td>▁▁▃▃▄▄▅▄▅▆▇▆▇▇▇▇██▇████████████</td></tr><tr><td>Fold_3/eval/eval_loss</td><td>█▅▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_3/eval/eval_meteor</td><td>▁▂▅▅▆▆▇▆▇▇▇▇█▇█████████████████</td></tr><tr><td>Fold_3/eval/eval_rouge1</td><td>▁▂▅▅▆▆▇▇▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>Fold_3/eval/eval_rouge2</td><td>▁▂▄▄▅▆▆▆▆▆▇▇▇▇▇████████████████</td></tr><tr><td>Fold_3/eval/eval_rougeL</td><td>▁▂▅▅▆▆▆▆▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>Fold_3/eval/eval_runtime</td><td>▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▂▇█▇█▇</td></tr><tr><td>Fold_3/eval/eval_samples_per_second</td><td>▇███▇▇██████▇██▇█▇███████▆▂▁▂▁▁</td></tr><tr><td>Fold_3/eval/eval_steps_per_second</td><td>▇███▇▇██████▇██▇█▇███████▆▂▁▂▁▁</td></tr><tr><td>Fold_4/eval/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>Fold_4/eval/eval_bleu</td><td>▁▂▄▄▅▆▆▇▇▇▇▇▇▇▇████████████████</td></tr><tr><td>Fold_4/eval/eval_loss</td><td>█▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_4/eval/eval_meteor</td><td>▁▂▅▅▆▇▇▇▇▇▇████████████████████</td></tr><tr><td>Fold_4/eval/eval_rouge1</td><td>▁▃▅▆▇▇▇▇▇▇█████████████████████</td></tr><tr><td>Fold_4/eval/eval_rouge2</td><td>▁▂▅▅▇▇▇▇█▇█████████████████████</td></tr><tr><td>Fold_4/eval/eval_rougeL</td><td>▁▃▅▆▇▇▇▇▇▇█████████████████████</td></tr><tr><td>Fold_4/eval/eval_runtime</td><td>▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂█▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>Fold_4/eval/eval_samples_per_second</td><td>█▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▁▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>Fold_4/eval/eval_steps_per_second</td><td>█▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▁▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>Fold_5/eval/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>Fold_5/eval/eval_bleu</td><td>▁▂▃▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇██▇▇▇▇███████</td></tr><tr><td>Fold_5/eval/eval_loss</td><td>█▅▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_5/eval/eval_meteor</td><td>▁▂▄▅▆▆▇▇▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>Fold_5/eval/eval_rouge1</td><td>▁▂▅▅▆▆▇▇▇▇▇▇███████████████████</td></tr><tr><td>Fold_5/eval/eval_rouge2</td><td>▁▁▃▄▅▅▆▆▇▆▆▇▇▇▇▇▇▇████▇▇███████</td></tr><tr><td>Fold_5/eval/eval_rougeL</td><td>▁▂▄▅▆▆▇▇▇▇▇▇███████████████████</td></tr><tr><td>Fold_5/eval/eval_runtime</td><td>▆▂▃▂▅▅▅█▅▇▄▄▇▄▁█▆▅▆▃▃▄▃█▅█▃▅▆▆▆</td></tr><tr><td>Fold_5/eval/eval_samples_per_second</td><td>▃▇▆▆▄▃▄▁▄▂▅▅▂▅█▁▃▄▃▆▆▅▆▁▄▁▆▄▃▃▃</td></tr><tr><td>Fold_5/eval/eval_steps_per_second</td><td>▃▆▆▆▄▃▄▁▄▂▅▅▂▅█▁▃▄▃▆▆▅▆▁▄▁▆▄▃▃▃</td></tr><tr><td>eval/bleu</td><td>▁▆▆▆▇█████▅▆▇▇▇▇▇▇▇▇▄▅▅▆▆▃▅▅▆▆▇▇▄▅▆▆▇▇▇▇</td></tr><tr><td>eval/loss</td><td>▅▂▂▂▂▂█▅▃▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▅▂▂▂▂▂▂▂▂▅▂▁▁▁▁▁</td></tr><tr><td>eval/meteor</td><td>▁▇██████▁▅▇▇▇▇▇████▄▆▆▇▇▇▇▄▆▆▇▇▇▇▇▃▅▆▇▇█</td></tr><tr><td>eval/rouge1</td><td>▄▇▇▇█████▆▇▇▇▇█▄▆▆▇▇▇▇▇▆▆▆▇▇▇▇▁▅▆▆▇▇▇▇██</td></tr><tr><td>eval/rouge2</td><td>▁▄▆████▂▇▇████▂▄▅▆▆▆▆▂▄▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇█</td></tr><tr><td>eval/rougeL</td><td>▁▅▆▆██▃▇▇▇▇███████▄▅▆▇▇▇▇▇▅▆▇▇▇▇▇▇▂▆▇▇██</td></tr><tr><td>eval/runtime</td><td>▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▅▆▇▇██▇▆▆▆▅▆</td></tr><tr><td>eval/samples_per_second</td><td>███████████████████▆▆▆▆▁▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>eval/steps_per_second</td><td>███████████████████████▆▆▆▆▆▆▆▁▆▆▆▆▆▆▆▆▆</td></tr><tr><td>test/bleu</td><td>█▅▁▃▅</td></tr><tr><td>test/loss</td><td>▇▂▁█▅</td></tr><tr><td>test/meteor</td><td>█▆▂▁▅</td></tr><tr><td>test/rouge1</td><td>█▆▂▁▇</td></tr><tr><td>test/rouge2</td><td>█▆▁▁▇</td></tr><tr><td>test/rougeL</td><td>▇▆▁▁█</td></tr><tr><td>test/runtime</td><td>▁▁▆█▇</td></tr><tr><td>test/samples_per_second</td><td>██▂▁▂</td></tr><tr><td>test/steps_per_second</td><td>██▂▁▂</td></tr><tr><td>train/epoch</td><td>▁▃▃▄▄▅▆▆▆▁▃▃▄██▂▂▂▄▄▅▅▆▆▇█▁▁▂▃▆▆█▂▂▄▅▅▆█</td></tr><tr><td>train/global_step</td><td>▂▄▄▄▅▆▆▆▂▂▅▅▆▆▇█▁▁▁▃▅▅▁▁▂▃▅▅▆▇█▁▁▄▄▄▅▅▆█</td></tr><tr><td>train/grad_norm</td><td>▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▇▇▇▇▅▄▂▂▇▅▃▃▁██▇▆▅▅▃▂▁▁█▇▅▅▄▃▃▂▇▇▆▆▅▃▃▃▁</td></tr><tr><td>train/loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Averages/epoch</td><td>29.8</td></tr><tr><td>Averages/eval_bleu</td><td>0.19749</td></tr><tr><td>Averages/eval_loss</td><td>0.56171</td></tr><tr><td>Averages/eval_meteor</td><td>0.45672</td></tr><tr><td>Averages/eval_rouge1</td><td>0.49942</td></tr><tr><td>Averages/eval_rouge2</td><td>0.26903</td></tr><tr><td>Averages/eval_rougeL</td><td>0.49122</td></tr><tr><td>Averages/eval_runtime</td><td>2.31742</td></tr><tr><td>Averages/eval_samples_per_second</td><td>51.7918</td></tr><tr><td>Averages/eval_steps_per_second</td><td>6.5162</td></tr><tr><td>Fold_1/eval/epoch</td><td>29</td></tr><tr><td>Fold_1/eval/eval_bleu</td><td>0.21064</td></tr><tr><td>Fold_1/eval/eval_loss</td><td>0.5772</td></tr><tr><td>Fold_1/eval/eval_meteor</td><td>0.46832</td></tr><tr><td>Fold_1/eval/eval_rouge1</td><td>0.51145</td></tr><tr><td>Fold_1/eval/eval_rouge2</td><td>0.28303</td></tr><tr><td>Fold_1/eval/eval_rougeL</td><td>0.5012</td></tr><tr><td>Fold_1/eval/eval_runtime</td><td>2.1001</td></tr><tr><td>Fold_1/eval/eval_samples_per_second</td><td>57.139</td></tr><tr><td>Fold_1/eval/eval_steps_per_second</td><td>7.142</td></tr><tr><td>Fold_2/eval/epoch</td><td>30</td></tr><tr><td>Fold_2/eval/eval_bleu</td><td>0.20017</td></tr><tr><td>Fold_2/eval/eval_loss</td><td>0.54566</td></tr><tr><td>Fold_2/eval/eval_meteor</td><td>0.46276</td></tr><tr><td>Fold_2/eval/eval_rouge1</td><td>0.50244</td></tr><tr><td>Fold_2/eval/eval_rouge2</td><td>0.27597</td></tr><tr><td>Fold_2/eval/eval_rougeL</td><td>0.49455</td></tr><tr><td>Fold_2/eval/eval_runtime</td><td>2.0822</td></tr><tr><td>Fold_2/eval/eval_samples_per_second</td><td>57.151</td></tr><tr><td>Fold_2/eval/eval_steps_per_second</td><td>7.204</td></tr><tr><td>Fold_3/eval/epoch</td><td>30</td></tr><tr><td>Fold_3/eval/eval_bleu</td><td>0.1848</td></tr><tr><td>Fold_3/eval/eval_loss</td><td>0.53949</td></tr><tr><td>Fold_3/eval/eval_meteor</td><td>0.44896</td></tr><tr><td>Fold_3/eval/eval_rouge1</td><td>0.48882</td></tr><tr><td>Fold_3/eval/eval_rouge2</td><td>0.25325</td></tr><tr><td>Fold_3/eval/eval_rougeL</td><td>0.47774</td></tr><tr><td>Fold_3/eval/eval_runtime</td><td>2.4337</td></tr><tr><td>Fold_3/eval/eval_samples_per_second</td><td>48.896</td></tr><tr><td>Fold_3/eval/eval_steps_per_second</td><td>6.163</td></tr><tr><td>Fold_4/eval/epoch</td><td>30</td></tr><tr><td>Fold_4/eval/eval_bleu</td><td>0.19341</td></tr><tr><td>Fold_4/eval/eval_loss</td><td>0.58107</td></tr><tr><td>Fold_4/eval/eval_meteor</td><td>0.44519</td></tr><tr><td>Fold_4/eval/eval_rouge1</td><td>0.48569</td></tr><tr><td>Fold_4/eval/eval_rouge2</td><td>0.25462</td></tr><tr><td>Fold_4/eval/eval_rougeL</td><td>0.47927</td></tr><tr><td>Fold_4/eval/eval_runtime</td><td>2.5209</td></tr><tr><td>Fold_4/eval/eval_samples_per_second</td><td>47.206</td></tr><tr><td>Fold_4/eval/eval_steps_per_second</td><td>5.95</td></tr><tr><td>Fold_5/eval/epoch</td><td>30</td></tr><tr><td>Fold_5/eval/eval_bleu</td><td>0.19842</td></tr><tr><td>Fold_5/eval/eval_loss</td><td>0.56515</td></tr><tr><td>Fold_5/eval/eval_meteor</td><td>0.45838</td></tr><tr><td>Fold_5/eval/eval_rouge1</td><td>0.5087</td></tr><tr><td>Fold_5/eval/eval_rouge2</td><td>0.27829</td></tr><tr><td>Fold_5/eval/eval_rougeL</td><td>0.50335</td></tr><tr><td>Fold_5/eval/eval_runtime</td><td>2.4502</td></tr><tr><td>Fold_5/eval/eval_samples_per_second</td><td>48.567</td></tr><tr><td>Fold_5/eval/eval_steps_per_second</td><td>6.122</td></tr><tr><td>eval/bleu</td><td>0.19842</td></tr><tr><td>eval/loss</td><td>0.56515</td></tr><tr><td>eval/meteor</td><td>0.45838</td></tr><tr><td>eval/rouge1</td><td>0.5087</td></tr><tr><td>eval/rouge2</td><td>0.27829</td></tr><tr><td>eval/rougeL</td><td>0.50335</td></tr><tr><td>eval/runtime</td><td>2.4502</td></tr><tr><td>eval/samples_per_second</td><td>48.567</td></tr><tr><td>eval/steps_per_second</td><td>6.122</td></tr><tr><td>test/bleu</td><td>0.19842</td></tr><tr><td>test/loss</td><td>0.56515</td></tr><tr><td>test/meteor</td><td>0.45838</td></tr><tr><td>test/rouge1</td><td>0.5087</td></tr><tr><td>test/rouge2</td><td>0.27829</td></tr><tr><td>test/rougeL</td><td>0.50335</td></tr><tr><td>test/runtime</td><td>2.4318</td></tr><tr><td>test/samples_per_second</td><td>48.934</td></tr><tr><td>test/steps_per_second</td><td>6.168</td></tr><tr><td>total_flos</td><td>1936741179064320.0</td></tr><tr><td>train/epoch</td><td>30</td></tr><tr><td>train/global_step</td><td>1800</td></tr><tr><td>train/grad_norm</td><td>0.85124</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>0.4759</td></tr><tr><td>train_loss</td><td>0.67812</td></tr><tr><td>train_runtime</td><td>679.0062</td></tr><tr><td>train_samples_per_second</td><td>21.075</td></tr><tr><td>train_steps_per_second</td><td>2.651</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">FR_T5_SMALL_QUERY_ONLY</strong> at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/rpai6d3d' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/rpai6d3d</a><br> View project at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250513_020948-rpai6d3d/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final predicitons for all folds foudn in FR_crossval_T5_SMALL_QUERY_ONLY.jsonl\n"
     ]
    }
   ],
   "source": [
    "cross_val_train(modelname=\"t5-small\", run_name=\"FR_T5_SMALL_QUERY_ONLY\",filename=\"FR_crossval_T5_SMALL_QUERY_ONLY.jsonl\", \n",
    "                tokenizer=tokenizer, tokenized_dataset=tokenized_dataset, \n",
    "                folds=folds, nb_epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7668e0a-cc70-4e2a-9109-2a8ddaf21e6b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# ---------- TOP 5 DOUCMENTS SUMMARIZED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6f325fa-3e1f-4093-98f3-9b9d33869d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a1560c7117e40878d3c653e89f73254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': \"[QUERY] Find information on President Barack Obama's family history, including genealogy, national origins, places and dates of birth, etc.\\n[DOCUMENTS]\\n[DOC 1] This page was last modified on 14 March 2009, at 05:50 . It contains articles with unsourced statements and articles needing factual verification since October 2008 .\\n[DOC 2] This page was last modified on 14 March 2009, at 05:50 . It contains articles needing factual verification since October 2008 . It is available under the terms of the GNU Free Documentation License .\\n[DOC 3] This page was last modified on 16 March 2009, at 03:22 . It contains articles with unsourced statements and articles needing factual verification since October 2008 . Hidden categories: Wikipedia semi-protected pages | All pages needing cleanup .\\n[DOC 4] This page was last modified on 14 March 2009, at 05:50 . Use this page to help users with reading comprehension and vocabulary . Use the weekly Newsquiz to test your knowledge of stories you need to share .\\n[DOC 5] This page was last modified on 14 March 2009, at 05:50 . It contains articles with unsourced statements . The Audacity of Hope was written in 2009 .\", 'output': \"Where did Barack Obama's parents and grandparents come from?\", 'input_ids': [784, 15367, 11824, 908, 2588, 251, 30, 1661, 20653, 4534, 31, 7, 384, 892, 6, 379, 31960, 6, 1157, 5233, 7, 6, 1747, 11, 5128, 13, 3879, 6, 672, 5, 784, 9857, 5211, 11810, 134, 908, 784, 26472, 209, 908, 100, 543, 47, 336, 8473, 30, 968, 1332, 2464, 6, 44, 3, 3076, 10, 1752, 3, 5, 94, 2579, 2984, 28, 73, 15551, 6643, 11, 2984, 174, 53, 685, 3471, 17549, 437, 1797, 2628, 3, 5, 784, 26472, 204, 908, 100, 543, 47, 336, 8473, 30, 968, 1332, 2464, 6, 44, 3, 3076, 10, 1752, 3, 5, 94, 2579, 2984, 174, 53, 685, 3471, 17549, 437, 1797, 2628, 3, 5, 94, 19, 347, 365, 8, 1353, 13, 8, 350, 17052, 1443, 11167, 257, 16452, 3, 5, 784, 26472, 220, 908, 100, 543, 47, 336, 8473, 30, 898, 1332, 2464, 6, 44, 12811, 10, 2884, 3, 5, 94, 2579, 2984, 28, 73, 15551, 6643, 11, 2984, 174, 53, 685, 3471, 17549, 437, 1797, 2628, 3, 5, 27194, 5897, 10, 16885, 4772, 18, 19812, 15, 26, 1688, 1820, 432, 1688, 174, 53, 23175, 3, 5, 784, 26472, 314, 908, 100, 543, 47, 336, 8473, 30, 968, 1332, 2464, 6, 44, 3, 3076, 10, 1752, 3, 5, 2048, 48, 543, 12, 199, 1105, 28, 1183, 27160, 11, 19067, 3, 5, 2048, 8, 5547, 3529, 1169, 172, 12, 794, 39, 1103, 13, 1937, 25, 174, 12, 698, 3, 5, 784, 26472, 305, 908, 100, 543, 47, 336, 8473, 30, 968, 1332, 2464, 6, 44, 3, 3076, 10, 1752, 3, 5, 94, 2579, 2984, 28, 73, 15551, 6643, 3, 5, 37, 1957, 26, 9, 6726, 13, 6000, 47, 1545, 16, 2464, 3, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [2840, 410, 20653, 4534, 31, 7, 1362, 11, 22229, 369, 45, 58, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "tokenizer,tokenized_dataset= tokenize_and_split_dataset(\"training_top5_qulac_SUMMARIZED_FOR_MODEL.json\",\"t5-small\")\n",
    "print(tokenized_dataset[0])  # Optional: check one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c3f69a1-2bcc-4294-b048-2e129e8f24b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds= generate_folds(tokenized_dataset, n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3efe5ac-26f7-4f19-b8b1-cc3e817e37ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/users/nfs/Etu0/21402600/wandb/run-20250510_224706-yqrgb5pz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/yqrgb5pz' target=\"_blank\">T5_SMALL_TOP5DOCS_SUMMARIZED</a></strong> to <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/yqrgb5pz' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/yqrgb5pz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_14586/3145572364.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='900' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 900/1800 05:37 < 05:37, 2.66 it/s, Epoch 15/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.393400</td>\n",
       "      <td>0.844550</td>\n",
       "      <td>0.098140</td>\n",
       "      <td>0.393720</td>\n",
       "      <td>0.172560</td>\n",
       "      <td>0.383675</td>\n",
       "      <td>0.300530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.730600</td>\n",
       "      <td>0.601212</td>\n",
       "      <td>0.150760</td>\n",
       "      <td>0.431580</td>\n",
       "      <td>0.206855</td>\n",
       "      <td>0.424689</td>\n",
       "      <td>0.377058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.586500</td>\n",
       "      <td>0.545290</td>\n",
       "      <td>0.170607</td>\n",
       "      <td>0.469422</td>\n",
       "      <td>0.220446</td>\n",
       "      <td>0.459851</td>\n",
       "      <td>0.417890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.519100</td>\n",
       "      <td>0.530222</td>\n",
       "      <td>0.178776</td>\n",
       "      <td>0.470718</td>\n",
       "      <td>0.228101</td>\n",
       "      <td>0.461492</td>\n",
       "      <td>0.423201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.491400</td>\n",
       "      <td>0.515488</td>\n",
       "      <td>0.182384</td>\n",
       "      <td>0.477691</td>\n",
       "      <td>0.228463</td>\n",
       "      <td>0.468648</td>\n",
       "      <td>0.429876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.460800</td>\n",
       "      <td>0.508271</td>\n",
       "      <td>0.182371</td>\n",
       "      <td>0.478430</td>\n",
       "      <td>0.229796</td>\n",
       "      <td>0.469196</td>\n",
       "      <td>0.429252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.438900</td>\n",
       "      <td>0.503539</td>\n",
       "      <td>0.185633</td>\n",
       "      <td>0.474441</td>\n",
       "      <td>0.231480</td>\n",
       "      <td>0.463858</td>\n",
       "      <td>0.425513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.429800</td>\n",
       "      <td>0.499806</td>\n",
       "      <td>0.180491</td>\n",
       "      <td>0.475617</td>\n",
       "      <td>0.225040</td>\n",
       "      <td>0.467359</td>\n",
       "      <td>0.426904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.432800</td>\n",
       "      <td>0.499436</td>\n",
       "      <td>0.186712</td>\n",
       "      <td>0.484756</td>\n",
       "      <td>0.230496</td>\n",
       "      <td>0.472845</td>\n",
       "      <td>0.436418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.413600</td>\n",
       "      <td>0.499159</td>\n",
       "      <td>0.193403</td>\n",
       "      <td>0.480349</td>\n",
       "      <td>0.237614</td>\n",
       "      <td>0.470217</td>\n",
       "      <td>0.433564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.384800</td>\n",
       "      <td>0.498770</td>\n",
       "      <td>0.189728</td>\n",
       "      <td>0.488274</td>\n",
       "      <td>0.236147</td>\n",
       "      <td>0.476311</td>\n",
       "      <td>0.440612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.392300</td>\n",
       "      <td>0.498631</td>\n",
       "      <td>0.190746</td>\n",
       "      <td>0.489043</td>\n",
       "      <td>0.231242</td>\n",
       "      <td>0.474459</td>\n",
       "      <td>0.438389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.390700</td>\n",
       "      <td>0.502049</td>\n",
       "      <td>0.187945</td>\n",
       "      <td>0.487172</td>\n",
       "      <td>0.228681</td>\n",
       "      <td>0.471949</td>\n",
       "      <td>0.436189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.396000</td>\n",
       "      <td>0.499222</td>\n",
       "      <td>0.191505</td>\n",
       "      <td>0.486842</td>\n",
       "      <td>0.229736</td>\n",
       "      <td>0.472041</td>\n",
       "      <td>0.438110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.387600</td>\n",
       "      <td>0.500895</td>\n",
       "      <td>0.195191</td>\n",
       "      <td>0.488448</td>\n",
       "      <td>0.231025</td>\n",
       "      <td>0.471668</td>\n",
       "      <td>0.439554</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 results: {'eval_loss': 0.4986312985420227, 'eval_bleu': 0.19074583115428184, 'eval_rouge1': 0.48904300069121504, 'eval_rouge2': 0.23124195063454211, 'eval_rougeL': 0.4744587812246184, 'eval_meteor': 0.43838852433833125, 'eval_runtime': 2.1723, 'eval_samples_per_second': 55.242, 'eval_steps_per_second': 6.905, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 40152.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 1 saved to crossval_T5_TOP5DOCS_SUMMARIZED_fold_1.jsonl\n",
      "Processing Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_14586/3145572364.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='960' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 960/1800 05:57 < 05:13, 2.68 it/s, Epoch 16/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.367300</td>\n",
       "      <td>0.840007</td>\n",
       "      <td>0.113379</td>\n",
       "      <td>0.387473</td>\n",
       "      <td>0.178009</td>\n",
       "      <td>0.379653</td>\n",
       "      <td>0.301322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.738900</td>\n",
       "      <td>0.610343</td>\n",
       "      <td>0.156122</td>\n",
       "      <td>0.445008</td>\n",
       "      <td>0.223891</td>\n",
       "      <td>0.438057</td>\n",
       "      <td>0.399772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.540500</td>\n",
       "      <td>0.529365</td>\n",
       "      <td>0.168870</td>\n",
       "      <td>0.471875</td>\n",
       "      <td>0.242810</td>\n",
       "      <td>0.465599</td>\n",
       "      <td>0.435242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.510330</td>\n",
       "      <td>0.179504</td>\n",
       "      <td>0.481421</td>\n",
       "      <td>0.244129</td>\n",
       "      <td>0.471905</td>\n",
       "      <td>0.445479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.465300</td>\n",
       "      <td>0.500303</td>\n",
       "      <td>0.182987</td>\n",
       "      <td>0.485587</td>\n",
       "      <td>0.245132</td>\n",
       "      <td>0.475302</td>\n",
       "      <td>0.448616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.473500</td>\n",
       "      <td>0.493620</td>\n",
       "      <td>0.189127</td>\n",
       "      <td>0.494526</td>\n",
       "      <td>0.252337</td>\n",
       "      <td>0.484085</td>\n",
       "      <td>0.454191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.453500</td>\n",
       "      <td>0.488064</td>\n",
       "      <td>0.191051</td>\n",
       "      <td>0.498080</td>\n",
       "      <td>0.262739</td>\n",
       "      <td>0.490697</td>\n",
       "      <td>0.462276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.425500</td>\n",
       "      <td>0.486565</td>\n",
       "      <td>0.201974</td>\n",
       "      <td>0.508990</td>\n",
       "      <td>0.276899</td>\n",
       "      <td>0.502499</td>\n",
       "      <td>0.475723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.411100</td>\n",
       "      <td>0.484984</td>\n",
       "      <td>0.199641</td>\n",
       "      <td>0.501817</td>\n",
       "      <td>0.270618</td>\n",
       "      <td>0.495777</td>\n",
       "      <td>0.471454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.423700</td>\n",
       "      <td>0.483069</td>\n",
       "      <td>0.204154</td>\n",
       "      <td>0.509511</td>\n",
       "      <td>0.274646</td>\n",
       "      <td>0.501453</td>\n",
       "      <td>0.477426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.395100</td>\n",
       "      <td>0.482153</td>\n",
       "      <td>0.206338</td>\n",
       "      <td>0.510360</td>\n",
       "      <td>0.277479</td>\n",
       "      <td>0.502644</td>\n",
       "      <td>0.476093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.401700</td>\n",
       "      <td>0.480806</td>\n",
       "      <td>0.212268</td>\n",
       "      <td>0.514025</td>\n",
       "      <td>0.286867</td>\n",
       "      <td>0.508378</td>\n",
       "      <td>0.481110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.393100</td>\n",
       "      <td>0.480254</td>\n",
       "      <td>0.213526</td>\n",
       "      <td>0.520957</td>\n",
       "      <td>0.293721</td>\n",
       "      <td>0.511678</td>\n",
       "      <td>0.488205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.365400</td>\n",
       "      <td>0.480462</td>\n",
       "      <td>0.221452</td>\n",
       "      <td>0.523811</td>\n",
       "      <td>0.298465</td>\n",
       "      <td>0.516387</td>\n",
       "      <td>0.492296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.364700</td>\n",
       "      <td>0.480471</td>\n",
       "      <td>0.217460</td>\n",
       "      <td>0.518835</td>\n",
       "      <td>0.291437</td>\n",
       "      <td>0.509693</td>\n",
       "      <td>0.488028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.379200</td>\n",
       "      <td>0.480256</td>\n",
       "      <td>0.220664</td>\n",
       "      <td>0.518195</td>\n",
       "      <td>0.295599</td>\n",
       "      <td>0.509999</td>\n",
       "      <td>0.486927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 results: {'eval_loss': 0.4802541732788086, 'eval_bleu': 0.2135260089579745, 'eval_rouge1': 0.5209566480659632, 'eval_rouge2': 0.29372120823634895, 'eval_rougeL': 0.5116782657622396, 'eval_meteor': 0.48820488273763574, 'eval_runtime': 2.1116, 'eval_samples_per_second': 56.828, 'eval_steps_per_second': 7.103, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 40508.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 2 saved to crossval_T5_TOP5DOCS_SUMMARIZED_fold_2.jsonl\n",
      "Processing Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_14586/3145572364.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='840' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 840/1800 05:11 < 05:57, 2.69 it/s, Epoch 14/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.417400</td>\n",
       "      <td>0.801048</td>\n",
       "      <td>0.095761</td>\n",
       "      <td>0.352721</td>\n",
       "      <td>0.137421</td>\n",
       "      <td>0.348417</td>\n",
       "      <td>0.272684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.764300</td>\n",
       "      <td>0.555041</td>\n",
       "      <td>0.145036</td>\n",
       "      <td>0.417869</td>\n",
       "      <td>0.187483</td>\n",
       "      <td>0.406528</td>\n",
       "      <td>0.368363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.485662</td>\n",
       "      <td>0.166677</td>\n",
       "      <td>0.464391</td>\n",
       "      <td>0.207743</td>\n",
       "      <td>0.452142</td>\n",
       "      <td>0.414841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.498800</td>\n",
       "      <td>0.465997</td>\n",
       "      <td>0.164523</td>\n",
       "      <td>0.467255</td>\n",
       "      <td>0.215336</td>\n",
       "      <td>0.458143</td>\n",
       "      <td>0.424279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.493300</td>\n",
       "      <td>0.454443</td>\n",
       "      <td>0.174174</td>\n",
       "      <td>0.469166</td>\n",
       "      <td>0.220938</td>\n",
       "      <td>0.460548</td>\n",
       "      <td>0.427358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.435700</td>\n",
       "      <td>0.445464</td>\n",
       "      <td>0.178225</td>\n",
       "      <td>0.485080</td>\n",
       "      <td>0.236502</td>\n",
       "      <td>0.474365</td>\n",
       "      <td>0.443416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.464100</td>\n",
       "      <td>0.439881</td>\n",
       "      <td>0.175345</td>\n",
       "      <td>0.484308</td>\n",
       "      <td>0.234134</td>\n",
       "      <td>0.474197</td>\n",
       "      <td>0.438026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.446800</td>\n",
       "      <td>0.438407</td>\n",
       "      <td>0.187259</td>\n",
       "      <td>0.488912</td>\n",
       "      <td>0.242603</td>\n",
       "      <td>0.476630</td>\n",
       "      <td>0.442679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.428900</td>\n",
       "      <td>0.436724</td>\n",
       "      <td>0.186676</td>\n",
       "      <td>0.488738</td>\n",
       "      <td>0.243386</td>\n",
       "      <td>0.479285</td>\n",
       "      <td>0.446805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.414600</td>\n",
       "      <td>0.434739</td>\n",
       "      <td>0.188200</td>\n",
       "      <td>0.485970</td>\n",
       "      <td>0.248061</td>\n",
       "      <td>0.475252</td>\n",
       "      <td>0.448227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.439600</td>\n",
       "      <td>0.432647</td>\n",
       "      <td>0.191484</td>\n",
       "      <td>0.489137</td>\n",
       "      <td>0.249856</td>\n",
       "      <td>0.478485</td>\n",
       "      <td>0.450919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.382400</td>\n",
       "      <td>0.433788</td>\n",
       "      <td>0.179684</td>\n",
       "      <td>0.482556</td>\n",
       "      <td>0.235384</td>\n",
       "      <td>0.472421</td>\n",
       "      <td>0.441930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.414200</td>\n",
       "      <td>0.432732</td>\n",
       "      <td>0.185265</td>\n",
       "      <td>0.491529</td>\n",
       "      <td>0.242147</td>\n",
       "      <td>0.478092</td>\n",
       "      <td>0.446372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.388600</td>\n",
       "      <td>0.433045</td>\n",
       "      <td>0.191464</td>\n",
       "      <td>0.487693</td>\n",
       "      <td>0.249661</td>\n",
       "      <td>0.477232</td>\n",
       "      <td>0.449795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 results: {'eval_loss': 0.4326474964618683, 'eval_bleu': 0.1914836933547287, 'eval_rouge1': 0.48913709117429016, 'eval_rouge2': 0.2498560464342759, 'eval_rougeL': 0.478485452172861, 'eval_meteor': 0.45091889778587657, 'eval_runtime': 2.169, 'eval_samples_per_second': 55.324, 'eval_steps_per_second': 6.916, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 39740.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 3 saved to crossval_T5_TOP5DOCS_SUMMARIZED_fold_3.jsonl\n",
      "Processing Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_14586/3145572364.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1020' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1020/1800 06:17 < 04:49, 2.70 it/s, Epoch 17/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.377000</td>\n",
       "      <td>0.791551</td>\n",
       "      <td>0.090261</td>\n",
       "      <td>0.366213</td>\n",
       "      <td>0.157842</td>\n",
       "      <td>0.356367</td>\n",
       "      <td>0.278005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.704200</td>\n",
       "      <td>0.597976</td>\n",
       "      <td>0.157428</td>\n",
       "      <td>0.418159</td>\n",
       "      <td>0.219276</td>\n",
       "      <td>0.412624</td>\n",
       "      <td>0.369767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.566300</td>\n",
       "      <td>0.506203</td>\n",
       "      <td>0.169720</td>\n",
       "      <td>0.463146</td>\n",
       "      <td>0.223751</td>\n",
       "      <td>0.457998</td>\n",
       "      <td>0.415361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.520500</td>\n",
       "      <td>0.488971</td>\n",
       "      <td>0.176850</td>\n",
       "      <td>0.470499</td>\n",
       "      <td>0.226383</td>\n",
       "      <td>0.463468</td>\n",
       "      <td>0.424061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.500200</td>\n",
       "      <td>0.481674</td>\n",
       "      <td>0.184402</td>\n",
       "      <td>0.477144</td>\n",
       "      <td>0.234679</td>\n",
       "      <td>0.471809</td>\n",
       "      <td>0.427656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.465700</td>\n",
       "      <td>0.474956</td>\n",
       "      <td>0.186629</td>\n",
       "      <td>0.482668</td>\n",
       "      <td>0.238557</td>\n",
       "      <td>0.475631</td>\n",
       "      <td>0.434827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.481300</td>\n",
       "      <td>0.471873</td>\n",
       "      <td>0.190036</td>\n",
       "      <td>0.488868</td>\n",
       "      <td>0.241084</td>\n",
       "      <td>0.482209</td>\n",
       "      <td>0.440292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.456500</td>\n",
       "      <td>0.470310</td>\n",
       "      <td>0.187724</td>\n",
       "      <td>0.484474</td>\n",
       "      <td>0.237553</td>\n",
       "      <td>0.479127</td>\n",
       "      <td>0.436081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.440500</td>\n",
       "      <td>0.469708</td>\n",
       "      <td>0.192194</td>\n",
       "      <td>0.484080</td>\n",
       "      <td>0.240327</td>\n",
       "      <td>0.477736</td>\n",
       "      <td>0.439035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.414500</td>\n",
       "      <td>0.467988</td>\n",
       "      <td>0.191969</td>\n",
       "      <td>0.484945</td>\n",
       "      <td>0.238418</td>\n",
       "      <td>0.477834</td>\n",
       "      <td>0.439417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.395100</td>\n",
       "      <td>0.467141</td>\n",
       "      <td>0.192317</td>\n",
       "      <td>0.485998</td>\n",
       "      <td>0.229999</td>\n",
       "      <td>0.474854</td>\n",
       "      <td>0.436621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.386300</td>\n",
       "      <td>0.467068</td>\n",
       "      <td>0.194684</td>\n",
       "      <td>0.481532</td>\n",
       "      <td>0.237875</td>\n",
       "      <td>0.474804</td>\n",
       "      <td>0.438131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.404600</td>\n",
       "      <td>0.466466</td>\n",
       "      <td>0.188263</td>\n",
       "      <td>0.487101</td>\n",
       "      <td>0.233310</td>\n",
       "      <td>0.476960</td>\n",
       "      <td>0.438797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.385600</td>\n",
       "      <td>0.466139</td>\n",
       "      <td>0.194575</td>\n",
       "      <td>0.482742</td>\n",
       "      <td>0.232682</td>\n",
       "      <td>0.476693</td>\n",
       "      <td>0.437826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.398900</td>\n",
       "      <td>0.466219</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.487013</td>\n",
       "      <td>0.236613</td>\n",
       "      <td>0.478876</td>\n",
       "      <td>0.439632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.400700</td>\n",
       "      <td>0.467383</td>\n",
       "      <td>0.201620</td>\n",
       "      <td>0.487977</td>\n",
       "      <td>0.239830</td>\n",
       "      <td>0.479239</td>\n",
       "      <td>0.438173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.368700</td>\n",
       "      <td>0.467425</td>\n",
       "      <td>0.201982</td>\n",
       "      <td>0.486406</td>\n",
       "      <td>0.237136</td>\n",
       "      <td>0.478852</td>\n",
       "      <td>0.438646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 results: {'eval_loss': 0.4661385715007782, 'eval_bleu': 0.19457517805226662, 'eval_rouge1': 0.482741747158943, 'eval_rouge2': 0.2326822519641224, 'eval_rougeL': 0.4766927359735248, 'eval_meteor': 0.43782645239734896, 'eval_runtime': 2.1092, 'eval_samples_per_second': 56.895, 'eval_steps_per_second': 7.112, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 40143.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 4 saved to crossval_T5_TOP5DOCS_SUMMARIZED_fold_4.jsonl\n",
      "Processing Fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_14586/3145572364.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1020' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1020/1800 06:17 < 04:48, 2.70 it/s, Epoch 17/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.397000</td>\n",
       "      <td>0.794929</td>\n",
       "      <td>0.108309</td>\n",
       "      <td>0.382341</td>\n",
       "      <td>0.161152</td>\n",
       "      <td>0.374477</td>\n",
       "      <td>0.279044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.750700</td>\n",
       "      <td>0.594559</td>\n",
       "      <td>0.149185</td>\n",
       "      <td>0.427434</td>\n",
       "      <td>0.205773</td>\n",
       "      <td>0.421939</td>\n",
       "      <td>0.373557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.619200</td>\n",
       "      <td>0.511499</td>\n",
       "      <td>0.175556</td>\n",
       "      <td>0.485338</td>\n",
       "      <td>0.216709</td>\n",
       "      <td>0.478702</td>\n",
       "      <td>0.424604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.528200</td>\n",
       "      <td>0.491683</td>\n",
       "      <td>0.181541</td>\n",
       "      <td>0.494446</td>\n",
       "      <td>0.229014</td>\n",
       "      <td>0.486793</td>\n",
       "      <td>0.438727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.483400</td>\n",
       "      <td>0.482940</td>\n",
       "      <td>0.194483</td>\n",
       "      <td>0.503939</td>\n",
       "      <td>0.237962</td>\n",
       "      <td>0.495802</td>\n",
       "      <td>0.451597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.453000</td>\n",
       "      <td>0.474159</td>\n",
       "      <td>0.183317</td>\n",
       "      <td>0.500930</td>\n",
       "      <td>0.232308</td>\n",
       "      <td>0.494014</td>\n",
       "      <td>0.447944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.436700</td>\n",
       "      <td>0.468930</td>\n",
       "      <td>0.197238</td>\n",
       "      <td>0.510891</td>\n",
       "      <td>0.241939</td>\n",
       "      <td>0.502802</td>\n",
       "      <td>0.460617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.414400</td>\n",
       "      <td>0.466137</td>\n",
       "      <td>0.202176</td>\n",
       "      <td>0.512554</td>\n",
       "      <td>0.248068</td>\n",
       "      <td>0.502779</td>\n",
       "      <td>0.464847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.411000</td>\n",
       "      <td>0.464585</td>\n",
       "      <td>0.199741</td>\n",
       "      <td>0.516188</td>\n",
       "      <td>0.244048</td>\n",
       "      <td>0.506714</td>\n",
       "      <td>0.466188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.420300</td>\n",
       "      <td>0.463048</td>\n",
       "      <td>0.206423</td>\n",
       "      <td>0.517674</td>\n",
       "      <td>0.246133</td>\n",
       "      <td>0.505992</td>\n",
       "      <td>0.468281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.410700</td>\n",
       "      <td>0.462020</td>\n",
       "      <td>0.204180</td>\n",
       "      <td>0.521864</td>\n",
       "      <td>0.248746</td>\n",
       "      <td>0.510810</td>\n",
       "      <td>0.467678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.402500</td>\n",
       "      <td>0.460841</td>\n",
       "      <td>0.205031</td>\n",
       "      <td>0.527238</td>\n",
       "      <td>0.251864</td>\n",
       "      <td>0.514758</td>\n",
       "      <td>0.470509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.395800</td>\n",
       "      <td>0.460246</td>\n",
       "      <td>0.202611</td>\n",
       "      <td>0.521127</td>\n",
       "      <td>0.252698</td>\n",
       "      <td>0.511057</td>\n",
       "      <td>0.471009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.388500</td>\n",
       "      <td>0.458438</td>\n",
       "      <td>0.205575</td>\n",
       "      <td>0.516602</td>\n",
       "      <td>0.250902</td>\n",
       "      <td>0.508839</td>\n",
       "      <td>0.469133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.367000</td>\n",
       "      <td>0.460046</td>\n",
       "      <td>0.204248</td>\n",
       "      <td>0.523148</td>\n",
       "      <td>0.245730</td>\n",
       "      <td>0.509342</td>\n",
       "      <td>0.469296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.369600</td>\n",
       "      <td>0.460176</td>\n",
       "      <td>0.202020</td>\n",
       "      <td>0.523817</td>\n",
       "      <td>0.248358</td>\n",
       "      <td>0.511206</td>\n",
       "      <td>0.472629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.372400</td>\n",
       "      <td>0.458559</td>\n",
       "      <td>0.200832</td>\n",
       "      <td>0.518609</td>\n",
       "      <td>0.241211</td>\n",
       "      <td>0.505559</td>\n",
       "      <td>0.463901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 results: {'eval_loss': 0.45843765139579773, 'eval_bleu': 0.20557456979612207, 'eval_rouge1': 0.5166019243725455, 'eval_rouge2': 0.25090219860397556, 'eval_rougeL': 0.5088385907134892, 'eval_meteor': 0.4691332159650356, 'eval_runtime': 2.0821, 'eval_samples_per_second': 57.155, 'eval_steps_per_second': 7.204, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:00<00:00, 41414.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 5 saved to crossval_T5_TOP5DOCS_SUMMARIZED_fold_5.jsonl\n",
      "Average metrics over all folds: {'eval_loss': 0.4672218382358551, 'eval_bleu': 0.19918105626307475, 'eval_rouge1': 0.4996960822925914, 'eval_rouge2': 0.251680731174653, 'eval_rougeL': 0.4900307651693466, 'eval_meteor': 0.45689439464484566, 'eval_runtime': 2.12884, 'eval_samples_per_second': 56.2888, 'eval_steps_per_second': 7.048, 'epoch': 15.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Averages/epoch</td><td>▁</td></tr><tr><td>Averages/eval_bleu</td><td>▁</td></tr><tr><td>Averages/eval_loss</td><td>▁</td></tr><tr><td>Averages/eval_meteor</td><td>▁</td></tr><tr><td>Averages/eval_rouge1</td><td>▁</td></tr><tr><td>Averages/eval_rouge2</td><td>▁</td></tr><tr><td>Averages/eval_rougeL</td><td>▁</td></tr><tr><td>Averages/eval_runtime</td><td>▁</td></tr><tr><td>Averages/eval_samples_per_second</td><td>▁</td></tr><tr><td>Averages/eval_steps_per_second</td><td>▁</td></tr><tr><td>Fold_1/eval/epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇██</td></tr><tr><td>Fold_1/eval/eval_bleu</td><td>▁▅▆▇▇▇▇▇▇███▇███</td></tr><tr><td>Fold_1/eval/eval_loss</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_1/eval/eval_meteor</td><td>▁▅▇▇▇▇▇▇████████</td></tr><tr><td>Fold_1/eval/eval_rouge1</td><td>▁▄▇▇▇▇▇▇█▇██████</td></tr><tr><td>Fold_1/eval/eval_rouge2</td><td>▁▅▆▇▇▇▇▇▇██▇▇▇▇▇</td></tr><tr><td>Fold_1/eval/eval_rougeL</td><td>▁▄▇▇▇▇▇▇████████</td></tr><tr><td>Fold_1/eval/eval_runtime</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_1/eval/eval_samples_per_second</td><td>▁█████▇▇█▇██████</td></tr><tr><td>Fold_1/eval/eval_steps_per_second</td><td>▁█████▇▇█▇██████</td></tr><tr><td>Fold_2/eval/epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇███</td></tr><tr><td>Fold_2/eval/eval_bleu</td><td>▁▄▅▅▆▆▆▇▇▇▇▇▇███▇</td></tr><tr><td>Fold_2/eval/eval_loss</td><td>█▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_2/eval/eval_meteor</td><td>▁▅▆▆▆▇▇▇▇▇▇██████</td></tr><tr><td>Fold_2/eval/eval_rouge1</td><td>▁▄▅▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>Fold_2/eval/eval_rouge2</td><td>▁▄▅▅▅▅▆▇▆▇▇▇█████</td></tr><tr><td>Fold_2/eval/eval_rougeL</td><td>▁▄▅▆▆▆▇▇▇▇▇██████</td></tr><tr><td>Fold_2/eval/eval_runtime</td><td>█▇▇▇▁▂▁▁▇▁▂▁▁▁▇▃▂</td></tr><tr><td>Fold_2/eval/eval_samples_per_second</td><td>▁▂▂▂█▇██▂█▇███▂▆▇</td></tr><tr><td>Fold_2/eval/eval_steps_per_second</td><td>▁▂▂▂█▇██▂█▇███▂▆▇</td></tr><tr><td>Fold_3/eval/epoch</td><td>▁▂▂▃▃▄▄▅▅▆▆▇▇██</td></tr><tr><td>Fold_3/eval/eval_bleu</td><td>▁▅▆▆▇▇▇████▇███</td></tr><tr><td>Fold_3/eval/eval_loss</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_3/eval/eval_meteor</td><td>▁▅▇▇▇█▇████████</td></tr><tr><td>Fold_3/eval/eval_rouge1</td><td>▁▄▇▇▇██████████</td></tr><tr><td>Fold_3/eval/eval_rouge2</td><td>▁▄▅▆▆▇▇████▇███</td></tr><tr><td>Fold_3/eval/eval_rougeL</td><td>▁▄▇▇▇██████████</td></tr><tr><td>Fold_3/eval/eval_runtime</td><td>▃▁▁▁█▆▇▅▁▂▁▇▂▂▆</td></tr><tr><td>Fold_3/eval/eval_samples_per_second</td><td>▆███▁▃▂▃█▇█▂▇▇▃</td></tr><tr><td>Fold_3/eval/eval_steps_per_second</td><td>▆███▁▃▂▃█▇█▂▇▇▃</td></tr><tr><td>Fold_4/eval/epoch</td><td>▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇███</td></tr><tr><td>Fold_4/eval/eval_bleu</td><td>▁▅▆▆▇▇▇▇▇▇▇▇▇▇███▇</td></tr><tr><td>Fold_4/eval/eval_loss</td><td>█▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_4/eval/eval_meteor</td><td>▁▅▇▇▇█████████████</td></tr><tr><td>Fold_4/eval/eval_rouge1</td><td>▁▄▇▇▇█████████████</td></tr><tr><td>Fold_4/eval/eval_rouge2</td><td>▁▆▇▇▇█████▇█▇▇███▇</td></tr><tr><td>Fold_4/eval/eval_rougeL</td><td>▁▄▇▇▇█████████████</td></tr><tr><td>Fold_4/eval/eval_runtime</td><td>█▅▄▆▃▄▂▄▁▅▄▇▂▄▁▄▁▇</td></tr><tr><td>Fold_4/eval/eval_samples_per_second</td><td>▁▄▅▃▆▅▇▅█▄▅▂▇▅█▅█▂</td></tr><tr><td>Fold_4/eval/eval_steps_per_second</td><td>▁▄▅▃▆▅▇▅█▄▅▂▇▅█▅█▂</td></tr><tr><td>Fold_5/eval/epoch</td><td>▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇███</td></tr><tr><td>Fold_5/eval/eval_bleu</td><td>▁▄▆▆▇▆▇███████████</td></tr><tr><td>Fold_5/eval/eval_loss</td><td>█▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_5/eval/eval_meteor</td><td>▁▄▆▇▇▇████████████</td></tr><tr><td>Fold_5/eval/eval_rouge1</td><td>▁▃▆▆▇▇▇▇▇████▇███▇</td></tr><tr><td>Fold_5/eval/eval_rouge2</td><td>▁▄▅▆▇▆▇█▇▇████▇█▇█</td></tr><tr><td>Fold_5/eval/eval_rougeL</td><td>▁▃▆▇▇▇▇▇██████████</td></tr><tr><td>Fold_5/eval/eval_runtime</td><td>█▅▅▅▃▇▁▆▃▅▂▅▃▄▃▄▃▄</td></tr><tr><td>Fold_5/eval/eval_samples_per_second</td><td>▁▄▄▄▆▂█▃▆▄▇▄▆▅▆▅▆▅</td></tr><tr><td>Fold_5/eval/eval_steps_per_second</td><td>▁▄▄▄▆▂█▃▆▄▇▄▆▅▆▅▆▅</td></tr><tr><td>eval/bleu</td><td>▁▆▆▆▆▇▇▇▆▇▇▇▇███▁▅▆▇▇▇▅▆▆▆▇▆▇▇▂▄▆▆▇▇▇▇▇█</td></tr><tr><td>eval/loss</td><td>█▃▂▂▂█▄▃▂▂▂▂▂▇▂▁▁▁▁▁▇▂▂▂▂▂▂▂▂▂▄▂▂▂▂▂▂▁▁▁</td></tr><tr><td>eval/meteor</td><td>▂▄▆▆▆▆▆▆▆▂▆▇██████▆▆▆▇▇▇▆▄▆▆▆▆▆▆▆▁▄▆▇▇▇▇</td></tr><tr><td>eval/rouge1</td><td>▂▄▆▆▆▆▆▆▆▄▆▆▇▇▇███▃▆▆▆▆▁▃▆▆▆▆▆▆▆▆▄▇▇████</td></tr><tr><td>eval/rouge2</td><td>▄▅▅▅▅▅▅▅▅▅▆▆▆▇▇█▁▅▅▆▆▆▂▅▅▆▆▆▆▅▂▅▅▆▅▆▆▆▆▆</td></tr><tr><td>eval/rougeL</td><td>▁▃▅▅▅▆▆▆▁▄▇▇▇███▅▆▆▆▆▆▅▆▆▆▆▆▆▆▃▆▇▇██████</td></tr><tr><td>eval/runtime</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/samples_per_second</td><td>▁███▇█▇▇▇█████████▇▇████████████████████</td></tr><tr><td>eval/steps_per_second</td><td>▄▅▅▅▂▃▂▅▆▅▁▁▆▁▆▁▅▆██▇▃▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇</td></tr><tr><td>test/bleu</td><td>▁█▁▂▆</td></tr><tr><td>test/loss</td><td>█▆▁▅▄</td></tr><tr><td>test/meteor</td><td>▁█▃▁▅</td></tr><tr><td>test/rouge1</td><td>▂█▂▁▇</td></tr><tr><td>test/rouge2</td><td>▁█▃▁▃</td></tr><tr><td>test/rougeL</td><td>▁█▂▁▇</td></tr><tr><td>test/runtime</td><td>█▂█▂▁</td></tr><tr><td>test/samples_per_second</td><td>▁█▁██</td></tr><tr><td>test/steps_per_second</td><td>▁▇▁▇█</td></tr><tr><td>train/epoch</td><td>▂▃▄▄▄▇▂▂▂▃▄▇███▁▁▂▂▂▄▄▄▄▅▇▂▅▆▆▆▆▇▇█▂▂▃▃█</td></tr><tr><td>train/global_step</td><td>▃▃▃▃▄▅▅▅▅▆▁▂▂▂▂▃▄▆█▂▃▃▄▅▅▆▁▂▂▂▃▄▅▅▇██▁▄▇</td></tr><tr><td>train/grad_norm</td><td>▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▇▇▆▄▃▃▂▂▇▇▆▅▄▃▃█▇▆▆▆▅▂███▆▆▅▄▃▃▂▂▁▁▆▄▃▃▂</td></tr><tr><td>train/loss</td><td>█▁▁▁▁▁▁█▂▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Averages/epoch</td><td>15.8</td></tr><tr><td>Averages/eval_bleu</td><td>0.19918</td></tr><tr><td>Averages/eval_loss</td><td>0.46722</td></tr><tr><td>Averages/eval_meteor</td><td>0.45689</td></tr><tr><td>Averages/eval_rouge1</td><td>0.4997</td></tr><tr><td>Averages/eval_rouge2</td><td>0.25168</td></tr><tr><td>Averages/eval_rougeL</td><td>0.49003</td></tr><tr><td>Averages/eval_runtime</td><td>2.12884</td></tr><tr><td>Averages/eval_samples_per_second</td><td>56.2888</td></tr><tr><td>Averages/eval_steps_per_second</td><td>7.048</td></tr><tr><td>Fold_1/eval/epoch</td><td>15</td></tr><tr><td>Fold_1/eval/eval_bleu</td><td>0.19075</td></tr><tr><td>Fold_1/eval/eval_loss</td><td>0.49863</td></tr><tr><td>Fold_1/eval/eval_meteor</td><td>0.43839</td></tr><tr><td>Fold_1/eval/eval_rouge1</td><td>0.48904</td></tr><tr><td>Fold_1/eval/eval_rouge2</td><td>0.23124</td></tr><tr><td>Fold_1/eval/eval_rougeL</td><td>0.47446</td></tr><tr><td>Fold_1/eval/eval_runtime</td><td>2.1723</td></tr><tr><td>Fold_1/eval/eval_samples_per_second</td><td>55.242</td></tr><tr><td>Fold_1/eval/eval_steps_per_second</td><td>6.905</td></tr><tr><td>Fold_2/eval/epoch</td><td>16</td></tr><tr><td>Fold_2/eval/eval_bleu</td><td>0.21353</td></tr><tr><td>Fold_2/eval/eval_loss</td><td>0.48025</td></tr><tr><td>Fold_2/eval/eval_meteor</td><td>0.4882</td></tr><tr><td>Fold_2/eval/eval_rouge1</td><td>0.52096</td></tr><tr><td>Fold_2/eval/eval_rouge2</td><td>0.29372</td></tr><tr><td>Fold_2/eval/eval_rougeL</td><td>0.51168</td></tr><tr><td>Fold_2/eval/eval_runtime</td><td>2.1116</td></tr><tr><td>Fold_2/eval/eval_samples_per_second</td><td>56.828</td></tr><tr><td>Fold_2/eval/eval_steps_per_second</td><td>7.103</td></tr><tr><td>Fold_3/eval/epoch</td><td>14</td></tr><tr><td>Fold_3/eval/eval_bleu</td><td>0.19148</td></tr><tr><td>Fold_3/eval/eval_loss</td><td>0.43265</td></tr><tr><td>Fold_3/eval/eval_meteor</td><td>0.45092</td></tr><tr><td>Fold_3/eval/eval_rouge1</td><td>0.48914</td></tr><tr><td>Fold_3/eval/eval_rouge2</td><td>0.24986</td></tr><tr><td>Fold_3/eval/eval_rougeL</td><td>0.47849</td></tr><tr><td>Fold_3/eval/eval_runtime</td><td>2.169</td></tr><tr><td>Fold_3/eval/eval_samples_per_second</td><td>55.324</td></tr><tr><td>Fold_3/eval/eval_steps_per_second</td><td>6.916</td></tr><tr><td>Fold_4/eval/epoch</td><td>17</td></tr><tr><td>Fold_4/eval/eval_bleu</td><td>0.19458</td></tr><tr><td>Fold_4/eval/eval_loss</td><td>0.46614</td></tr><tr><td>Fold_4/eval/eval_meteor</td><td>0.43783</td></tr><tr><td>Fold_4/eval/eval_rouge1</td><td>0.48274</td></tr><tr><td>Fold_4/eval/eval_rouge2</td><td>0.23268</td></tr><tr><td>Fold_4/eval/eval_rougeL</td><td>0.47669</td></tr><tr><td>Fold_4/eval/eval_runtime</td><td>2.1092</td></tr><tr><td>Fold_4/eval/eval_samples_per_second</td><td>56.895</td></tr><tr><td>Fold_4/eval/eval_steps_per_second</td><td>7.112</td></tr><tr><td>Fold_5/eval/epoch</td><td>17</td></tr><tr><td>Fold_5/eval/eval_bleu</td><td>0.20557</td></tr><tr><td>Fold_5/eval/eval_loss</td><td>0.45844</td></tr><tr><td>Fold_5/eval/eval_meteor</td><td>0.46913</td></tr><tr><td>Fold_5/eval/eval_rouge1</td><td>0.5166</td></tr><tr><td>Fold_5/eval/eval_rouge2</td><td>0.2509</td></tr><tr><td>Fold_5/eval/eval_rougeL</td><td>0.50884</td></tr><tr><td>Fold_5/eval/eval_runtime</td><td>2.0821</td></tr><tr><td>Fold_5/eval/eval_samples_per_second</td><td>57.155</td></tr><tr><td>Fold_5/eval/eval_steps_per_second</td><td>7.204</td></tr><tr><td>eval/bleu</td><td>0.20557</td></tr><tr><td>eval/loss</td><td>0.45844</td></tr><tr><td>eval/meteor</td><td>0.46913</td></tr><tr><td>eval/rouge1</td><td>0.5166</td></tr><tr><td>eval/rouge2</td><td>0.2509</td></tr><tr><td>eval/rougeL</td><td>0.50884</td></tr><tr><td>eval/runtime</td><td>2.0821</td></tr><tr><td>eval/samples_per_second</td><td>57.155</td></tr><tr><td>eval/steps_per_second</td><td>7.204</td></tr><tr><td>test/bleu</td><td>0.20557</td></tr><tr><td>test/loss</td><td>0.45844</td></tr><tr><td>test/meteor</td><td>0.46913</td></tr><tr><td>test/rouge1</td><td>0.5166</td></tr><tr><td>test/rouge2</td><td>0.2509</td></tr><tr><td>test/rougeL</td><td>0.50884</td></tr><tr><td>test/runtime</td><td>2.0791</td></tr><tr><td>test/samples_per_second</td><td>57.237</td></tr><tr><td>test/steps_per_second</td><td>7.215</td></tr><tr><td>total_flos</td><td>1104389100011520.0</td></tr><tr><td>train/epoch</td><td>17</td></tr><tr><td>train/global_step</td><td>1020</td></tr><tr><td>train/grad_norm</td><td>0.77074</td></tr><tr><td>train/learning_rate</td><td>2e-05</td></tr><tr><td>train/loss</td><td>0.3724</td></tr><tr><td>train_loss</td><td>0.77424</td></tr><tr><td>train_runtime</td><td>377.2822</td></tr><tr><td>train_samples_per_second</td><td>38.168</td></tr><tr><td>train_steps_per_second</td><td>4.771</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">T5_SMALL_TOP5DOCS_SUMMARIZED</strong> at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/yqrgb5pz' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/yqrgb5pz</a><br> View project at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250510_224706-yqrgb5pz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final predicitons for all folds foudn in crossval_T5_TOP5DOCS_SUMMARIZED.jsonl\n"
     ]
    }
   ],
   "source": [
    "cross_val_train(modelname=\"t5-small\", run_name=\"T5_SMALL_TOP5DOCS_SUMMARIZED\",filename=\"crossval_T5_TOP5DOCS_SUMMARIZED.jsonl\", \n",
    "                tokenizer=tokenizer, tokenized_dataset=tokenized_dataset, \n",
    "                folds=folds, nb_epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4374c301-3125-45ad-b576-8499cd8ff6b0",
   "metadata": {},
   "source": [
    "# HanDLE TFIDF WITH CROSS VAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1448a7d7-2820-4102-a141-85c0128e1a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "#fucntion to tokenize the dataset based on the model\n",
    "def tokenize_and_split_dataset(filename, modelname):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_data = json.load(f)\n",
    "        \n",
    "    # Choose your model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(modelname)\n",
    "\n",
    "    # Tokenization function\n",
    "    def preprocess(batch):\n",
    "        model_inputs = tokenizer(\n",
    "            batch[\"input\"],\n",
    "            max_length=512,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(\n",
    "                batch[\"output\"],\n",
    "                max_length=64,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True\n",
    "            )\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "    \n",
    "    # Convert raw data to Dataset object\n",
    "    dataset = Dataset.from_list(raw_data)\n",
    "    tokenized_dataset = dataset.map(preprocess, batched=True)\n",
    "\n",
    "    return tokenizer, tokenized_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "977de5fe-62bc-47cb-821e-9976ac9293f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "def compute_metrics(eval_pred, tokenizer):\n",
    "    \"\"\"\n",
    "    This function calculates BLEU, ROUGE, and METEOR for the model predictions.\n",
    "    eval_pred: tuple (predictions, references), where predictions are model outputs\n",
    "    and references are the true target labels.\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions= predictions[0]\n",
    "    pred_ids = np.argmax(predictions, axis=-1) \n",
    "    # Decode the model's predicted tokens and true labels\n",
    "    predicted_texts = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    true_texts = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Calculate BLEU score\n",
    "    bleu_score = bleu.compute(predictions=predicted_texts, references=true_texts)\n",
    "    rouge_score = rouge.compute(predictions=predicted_texts, references=true_texts)\n",
    "    meteor_score = meteor.compute(predictions=predicted_texts, references=true_texts)\n",
    "   \n",
    "    # Combine the metrics into a dictionary\n",
    "    return {\n",
    "        \"bleu\": bleu_score[\"bleu\"],\n",
    "        \"rouge1\": rouge_score[\"rouge1\"],\n",
    "        \"rouge2\": rouge_score[\"rouge2\"],\n",
    "        \"rougeL\": rouge_score[\"rougeL\"],\n",
    "        \"meteor\": meteor_score[\"meteor\"]\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d2befff-370a-484c-8f34-93feefd25171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#fucntion to split dataset into folds\n",
    "def generate_folds(tokenized_dataset, n_splits=5, seed=42):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    folds = list(kf.split(tokenized_dataset))\n",
    "    return folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f46c519a-b793-4b88-a1ac-92b008bfce13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81225fe364134ebca527b3a2688fd852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/596 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/Etu0/21402600/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# recuperer les folds pour les indices de chaque fichier tfidf\n",
    "tokenizer_global,tokenized_dataset_global= tokenize_and_split_dataset(\"FR_training_queryonly_qulac_PREPROCESSED_FOR_MODEL.json\",\"t5-small\")\n",
    "folds= generate_folds(tokenized_dataset_global, n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1965ad1b-658c-4871-8727-2bed1fa1c8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([  0,   1,   3,   4,   5,   7,   8,  12,  13,  14,  15,  16,  17,\n",
       "          18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,  31,  32,\n",
       "          33,  34,  35,  36,  37,  38,  40,  41,  42,  43,  44,  45,  46,\n",
       "          47,  48,  49,  50,  51,  52,  53,  54,  56,  57,  58,  59,  61,\n",
       "          62,  64,  65,  66,  67,  68,  69,  71,  74,  75,  80,  85,  87,\n",
       "          88,  89,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 102,\n",
       "         103, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117,\n",
       "         119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 133,\n",
       "         134, 136, 137, 138, 139, 141, 142, 143, 144, 146, 147, 149, 150,\n",
       "         151, 152, 154, 155, 156, 157, 159, 160, 161, 162, 164, 166, 167,\n",
       "         168, 169, 170, 171, 172, 173, 174, 175, 176, 178, 179, 180, 183,\n",
       "         185, 186, 187, 189, 190, 191, 192, 193, 194, 195, 197, 198, 200,\n",
       "         201, 202, 203, 205, 206, 207, 212, 213, 214, 215, 216, 217, 219,\n",
       "         220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 232, 233,\n",
       "         236, 237, 239, 240, 241, 242, 243, 244, 245, 246, 249, 251, 252,\n",
       "         253, 254, 255, 256, 258, 259, 260, 261, 262, 263, 264, 266, 267,\n",
       "         268, 269, 270, 272, 273, 275, 276, 279, 280, 282, 283, 285, 287,\n",
       "         288, 290, 292, 293, 294, 295, 296, 297, 298, 301, 303, 305, 306,\n",
       "         307, 308, 310, 311, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
       "         322, 323, 324, 325, 326, 327, 328, 329, 330, 332, 333, 334, 335,\n",
       "         336, 337, 338, 339, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
       "         350, 351, 353, 356, 358, 359, 360, 362, 363, 365, 366, 369, 370,\n",
       "         371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383,\n",
       "         384, 385, 386, 387, 389, 390, 391, 392, 393, 396, 397, 398, 399,\n",
       "         400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412,\n",
       "         413, 414, 415, 416, 417, 418, 420, 421, 422, 423, 424, 425, 426,\n",
       "         427, 428, 429, 430, 432, 434, 435, 436, 438, 439, 440, 441, 443,\n",
       "         445, 446, 447, 448, 450, 452, 453, 454, 455, 456, 457, 458, 459,\n",
       "         460, 461, 463, 466, 467, 469, 470, 471, 472, 473, 474, 475, 476,\n",
       "         477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
       "         491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 502, 503, 504,\n",
       "         505, 506, 507, 508, 509, 510, 511, 513, 514, 515, 516, 517, 518,\n",
       "         519, 520, 521, 522, 523, 525, 526, 527, 528, 529, 530, 531, 532,\n",
       "         535, 537, 538, 539, 540, 541, 543, 544, 545, 546, 547, 548, 549,\n",
       "         550, 551, 553, 554, 558, 559, 560, 561, 562, 563, 564, 565, 567,\n",
       "         568, 569, 570, 572, 573, 574, 575, 576, 577, 578, 579, 581, 583,\n",
       "         585, 586, 588, 589, 590, 593, 594, 595]),\n",
       "  array([  2,   6,   9,  10,  11,  29,  30,  39,  55,  60,  63,  70,  72,\n",
       "          73,  76,  77,  78,  79,  81,  82,  83,  84,  86,  90, 101, 104,\n",
       "         110, 118, 131, 132, 135, 140, 145, 148, 153, 158, 163, 165, 177,\n",
       "         181, 182, 184, 188, 196, 199, 204, 208, 209, 210, 211, 218, 231,\n",
       "         234, 235, 238, 247, 248, 250, 257, 265, 271, 274, 277, 278, 281,\n",
       "         284, 286, 289, 291, 299, 300, 302, 304, 309, 312, 331, 340, 352,\n",
       "         354, 355, 357, 361, 364, 367, 368, 388, 394, 395, 419, 431, 433,\n",
       "         437, 442, 444, 449, 451, 462, 464, 465, 468, 490, 501, 512, 524,\n",
       "         533, 534, 536, 542, 552, 555, 556, 557, 566, 571, 580, 582, 584,\n",
       "         587, 591, 592])),\n",
       " (array([  1,   2,   3,   4,   5,   6,   8,   9,  10,  11,  12,  13,  14,\n",
       "          16,  17,  20,  21,  23,  24,  26,  27,  28,  29,  30,  31,  32,\n",
       "          34,  35,  36,  37,  38,  39,  40,  41,  43,  44,  45,  47,  48,\n",
       "          50,  51,  52,  53,  55,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "          65,  66,  67,  70,  71,  72,  73,  74,  76,  77,  78,  79,  80,\n",
       "          81,  82,  83,  84,  85,  86,  87,  90,  91,  94,  95,  96,  97,\n",
       "          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 110, 111, 112,\n",
       "         113, 115, 118, 119, 120, 121, 122, 123, 125, 126, 127, 128, 129,\n",
       "         130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 142, 143, 145,\n",
       "         146, 147, 148, 150, 151, 152, 153, 154, 156, 157, 158, 159, 160,\n",
       "         161, 162, 163, 164, 165, 166, 168, 169, 170, 171, 173, 174, 175,\n",
       "         177, 178, 179, 180, 181, 182, 183, 184, 186, 187, 188, 189, 190,\n",
       "         191, 193, 194, 196, 197, 198, 199, 200, 201, 202, 204, 205, 206,\n",
       "         207, 208, 209, 210, 211, 213, 214, 215, 216, 217, 218, 219, 221,\n",
       "         222, 223, 224, 225, 226, 229, 230, 231, 232, 233, 234, 235, 236,\n",
       "         237, 238, 239, 240, 241, 242, 243, 246, 247, 248, 250, 251, 252,\n",
       "         253, 254, 255, 256, 257, 258, 260, 262, 263, 265, 266, 267, 269,\n",
       "         270, 271, 273, 274, 276, 277, 278, 279, 281, 282, 283, 284, 285,\n",
       "         286, 288, 289, 290, 291, 292, 293, 294, 295, 297, 298, 299, 300,\n",
       "         301, 302, 303, 304, 305, 306, 307, 308, 309, 311, 312, 313, 314,\n",
       "         315, 317, 320, 322, 323, 324, 325, 326, 327, 329, 330, 331, 333,\n",
       "         337, 339, 340, 342, 343, 344, 345, 347, 348, 349, 350, 351, 352,\n",
       "         353, 354, 355, 357, 358, 359, 361, 363, 364, 366, 367, 368, 370,\n",
       "         372, 373, 375, 376, 378, 379, 381, 382, 383, 384, 385, 386, 387,\n",
       "         388, 389, 391, 392, 393, 394, 395, 397, 398, 400, 401, 402, 403,\n",
       "         404, 405, 406, 407, 409, 410, 411, 413, 414, 415, 416, 417, 418,\n",
       "         419, 420, 422, 423, 427, 428, 430, 431, 433, 435, 437, 440, 441,\n",
       "         442, 443, 444, 445, 448, 449, 451, 452, 453, 454, 455, 456, 457,\n",
       "         458, 459, 460, 461, 462, 463, 464, 465, 466, 468, 469, 470, 471,\n",
       "         472, 473, 474, 475, 476, 477, 479, 480, 481, 483, 484, 486, 487,\n",
       "         488, 489, 490, 491, 492, 494, 495, 496, 497, 498, 499, 500, 501,\n",
       "         502, 504, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 518,\n",
       "         520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 532, 533,\n",
       "         534, 536, 538, 539, 540, 541, 542, 543, 544, 545, 549, 550, 552,\n",
       "         553, 554, 555, 556, 557, 558, 559, 560, 562, 563, 565, 566, 567,\n",
       "         569, 570, 571, 572, 573, 574, 575, 576, 578, 579, 580, 581, 582,\n",
       "         583, 584, 586, 587, 588, 590, 591, 592, 593]),\n",
       "  array([  0,   7,  15,  18,  19,  22,  25,  33,  42,  46,  49,  54,  56,\n",
       "          68,  69,  75,  88,  89,  92,  93, 108, 109, 114, 116, 117, 124,\n",
       "         137, 141, 144, 149, 155, 167, 172, 176, 185, 192, 195, 203, 212,\n",
       "         220, 227, 228, 244, 245, 249, 259, 261, 264, 268, 272, 275, 280,\n",
       "         287, 296, 310, 316, 318, 319, 321, 328, 332, 334, 335, 336, 338,\n",
       "         341, 346, 356, 360, 362, 365, 369, 371, 374, 377, 380, 390, 396,\n",
       "         399, 408, 412, 421, 424, 425, 426, 429, 432, 434, 436, 438, 439,\n",
       "         446, 447, 450, 467, 478, 482, 485, 493, 503, 505, 516, 517, 519,\n",
       "         531, 535, 537, 546, 547, 548, 551, 561, 564, 568, 577, 585, 589,\n",
       "         594, 595])),\n",
       " (array([  0,   1,   2,   4,   6,   7,   9,  10,  11,  12,  13,  14,  15,\n",
       "          18,  19,  20,  21,  22,  25,  27,  28,  29,  30,  32,  33,  34,\n",
       "          35,  39,  40,  41,  42,  43,  44,  46,  47,  49,  51,  52,  53,\n",
       "          54,  55,  56,  58,  60,  61,  62,  63,  64,  65,  68,  69,  70,\n",
       "          71,  72,  73,  75,  76,  77,  78,  79,  80,  81,  82,  83,  84,\n",
       "          85,  86,  87,  88,  89,  90,  91,  92,  93,  95,  96,  97,  98,\n",
       "          99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 110, 112, 114,\n",
       "         115, 116, 117, 118, 120, 121, 122, 124, 125, 127, 128, 129, 130,\n",
       "         131, 132, 133, 134, 135, 136, 137, 138, 140, 141, 142, 143, 144,\n",
       "         145, 148, 149, 151, 153, 155, 156, 158, 159, 160, 161, 162, 163,\n",
       "         164, 165, 166, 167, 169, 170, 171, 172, 176, 177, 178, 179, 181,\n",
       "         182, 183, 184, 185, 186, 187, 188, 189, 191, 192, 195, 196, 197,\n",
       "         199, 200, 201, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212,\n",
       "         213, 214, 215, 216, 217, 218, 219, 220, 221, 223, 224, 226, 227,\n",
       "         228, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242,\n",
       "         243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 254, 256, 257,\n",
       "         258, 259, 260, 261, 264, 265, 267, 268, 269, 270, 271, 272, 273,\n",
       "         274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286,\n",
       "         287, 288, 289, 291, 292, 293, 294, 295, 296, 299, 300, 302, 304,\n",
       "         306, 308, 309, 310, 312, 313, 314, 315, 316, 317, 318, 319, 321,\n",
       "         323, 324, 326, 327, 328, 330, 331, 332, 334, 335, 336, 337, 338,\n",
       "         339, 340, 341, 342, 343, 345, 346, 347, 351, 352, 354, 355, 356,\n",
       "         357, 358, 359, 360, 361, 362, 364, 365, 366, 367, 368, 369, 371,\n",
       "         372, 373, 374, 376, 377, 378, 379, 380, 384, 385, 386, 387, 388,\n",
       "         389, 390, 391, 392, 394, 395, 396, 397, 398, 399, 401, 402, 406,\n",
       "         408, 409, 410, 411, 412, 413, 414, 417, 418, 419, 421, 422, 423,\n",
       "         424, 425, 426, 427, 429, 430, 431, 432, 433, 434, 435, 436, 437,\n",
       "         438, 439, 440, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451,\n",
       "         452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 464, 465,\n",
       "         466, 467, 468, 470, 471, 473, 474, 475, 476, 478, 480, 482, 484,\n",
       "         485, 487, 489, 490, 491, 492, 493, 495, 496, 498, 500, 501, 502,\n",
       "         503, 504, 505, 508, 509, 510, 512, 513, 514, 516, 517, 518, 519,\n",
       "         520, 521, 522, 524, 525, 529, 531, 533, 534, 535, 536, 537, 538,\n",
       "         539, 540, 541, 542, 543, 546, 547, 548, 549, 550, 551, 552, 554,\n",
       "         555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567,\n",
       "         568, 571, 572, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585,\n",
       "         586, 587, 589, 590, 591, 592, 593, 594, 595]),\n",
       "  array([  3,   5,   8,  16,  17,  23,  24,  26,  31,  36,  37,  38,  45,\n",
       "          48,  50,  57,  59,  66,  67,  74,  94, 103, 111, 113, 119, 123,\n",
       "         126, 139, 146, 147, 150, 152, 154, 157, 168, 173, 174, 175, 180,\n",
       "         190, 193, 194, 198, 207, 222, 225, 229, 237, 253, 255, 262, 263,\n",
       "         266, 290, 297, 298, 301, 303, 305, 307, 311, 320, 322, 325, 329,\n",
       "         333, 344, 348, 349, 350, 353, 363, 370, 375, 381, 382, 383, 393,\n",
       "         400, 403, 404, 405, 407, 415, 416, 420, 428, 441, 463, 469, 472,\n",
       "         477, 479, 481, 483, 486, 488, 494, 497, 499, 506, 507, 511, 515,\n",
       "         523, 526, 527, 528, 530, 532, 544, 545, 553, 569, 570, 573, 574,\n",
       "         575, 588])),\n",
       " (array([  0,   1,   2,   3,   5,   6,   7,   8,   9,  10,  11,  13,  14,\n",
       "          15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
       "          29,  30,  31,  32,  33,  34,  36,  37,  38,  39,  40,  42,  45,\n",
       "          46,  47,  48,  49,  50,  52,  54,  55,  56,  57,  58,  59,  60,\n",
       "          61,  62,  63,  64,  66,  67,  68,  69,  70,  71,  72,  73,  74,\n",
       "          75,  76,  77,  78,  79,  80,  81,  82,  83,  84,  86,  87,  88,\n",
       "          89,  90,  91,  92,  93,  94,  98,  99, 100, 101, 102, 103, 104,\n",
       "         105, 106, 108, 109, 110, 111, 113, 114, 116, 117, 118, 119, 121,\n",
       "         123, 124, 126, 128, 130, 131, 132, 134, 135, 137, 138, 139, 140,\n",
       "         141, 144, 145, 146, 147, 148, 149, 150, 152, 153, 154, 155, 156,\n",
       "         157, 158, 160, 161, 162, 163, 165, 166, 167, 168, 171, 172, 173,\n",
       "         174, 175, 176, 177, 180, 181, 182, 184, 185, 187, 188, 189, 190,\n",
       "         191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 203, 204, 205,\n",
       "         207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 218, 220, 222,\n",
       "         225, 226, 227, 228, 229, 230, 231, 234, 235, 237, 238, 241, 243,\n",
       "         244, 245, 247, 248, 249, 250, 251, 252, 253, 255, 257, 259, 260,\n",
       "         261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273,\n",
       "         274, 275, 276, 277, 278, 280, 281, 284, 286, 287, 288, 289, 290,\n",
       "         291, 292, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305,\n",
       "         307, 308, 309, 310, 311, 312, 313, 315, 316, 318, 319, 320, 321,\n",
       "         322, 325, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
       "         338, 339, 340, 341, 343, 344, 345, 346, 348, 349, 350, 352, 353,\n",
       "         354, 355, 356, 357, 359, 360, 361, 362, 363, 364, 365, 366, 367,\n",
       "         368, 369, 370, 371, 372, 374, 375, 377, 378, 379, 380, 381, 382,\n",
       "         383, 385, 387, 388, 389, 390, 391, 393, 394, 395, 396, 399, 400,\n",
       "         401, 403, 404, 405, 406, 407, 408, 412, 413, 415, 416, 417, 418,\n",
       "         419, 420, 421, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
       "         434, 435, 436, 437, 438, 439, 441, 442, 444, 446, 447, 448, 449,\n",
       "         450, 451, 454, 455, 458, 459, 461, 462, 463, 464, 465, 466, 467,\n",
       "         468, 469, 471, 472, 474, 475, 476, 477, 478, 479, 481, 482, 483,\n",
       "         484, 485, 486, 488, 489, 490, 491, 492, 493, 494, 497, 498, 499,\n",
       "         501, 503, 504, 505, 506, 507, 508, 510, 511, 512, 515, 516, 517,\n",
       "         519, 520, 523, 524, 526, 527, 528, 529, 530, 531, 532, 533, 534,\n",
       "         535, 536, 537, 541, 542, 543, 544, 545, 546, 547, 548, 551, 552,\n",
       "         553, 554, 555, 556, 557, 559, 560, 561, 562, 564, 566, 567, 568,\n",
       "         569, 570, 571, 573, 574, 575, 577, 579, 580, 581, 582, 584, 585,\n",
       "         587, 588, 589, 590, 591, 592, 593, 594, 595]),\n",
       "  array([  4,  12,  28,  35,  41,  43,  44,  51,  53,  65,  85,  95,  96,\n",
       "          97, 107, 112, 115, 120, 122, 125, 127, 129, 133, 136, 142, 143,\n",
       "         151, 159, 164, 169, 170, 178, 179, 183, 186, 197, 202, 206, 217,\n",
       "         219, 221, 223, 224, 232, 233, 236, 239, 240, 242, 246, 254, 256,\n",
       "         258, 279, 282, 283, 285, 293, 294, 306, 314, 317, 323, 324, 326,\n",
       "         342, 347, 351, 358, 373, 376, 384, 386, 392, 397, 398, 402, 409,\n",
       "         410, 411, 414, 422, 423, 440, 443, 445, 452, 453, 456, 457, 460,\n",
       "         470, 473, 480, 487, 495, 496, 500, 502, 509, 513, 514, 518, 521,\n",
       "         522, 525, 538, 539, 540, 549, 550, 558, 563, 565, 572, 576, 578,\n",
       "         583, 586])),\n",
       " (array([  0,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  15,\n",
       "          16,  17,  18,  19,  22,  23,  24,  25,  26,  28,  29,  30,  31,\n",
       "          33,  35,  36,  37,  38,  39,  41,  42,  43,  44,  45,  46,  48,\n",
       "          49,  50,  51,  53,  54,  55,  56,  57,  59,  60,  63,  65,  66,\n",
       "          67,  68,  69,  70,  72,  73,  74,  75,  76,  77,  78,  79,  81,\n",
       "          82,  83,  84,  85,  86,  88,  89,  90,  92,  93,  94,  95,  96,\n",
       "          97, 101, 103, 104, 107, 108, 109, 110, 111, 112, 113, 114, 115,\n",
       "         116, 117, 118, 119, 120, 122, 123, 124, 125, 126, 127, 129, 131,\n",
       "         132, 133, 135, 136, 137, 139, 140, 141, 142, 143, 144, 145, 146,\n",
       "         147, 148, 149, 150, 151, 152, 153, 154, 155, 157, 158, 159, 163,\n",
       "         164, 165, 167, 168, 169, 170, 172, 173, 174, 175, 176, 177, 178,\n",
       "         179, 180, 181, 182, 183, 184, 185, 186, 188, 190, 192, 193, 194,\n",
       "         195, 196, 197, 198, 199, 202, 203, 204, 206, 207, 208, 209, 210,\n",
       "         211, 212, 217, 218, 219, 220, 221, 222, 223, 224, 225, 227, 228,\n",
       "         229, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 242, 244,\n",
       "         245, 246, 247, 248, 249, 250, 253, 254, 255, 256, 257, 258, 259,\n",
       "         261, 262, 263, 264, 265, 266, 268, 271, 272, 274, 275, 277, 278,\n",
       "         279, 280, 281, 282, 283, 284, 285, 286, 287, 289, 290, 291, 293,\n",
       "         294, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
       "         309, 310, 311, 312, 314, 316, 317, 318, 319, 320, 321, 322, 323,\n",
       "         324, 325, 326, 328, 329, 331, 332, 333, 334, 335, 336, 338, 340,\n",
       "         341, 342, 344, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355,\n",
       "         356, 357, 358, 360, 361, 362, 363, 364, 365, 367, 368, 369, 370,\n",
       "         371, 373, 374, 375, 376, 377, 380, 381, 382, 383, 384, 386, 388,\n",
       "         390, 392, 393, 394, 395, 396, 397, 398, 399, 400, 402, 403, 404,\n",
       "         405, 407, 408, 409, 410, 411, 412, 414, 415, 416, 419, 420, 421,\n",
       "         422, 423, 424, 425, 426, 428, 429, 431, 432, 433, 434, 436, 437,\n",
       "         438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 449, 450, 451,\n",
       "         452, 453, 456, 457, 460, 462, 463, 464, 465, 467, 468, 469, 470,\n",
       "         472, 473, 477, 478, 479, 480, 481, 482, 483, 485, 486, 487, 488,\n",
       "         490, 493, 494, 495, 496, 497, 499, 500, 501, 502, 503, 505, 506,\n",
       "         507, 509, 511, 512, 513, 514, 515, 516, 517, 518, 519, 521, 522,\n",
       "         523, 524, 525, 526, 527, 528, 530, 531, 532, 533, 534, 535, 536,\n",
       "         537, 538, 539, 540, 542, 544, 545, 546, 547, 548, 549, 550, 551,\n",
       "         552, 553, 555, 556, 557, 558, 561, 563, 564, 565, 566, 568, 569,\n",
       "         570, 571, 572, 573, 574, 575, 576, 577, 578, 580, 582, 583, 584,\n",
       "         585, 586, 587, 588, 589, 591, 592, 594, 595]),\n",
       "  array([  1,  13,  14,  20,  21,  27,  32,  34,  40,  47,  52,  58,  61,\n",
       "          62,  64,  71,  80,  87,  91,  98,  99, 100, 102, 105, 106, 121,\n",
       "         128, 130, 134, 138, 156, 160, 161, 162, 166, 171, 187, 189, 191,\n",
       "         200, 201, 205, 213, 214, 215, 216, 226, 230, 241, 243, 251, 252,\n",
       "         260, 267, 269, 270, 273, 276, 288, 292, 295, 308, 313, 315, 327,\n",
       "         330, 337, 339, 343, 345, 359, 366, 372, 378, 379, 385, 387, 389,\n",
       "         391, 401, 406, 413, 417, 418, 427, 430, 435, 448, 454, 455, 458,\n",
       "         459, 461, 466, 471, 474, 475, 476, 484, 489, 491, 492, 498, 504,\n",
       "         508, 510, 520, 529, 541, 543, 554, 559, 560, 562, 567, 579, 581,\n",
       "         590, 593]))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92589b26-d9ad-42ec-9a15-258873181d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from transformers import AutoModelForSeq2SeqLM, Trainer, TrainingArguments, EarlyStoppingCallback, TrainerCallback\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Custom callback to log metrics after each evaluation step\n",
    "class WandbLoggingCallback(TrainerCallback):\n",
    "    def __init__(self, fold_num):\n",
    "        self.fold_num = fold_num\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        if state.is_world_process_zero:\n",
    "            log_data = {f\"Fold_{self.fold_num}/eval/{key}\": value for key, value in metrics.items()}\n",
    "            wandb.log(log_data)\n",
    "\n",
    "def cross_val_train(modelname, fold_filename, filename, run_name, tokenizer_global, tokenized_dataset_global, folds, nb_epochs=30):\n",
    "    # Start a single WandB run for all folds\n",
    "    wandb.init(project=\"cross_val_T5\", name=run_name, config={\"epochs\": nb_epochs})\n",
    "    filenamebis= filename.split(\".jsonl\")[0]\n",
    "    fold_metrics = []\n",
    "\n",
    "    for fold_num, (train_idx, test_idx) in enumerate(folds):\n",
    "        print(f\"Processing Fold {fold_num + 1}\")\n",
    "        \n",
    "        #Ici on modifie pour recuperer le bon doc et appliquer le split dessus 3la 7ssab the fold\n",
    "        file_tfdidf=fold_filename\n",
    "        file_tfidf_fold=f\"FOLD{fold_num}_{file_tfdidf}\"\n",
    "        tokenizer,tokenized_dataset= tokenize_and_split_dataset(file_tfidf_fold,modelname)\n",
    "        print(f\"Recuperation du Fichier {file_tfidf_fold} pour le training du Fold {fold_num}\")\n",
    "        \n",
    "        # Split the dataset\n",
    "        train_dataset = tokenized_dataset.select(train_idx)\n",
    "        test_dataset = tokenized_dataset.select(test_idx)\n",
    "\n",
    "        # Initialize the model\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(modelname, from_tf=True)\n",
    "\n",
    "        # Training arguments with early stopping\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"./results/{run_name}/fold_{fold_num+1}\",\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=5e-5,\n",
    "            per_device_train_batch_size=8,\n",
    "            per_device_eval_batch_size=8,\n",
    "            num_train_epochs=nb_epochs,\n",
    "            load_best_model_at_end=True,\n",
    "            logging_steps=20,\n",
    "            logging_first_step=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            seed=42,\n",
    "        )\n",
    "\n",
    "        # Initialize Trainer with the logging callback and early stopping\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=test_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=lambda eval_pred: compute_metrics(eval_pred, tokenizer),\n",
    "            callbacks=[\n",
    "                WandbLoggingCallback(fold_num + 1),\n",
    "                EarlyStoppingCallback(early_stopping_patience=3)\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "\n",
    "        # Final evaluation for the fold\n",
    "        eval_results = trainer.evaluate()\n",
    "        print(f\"Fold {fold_num + 1} results: {eval_results}\")\n",
    "\n",
    "        # Store the final evaluation results for this fold\n",
    "        fold_metrics.append(eval_results)\n",
    "\n",
    "        # Predict on the test dataset\n",
    "        predictions = trainer.predict(test_dataset)\n",
    "\n",
    "    \n",
    "        # Decode predictions and save to file\n",
    "        pred_ids = np.argmax(predictions.predictions[0], axis=-1)\n",
    "        predicted_texts = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "        true_texts = tokenizer.batch_decode(predictions.label_ids, skip_special_tokens=True)\n",
    "        input_ids = test_dataset[\"input_ids\"]\n",
    "        input_texts = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Save predictions to a file\n",
    "\n",
    "        output_predictions_file = f\"{filenamebis}_fold_{fold_num+1}.jsonl\"\n",
    "        with open(output_predictions_file, \"w\", encoding=\"utf-8\") as writer:\n",
    "            for input_text, true_text, predicted_text in tqdm(zip(input_texts, true_texts, predicted_texts), total=len(true_texts)):\n",
    "                output_dict = {\n",
    "                    \"input\": input_text,\n",
    "                    \"true\": true_text,\n",
    "                    \"predicted\": predicted_text\n",
    "                }\n",
    "                writer.write(json.dumps(output_dict, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "        print(f\"Predictions for fold {fold_num + 1} saved to {output_predictions_file}\")\n",
    "\n",
    "    # Calculate and log average metrics\n",
    "    avg_metrics = {key: np.mean([fold[key] for fold in fold_metrics]) for key in fold_metrics[0]}\n",
    "    wandb.log({f\"Averages/{key}\": value for key, value in avg_metrics.items()})\n",
    "    print(\"Average metrics over all folds:\", avg_metrics)\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    #merge all to a single file for predcitions\n",
    "\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as merged_file:\n",
    "        for fold_num in range(len(folds)):\n",
    "            fold_file = f\"{filenamebis}_fold_{fold_num+1}.jsonl\"\n",
    "            if os.path.exists(fold_file):\n",
    "                with open(fold_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                    for line in f:\n",
    "                        merged_file.write(line)\n",
    "    print(f\"Final predicitons for all folds foudn in {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48612e7a-f4c4-4bac-8851-a0327efad7a0",
   "metadata": {},
   "source": [
    "# TEST TOP 5 DOCS TFDIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a4e485b-e8a1-4213-bf27-b54e97c880fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/users/nfs/Etu0/21402600/wandb/run-20250513_001739-vnunwory</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/vnunwory' target=\"_blank\">FR_T5_SMALL_TOP5DOCS_TFIDF</a></strong> to <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/vnunwory' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/vnunwory</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Fold 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336f753a27504f7b932cdd3b86eaa11f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/596 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recuperation du Fichier FOLD0_FR_training_top5_qulac_tfidf_CLEANED_rep_PREPROCESSED_FOR_MODEL.json pour le training du Fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1747088260.959222   56665 gpu_device.cc:2341] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_56665/3001149367.py:59: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1080' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1080/1500 07:11 < 02:48, 2.50 it/s, Epoch 18/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.438300</td>\n",
       "      <td>0.883603</td>\n",
       "      <td>0.111675</td>\n",
       "      <td>0.377948</td>\n",
       "      <td>0.172986</td>\n",
       "      <td>0.369415</td>\n",
       "      <td>0.301083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.889600</td>\n",
       "      <td>0.715853</td>\n",
       "      <td>0.120112</td>\n",
       "      <td>0.410543</td>\n",
       "      <td>0.188912</td>\n",
       "      <td>0.401559</td>\n",
       "      <td>0.338239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.690400</td>\n",
       "      <td>0.631655</td>\n",
       "      <td>0.162895</td>\n",
       "      <td>0.463888</td>\n",
       "      <td>0.232063</td>\n",
       "      <td>0.451113</td>\n",
       "      <td>0.395789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.646200</td>\n",
       "      <td>0.614955</td>\n",
       "      <td>0.169162</td>\n",
       "      <td>0.472976</td>\n",
       "      <td>0.247314</td>\n",
       "      <td>0.460721</td>\n",
       "      <td>0.415384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.599800</td>\n",
       "      <td>0.598372</td>\n",
       "      <td>0.170783</td>\n",
       "      <td>0.474768</td>\n",
       "      <td>0.249981</td>\n",
       "      <td>0.461383</td>\n",
       "      <td>0.424687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.592600</td>\n",
       "      <td>0.589590</td>\n",
       "      <td>0.174162</td>\n",
       "      <td>0.483400</td>\n",
       "      <td>0.251815</td>\n",
       "      <td>0.468180</td>\n",
       "      <td>0.428656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.570100</td>\n",
       "      <td>0.587388</td>\n",
       "      <td>0.180645</td>\n",
       "      <td>0.487023</td>\n",
       "      <td>0.261762</td>\n",
       "      <td>0.473955</td>\n",
       "      <td>0.438592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.549500</td>\n",
       "      <td>0.583594</td>\n",
       "      <td>0.181274</td>\n",
       "      <td>0.485835</td>\n",
       "      <td>0.261197</td>\n",
       "      <td>0.473159</td>\n",
       "      <td>0.435626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.570400</td>\n",
       "      <td>0.583231</td>\n",
       "      <td>0.186528</td>\n",
       "      <td>0.487300</td>\n",
       "      <td>0.263641</td>\n",
       "      <td>0.476203</td>\n",
       "      <td>0.437577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.531800</td>\n",
       "      <td>0.581274</td>\n",
       "      <td>0.183078</td>\n",
       "      <td>0.487451</td>\n",
       "      <td>0.264657</td>\n",
       "      <td>0.476167</td>\n",
       "      <td>0.437838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.506900</td>\n",
       "      <td>0.576769</td>\n",
       "      <td>0.183314</td>\n",
       "      <td>0.487605</td>\n",
       "      <td>0.264171</td>\n",
       "      <td>0.477712</td>\n",
       "      <td>0.434750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.485600</td>\n",
       "      <td>0.577168</td>\n",
       "      <td>0.186701</td>\n",
       "      <td>0.490206</td>\n",
       "      <td>0.269077</td>\n",
       "      <td>0.480062</td>\n",
       "      <td>0.444470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.488600</td>\n",
       "      <td>0.575254</td>\n",
       "      <td>0.191386</td>\n",
       "      <td>0.492176</td>\n",
       "      <td>0.270473</td>\n",
       "      <td>0.481670</td>\n",
       "      <td>0.444392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.446300</td>\n",
       "      <td>0.575928</td>\n",
       "      <td>0.193868</td>\n",
       "      <td>0.492554</td>\n",
       "      <td>0.275743</td>\n",
       "      <td>0.481370</td>\n",
       "      <td>0.447322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.482200</td>\n",
       "      <td>0.574018</td>\n",
       "      <td>0.193203</td>\n",
       "      <td>0.494024</td>\n",
       "      <td>0.277288</td>\n",
       "      <td>0.483234</td>\n",
       "      <td>0.446632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.474500</td>\n",
       "      <td>0.574181</td>\n",
       "      <td>0.192606</td>\n",
       "      <td>0.493816</td>\n",
       "      <td>0.276338</td>\n",
       "      <td>0.483161</td>\n",
       "      <td>0.448157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.455100</td>\n",
       "      <td>0.575923</td>\n",
       "      <td>0.197624</td>\n",
       "      <td>0.495979</td>\n",
       "      <td>0.279067</td>\n",
       "      <td>0.485025</td>\n",
       "      <td>0.453605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.482200</td>\n",
       "      <td>0.574992</td>\n",
       "      <td>0.192929</td>\n",
       "      <td>0.492125</td>\n",
       "      <td>0.272675</td>\n",
       "      <td>0.481257</td>\n",
       "      <td>0.449931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 results: {'eval_loss': 0.5740179419517517, 'eval_bleu': 0.19320296363114337, 'eval_rouge1': 0.49402381093956016, 'eval_rouge2': 0.2772884254470274, 'eval_rougeL': 0.4832339615726424, 'eval_meteor': 0.44663221802504166, 'eval_runtime': 2.1194, 'eval_samples_per_second': 56.619, 'eval_steps_per_second': 7.077, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 53732.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 1 saved to FR_crossval_T5_SMALL_TOP5DOCS_TFIDF_fold_1.jsonl\n",
      "Processing Fold 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc3f11688cd74beba86dcbff929d54c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/596 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/Etu0/21402600/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recuperation du Fichier FOLD1_FR_training_top5_qulac_tfidf_CLEANED_rep_PREPROCESSED_FOR_MODEL.json pour le training du Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_56665/3001149367.py:59: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1380' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1380/1500 08:45 < 00:45, 2.62 it/s, Epoch 23/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.361800</td>\n",
       "      <td>0.852836</td>\n",
       "      <td>0.103116</td>\n",
       "      <td>0.386282</td>\n",
       "      <td>0.171521</td>\n",
       "      <td>0.378438</td>\n",
       "      <td>0.312096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.711263</td>\n",
       "      <td>0.125938</td>\n",
       "      <td>0.418177</td>\n",
       "      <td>0.194125</td>\n",
       "      <td>0.409841</td>\n",
       "      <td>0.347324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.694500</td>\n",
       "      <td>0.622502</td>\n",
       "      <td>0.161188</td>\n",
       "      <td>0.464142</td>\n",
       "      <td>0.237151</td>\n",
       "      <td>0.453725</td>\n",
       "      <td>0.404093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.627600</td>\n",
       "      <td>0.592968</td>\n",
       "      <td>0.167058</td>\n",
       "      <td>0.476888</td>\n",
       "      <td>0.244924</td>\n",
       "      <td>0.467152</td>\n",
       "      <td>0.423433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.615400</td>\n",
       "      <td>0.581595</td>\n",
       "      <td>0.178459</td>\n",
       "      <td>0.483109</td>\n",
       "      <td>0.252853</td>\n",
       "      <td>0.473137</td>\n",
       "      <td>0.430006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.572600</td>\n",
       "      <td>0.570857</td>\n",
       "      <td>0.186939</td>\n",
       "      <td>0.496189</td>\n",
       "      <td>0.265681</td>\n",
       "      <td>0.487266</td>\n",
       "      <td>0.440721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.573500</td>\n",
       "      <td>0.565251</td>\n",
       "      <td>0.184101</td>\n",
       "      <td>0.498614</td>\n",
       "      <td>0.264602</td>\n",
       "      <td>0.485725</td>\n",
       "      <td>0.448189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.574000</td>\n",
       "      <td>0.560380</td>\n",
       "      <td>0.186810</td>\n",
       "      <td>0.499100</td>\n",
       "      <td>0.267380</td>\n",
       "      <td>0.487956</td>\n",
       "      <td>0.447096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.576900</td>\n",
       "      <td>0.558778</td>\n",
       "      <td>0.186020</td>\n",
       "      <td>0.491594</td>\n",
       "      <td>0.256555</td>\n",
       "      <td>0.481314</td>\n",
       "      <td>0.448923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.551200</td>\n",
       "      <td>0.555526</td>\n",
       "      <td>0.191646</td>\n",
       "      <td>0.499294</td>\n",
       "      <td>0.263310</td>\n",
       "      <td>0.488567</td>\n",
       "      <td>0.453782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.496800</td>\n",
       "      <td>0.552994</td>\n",
       "      <td>0.181254</td>\n",
       "      <td>0.493263</td>\n",
       "      <td>0.256565</td>\n",
       "      <td>0.485299</td>\n",
       "      <td>0.451610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.513300</td>\n",
       "      <td>0.547941</td>\n",
       "      <td>0.184227</td>\n",
       "      <td>0.497092</td>\n",
       "      <td>0.263860</td>\n",
       "      <td>0.487849</td>\n",
       "      <td>0.453786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.533000</td>\n",
       "      <td>0.548745</td>\n",
       "      <td>0.187447</td>\n",
       "      <td>0.499849</td>\n",
       "      <td>0.265280</td>\n",
       "      <td>0.489670</td>\n",
       "      <td>0.459822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.536900</td>\n",
       "      <td>0.547457</td>\n",
       "      <td>0.190293</td>\n",
       "      <td>0.500688</td>\n",
       "      <td>0.266737</td>\n",
       "      <td>0.490388</td>\n",
       "      <td>0.461380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.477400</td>\n",
       "      <td>0.547391</td>\n",
       "      <td>0.186759</td>\n",
       "      <td>0.498666</td>\n",
       "      <td>0.261696</td>\n",
       "      <td>0.487352</td>\n",
       "      <td>0.458203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.476200</td>\n",
       "      <td>0.546285</td>\n",
       "      <td>0.186964</td>\n",
       "      <td>0.499674</td>\n",
       "      <td>0.262692</td>\n",
       "      <td>0.489869</td>\n",
       "      <td>0.461798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.488500</td>\n",
       "      <td>0.544836</td>\n",
       "      <td>0.191937</td>\n",
       "      <td>0.501520</td>\n",
       "      <td>0.266153</td>\n",
       "      <td>0.489838</td>\n",
       "      <td>0.461194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.496000</td>\n",
       "      <td>0.544108</td>\n",
       "      <td>0.193103</td>\n",
       "      <td>0.501967</td>\n",
       "      <td>0.267141</td>\n",
       "      <td>0.489787</td>\n",
       "      <td>0.461271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.485100</td>\n",
       "      <td>0.543644</td>\n",
       "      <td>0.189308</td>\n",
       "      <td>0.499267</td>\n",
       "      <td>0.265050</td>\n",
       "      <td>0.488215</td>\n",
       "      <td>0.456418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.456500</td>\n",
       "      <td>0.542639</td>\n",
       "      <td>0.187398</td>\n",
       "      <td>0.500811</td>\n",
       "      <td>0.261411</td>\n",
       "      <td>0.488763</td>\n",
       "      <td>0.459604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.483900</td>\n",
       "      <td>0.543645</td>\n",
       "      <td>0.187525</td>\n",
       "      <td>0.497891</td>\n",
       "      <td>0.261567</td>\n",
       "      <td>0.488055</td>\n",
       "      <td>0.457921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.420500</td>\n",
       "      <td>0.543262</td>\n",
       "      <td>0.185857</td>\n",
       "      <td>0.493440</td>\n",
       "      <td>0.258462</td>\n",
       "      <td>0.484116</td>\n",
       "      <td>0.455351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.457300</td>\n",
       "      <td>0.542938</td>\n",
       "      <td>0.185914</td>\n",
       "      <td>0.494522</td>\n",
       "      <td>0.259711</td>\n",
       "      <td>0.484688</td>\n",
       "      <td>0.455365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 results: {'eval_loss': 0.5426387190818787, 'eval_bleu': 0.1873981425473677, 'eval_rouge1': 0.5008114579198601, 'eval_rouge2': 0.26141098674883534, 'eval_rougeL': 0.4887629600891783, 'eval_meteor': 0.45960416985056235, 'eval_runtime': 2.1094, 'eval_samples_per_second': 56.413, 'eval_steps_per_second': 7.111, 'epoch': 23.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 119/119 [00:00<00:00, 53143.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 2 saved to FR_crossval_T5_SMALL_TOP5DOCS_TFIDF_fold_2.jsonl\n",
      "Processing Fold 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae98945bfb64a51a5246729caf0de67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/596 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/Etu0/21402600/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recuperation du Fichier FOLD2_FR_training_top5_qulac_tfidf_CLEANED_rep_PREPROCESSED_FOR_MODEL.json pour le training du Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_56665/3001149367.py:59: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1260' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1260/1500 07:52 < 01:30, 2.66 it/s, Epoch 21/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.369000</td>\n",
       "      <td>0.854148</td>\n",
       "      <td>0.083450</td>\n",
       "      <td>0.356568</td>\n",
       "      <td>0.141951</td>\n",
       "      <td>0.348184</td>\n",
       "      <td>0.276086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.887200</td>\n",
       "      <td>0.698422</td>\n",
       "      <td>0.097190</td>\n",
       "      <td>0.379966</td>\n",
       "      <td>0.158591</td>\n",
       "      <td>0.369018</td>\n",
       "      <td>0.313265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.745400</td>\n",
       "      <td>0.619613</td>\n",
       "      <td>0.114555</td>\n",
       "      <td>0.423660</td>\n",
       "      <td>0.196684</td>\n",
       "      <td>0.409245</td>\n",
       "      <td>0.369933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.662900</td>\n",
       "      <td>0.593468</td>\n",
       "      <td>0.122473</td>\n",
       "      <td>0.440710</td>\n",
       "      <td>0.201133</td>\n",
       "      <td>0.428658</td>\n",
       "      <td>0.377999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.626400</td>\n",
       "      <td>0.576535</td>\n",
       "      <td>0.137602</td>\n",
       "      <td>0.446939</td>\n",
       "      <td>0.213757</td>\n",
       "      <td>0.437431</td>\n",
       "      <td>0.394597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.586000</td>\n",
       "      <td>0.565055</td>\n",
       "      <td>0.137780</td>\n",
       "      <td>0.447560</td>\n",
       "      <td>0.214223</td>\n",
       "      <td>0.438091</td>\n",
       "      <td>0.399236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.592000</td>\n",
       "      <td>0.556793</td>\n",
       "      <td>0.155272</td>\n",
       "      <td>0.461047</td>\n",
       "      <td>0.226015</td>\n",
       "      <td>0.450269</td>\n",
       "      <td>0.422751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.543200</td>\n",
       "      <td>0.551870</td>\n",
       "      <td>0.153210</td>\n",
       "      <td>0.461994</td>\n",
       "      <td>0.228380</td>\n",
       "      <td>0.452664</td>\n",
       "      <td>0.419928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.583500</td>\n",
       "      <td>0.545046</td>\n",
       "      <td>0.157596</td>\n",
       "      <td>0.471306</td>\n",
       "      <td>0.230816</td>\n",
       "      <td>0.460901</td>\n",
       "      <td>0.422802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.515700</td>\n",
       "      <td>0.542913</td>\n",
       "      <td>0.157468</td>\n",
       "      <td>0.471271</td>\n",
       "      <td>0.228395</td>\n",
       "      <td>0.461112</td>\n",
       "      <td>0.415896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.499800</td>\n",
       "      <td>0.540343</td>\n",
       "      <td>0.161674</td>\n",
       "      <td>0.477895</td>\n",
       "      <td>0.238242</td>\n",
       "      <td>0.467144</td>\n",
       "      <td>0.431608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.519200</td>\n",
       "      <td>0.539181</td>\n",
       "      <td>0.158302</td>\n",
       "      <td>0.472285</td>\n",
       "      <td>0.234910</td>\n",
       "      <td>0.460821</td>\n",
       "      <td>0.429378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.532300</td>\n",
       "      <td>0.535296</td>\n",
       "      <td>0.161501</td>\n",
       "      <td>0.476626</td>\n",
       "      <td>0.237952</td>\n",
       "      <td>0.466886</td>\n",
       "      <td>0.431841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.508900</td>\n",
       "      <td>0.534746</td>\n",
       "      <td>0.166869</td>\n",
       "      <td>0.479148</td>\n",
       "      <td>0.242849</td>\n",
       "      <td>0.469236</td>\n",
       "      <td>0.436236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.499600</td>\n",
       "      <td>0.533478</td>\n",
       "      <td>0.171126</td>\n",
       "      <td>0.478285</td>\n",
       "      <td>0.240178</td>\n",
       "      <td>0.466821</td>\n",
       "      <td>0.438259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.499100</td>\n",
       "      <td>0.531957</td>\n",
       "      <td>0.175564</td>\n",
       "      <td>0.483389</td>\n",
       "      <td>0.242792</td>\n",
       "      <td>0.471022</td>\n",
       "      <td>0.440433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.460900</td>\n",
       "      <td>0.532736</td>\n",
       "      <td>0.175326</td>\n",
       "      <td>0.484971</td>\n",
       "      <td>0.246877</td>\n",
       "      <td>0.473249</td>\n",
       "      <td>0.446597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.504600</td>\n",
       "      <td>0.531171</td>\n",
       "      <td>0.175550</td>\n",
       "      <td>0.481640</td>\n",
       "      <td>0.243678</td>\n",
       "      <td>0.469451</td>\n",
       "      <td>0.443960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>0.531331</td>\n",
       "      <td>0.181117</td>\n",
       "      <td>0.482129</td>\n",
       "      <td>0.248445</td>\n",
       "      <td>0.470466</td>\n",
       "      <td>0.447353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.453800</td>\n",
       "      <td>0.531329</td>\n",
       "      <td>0.174412</td>\n",
       "      <td>0.481715</td>\n",
       "      <td>0.242335</td>\n",
       "      <td>0.469099</td>\n",
       "      <td>0.443180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.506600</td>\n",
       "      <td>0.531678</td>\n",
       "      <td>0.178575</td>\n",
       "      <td>0.481882</td>\n",
       "      <td>0.245082</td>\n",
       "      <td>0.469810</td>\n",
       "      <td>0.444897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 results: {'eval_loss': 0.5311710238456726, 'eval_bleu': 0.17555040645322237, 'eval_rouge1': 0.4816395921046086, 'eval_rouge2': 0.24367836418476257, 'eval_rougeL': 0.4694512046072364, 'eval_meteor': 0.4439596408898649, 'eval_runtime': 2.0864, 'eval_samples_per_second': 57.037, 'eval_steps_per_second': 7.19, 'epoch': 21.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 119/119 [00:00<00:00, 56455.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 3 saved to FR_crossval_T5_SMALL_TOP5DOCS_TFIDF_fold_3.jsonl\n",
      "Processing Fold 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b1b78a7ab6e4bee91bd5ef65d121ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/596 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/Etu0/21402600/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recuperation du Fichier FOLD3_FR_training_top5_qulac_tfidf_CLEANED_rep_PREPROCESSED_FOR_MODEL.json pour le training du Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_56665/3001149367.py:59: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 09:29, Epoch 25/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.335500</td>\n",
       "      <td>0.898214</td>\n",
       "      <td>0.110014</td>\n",
       "      <td>0.383970</td>\n",
       "      <td>0.169345</td>\n",
       "      <td>0.372973</td>\n",
       "      <td>0.295120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.856900</td>\n",
       "      <td>0.714580</td>\n",
       "      <td>0.127406</td>\n",
       "      <td>0.411403</td>\n",
       "      <td>0.191093</td>\n",
       "      <td>0.400483</td>\n",
       "      <td>0.329396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.699700</td>\n",
       "      <td>0.646456</td>\n",
       "      <td>0.151453</td>\n",
       "      <td>0.454414</td>\n",
       "      <td>0.226390</td>\n",
       "      <td>0.444263</td>\n",
       "      <td>0.382336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.641200</td>\n",
       "      <td>0.623805</td>\n",
       "      <td>0.162488</td>\n",
       "      <td>0.464951</td>\n",
       "      <td>0.233446</td>\n",
       "      <td>0.456751</td>\n",
       "      <td>0.406473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.611800</td>\n",
       "      <td>0.609629</td>\n",
       "      <td>0.179540</td>\n",
       "      <td>0.478654</td>\n",
       "      <td>0.251201</td>\n",
       "      <td>0.471706</td>\n",
       "      <td>0.429199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.580400</td>\n",
       "      <td>0.595688</td>\n",
       "      <td>0.183705</td>\n",
       "      <td>0.479181</td>\n",
       "      <td>0.254302</td>\n",
       "      <td>0.471954</td>\n",
       "      <td>0.431814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.571800</td>\n",
       "      <td>0.589981</td>\n",
       "      <td>0.175670</td>\n",
       "      <td>0.481282</td>\n",
       "      <td>0.252452</td>\n",
       "      <td>0.472970</td>\n",
       "      <td>0.437273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.568800</td>\n",
       "      <td>0.584588</td>\n",
       "      <td>0.183194</td>\n",
       "      <td>0.487094</td>\n",
       "      <td>0.253531</td>\n",
       "      <td>0.476247</td>\n",
       "      <td>0.442734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.554600</td>\n",
       "      <td>0.579958</td>\n",
       "      <td>0.187994</td>\n",
       "      <td>0.485687</td>\n",
       "      <td>0.251878</td>\n",
       "      <td>0.474225</td>\n",
       "      <td>0.443919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.528000</td>\n",
       "      <td>0.575324</td>\n",
       "      <td>0.192481</td>\n",
       "      <td>0.492650</td>\n",
       "      <td>0.260117</td>\n",
       "      <td>0.480627</td>\n",
       "      <td>0.450252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.520900</td>\n",
       "      <td>0.572550</td>\n",
       "      <td>0.198096</td>\n",
       "      <td>0.496362</td>\n",
       "      <td>0.264845</td>\n",
       "      <td>0.483116</td>\n",
       "      <td>0.451856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.526600</td>\n",
       "      <td>0.571409</td>\n",
       "      <td>0.192082</td>\n",
       "      <td>0.491579</td>\n",
       "      <td>0.257337</td>\n",
       "      <td>0.480486</td>\n",
       "      <td>0.445942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.510200</td>\n",
       "      <td>0.569245</td>\n",
       "      <td>0.200373</td>\n",
       "      <td>0.498259</td>\n",
       "      <td>0.267119</td>\n",
       "      <td>0.487947</td>\n",
       "      <td>0.455441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.498400</td>\n",
       "      <td>0.569354</td>\n",
       "      <td>0.196421</td>\n",
       "      <td>0.494507</td>\n",
       "      <td>0.262782</td>\n",
       "      <td>0.485594</td>\n",
       "      <td>0.450660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.502900</td>\n",
       "      <td>0.567281</td>\n",
       "      <td>0.201001</td>\n",
       "      <td>0.498376</td>\n",
       "      <td>0.267360</td>\n",
       "      <td>0.490672</td>\n",
       "      <td>0.456311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.480800</td>\n",
       "      <td>0.566388</td>\n",
       "      <td>0.199417</td>\n",
       "      <td>0.499860</td>\n",
       "      <td>0.269488</td>\n",
       "      <td>0.492097</td>\n",
       "      <td>0.458633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.470200</td>\n",
       "      <td>0.564261</td>\n",
       "      <td>0.196503</td>\n",
       "      <td>0.500319</td>\n",
       "      <td>0.266020</td>\n",
       "      <td>0.491995</td>\n",
       "      <td>0.456647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.496400</td>\n",
       "      <td>0.564837</td>\n",
       "      <td>0.195954</td>\n",
       "      <td>0.500923</td>\n",
       "      <td>0.263813</td>\n",
       "      <td>0.491626</td>\n",
       "      <td>0.458901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.473900</td>\n",
       "      <td>0.564324</td>\n",
       "      <td>0.196082</td>\n",
       "      <td>0.501975</td>\n",
       "      <td>0.262989</td>\n",
       "      <td>0.491674</td>\n",
       "      <td>0.457800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.453900</td>\n",
       "      <td>0.563731</td>\n",
       "      <td>0.196074</td>\n",
       "      <td>0.502232</td>\n",
       "      <td>0.262920</td>\n",
       "      <td>0.493544</td>\n",
       "      <td>0.458107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.472200</td>\n",
       "      <td>0.562825</td>\n",
       "      <td>0.196142</td>\n",
       "      <td>0.502216</td>\n",
       "      <td>0.263493</td>\n",
       "      <td>0.492511</td>\n",
       "      <td>0.456638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.482500</td>\n",
       "      <td>0.561930</td>\n",
       "      <td>0.195783</td>\n",
       "      <td>0.501806</td>\n",
       "      <td>0.264210</td>\n",
       "      <td>0.492112</td>\n",
       "      <td>0.455438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.441000</td>\n",
       "      <td>0.562591</td>\n",
       "      <td>0.196422</td>\n",
       "      <td>0.502563</td>\n",
       "      <td>0.265151</td>\n",
       "      <td>0.493002</td>\n",
       "      <td>0.457189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.433500</td>\n",
       "      <td>0.562640</td>\n",
       "      <td>0.197002</td>\n",
       "      <td>0.503897</td>\n",
       "      <td>0.266834</td>\n",
       "      <td>0.495086</td>\n",
       "      <td>0.459146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.451500</td>\n",
       "      <td>0.562553</td>\n",
       "      <td>0.196909</td>\n",
       "      <td>0.504177</td>\n",
       "      <td>0.266134</td>\n",
       "      <td>0.494917</td>\n",
       "      <td>0.459386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 results: {'eval_loss': 0.5619302988052368, 'eval_bleu': 0.19578265031334005, 'eval_rouge1': 0.5018060756604166, 'eval_rouge2': 0.26421018029421517, 'eval_rougeL': 0.49211244004022403, 'eval_meteor': 0.4554375850601602, 'eval_runtime': 2.5263, 'eval_samples_per_second': 47.104, 'eval_steps_per_second': 5.937, 'epoch': 25.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 119/119 [00:00<00:00, 54836.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 4 saved to FR_crossval_T5_SMALL_TOP5DOCS_TFIDF_fold_4.jsonl\n",
      "Processing Fold 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f81d202d3a475b87b525b44c50fd7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/596 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/Etu0/21402600/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recuperation du Fichier FOLD4_FR_training_top5_qulac_tfidf_CLEANED_rep_PREPROCESSED_FOR_MODEL.json pour le training du Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_56665/3001149367.py:59: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 09:49, Epoch 25/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.380200</td>\n",
       "      <td>0.860477</td>\n",
       "      <td>0.100930</td>\n",
       "      <td>0.387181</td>\n",
       "      <td>0.162646</td>\n",
       "      <td>0.371686</td>\n",
       "      <td>0.293010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.838400</td>\n",
       "      <td>0.703810</td>\n",
       "      <td>0.129808</td>\n",
       "      <td>0.448184</td>\n",
       "      <td>0.216908</td>\n",
       "      <td>0.433890</td>\n",
       "      <td>0.382828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.681100</td>\n",
       "      <td>0.639001</td>\n",
       "      <td>0.143929</td>\n",
       "      <td>0.471112</td>\n",
       "      <td>0.225474</td>\n",
       "      <td>0.456490</td>\n",
       "      <td>0.405377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.649200</td>\n",
       "      <td>0.613080</td>\n",
       "      <td>0.153776</td>\n",
       "      <td>0.475658</td>\n",
       "      <td>0.235523</td>\n",
       "      <td>0.462408</td>\n",
       "      <td>0.410654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.576400</td>\n",
       "      <td>0.600616</td>\n",
       "      <td>0.158172</td>\n",
       "      <td>0.481688</td>\n",
       "      <td>0.235532</td>\n",
       "      <td>0.471096</td>\n",
       "      <td>0.414189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.575100</td>\n",
       "      <td>0.587919</td>\n",
       "      <td>0.165720</td>\n",
       "      <td>0.490341</td>\n",
       "      <td>0.248911</td>\n",
       "      <td>0.479728</td>\n",
       "      <td>0.427667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.576100</td>\n",
       "      <td>0.582805</td>\n",
       "      <td>0.169668</td>\n",
       "      <td>0.491539</td>\n",
       "      <td>0.249929</td>\n",
       "      <td>0.481465</td>\n",
       "      <td>0.430270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.535000</td>\n",
       "      <td>0.575686</td>\n",
       "      <td>0.184601</td>\n",
       "      <td>0.494368</td>\n",
       "      <td>0.258916</td>\n",
       "      <td>0.484044</td>\n",
       "      <td>0.434393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.557500</td>\n",
       "      <td>0.571367</td>\n",
       "      <td>0.180950</td>\n",
       "      <td>0.493412</td>\n",
       "      <td>0.256652</td>\n",
       "      <td>0.482744</td>\n",
       "      <td>0.435796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.533700</td>\n",
       "      <td>0.568134</td>\n",
       "      <td>0.183423</td>\n",
       "      <td>0.493938</td>\n",
       "      <td>0.261184</td>\n",
       "      <td>0.485257</td>\n",
       "      <td>0.441690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.521500</td>\n",
       "      <td>0.563128</td>\n",
       "      <td>0.184823</td>\n",
       "      <td>0.499097</td>\n",
       "      <td>0.263580</td>\n",
       "      <td>0.489206</td>\n",
       "      <td>0.442138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.492600</td>\n",
       "      <td>0.563986</td>\n",
       "      <td>0.190297</td>\n",
       "      <td>0.499955</td>\n",
       "      <td>0.269563</td>\n",
       "      <td>0.489989</td>\n",
       "      <td>0.446388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.539700</td>\n",
       "      <td>0.560823</td>\n",
       "      <td>0.190491</td>\n",
       "      <td>0.504324</td>\n",
       "      <td>0.267669</td>\n",
       "      <td>0.493068</td>\n",
       "      <td>0.450366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.487900</td>\n",
       "      <td>0.561333</td>\n",
       "      <td>0.193981</td>\n",
       "      <td>0.507153</td>\n",
       "      <td>0.274865</td>\n",
       "      <td>0.496703</td>\n",
       "      <td>0.453662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.489100</td>\n",
       "      <td>0.559159</td>\n",
       "      <td>0.191180</td>\n",
       "      <td>0.503697</td>\n",
       "      <td>0.270778</td>\n",
       "      <td>0.493572</td>\n",
       "      <td>0.446658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.497300</td>\n",
       "      <td>0.558234</td>\n",
       "      <td>0.198292</td>\n",
       "      <td>0.506697</td>\n",
       "      <td>0.277122</td>\n",
       "      <td>0.497197</td>\n",
       "      <td>0.452061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.472000</td>\n",
       "      <td>0.558229</td>\n",
       "      <td>0.195647</td>\n",
       "      <td>0.512896</td>\n",
       "      <td>0.276942</td>\n",
       "      <td>0.500506</td>\n",
       "      <td>0.455083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.499400</td>\n",
       "      <td>0.557024</td>\n",
       "      <td>0.196538</td>\n",
       "      <td>0.509784</td>\n",
       "      <td>0.277136</td>\n",
       "      <td>0.498900</td>\n",
       "      <td>0.452247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.493100</td>\n",
       "      <td>0.557284</td>\n",
       "      <td>0.198891</td>\n",
       "      <td>0.511946</td>\n",
       "      <td>0.279363</td>\n",
       "      <td>0.500745</td>\n",
       "      <td>0.454480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.434300</td>\n",
       "      <td>0.555797</td>\n",
       "      <td>0.197039</td>\n",
       "      <td>0.511043</td>\n",
       "      <td>0.278334</td>\n",
       "      <td>0.500335</td>\n",
       "      <td>0.456012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.463800</td>\n",
       "      <td>0.555772</td>\n",
       "      <td>0.195854</td>\n",
       "      <td>0.509433</td>\n",
       "      <td>0.277280</td>\n",
       "      <td>0.498487</td>\n",
       "      <td>0.456287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.449800</td>\n",
       "      <td>0.555420</td>\n",
       "      <td>0.195749</td>\n",
       "      <td>0.509400</td>\n",
       "      <td>0.276608</td>\n",
       "      <td>0.498416</td>\n",
       "      <td>0.454754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.475900</td>\n",
       "      <td>0.555424</td>\n",
       "      <td>0.196974</td>\n",
       "      <td>0.507929</td>\n",
       "      <td>0.277685</td>\n",
       "      <td>0.495524</td>\n",
       "      <td>0.454091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.460400</td>\n",
       "      <td>0.555064</td>\n",
       "      <td>0.198949</td>\n",
       "      <td>0.507260</td>\n",
       "      <td>0.279548</td>\n",
       "      <td>0.495863</td>\n",
       "      <td>0.454866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.459300</td>\n",
       "      <td>0.555149</td>\n",
       "      <td>0.198949</td>\n",
       "      <td>0.507798</td>\n",
       "      <td>0.279234</td>\n",
       "      <td>0.496902</td>\n",
       "      <td>0.454630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 results: {'eval_loss': 0.5550636649131775, 'eval_bleu': 0.19894933028567022, 'eval_rouge1': 0.5072599493564104, 'eval_rouge2': 0.27954840084580745, 'eval_rougeL': 0.4958626556724899, 'eval_meteor': 0.45486632453188813, 'eval_runtime': 2.5436, 'eval_samples_per_second': 46.784, 'eval_steps_per_second': 5.897, 'epoch': 25.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 119/119 [00:00<00:00, 55649.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 5 saved to FR_crossval_T5_SMALL_TOP5DOCS_TFIDF_fold_5.jsonl\n",
      "Average metrics over all folds: {'eval_loss': 0.5529643297195435, 'eval_bleu': 0.19017669864614875, 'eval_rouge1': 0.49710817719617123, 'eval_rouge2': 0.2652272715041296, 'eval_rougeL': 0.4858846443963542, 'eval_meteor': 0.45209998767150344, 'eval_runtime': 2.27702, 'eval_samples_per_second': 52.791399999999996, 'eval_steps_per_second': 6.6424, 'epoch': 22.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Averages/epoch</td><td>▁</td></tr><tr><td>Averages/eval_bleu</td><td>▁</td></tr><tr><td>Averages/eval_loss</td><td>▁</td></tr><tr><td>Averages/eval_meteor</td><td>▁</td></tr><tr><td>Averages/eval_rouge1</td><td>▁</td></tr><tr><td>Averages/eval_rouge2</td><td>▁</td></tr><tr><td>Averages/eval_rougeL</td><td>▁</td></tr><tr><td>Averages/eval_runtime</td><td>▁</td></tr><tr><td>Averages/eval_samples_per_second</td><td>▁</td></tr><tr><td>Averages/eval_steps_per_second</td><td>▁</td></tr><tr><td>Fold_1/eval/epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇███</td></tr><tr><td>Fold_1/eval/eval_bleu</td><td>▁▂▅▆▆▆▇▇▇▇▇▇▇██████</td></tr><tr><td>Fold_1/eval/eval_loss</td><td>█▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_1/eval/eval_meteor</td><td>▁▃▅▆▇▇▇▇▇▇▇████████</td></tr><tr><td>Fold_1/eval/eval_rouge1</td><td>▁▃▆▇▇▇▇▇▇▇█████████</td></tr><tr><td>Fold_1/eval/eval_rouge2</td><td>▁▂▅▆▆▆▇▇▇▇▇▇▇██████</td></tr><tr><td>Fold_1/eval/eval_rougeL</td><td>▁▃▆▇▇▇▇▇▇▇█████████</td></tr><tr><td>Fold_1/eval/eval_runtime</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_1/eval/eval_samples_per_second</td><td>▁██████████████████</td></tr><tr><td>Fold_1/eval/eval_steps_per_second</td><td>▁██████████████████</td></tr><tr><td>Fold_2/eval/epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇███</td></tr><tr><td>Fold_2/eval/eval_bleu</td><td>▁▃▆▆▇█▇█▇█▇▇█████████▇▇█</td></tr><tr><td>Fold_2/eval/eval_loss</td><td>█▅▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_2/eval/eval_meteor</td><td>▁▃▅▆▇▇▇▇▇███████████████</td></tr><tr><td>Fold_2/eval/eval_rouge1</td><td>▁▃▆▆▇███▇█▇██████████▇██</td></tr><tr><td>Fold_2/eval/eval_rouge2</td><td>▁▃▆▆▇███▇█▇██████████▇▇█</td></tr><tr><td>Fold_2/eval/eval_rougeL</td><td>▁▃▆▇▇███▇███████████████</td></tr><tr><td>Fold_2/eval/eval_runtime</td><td>▁▅███▇▄▄▅█▄▅▃▆▇▆█▇▇▇▇▇▇▆</td></tr><tr><td>Fold_2/eval/eval_samples_per_second</td><td>█▄▁▁▁▂▅▅▄▁▅▄▆▃▂▃▁▂▂▂▂▂▂▂</td></tr><tr><td>Fold_2/eval/eval_steps_per_second</td><td>█▄▁▁▁▂▅▅▄▁▅▄▆▃▂▃▁▂▂▂▂▂▂▂</td></tr><tr><td>Fold_3/eval/epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇███</td></tr><tr><td>Fold_3/eval/eval_bleu</td><td>▁▂▃▄▅▅▆▆▆▆▇▆▇▇▇███████</td></tr><tr><td>Fold_3/eval/eval_loss</td><td>█▅▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_3/eval/eval_meteor</td><td>▁▃▅▅▆▆▇▇▇▇▇▇▇█████████</td></tr><tr><td>Fold_3/eval/eval_rouge1</td><td>▁▂▅▆▆▆▇▇▇▇█▇██████████</td></tr><tr><td>Fold_3/eval/eval_rouge2</td><td>▁▂▅▅▆▆▇▇▇▇▇▇▇█▇███████</td></tr><tr><td>Fold_3/eval/eval_rougeL</td><td>▁▂▄▆▆▆▇▇▇▇█▇██████████</td></tr><tr><td>Fold_3/eval/eval_runtime</td><td>▄▃██▂▂███▇▇▇▂▂▂▂▂▂▂▂▂▁</td></tr><tr><td>Fold_3/eval/eval_samples_per_second</td><td>▅▆▁▁▇▇▁▁▁▂▂▂▇▇▆▇▇▇▇▇▇█</td></tr><tr><td>Fold_3/eval/eval_steps_per_second</td><td>▅▆▁▁▇▆▁▁▁▂▂▂▇▇▆▇▆▇▇▇▇█</td></tr><tr><td>Fold_4/eval/epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇███</td></tr><tr><td>Fold_4/eval/eval_bleu</td><td>▁▂▄▅▆▇▆▇▇▇█▇██████████████</td></tr><tr><td>Fold_4/eval/eval_loss</td><td>█▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_4/eval/eval_meteor</td><td>▁▂▅▆▇▇▇▇▇██▇██████████████</td></tr><tr><td>Fold_4/eval/eval_rouge1</td><td>▁▃▅▆▇▇▇▇▇▇█▇█▇████████████</td></tr><tr><td>Fold_4/eval/eval_rouge2</td><td>▁▃▅▅▇▇▇▇▇▇█▇██████████████</td></tr><tr><td>Fold_4/eval/eval_rougeL</td><td>▁▃▅▆▇▇▇▇▇▇▇▇█▇████████████</td></tr><tr><td>Fold_4/eval/eval_runtime</td><td>▂▂▂▂▂▂▂▁▁▁▁▂▂▂▂▂▂▂▂▂▂▁▂▇██</td></tr><tr><td>Fold_4/eval/eval_samples_per_second</td><td>▇▇▇▇▇▇▇████▇▇▇▇▇▇▇▇▇▇█▇▂▁▁</td></tr><tr><td>Fold_4/eval/eval_steps_per_second</td><td>▇▇▇▇▇▇▇████▇▇▇▇▇▇▇▇▇▇█▇▂▁▁</td></tr><tr><td>Fold_5/eval/epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇███</td></tr><tr><td>Fold_5/eval/eval_bleu</td><td>▁▃▄▅▅▆▆▇▇▇▇▇▇█▇███████████</td></tr><tr><td>Fold_5/eval/eval_loss</td><td>█▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_5/eval/eval_meteor</td><td>▁▅▆▆▆▇▇▇▇▇▇███████████████</td></tr><tr><td>Fold_5/eval/eval_rouge1</td><td>▁▄▆▆▆▇▇▇▇▇▇▇██▇███████████</td></tr><tr><td>Fold_5/eval/eval_rouge2</td><td>▁▄▅▅▅▆▆▇▇▇▇▇▇█▇███████████</td></tr><tr><td>Fold_5/eval/eval_rougeL</td><td>▁▄▆▆▆▇▇▇▇▇▇▇██████████████</td></tr><tr><td>Fold_5/eval/eval_runtime</td><td>▁▆████████████████████████</td></tr><tr><td>Fold_5/eval/eval_samples_per_second</td><td>█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_5/eval/eval_steps_per_second</td><td>█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/bleu</td><td>▃▆▆▇▇▇████▇▇▇▇▇▇▇▇▁▂▄▅▅▆▇▇██████▂▅▅▇████</td></tr><tr><td>eval/loss</td><td>▅▂▂▂▂▂▂▅▃▂▁▁▁▁█▂▂▁▁▁▁▁▁▁▃▃▂▂▂▂▂▂▂█▂▂▂▂▂▂</td></tr><tr><td>eval/meteor</td><td>▁▅▆▇▇▇▅▇▇████▄▅▆▆▆▇▇▁▄▅▇▇███████▆▆▇█████</td></tr><tr><td>eval/rouge1</td><td>▁▆▆▇▇▇▇▇▇▇▆▇▇▇▇▇██▁▄▆▆▇▇▅▇▇▇███▁▅▆▇▇████</td></tr><tr><td>eval/rouge2</td><td>▁▆▆▆▇█▅▆▇▆▇▇▇▇▇▆▇▅▅▅▆▇▆▇▇▇▇▇▇▇▇▇▇▄▆▆▇███</td></tr><tr><td>eval/rougeL</td><td>▂▆▆▇▇▇▇▇▇▇▆▇▇▇█▇▇▇▁▅▇▇▇▇▃▇████▆▇▇▇▇█████</td></tr><tr><td>eval/runtime</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂</td></tr><tr><td>eval/samples_per_second</td><td>███████▇████████████▇▇█▇▇▇▇▇▇▇█▇▁▃▁▁▁▁▁▁</td></tr><tr><td>eval/steps_per_second</td><td>▁████████████████████████████████████▆▆▆</td></tr><tr><td>test/bleu</td><td>▆▅▁▇█</td></tr><tr><td>test/loss</td><td>█▃▁▆▅</td></tr><tr><td>test/meteor</td><td>▂█▁▆▆</td></tr><tr><td>test/rouge1</td><td>▄▆▁▇█</td></tr><tr><td>test/rouge2</td><td>█▄▁▅█</td></tr><tr><td>test/rougeL</td><td>▅▆▁▇█</td></tr><tr><td>test/runtime</td><td>▁▁▁██</td></tr><tr><td>test/samples_per_second</td><td>███▁▁</td></tr><tr><td>test/steps_per_second</td><td>▇██▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▃▃▅▅▅▂▂▂▂▄▄▅▅▅▅▇▇▂▃▄▆▃▃▃▃▄▄▆▁▁▄▅▅▇█</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▅▆▆▆▃▃▄▄▅▆▆▇▇▁▂▂▄▄▅▅▁▃▄▄▅▅█▂▃▄▆▇▇▇██</td></tr><tr><td>train/grad_norm</td><td>▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂</td></tr><tr><td>train/learning_rate</td><td>███▄▄▇▅▄▄▃▃▂█▇▆▅▅▄▄▂▇▇▇▆▅▃▂▂▂▁▁██▆▆▅▅▅▄▂</td></tr><tr><td>train/loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Averages/epoch</td><td>22.4</td></tr><tr><td>Averages/eval_bleu</td><td>0.19018</td></tr><tr><td>Averages/eval_loss</td><td>0.55296</td></tr><tr><td>Averages/eval_meteor</td><td>0.4521</td></tr><tr><td>Averages/eval_rouge1</td><td>0.49711</td></tr><tr><td>Averages/eval_rouge2</td><td>0.26523</td></tr><tr><td>Averages/eval_rougeL</td><td>0.48588</td></tr><tr><td>Averages/eval_runtime</td><td>2.27702</td></tr><tr><td>Averages/eval_samples_per_second</td><td>52.7914</td></tr><tr><td>Averages/eval_steps_per_second</td><td>6.6424</td></tr><tr><td>Fold_1/eval/epoch</td><td>18</td></tr><tr><td>Fold_1/eval/eval_bleu</td><td>0.1932</td></tr><tr><td>Fold_1/eval/eval_loss</td><td>0.57402</td></tr><tr><td>Fold_1/eval/eval_meteor</td><td>0.44663</td></tr><tr><td>Fold_1/eval/eval_rouge1</td><td>0.49402</td></tr><tr><td>Fold_1/eval/eval_rouge2</td><td>0.27729</td></tr><tr><td>Fold_1/eval/eval_rougeL</td><td>0.48323</td></tr><tr><td>Fold_1/eval/eval_runtime</td><td>2.1194</td></tr><tr><td>Fold_1/eval/eval_samples_per_second</td><td>56.619</td></tr><tr><td>Fold_1/eval/eval_steps_per_second</td><td>7.077</td></tr><tr><td>Fold_2/eval/epoch</td><td>23</td></tr><tr><td>Fold_2/eval/eval_bleu</td><td>0.1874</td></tr><tr><td>Fold_2/eval/eval_loss</td><td>0.54264</td></tr><tr><td>Fold_2/eval/eval_meteor</td><td>0.4596</td></tr><tr><td>Fold_2/eval/eval_rouge1</td><td>0.50081</td></tr><tr><td>Fold_2/eval/eval_rouge2</td><td>0.26141</td></tr><tr><td>Fold_2/eval/eval_rougeL</td><td>0.48876</td></tr><tr><td>Fold_2/eval/eval_runtime</td><td>2.1094</td></tr><tr><td>Fold_2/eval/eval_samples_per_second</td><td>56.413</td></tr><tr><td>Fold_2/eval/eval_steps_per_second</td><td>7.111</td></tr><tr><td>Fold_3/eval/epoch</td><td>21</td></tr><tr><td>Fold_3/eval/eval_bleu</td><td>0.17555</td></tr><tr><td>Fold_3/eval/eval_loss</td><td>0.53117</td></tr><tr><td>Fold_3/eval/eval_meteor</td><td>0.44396</td></tr><tr><td>Fold_3/eval/eval_rouge1</td><td>0.48164</td></tr><tr><td>Fold_3/eval/eval_rouge2</td><td>0.24368</td></tr><tr><td>Fold_3/eval/eval_rougeL</td><td>0.46945</td></tr><tr><td>Fold_3/eval/eval_runtime</td><td>2.0864</td></tr><tr><td>Fold_3/eval/eval_samples_per_second</td><td>57.037</td></tr><tr><td>Fold_3/eval/eval_steps_per_second</td><td>7.19</td></tr><tr><td>Fold_4/eval/epoch</td><td>25</td></tr><tr><td>Fold_4/eval/eval_bleu</td><td>0.19578</td></tr><tr><td>Fold_4/eval/eval_loss</td><td>0.56193</td></tr><tr><td>Fold_4/eval/eval_meteor</td><td>0.45544</td></tr><tr><td>Fold_4/eval/eval_rouge1</td><td>0.50181</td></tr><tr><td>Fold_4/eval/eval_rouge2</td><td>0.26421</td></tr><tr><td>Fold_4/eval/eval_rougeL</td><td>0.49211</td></tr><tr><td>Fold_4/eval/eval_runtime</td><td>2.5263</td></tr><tr><td>Fold_4/eval/eval_samples_per_second</td><td>47.104</td></tr><tr><td>Fold_4/eval/eval_steps_per_second</td><td>5.937</td></tr><tr><td>Fold_5/eval/epoch</td><td>25</td></tr><tr><td>Fold_5/eval/eval_bleu</td><td>0.19895</td></tr><tr><td>Fold_5/eval/eval_loss</td><td>0.55506</td></tr><tr><td>Fold_5/eval/eval_meteor</td><td>0.45487</td></tr><tr><td>Fold_5/eval/eval_rouge1</td><td>0.50726</td></tr><tr><td>Fold_5/eval/eval_rouge2</td><td>0.27955</td></tr><tr><td>Fold_5/eval/eval_rougeL</td><td>0.49586</td></tr><tr><td>Fold_5/eval/eval_runtime</td><td>2.5436</td></tr><tr><td>Fold_5/eval/eval_samples_per_second</td><td>46.784</td></tr><tr><td>Fold_5/eval/eval_steps_per_second</td><td>5.897</td></tr><tr><td>eval/bleu</td><td>0.19895</td></tr><tr><td>eval/loss</td><td>0.55506</td></tr><tr><td>eval/meteor</td><td>0.45487</td></tr><tr><td>eval/rouge1</td><td>0.50726</td></tr><tr><td>eval/rouge2</td><td>0.27955</td></tr><tr><td>eval/rougeL</td><td>0.49586</td></tr><tr><td>eval/runtime</td><td>2.5436</td></tr><tr><td>eval/samples_per_second</td><td>46.784</td></tr><tr><td>eval/steps_per_second</td><td>5.897</td></tr><tr><td>test/bleu</td><td>0.19895</td></tr><tr><td>test/loss</td><td>0.55506</td></tr><tr><td>test/meteor</td><td>0.45487</td></tr><tr><td>test/rouge1</td><td>0.50726</td></tr><tr><td>test/rouge2</td><td>0.27955</td></tr><tr><td>test/rougeL</td><td>0.49586</td></tr><tr><td>test/runtime</td><td>2.5213</td></tr><tr><td>test/samples_per_second</td><td>47.199</td></tr><tr><td>test/steps_per_second</td><td>5.949</td></tr><tr><td>total_flos</td><td>1613950982553600.0</td></tr><tr><td>train/epoch</td><td>25</td></tr><tr><td>train/global_step</td><td>1500</td></tr><tr><td>train/grad_norm</td><td>1.44704</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>0.4593</td></tr><tr><td>train_loss</td><td>0.68724</td></tr><tr><td>train_runtime</td><td>589.9479</td></tr><tr><td>train_samples_per_second</td><td>20.214</td></tr><tr><td>train_steps_per_second</td><td>2.543</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">FR_T5_SMALL_TOP5DOCS_TFIDF</strong> at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/vnunwory' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/vnunwory</a><br> View project at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250513_001739-vnunwory/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final predicitons for all folds foudn in FR_crossval_T5_SMALL_TOP5DOCS_TFIDF.jsonl\n"
     ]
    }
   ],
   "source": [
    "cross_val_train(modelname=\"t5-small\", run_name=\"FR_T5_SMALL_TOP5DOCS_TFIDF\",fold_filename=\"FR_training_top5_qulac_tfidf_CLEANED_rep_PREPROCESSED_FOR_MODEL.json\",filename=\"FR_crossval_T5_SMALL_TOP5DOCS_TFIDF.jsonl\", \n",
    "                tokenizer_global=tokenizer_global, tokenized_dataset_global=tokenized_dataset_global, \n",
    "                folds=folds, nb_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ba9e53-6072-4d47-ba1e-5e2f4bf87b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0460ea58-2035-4f95-b089-44d3b6b62220",
   "metadata": {},
   "source": [
    "# TEST of top10 TFIDF INSTEAD OF TOP 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc6814d5-34f9-411a-b9c3-52a0ee9d3c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/users/nfs/Etu0/21402600/wandb/run-20250513_011337-uw4wjrpp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/uw4wjrpp' target=\"_blank\">FR_T5_SMALL_TOP10DOCS_TFIDF</a></strong> to <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/uw4wjrpp' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/uw4wjrpp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Fold 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "969a2f7891d14da29c336e1f54a957e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/596 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recuperation du Fichier FOLD0_FR_training_top10_qulac_tfidf_CLEANED_rep_PREPROCESSED_FOR_MODEL.json pour le training du Fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1747091618.884770   59856 gpu_device.cc:2341] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_59856/3001149367.py:59: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1140' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1140/1500 07:28 < 02:21, 2.54 it/s, Epoch 19/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.385900</td>\n",
       "      <td>0.861544</td>\n",
       "      <td>0.099598</td>\n",
       "      <td>0.378332</td>\n",
       "      <td>0.161224</td>\n",
       "      <td>0.367960</td>\n",
       "      <td>0.292824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.891700</td>\n",
       "      <td>0.716012</td>\n",
       "      <td>0.124119</td>\n",
       "      <td>0.409322</td>\n",
       "      <td>0.191150</td>\n",
       "      <td>0.397628</td>\n",
       "      <td>0.336304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.688400</td>\n",
       "      <td>0.633074</td>\n",
       "      <td>0.160094</td>\n",
       "      <td>0.457190</td>\n",
       "      <td>0.224685</td>\n",
       "      <td>0.442603</td>\n",
       "      <td>0.390003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.649000</td>\n",
       "      <td>0.615146</td>\n",
       "      <td>0.166623</td>\n",
       "      <td>0.456563</td>\n",
       "      <td>0.234318</td>\n",
       "      <td>0.442833</td>\n",
       "      <td>0.401079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.597600</td>\n",
       "      <td>0.597549</td>\n",
       "      <td>0.178528</td>\n",
       "      <td>0.477105</td>\n",
       "      <td>0.253367</td>\n",
       "      <td>0.463819</td>\n",
       "      <td>0.424821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.592500</td>\n",
       "      <td>0.587435</td>\n",
       "      <td>0.175652</td>\n",
       "      <td>0.479105</td>\n",
       "      <td>0.253986</td>\n",
       "      <td>0.465509</td>\n",
       "      <td>0.425545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.566100</td>\n",
       "      <td>0.586338</td>\n",
       "      <td>0.186651</td>\n",
       "      <td>0.480448</td>\n",
       "      <td>0.264036</td>\n",
       "      <td>0.466015</td>\n",
       "      <td>0.431776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.544500</td>\n",
       "      <td>0.581283</td>\n",
       "      <td>0.188079</td>\n",
       "      <td>0.495036</td>\n",
       "      <td>0.264739</td>\n",
       "      <td>0.481592</td>\n",
       "      <td>0.441076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.562600</td>\n",
       "      <td>0.579822</td>\n",
       "      <td>0.195121</td>\n",
       "      <td>0.497729</td>\n",
       "      <td>0.270666</td>\n",
       "      <td>0.485691</td>\n",
       "      <td>0.446504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.528300</td>\n",
       "      <td>0.576138</td>\n",
       "      <td>0.192549</td>\n",
       "      <td>0.496250</td>\n",
       "      <td>0.273552</td>\n",
       "      <td>0.484572</td>\n",
       "      <td>0.445065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.501900</td>\n",
       "      <td>0.572475</td>\n",
       "      <td>0.189264</td>\n",
       "      <td>0.497470</td>\n",
       "      <td>0.271437</td>\n",
       "      <td>0.484551</td>\n",
       "      <td>0.445353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.483800</td>\n",
       "      <td>0.573354</td>\n",
       "      <td>0.186722</td>\n",
       "      <td>0.493688</td>\n",
       "      <td>0.267365</td>\n",
       "      <td>0.481930</td>\n",
       "      <td>0.443615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.491900</td>\n",
       "      <td>0.570625</td>\n",
       "      <td>0.190664</td>\n",
       "      <td>0.497225</td>\n",
       "      <td>0.271661</td>\n",
       "      <td>0.484791</td>\n",
       "      <td>0.449400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.451900</td>\n",
       "      <td>0.572270</td>\n",
       "      <td>0.190795</td>\n",
       "      <td>0.496900</td>\n",
       "      <td>0.272916</td>\n",
       "      <td>0.483981</td>\n",
       "      <td>0.448280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.480300</td>\n",
       "      <td>0.571459</td>\n",
       "      <td>0.191389</td>\n",
       "      <td>0.502152</td>\n",
       "      <td>0.276058</td>\n",
       "      <td>0.489516</td>\n",
       "      <td>0.450042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.473900</td>\n",
       "      <td>0.570528</td>\n",
       "      <td>0.192016</td>\n",
       "      <td>0.506189</td>\n",
       "      <td>0.276358</td>\n",
       "      <td>0.493101</td>\n",
       "      <td>0.453822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.452700</td>\n",
       "      <td>0.572923</td>\n",
       "      <td>0.191278</td>\n",
       "      <td>0.497389</td>\n",
       "      <td>0.276567</td>\n",
       "      <td>0.484963</td>\n",
       "      <td>0.452538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.478300</td>\n",
       "      <td>0.571026</td>\n",
       "      <td>0.193591</td>\n",
       "      <td>0.499004</td>\n",
       "      <td>0.274957</td>\n",
       "      <td>0.487210</td>\n",
       "      <td>0.452057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.438300</td>\n",
       "      <td>0.570609</td>\n",
       "      <td>0.197574</td>\n",
       "      <td>0.500866</td>\n",
       "      <td>0.280619</td>\n",
       "      <td>0.489379</td>\n",
       "      <td>0.455728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 results: {'eval_loss': 0.5705281496047974, 'eval_bleu': 0.1920158539372309, 'eval_rouge1': 0.5061890789953674, 'eval_rouge2': 0.27635824158462213, 'eval_rougeL': 0.49310080808303575, 'eval_meteor': 0.45382203485247885, 'eval_runtime': 2.1499, 'eval_samples_per_second': 55.817, 'eval_steps_per_second': 6.977, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 41723.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 1 saved to FR_crossval_T5_SMALL_TOP10DOCS_TFIDF_fold_1.jsonl\n",
      "Processing Fold 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1af0dc17d4104192b347a655a72e4ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/596 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/Etu0/21402600/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recuperation du Fichier FOLD1_FR_training_top10_qulac_tfidf_CLEANED_rep_PREPROCESSED_FOR_MODEL.json pour le training du Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_59856/3001149367.py:59: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1380' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1380/1500 08:29 < 00:44, 2.70 it/s, Epoch 23/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.275200</td>\n",
       "      <td>0.847429</td>\n",
       "      <td>0.104279</td>\n",
       "      <td>0.400945</td>\n",
       "      <td>0.177559</td>\n",
       "      <td>0.393512</td>\n",
       "      <td>0.315786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.830500</td>\n",
       "      <td>0.701813</td>\n",
       "      <td>0.117226</td>\n",
       "      <td>0.406438</td>\n",
       "      <td>0.189582</td>\n",
       "      <td>0.401621</td>\n",
       "      <td>0.345123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.700900</td>\n",
       "      <td>0.622365</td>\n",
       "      <td>0.151133</td>\n",
       "      <td>0.464433</td>\n",
       "      <td>0.234297</td>\n",
       "      <td>0.457046</td>\n",
       "      <td>0.403243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.623100</td>\n",
       "      <td>0.597929</td>\n",
       "      <td>0.150024</td>\n",
       "      <td>0.471040</td>\n",
       "      <td>0.227251</td>\n",
       "      <td>0.459734</td>\n",
       "      <td>0.413866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.613700</td>\n",
       "      <td>0.582049</td>\n",
       "      <td>0.175451</td>\n",
       "      <td>0.488613</td>\n",
       "      <td>0.250329</td>\n",
       "      <td>0.476997</td>\n",
       "      <td>0.437055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.567700</td>\n",
       "      <td>0.572901</td>\n",
       "      <td>0.178668</td>\n",
       "      <td>0.490309</td>\n",
       "      <td>0.258114</td>\n",
       "      <td>0.482501</td>\n",
       "      <td>0.443283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.566700</td>\n",
       "      <td>0.564700</td>\n",
       "      <td>0.177709</td>\n",
       "      <td>0.496175</td>\n",
       "      <td>0.261458</td>\n",
       "      <td>0.485946</td>\n",
       "      <td>0.444460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.562600</td>\n",
       "      <td>0.558042</td>\n",
       "      <td>0.183677</td>\n",
       "      <td>0.496872</td>\n",
       "      <td>0.262690</td>\n",
       "      <td>0.486468</td>\n",
       "      <td>0.445765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.565800</td>\n",
       "      <td>0.556769</td>\n",
       "      <td>0.186059</td>\n",
       "      <td>0.493123</td>\n",
       "      <td>0.261788</td>\n",
       "      <td>0.484172</td>\n",
       "      <td>0.451229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.536900</td>\n",
       "      <td>0.552870</td>\n",
       "      <td>0.187852</td>\n",
       "      <td>0.495173</td>\n",
       "      <td>0.265375</td>\n",
       "      <td>0.484903</td>\n",
       "      <td>0.453881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.484600</td>\n",
       "      <td>0.549230</td>\n",
       "      <td>0.186699</td>\n",
       "      <td>0.492824</td>\n",
       "      <td>0.263691</td>\n",
       "      <td>0.483192</td>\n",
       "      <td>0.455667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.502000</td>\n",
       "      <td>0.546036</td>\n",
       "      <td>0.183819</td>\n",
       "      <td>0.502594</td>\n",
       "      <td>0.268749</td>\n",
       "      <td>0.492997</td>\n",
       "      <td>0.461754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.524200</td>\n",
       "      <td>0.547361</td>\n",
       "      <td>0.186454</td>\n",
       "      <td>0.495394</td>\n",
       "      <td>0.265344</td>\n",
       "      <td>0.486128</td>\n",
       "      <td>0.458879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.522100</td>\n",
       "      <td>0.546477</td>\n",
       "      <td>0.186756</td>\n",
       "      <td>0.494112</td>\n",
       "      <td>0.266437</td>\n",
       "      <td>0.485279</td>\n",
       "      <td>0.459432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.468700</td>\n",
       "      <td>0.545150</td>\n",
       "      <td>0.187289</td>\n",
       "      <td>0.498948</td>\n",
       "      <td>0.268669</td>\n",
       "      <td>0.489394</td>\n",
       "      <td>0.463900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.466300</td>\n",
       "      <td>0.544723</td>\n",
       "      <td>0.188751</td>\n",
       "      <td>0.502717</td>\n",
       "      <td>0.268329</td>\n",
       "      <td>0.492950</td>\n",
       "      <td>0.465614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.473100</td>\n",
       "      <td>0.543172</td>\n",
       "      <td>0.188574</td>\n",
       "      <td>0.504676</td>\n",
       "      <td>0.266172</td>\n",
       "      <td>0.494131</td>\n",
       "      <td>0.468076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.485300</td>\n",
       "      <td>0.541420</td>\n",
       "      <td>0.191257</td>\n",
       "      <td>0.503213</td>\n",
       "      <td>0.266996</td>\n",
       "      <td>0.493275</td>\n",
       "      <td>0.465082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.471800</td>\n",
       "      <td>0.541072</td>\n",
       "      <td>0.194104</td>\n",
       "      <td>0.503645</td>\n",
       "      <td>0.272541</td>\n",
       "      <td>0.494729</td>\n",
       "      <td>0.470312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.452500</td>\n",
       "      <td>0.539793</td>\n",
       "      <td>0.195844</td>\n",
       "      <td>0.502966</td>\n",
       "      <td>0.271761</td>\n",
       "      <td>0.493634</td>\n",
       "      <td>0.470430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.472200</td>\n",
       "      <td>0.540461</td>\n",
       "      <td>0.196377</td>\n",
       "      <td>0.504948</td>\n",
       "      <td>0.273181</td>\n",
       "      <td>0.495376</td>\n",
       "      <td>0.471183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.413200</td>\n",
       "      <td>0.540444</td>\n",
       "      <td>0.196573</td>\n",
       "      <td>0.505199</td>\n",
       "      <td>0.272889</td>\n",
       "      <td>0.496458</td>\n",
       "      <td>0.472984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.447900</td>\n",
       "      <td>0.540633</td>\n",
       "      <td>0.197169</td>\n",
       "      <td>0.503814</td>\n",
       "      <td>0.272992</td>\n",
       "      <td>0.494026</td>\n",
       "      <td>0.470828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 results: {'eval_loss': 0.5397933721542358, 'eval_bleu': 0.19584442957718268, 'eval_rouge1': 0.5029661272398562, 'eval_rouge2': 0.271761234525867, 'eval_rougeL': 0.49363386250192126, 'eval_meteor': 0.470430097815882, 'eval_runtime': 2.1259, 'eval_samples_per_second': 55.976, 'eval_steps_per_second': 7.056, 'epoch': 23.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 119/119 [00:00<00:00, 41107.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 2 saved to FR_crossval_T5_SMALL_TOP10DOCS_TFIDF_fold_2.jsonl\n",
      "Processing Fold 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9d232dff4c74e91815b8f56ccb5841f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/596 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/Etu0/21402600/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recuperation du Fichier FOLD2_FR_training_top10_qulac_tfidf_CLEANED_rep_PREPROCESSED_FOR_MODEL.json pour le training du Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_59856/3001149367.py:59: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1260' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1260/1500 07:55 < 01:30, 2.65 it/s, Epoch 21/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.334000</td>\n",
       "      <td>0.858511</td>\n",
       "      <td>0.086773</td>\n",
       "      <td>0.351578</td>\n",
       "      <td>0.140385</td>\n",
       "      <td>0.344631</td>\n",
       "      <td>0.275053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.876400</td>\n",
       "      <td>0.681933</td>\n",
       "      <td>0.102780</td>\n",
       "      <td>0.390047</td>\n",
       "      <td>0.164130</td>\n",
       "      <td>0.377854</td>\n",
       "      <td>0.318298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.735300</td>\n",
       "      <td>0.613643</td>\n",
       "      <td>0.121725</td>\n",
       "      <td>0.429422</td>\n",
       "      <td>0.196820</td>\n",
       "      <td>0.417263</td>\n",
       "      <td>0.374861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.659300</td>\n",
       "      <td>0.585171</td>\n",
       "      <td>0.140460</td>\n",
       "      <td>0.447787</td>\n",
       "      <td>0.210134</td>\n",
       "      <td>0.435461</td>\n",
       "      <td>0.395456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.619200</td>\n",
       "      <td>0.572960</td>\n",
       "      <td>0.147936</td>\n",
       "      <td>0.455808</td>\n",
       "      <td>0.222793</td>\n",
       "      <td>0.445706</td>\n",
       "      <td>0.401376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.579300</td>\n",
       "      <td>0.562565</td>\n",
       "      <td>0.154059</td>\n",
       "      <td>0.463403</td>\n",
       "      <td>0.230822</td>\n",
       "      <td>0.453194</td>\n",
       "      <td>0.422177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.584400</td>\n",
       "      <td>0.555526</td>\n",
       "      <td>0.161112</td>\n",
       "      <td>0.469343</td>\n",
       "      <td>0.233304</td>\n",
       "      <td>0.460453</td>\n",
       "      <td>0.429289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.539300</td>\n",
       "      <td>0.551242</td>\n",
       "      <td>0.161592</td>\n",
       "      <td>0.472139</td>\n",
       "      <td>0.236875</td>\n",
       "      <td>0.462282</td>\n",
       "      <td>0.427656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.575800</td>\n",
       "      <td>0.545476</td>\n",
       "      <td>0.165495</td>\n",
       "      <td>0.479881</td>\n",
       "      <td>0.247240</td>\n",
       "      <td>0.470231</td>\n",
       "      <td>0.439830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.509300</td>\n",
       "      <td>0.543497</td>\n",
       "      <td>0.167796</td>\n",
       "      <td>0.478595</td>\n",
       "      <td>0.244007</td>\n",
       "      <td>0.469350</td>\n",
       "      <td>0.431431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.495300</td>\n",
       "      <td>0.540566</td>\n",
       "      <td>0.177365</td>\n",
       "      <td>0.483936</td>\n",
       "      <td>0.253892</td>\n",
       "      <td>0.475925</td>\n",
       "      <td>0.439796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.512500</td>\n",
       "      <td>0.537685</td>\n",
       "      <td>0.172415</td>\n",
       "      <td>0.485707</td>\n",
       "      <td>0.248350</td>\n",
       "      <td>0.475764</td>\n",
       "      <td>0.440633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.527800</td>\n",
       "      <td>0.535948</td>\n",
       "      <td>0.176414</td>\n",
       "      <td>0.486236</td>\n",
       "      <td>0.253790</td>\n",
       "      <td>0.478828</td>\n",
       "      <td>0.443086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.501400</td>\n",
       "      <td>0.534299</td>\n",
       "      <td>0.179383</td>\n",
       "      <td>0.490067</td>\n",
       "      <td>0.253632</td>\n",
       "      <td>0.480130</td>\n",
       "      <td>0.442033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.496300</td>\n",
       "      <td>0.533821</td>\n",
       "      <td>0.178816</td>\n",
       "      <td>0.487035</td>\n",
       "      <td>0.254842</td>\n",
       "      <td>0.478833</td>\n",
       "      <td>0.451082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.490200</td>\n",
       "      <td>0.532558</td>\n",
       "      <td>0.183878</td>\n",
       "      <td>0.486970</td>\n",
       "      <td>0.256409</td>\n",
       "      <td>0.478550</td>\n",
       "      <td>0.447352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.454000</td>\n",
       "      <td>0.532278</td>\n",
       "      <td>0.186086</td>\n",
       "      <td>0.486935</td>\n",
       "      <td>0.257222</td>\n",
       "      <td>0.477844</td>\n",
       "      <td>0.445744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.491800</td>\n",
       "      <td>0.530528</td>\n",
       "      <td>0.188466</td>\n",
       "      <td>0.489730</td>\n",
       "      <td>0.260356</td>\n",
       "      <td>0.480727</td>\n",
       "      <td>0.453730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.460600</td>\n",
       "      <td>0.531519</td>\n",
       "      <td>0.179872</td>\n",
       "      <td>0.484707</td>\n",
       "      <td>0.252664</td>\n",
       "      <td>0.476021</td>\n",
       "      <td>0.449985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.448600</td>\n",
       "      <td>0.531491</td>\n",
       "      <td>0.184163</td>\n",
       "      <td>0.489203</td>\n",
       "      <td>0.256993</td>\n",
       "      <td>0.479734</td>\n",
       "      <td>0.452328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.499700</td>\n",
       "      <td>0.532190</td>\n",
       "      <td>0.181273</td>\n",
       "      <td>0.488580</td>\n",
       "      <td>0.256214</td>\n",
       "      <td>0.477921</td>\n",
       "      <td>0.455973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 results: {'eval_loss': 0.5305278301239014, 'eval_bleu': 0.18846607534669982, 'eval_rouge1': 0.48972994364328093, 'eval_rouge2': 0.26035572806690677, 'eval_rougeL': 0.4807274417069729, 'eval_meteor': 0.45373007901781265, 'eval_runtime': 2.0801, 'eval_samples_per_second': 57.208, 'eval_steps_per_second': 7.211, 'epoch': 21.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 119/119 [00:00<00:00, 41012.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 3 saved to FR_crossval_T5_SMALL_TOP10DOCS_TFIDF_fold_3.jsonl\n",
      "Processing Fold 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbbc0a4d340e4784a02c2dbf883e28bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/596 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/Etu0/21402600/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recuperation du Fichier FOLD3_FR_training_top10_qulac_tfidf_CLEANED_rep_PREPROCESSED_FOR_MODEL.json pour le training du Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_59856/3001149367.py:59: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1440' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1440/1500 09:17 < 00:23, 2.58 it/s, Epoch 24/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.294400</td>\n",
       "      <td>0.910990</td>\n",
       "      <td>0.113520</td>\n",
       "      <td>0.378621</td>\n",
       "      <td>0.164446</td>\n",
       "      <td>0.369492</td>\n",
       "      <td>0.297080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.841700</td>\n",
       "      <td>0.705245</td>\n",
       "      <td>0.130820</td>\n",
       "      <td>0.416160</td>\n",
       "      <td>0.191443</td>\n",
       "      <td>0.405624</td>\n",
       "      <td>0.337083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.695200</td>\n",
       "      <td>0.648158</td>\n",
       "      <td>0.160037</td>\n",
       "      <td>0.453424</td>\n",
       "      <td>0.228752</td>\n",
       "      <td>0.445613</td>\n",
       "      <td>0.383116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.640400</td>\n",
       "      <td>0.621251</td>\n",
       "      <td>0.170502</td>\n",
       "      <td>0.466743</td>\n",
       "      <td>0.233831</td>\n",
       "      <td>0.459260</td>\n",
       "      <td>0.415212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.607400</td>\n",
       "      <td>0.609014</td>\n",
       "      <td>0.184063</td>\n",
       "      <td>0.472536</td>\n",
       "      <td>0.248279</td>\n",
       "      <td>0.463528</td>\n",
       "      <td>0.422216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.578300</td>\n",
       "      <td>0.592740</td>\n",
       "      <td>0.191620</td>\n",
       "      <td>0.475778</td>\n",
       "      <td>0.252381</td>\n",
       "      <td>0.467189</td>\n",
       "      <td>0.430494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.570900</td>\n",
       "      <td>0.585323</td>\n",
       "      <td>0.183918</td>\n",
       "      <td>0.478578</td>\n",
       "      <td>0.247489</td>\n",
       "      <td>0.470132</td>\n",
       "      <td>0.431494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.558700</td>\n",
       "      <td>0.578911</td>\n",
       "      <td>0.184512</td>\n",
       "      <td>0.486264</td>\n",
       "      <td>0.255848</td>\n",
       "      <td>0.475948</td>\n",
       "      <td>0.438098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.542700</td>\n",
       "      <td>0.574944</td>\n",
       "      <td>0.184457</td>\n",
       "      <td>0.487774</td>\n",
       "      <td>0.251582</td>\n",
       "      <td>0.476108</td>\n",
       "      <td>0.439566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.520100</td>\n",
       "      <td>0.572062</td>\n",
       "      <td>0.192016</td>\n",
       "      <td>0.489683</td>\n",
       "      <td>0.257839</td>\n",
       "      <td>0.478804</td>\n",
       "      <td>0.448856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.516600</td>\n",
       "      <td>0.568268</td>\n",
       "      <td>0.196205</td>\n",
       "      <td>0.495755</td>\n",
       "      <td>0.261536</td>\n",
       "      <td>0.485353</td>\n",
       "      <td>0.450050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.512400</td>\n",
       "      <td>0.566988</td>\n",
       "      <td>0.196080</td>\n",
       "      <td>0.495650</td>\n",
       "      <td>0.262736</td>\n",
       "      <td>0.484953</td>\n",
       "      <td>0.449895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.501200</td>\n",
       "      <td>0.564125</td>\n",
       "      <td>0.199955</td>\n",
       "      <td>0.497875</td>\n",
       "      <td>0.266142</td>\n",
       "      <td>0.486948</td>\n",
       "      <td>0.454394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.493200</td>\n",
       "      <td>0.564750</td>\n",
       "      <td>0.200547</td>\n",
       "      <td>0.497614</td>\n",
       "      <td>0.266193</td>\n",
       "      <td>0.488438</td>\n",
       "      <td>0.454076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.498700</td>\n",
       "      <td>0.563831</td>\n",
       "      <td>0.204617</td>\n",
       "      <td>0.500606</td>\n",
       "      <td>0.270914</td>\n",
       "      <td>0.491244</td>\n",
       "      <td>0.458440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.562212</td>\n",
       "      <td>0.202390</td>\n",
       "      <td>0.498054</td>\n",
       "      <td>0.268932</td>\n",
       "      <td>0.489091</td>\n",
       "      <td>0.455393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.462200</td>\n",
       "      <td>0.561405</td>\n",
       "      <td>0.199065</td>\n",
       "      <td>0.498335</td>\n",
       "      <td>0.266394</td>\n",
       "      <td>0.490369</td>\n",
       "      <td>0.455782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.489000</td>\n",
       "      <td>0.561625</td>\n",
       "      <td>0.198429</td>\n",
       "      <td>0.501804</td>\n",
       "      <td>0.265550</td>\n",
       "      <td>0.491910</td>\n",
       "      <td>0.456244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.468200</td>\n",
       "      <td>0.560983</td>\n",
       "      <td>0.197262</td>\n",
       "      <td>0.499261</td>\n",
       "      <td>0.264077</td>\n",
       "      <td>0.490681</td>\n",
       "      <td>0.454955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.444400</td>\n",
       "      <td>0.561250</td>\n",
       "      <td>0.194767</td>\n",
       "      <td>0.499979</td>\n",
       "      <td>0.263387</td>\n",
       "      <td>0.491277</td>\n",
       "      <td>0.455462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.459300</td>\n",
       "      <td>0.559966</td>\n",
       "      <td>0.196782</td>\n",
       "      <td>0.498245</td>\n",
       "      <td>0.264197</td>\n",
       "      <td>0.490079</td>\n",
       "      <td>0.452912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.473500</td>\n",
       "      <td>0.560098</td>\n",
       "      <td>0.199125</td>\n",
       "      <td>0.500682</td>\n",
       "      <td>0.267408</td>\n",
       "      <td>0.492525</td>\n",
       "      <td>0.456615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.432300</td>\n",
       "      <td>0.560543</td>\n",
       "      <td>0.197141</td>\n",
       "      <td>0.499618</td>\n",
       "      <td>0.265144</td>\n",
       "      <td>0.491328</td>\n",
       "      <td>0.455164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.560688</td>\n",
       "      <td>0.195473</td>\n",
       "      <td>0.499237</td>\n",
       "      <td>0.263094</td>\n",
       "      <td>0.491105</td>\n",
       "      <td>0.453493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 results: {'eval_loss': 0.5599663853645325, 'eval_bleu': 0.1967815593900268, 'eval_rouge1': 0.49824541461257144, 'eval_rouge2': 0.26419681051376886, 'eval_rougeL': 0.49007857882358097, 'eval_meteor': 0.45291202009305925, 'eval_runtime': 2.5569, 'eval_samples_per_second': 46.54, 'eval_steps_per_second': 5.866, 'epoch': 24.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 119/119 [00:00<00:00, 41191.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 4 saved to FR_crossval_T5_SMALL_TOP10DOCS_TFIDF_fold_4.jsonl\n",
      "Processing Fold 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "769492003a234c5aa9eef66001d845ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/596 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/Etu0/21402600/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recuperation du Fichier FOLD4_FR_training_top10_qulac_tfidf_CLEANED_rep_PREPROCESSED_FOR_MODEL.json pour le training du Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the TF 2.0 model and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_59856/3001149367.py:59: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1260' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1260/1500 08:00 < 01:31, 2.62 it/s, Epoch 21/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.387100</td>\n",
       "      <td>0.870362</td>\n",
       "      <td>0.094568</td>\n",
       "      <td>0.397046</td>\n",
       "      <td>0.171647</td>\n",
       "      <td>0.384858</td>\n",
       "      <td>0.297629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.828600</td>\n",
       "      <td>0.690500</td>\n",
       "      <td>0.129266</td>\n",
       "      <td>0.445212</td>\n",
       "      <td>0.216016</td>\n",
       "      <td>0.435144</td>\n",
       "      <td>0.378908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.674300</td>\n",
       "      <td>0.631472</td>\n",
       "      <td>0.150388</td>\n",
       "      <td>0.470093</td>\n",
       "      <td>0.229360</td>\n",
       "      <td>0.461479</td>\n",
       "      <td>0.403697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.642700</td>\n",
       "      <td>0.606374</td>\n",
       "      <td>0.157218</td>\n",
       "      <td>0.483014</td>\n",
       "      <td>0.233328</td>\n",
       "      <td>0.470403</td>\n",
       "      <td>0.415056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.570200</td>\n",
       "      <td>0.595924</td>\n",
       "      <td>0.163738</td>\n",
       "      <td>0.478858</td>\n",
       "      <td>0.243016</td>\n",
       "      <td>0.468352</td>\n",
       "      <td>0.414415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.568200</td>\n",
       "      <td>0.583994</td>\n",
       "      <td>0.160539</td>\n",
       "      <td>0.482886</td>\n",
       "      <td>0.240786</td>\n",
       "      <td>0.473548</td>\n",
       "      <td>0.416338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.573500</td>\n",
       "      <td>0.579010</td>\n",
       "      <td>0.169753</td>\n",
       "      <td>0.493517</td>\n",
       "      <td>0.253067</td>\n",
       "      <td>0.483073</td>\n",
       "      <td>0.434241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.529300</td>\n",
       "      <td>0.572014</td>\n",
       "      <td>0.176115</td>\n",
       "      <td>0.496037</td>\n",
       "      <td>0.261650</td>\n",
       "      <td>0.485875</td>\n",
       "      <td>0.439137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.548800</td>\n",
       "      <td>0.567716</td>\n",
       "      <td>0.185514</td>\n",
       "      <td>0.501925</td>\n",
       "      <td>0.263408</td>\n",
       "      <td>0.490941</td>\n",
       "      <td>0.446570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.525300</td>\n",
       "      <td>0.564656</td>\n",
       "      <td>0.174854</td>\n",
       "      <td>0.501661</td>\n",
       "      <td>0.261101</td>\n",
       "      <td>0.491179</td>\n",
       "      <td>0.443916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.515400</td>\n",
       "      <td>0.560268</td>\n",
       "      <td>0.186695</td>\n",
       "      <td>0.506455</td>\n",
       "      <td>0.267025</td>\n",
       "      <td>0.496926</td>\n",
       "      <td>0.453220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.493100</td>\n",
       "      <td>0.559341</td>\n",
       "      <td>0.187128</td>\n",
       "      <td>0.502140</td>\n",
       "      <td>0.265753</td>\n",
       "      <td>0.492859</td>\n",
       "      <td>0.448785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.531400</td>\n",
       "      <td>0.556706</td>\n",
       "      <td>0.195944</td>\n",
       "      <td>0.510845</td>\n",
       "      <td>0.273093</td>\n",
       "      <td>0.498858</td>\n",
       "      <td>0.457340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.480800</td>\n",
       "      <td>0.557160</td>\n",
       "      <td>0.192523</td>\n",
       "      <td>0.510480</td>\n",
       "      <td>0.273002</td>\n",
       "      <td>0.498924</td>\n",
       "      <td>0.454714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.485100</td>\n",
       "      <td>0.555273</td>\n",
       "      <td>0.189901</td>\n",
       "      <td>0.514441</td>\n",
       "      <td>0.274034</td>\n",
       "      <td>0.502202</td>\n",
       "      <td>0.456518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.490900</td>\n",
       "      <td>0.554095</td>\n",
       "      <td>0.191284</td>\n",
       "      <td>0.518460</td>\n",
       "      <td>0.278906</td>\n",
       "      <td>0.506260</td>\n",
       "      <td>0.464157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.462900</td>\n",
       "      <td>0.553819</td>\n",
       "      <td>0.191922</td>\n",
       "      <td>0.513265</td>\n",
       "      <td>0.273462</td>\n",
       "      <td>0.501849</td>\n",
       "      <td>0.458966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.491400</td>\n",
       "      <td>0.552585</td>\n",
       "      <td>0.196218</td>\n",
       "      <td>0.514191</td>\n",
       "      <td>0.279820</td>\n",
       "      <td>0.504521</td>\n",
       "      <td>0.461798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.487000</td>\n",
       "      <td>0.553686</td>\n",
       "      <td>0.198842</td>\n",
       "      <td>0.515474</td>\n",
       "      <td>0.281871</td>\n",
       "      <td>0.505085</td>\n",
       "      <td>0.463086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.429600</td>\n",
       "      <td>0.553999</td>\n",
       "      <td>0.197287</td>\n",
       "      <td>0.514120</td>\n",
       "      <td>0.281089</td>\n",
       "      <td>0.503807</td>\n",
       "      <td>0.461011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.452700</td>\n",
       "      <td>0.552910</td>\n",
       "      <td>0.198272</td>\n",
       "      <td>0.515320</td>\n",
       "      <td>0.281779</td>\n",
       "      <td>0.505297</td>\n",
       "      <td>0.463714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 results: {'eval_loss': 0.5525848865509033, 'eval_bleu': 0.1962181489038196, 'eval_rouge1': 0.5141910217130199, 'eval_rouge2': 0.2798204956304613, 'eval_rougeL': 0.5045214978539461, 'eval_meteor': 0.4617980904015956, 'eval_runtime': 2.5434, 'eval_samples_per_second': 46.788, 'eval_steps_per_second': 5.898, 'epoch': 21.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 119/119 [00:00<00:00, 42287.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for fold 5 saved to FR_crossval_T5_SMALL_TOP10DOCS_TFIDF_fold_5.jsonl\n",
      "Average metrics over all folds: {'eval_loss': 0.5506801247596741, 'eval_bleu': 0.19386521343099197, 'eval_rouge1': 0.5022643172408191, 'eval_rouge2': 0.2704985020643252, 'eval_rougeL': 0.4924124377938914, 'eval_meteor': 0.4585384644361657, 'eval_runtime': 2.29124, 'eval_samples_per_second': 52.4658, 'eval_steps_per_second': 6.6015999999999995, 'epoch': 21.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Averages/epoch</td><td>▁</td></tr><tr><td>Averages/eval_bleu</td><td>▁</td></tr><tr><td>Averages/eval_loss</td><td>▁</td></tr><tr><td>Averages/eval_meteor</td><td>▁</td></tr><tr><td>Averages/eval_rouge1</td><td>▁</td></tr><tr><td>Averages/eval_rouge2</td><td>▁</td></tr><tr><td>Averages/eval_rougeL</td><td>▁</td></tr><tr><td>Averages/eval_runtime</td><td>▁</td></tr><tr><td>Averages/eval_samples_per_second</td><td>▁</td></tr><tr><td>Averages/eval_steps_per_second</td><td>▁</td></tr><tr><td>Fold_1/eval/epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇███</td></tr><tr><td>Fold_1/eval/eval_bleu</td><td>▁▃▅▆▇▆▇▇██▇▇████████</td></tr><tr><td>Fold_1/eval/eval_loss</td><td>█▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_1/eval/eval_meteor</td><td>▁▃▅▆▇▇▇▇███▇████████</td></tr><tr><td>Fold_1/eval/eval_rouge1</td><td>▁▃▅▅▆▇▇▇█▇█▇█▇██████</td></tr><tr><td>Fold_1/eval/eval_rouge2</td><td>▁▃▅▅▆▆▇▇▇█▇▇▇███████</td></tr><tr><td>Fold_1/eval/eval_rougeL</td><td>▁▃▅▅▆▆▆▇███▇█▇██████</td></tr><tr><td>Fold_1/eval/eval_runtime</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_1/eval/eval_samples_per_second</td><td>▁███████████████████</td></tr><tr><td>Fold_1/eval/eval_steps_per_second</td><td>▁███████████████████</td></tr><tr><td>Fold_2/eval/epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇███</td></tr><tr><td>Fold_2/eval/eval_bleu</td><td>▁▂▅▄▆▇▇▇▇▇▇▇▇▇▇▇▇███████</td></tr><tr><td>Fold_2/eval/eval_loss</td><td>█▅▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_2/eval/eval_meteor</td><td>▁▂▅▅▆▇▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>Fold_2/eval/eval_rouge1</td><td>▁▁▅▆▇▇▇▇▇▇▇█▇▇██████████</td></tr><tr><td>Fold_2/eval/eval_rouge2</td><td>▁▂▅▅▆▇▇▇▇▇▇█▇███▇███████</td></tr><tr><td>Fold_2/eval/eval_rougeL</td><td>▁▂▅▆▇▇▇▇▇▇▇█▇▇██████████</td></tr><tr><td>Fold_2/eval/eval_runtime</td><td>█▄█▂▂▂▄▁▄▂▁▄▅▅▆▅▁▁▁▁▅▆▅▆</td></tr><tr><td>Fold_2/eval/eval_samples_per_second</td><td>▁▅▁▇▇▇▅█▅▇█▅▃▄▃▄████▄▃▄▃</td></tr><tr><td>Fold_2/eval/eval_steps_per_second</td><td>▁▅▁▇▇▇▅█▅▇█▅▃▄▃▄████▄▃▄▃</td></tr><tr><td>Fold_3/eval/epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇███</td></tr><tr><td>Fold_3/eval/eval_bleu</td><td>▁▂▃▅▅▆▆▆▆▇▇▇▇▇▇███▇███</td></tr><tr><td>Fold_3/eval/eval_loss</td><td>█▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_3/eval/eval_meteor</td><td>▁▃▅▆▆▇▇▇▇▇▇▇█▇████████</td></tr><tr><td>Fold_3/eval/eval_rouge1</td><td>▁▃▅▆▆▇▇▇▇▇████████████</td></tr><tr><td>Fold_3/eval/eval_rouge2</td><td>▁▂▄▅▆▆▆▇▇▇█▇██████████</td></tr><tr><td>Fold_3/eval/eval_rougeL</td><td>▁▃▅▆▆▇▇▇▇▇████████████</td></tr><tr><td>Fold_3/eval/eval_runtime</td><td>█▁▄▄▁▂▁▁▃▃▃▃▁▅▄▄▅▁▁▅▄▁</td></tr><tr><td>Fold_3/eval/eval_samples_per_second</td><td>▁█▅▅█▇██▆▆▆▆█▄▅▅▄██▄▅█</td></tr><tr><td>Fold_3/eval/eval_steps_per_second</td><td>▁█▅▅█▇██▆▆▆▆█▄▅▅▄██▄▅█</td></tr><tr><td>Fold_4/eval/epoch</td><td>▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇███</td></tr><tr><td>Fold_4/eval/eval_bleu</td><td>▁▂▅▅▆▇▆▆▆▇▇▇██████▇▇▇█▇▇▇</td></tr><tr><td>Fold_4/eval/eval_loss</td><td>█▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_4/eval/eval_meteor</td><td>▁▃▅▆▆▇▇▇▇████████████████</td></tr><tr><td>Fold_4/eval/eval_rouge1</td><td>▁▃▅▆▆▇▇▇▇▇███████████████</td></tr><tr><td>Fold_4/eval/eval_rouge2</td><td>▁▃▅▆▇▇▆▇▇▇▇▇███████████▇█</td></tr><tr><td>Fold_4/eval/eval_rougeL</td><td>▁▃▅▆▆▇▇▇▇▇███████████████</td></tr><tr><td>Fold_4/eval/eval_runtime</td><td>▂▁▁▂▁▁▁▁▁▁▁▁▁▂▁▁▁▂▆█▇▅█▇█</td></tr><tr><td>Fold_4/eval/eval_samples_per_second</td><td>▇█▇▇▇████████▇▇██▇▃▁▁▄▁▂▁</td></tr><tr><td>Fold_4/eval/eval_steps_per_second</td><td>▇█▇▇▇████████▇▇██▇▃▁▁▄▁▂▁</td></tr><tr><td>Fold_5/eval/epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇███</td></tr><tr><td>Fold_5/eval/eval_bleu</td><td>▁▃▅▅▆▅▆▆▇▆▇▇██▇▇██████</td></tr><tr><td>Fold_5/eval/eval_loss</td><td>█▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Fold_5/eval/eval_meteor</td><td>▁▄▅▆▆▆▇▇▇▇█▇██████████</td></tr><tr><td>Fold_5/eval/eval_rouge1</td><td>▁▄▅▆▆▆▇▇▇▇▇▇██████████</td></tr><tr><td>Fold_5/eval/eval_rouge2</td><td>▁▄▅▅▆▅▆▇▇▇▇▇▇▇██▇█████</td></tr><tr><td>Fold_5/eval/eval_rougeL</td><td>▁▄▅▆▆▆▇▇▇▇▇▇██████████</td></tr><tr><td>Fold_5/eval/eval_runtime</td><td>▁▇█████▇▇███████▇▇▇███</td></tr><tr><td>Fold_5/eval/eval_samples_per_second</td><td>█▂▁▁▁▁▁▂▂▁▁▁▁▁▁▁▂▂▂▁▁▁</td></tr><tr><td>Fold_5/eval/eval_steps_per_second</td><td>█▂▁▁▁▁▁▂▂▁▁▁▁▁▁▁▂▂▂▁▁▁</td></tr><tr><td>eval/bleu</td><td>▅▆▇▇▇▇█▂▅▇▇▇██▂▄▅▇▇▇▇▇▇▃▆▇█████▁▃▅▅▆▇█▇█</td></tr><tr><td>eval/loss</td><td>▇▂▂▂▂▂▂▇▄▃▂▂▁▁▁▁▁▁▁▇▂▂▁▁▁▁█▄▃▃▂▂▂▃▂▂▂▂▁▁</td></tr><tr><td>eval/meteor</td><td>▃▅▆▇▇▇▇▇▇▂▆▇▇▇▇████▁▅▆▆▇▇▇▇▂▃▅▇▇▇▇▇▆▇▇▇█</td></tr><tr><td>eval/rouge1</td><td>▁▃▆▇▇▇▇▇▇▇▂▆▇▇▇▇▇▇▇▅▆▆▆▆▆▇▁▅▆▆▇▇▇▇▇▇████</td></tr><tr><td>eval/rouge2</td><td>▂▆███████▃▅▆▇▇▇▇███▁▆▆▇▇▇▇▇▇▇▇▅▆▆▇▇▇▇▇██</td></tr><tr><td>eval/rougeL</td><td>▁▆▇▇▇▇▇▂▇▇▇▇▇▇▇▇▇▂▅▆▆▇▇▇▇▁▃▆▆▆▇▇▇▇▇▄▆▆▇█</td></tr><tr><td>eval/runtime</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂</td></tr><tr><td>eval/samples_per_second</td><td>▁██████████████████████████████████▆▆▆▆▆</td></tr><tr><td>eval/steps_per_second</td><td>▇▇▆▆▆▆▇▇██▇██▇▇▇▇█▇████▇▇▇█▇▇▃▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test/bleu</td><td>▄▇▁██</td></tr><tr><td>test/loss</td><td>█▃▁▆▅</td></tr><tr><td>test/meteor</td><td>▁█▁▁▅</td></tr><tr><td>test/rouge1</td><td>▆▅▁▃█</td></tr><tr><td>test/rouge2</td><td>▇▅▁▂█</td></tr><tr><td>test/rougeL</td><td>▅▅▁▄█</td></tr><tr><td>test/runtime</td><td>▂▂▁██</td></tr><tr><td>test/samples_per_second</td><td>▇▇█▁▁</td></tr><tr><td>test/steps_per_second</td><td>▇▇█▁▁</td></tr><tr><td>train/epoch</td><td>▃▃▃▄▅▆▂▃▄▆▆▇██▂▂▂▅▅▆▇▇▁▁▂▃▄▄▅▆██▂▃▄▅▅▆▆▇</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▃▆▁▂▂▄▅▅▅▅▆▇█▁▂▃▃▅▅▆▇▅▅▆▆▇▇▇███▅▅▆▇▇</td></tr><tr><td>train/grad_norm</td><td>▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▄▄▃▃▃▃▇▇▅▅▅▂▂▂▁▁█▇▇▆▆▄▄▄▃▃▇▆▆▅▃▂█▆▅▄</td></tr><tr><td>train/loss</td><td>▂▂▂▁▁▁▁▁▃▂▂▁▁▁▁▁▁▁▂▂▁▁▁▁▃▂▁▁▁▁▁▁▁█▃▂▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Averages/epoch</td><td>21.6</td></tr><tr><td>Averages/eval_bleu</td><td>0.19387</td></tr><tr><td>Averages/eval_loss</td><td>0.55068</td></tr><tr><td>Averages/eval_meteor</td><td>0.45854</td></tr><tr><td>Averages/eval_rouge1</td><td>0.50226</td></tr><tr><td>Averages/eval_rouge2</td><td>0.2705</td></tr><tr><td>Averages/eval_rougeL</td><td>0.49241</td></tr><tr><td>Averages/eval_runtime</td><td>2.29124</td></tr><tr><td>Averages/eval_samples_per_second</td><td>52.4658</td></tr><tr><td>Averages/eval_steps_per_second</td><td>6.6016</td></tr><tr><td>Fold_1/eval/epoch</td><td>19</td></tr><tr><td>Fold_1/eval/eval_bleu</td><td>0.19202</td></tr><tr><td>Fold_1/eval/eval_loss</td><td>0.57053</td></tr><tr><td>Fold_1/eval/eval_meteor</td><td>0.45382</td></tr><tr><td>Fold_1/eval/eval_rouge1</td><td>0.50619</td></tr><tr><td>Fold_1/eval/eval_rouge2</td><td>0.27636</td></tr><tr><td>Fold_1/eval/eval_rougeL</td><td>0.4931</td></tr><tr><td>Fold_1/eval/eval_runtime</td><td>2.1499</td></tr><tr><td>Fold_1/eval/eval_samples_per_second</td><td>55.817</td></tr><tr><td>Fold_1/eval/eval_steps_per_second</td><td>6.977</td></tr><tr><td>Fold_2/eval/epoch</td><td>23</td></tr><tr><td>Fold_2/eval/eval_bleu</td><td>0.19584</td></tr><tr><td>Fold_2/eval/eval_loss</td><td>0.53979</td></tr><tr><td>Fold_2/eval/eval_meteor</td><td>0.47043</td></tr><tr><td>Fold_2/eval/eval_rouge1</td><td>0.50297</td></tr><tr><td>Fold_2/eval/eval_rouge2</td><td>0.27176</td></tr><tr><td>Fold_2/eval/eval_rougeL</td><td>0.49363</td></tr><tr><td>Fold_2/eval/eval_runtime</td><td>2.1259</td></tr><tr><td>Fold_2/eval/eval_samples_per_second</td><td>55.976</td></tr><tr><td>Fold_2/eval/eval_steps_per_second</td><td>7.056</td></tr><tr><td>Fold_3/eval/epoch</td><td>21</td></tr><tr><td>Fold_3/eval/eval_bleu</td><td>0.18847</td></tr><tr><td>Fold_3/eval/eval_loss</td><td>0.53053</td></tr><tr><td>Fold_3/eval/eval_meteor</td><td>0.45373</td></tr><tr><td>Fold_3/eval/eval_rouge1</td><td>0.48973</td></tr><tr><td>Fold_3/eval/eval_rouge2</td><td>0.26036</td></tr><tr><td>Fold_3/eval/eval_rougeL</td><td>0.48073</td></tr><tr><td>Fold_3/eval/eval_runtime</td><td>2.0801</td></tr><tr><td>Fold_3/eval/eval_samples_per_second</td><td>57.208</td></tr><tr><td>Fold_3/eval/eval_steps_per_second</td><td>7.211</td></tr><tr><td>Fold_4/eval/epoch</td><td>24</td></tr><tr><td>Fold_4/eval/eval_bleu</td><td>0.19678</td></tr><tr><td>Fold_4/eval/eval_loss</td><td>0.55997</td></tr><tr><td>Fold_4/eval/eval_meteor</td><td>0.45291</td></tr><tr><td>Fold_4/eval/eval_rouge1</td><td>0.49825</td></tr><tr><td>Fold_4/eval/eval_rouge2</td><td>0.2642</td></tr><tr><td>Fold_4/eval/eval_rougeL</td><td>0.49008</td></tr><tr><td>Fold_4/eval/eval_runtime</td><td>2.5569</td></tr><tr><td>Fold_4/eval/eval_samples_per_second</td><td>46.54</td></tr><tr><td>Fold_4/eval/eval_steps_per_second</td><td>5.866</td></tr><tr><td>Fold_5/eval/epoch</td><td>21</td></tr><tr><td>Fold_5/eval/eval_bleu</td><td>0.19622</td></tr><tr><td>Fold_5/eval/eval_loss</td><td>0.55258</td></tr><tr><td>Fold_5/eval/eval_meteor</td><td>0.4618</td></tr><tr><td>Fold_5/eval/eval_rouge1</td><td>0.51419</td></tr><tr><td>Fold_5/eval/eval_rouge2</td><td>0.27982</td></tr><tr><td>Fold_5/eval/eval_rougeL</td><td>0.50452</td></tr><tr><td>Fold_5/eval/eval_runtime</td><td>2.5434</td></tr><tr><td>Fold_5/eval/eval_samples_per_second</td><td>46.788</td></tr><tr><td>Fold_5/eval/eval_steps_per_second</td><td>5.898</td></tr><tr><td>eval/bleu</td><td>0.19622</td></tr><tr><td>eval/loss</td><td>0.55258</td></tr><tr><td>eval/meteor</td><td>0.4618</td></tr><tr><td>eval/rouge1</td><td>0.51419</td></tr><tr><td>eval/rouge2</td><td>0.27982</td></tr><tr><td>eval/rougeL</td><td>0.50452</td></tr><tr><td>eval/runtime</td><td>2.5434</td></tr><tr><td>eval/samples_per_second</td><td>46.788</td></tr><tr><td>eval/steps_per_second</td><td>5.898</td></tr><tr><td>test/bleu</td><td>0.19622</td></tr><tr><td>test/loss</td><td>0.55258</td></tr><tr><td>test/meteor</td><td>0.4618</td></tr><tr><td>test/rouge1</td><td>0.51419</td></tr><tr><td>test/rouge2</td><td>0.27982</td></tr><tr><td>test/rougeL</td><td>0.50452</td></tr><tr><td>test/runtime</td><td>2.5349</td></tr><tr><td>test/samples_per_second</td><td>46.945</td></tr><tr><td>test/steps_per_second</td><td>5.917</td></tr><tr><td>total_flos</td><td>1355718825345024.0</td></tr><tr><td>train/epoch</td><td>21</td></tr><tr><td>train/global_step</td><td>1260</td></tr><tr><td>train/grad_norm</td><td>1.21498</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.4527</td></tr><tr><td>train_loss</td><td>0.7292</td></tr><tr><td>train_runtime</td><td>480.3561</td></tr><tr><td>train_samples_per_second</td><td>24.825</td></tr><tr><td>train_steps_per_second</td><td>3.123</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">FR_T5_SMALL_TOP10DOCS_TFIDF</strong> at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/uw4wjrpp' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5/runs/uw4wjrpp</a><br> View project at: <a href='https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5' target=\"_blank\">https://wandb.ai/hibaakbi-sorbonne-universit-/cross_val_T5</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250513_011337-uw4wjrpp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final predicitons for all folds foudn in FR_crossval_T5_SMALL_TOP10DOCS_TFIDF.jsonl\n"
     ]
    }
   ],
   "source": [
    "cross_val_train(modelname=\"t5-small\", run_name=\"FR_T5_SMALL_TOP10DOCS_TFIDF\",fold_filename=\"FR_training_top10_qulac_tfidf_CLEANED_rep_PREPROCESSED_FOR_MODEL.json\",filename=\"FR_crossval_T5_SMALL_TOP10DOCS_TFIDF.jsonl\", \n",
    "                tokenizer_global=tokenizer_global, tokenized_dataset_global=tokenized_dataset_global, \n",
    "                folds=folds, nb_epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d469a66-bc2e-4d3e-9795-bdad4fd0e076",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc8a22e-03fa-4ca7-9403-6259e90cb227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06d0f2a-7ab1-40c6-a76c-38927890c6f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
